[
  {
    "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis",
    "url": "http://arxiv.org/abs/2504.19223v1",
    "authors": [
      "Alexander Baumann",
      "Leonardo Ayala",
      "Silvia Seidlitz",
      "Jan Sellner",
      "Alexander Studier-Fischer",
      "Berkin \u00d6zdemir",
      "Lena Maier-Hein",
      "Slobodan Ilic"
    ],
    "published": "2025-04-27",
    "abstract": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "JEPA"
    ],
    "applications": []
  },
  {
    "title": "PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data",
    "url": "http://arxiv.org/abs/2504.18770v1",
    "authors": [
      "Manuel Weber",
      "Carly Beneke"
    ],
    "published": "2025-04-26",
    "abstract": "We propose PyViT-FUSE, a foundation model for earth observation data\nexplicitly designed to handle multi-modal imagery by learning to fuse an\narbitrary number of mixed-resolution input bands into a single representation\nthrough an attention mechanism. The learned patch tokens are further processed\nby a stack of vision transformers with a novel pyramidal structure. We train\nthe model on a globally sampled dataset in a self-supervised manner, leveraging\ncore concepts of the SwAV algorithm. We show the interpretability of the fusion\nmechanism by visualization of the attention scores and the models applicability\nto downstream tasks.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology",
    "url": "http://arxiv.org/abs/2504.18256v1",
    "authors": [
      "Elena Plekhanova",
      "Damien Robert",
      "Johannes Dollinger",
      "Emilia Arens",
      "Philipp Brun",
      "Jan Dirk Wegner",
      "Niklaus Zimmermann"
    ],
    "published": "2025-04-25",
    "abstract": "With the exacerbation of the biodiversity and climate crises, macroecological\npursuits such as global biodiversity mapping become more urgent. Remote sensing\noffers a wealth of Earth observation data for ecological studies, but the\nscarcity of labeled datasets remains a major challenge. Recently,\nself-supervised learning has enabled learning representations from unlabeled\ndata, triggering the development of pretrained geospatial models with\ngeneralizable features. However, these models are often trained on datasets\nbiased toward areas of high human activity, leaving entire ecological regions\nunderrepresented. Additionally, while some datasets attempt to address\nseasonality through multi-date imagery, they typically follow calendar seasons\nrather than local phenological cycles. To better capture vegetation seasonality\nat a global scale, we propose a simple phenology-informed sampling strategy and\nintroduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we\ntrain an existing model with a season-contrastive objective. We compare\nrepresentations learned from SSL4Eco against other datasets on diverse\necological downstream tasks and demonstrate that our straightforward sampling\nmethod consistently improves representation quality, highlighting the\nimportance of dataset construction. The model pretrained on SSL4Eco reaches\nstate of the art performance on 7 out of 8 downstream tasks spanning\n(multi-label) classification and regression. We release our code, data, and\nmodel weights to support macroecological and computer vision research at\nhttps://github.com/PlekhanovaElena/ssl4eco.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2504.17397v1",
    "authors": [
      "Francesc Marti-Escofet",
      "Benedikt Blumenstiel",
      "Linus Scheibenreif",
      "Paolo Fraccaro",
      "Konrad Schindler"
    ],
    "published": "2025-04-24",
    "abstract": "Earth observation (EO) is crucial for monitoring environmental changes,\nresponding to disasters, and managing natural resources. In this context,\nfoundation models facilitate remote sensing image analysis to retrieve relevant\ngeoinformation accurately and efficiently. However, as these models grow in\nsize, fine-tuning becomes increasingly challenging due to the associated\ncomputational resources and costs, limiting their accessibility and\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\nfeatures and even degrade model generalization. To address this,\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\nIn this paper, we conduct extensive experiments with various foundation model\narchitectures and PEFT techniques to evaluate their effectiveness on five\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\ninsights into when and how PEFT methods support the adaptation of pre-trained\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\nfull fine-tuning performance and enhance model generalisation to unseen\ngeographic regions, while reducing training time and memory requirements.\nAdditional experiments investigate the effect of architecture choices such as\nthe decoder type or the use of metadata, suggesting UNet decoders and\nfine-tuning without metadata as the recommended configuration. We have\nintegrated all evaluated foundation models and techniques into the open-source\npackage TerraTorch to support quick, scalable, and cost-effective model\nadaptation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "A Genealogy of Multi-Sensor Foundation Models in Remote Sensing",
    "url": "http://arxiv.org/abs/2504.17177v1",
    "authors": [
      "Kevin Lane",
      "Morteza Karimzadeh"
    ],
    "published": "2025-04-24",
    "abstract": "Foundation models have garnered increasing attention for representation\nlearning in remote sensing, primarily adopting approaches that have\ndemonstrated success in computer vision with minimal domain-specific\nmodification. However, the development and application of foundation models in\nthis field are still burgeoning, as there are a variety of competing approaches\nthat each come with significant benefits and drawbacks. This paper examines\nthese approaches along with their roots in the computer vision field in order\nto characterize potential advantages and pitfalls while outlining future\ndirections to further improve remote sensing-specific foundation models. We\ndiscuss the quality of the learned representations and methods to alleviate the\nneed for massive compute resources. We place emphasis on the multi-sensor\naspect of Earth observations, and the extent to which existing approaches\nleverage multiple sensors in training foundation models in relation to\nmulti-modal foundation models. Finally, we identify opportunities for further\nharnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote\nsensing observations.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatelliteCalculator: A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion",
    "url": "http://arxiv.org/abs/2504.13442v1",
    "authors": [
      "Zhenyu Yu",
      "Mohd. Yamani Idna Idris",
      "Pei Wang"
    ],
    "published": "2025-04-18",
    "abstract": "Quantitative remote sensing inversion plays a critical role in environmental\nmonitoring, enabling the estimation of key ecological variables such as\nvegetation indices, canopy structure, and carbon stock. Although vision\nfoundation models have achieved remarkable progress in classification and\nsegmentation tasks, their application to physically interpretable regression\nremains largely unexplored. Furthermore, the multi-spectral nature and\ngeospatial heterogeneity of remote sensing data pose significant challenges for\ngeneralization and transferability. To address these issues, we introduce\nSatelliteCalculator, the first vision foundation model tailored for\nquantitative remote sensing inversion. By leveraging physically defined index\nformulas, we automatically construct a large-scale dataset of over one million\npaired samples across eight core ecological indicators. The model integrates a\nfrozen Swin Transformer backbone with a prompt-guided architecture, featuring\ncross-attentive adapters and lightweight task-specific MLP decoders.\nExperiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator\nachieves competitive accuracy across all tasks while significantly reducing\ninference cost. Our results validate the feasibility of applying foundation\nmodels to quantitative inversion, and provide a scalable framework for\ntask-adaptive remote sensing estimation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "SAM-Based Building Change Detection with Distribution-Aware Fourier Adaptation and Edge-Constrained Warping",
    "url": "http://arxiv.org/abs/2504.12619v1",
    "authors": [
      "Yun-Cheng Li",
      "Sen Lei",
      "Yi-Tao Zhao",
      "Heng-Chao Li",
      "Jun Li",
      "Antonio Plaza"
    ],
    "published": "2025-04-17",
    "abstract": "Building change detection remains challenging for urban development, disaster\nassessment, and military reconnaissance. While foundation models like Segment\nAnything Model (SAM) show strong segmentation capabilities, SAM is limited in\nthe task of building change detection due to domain gap issues. Existing\nadapter-based fine-tuning approaches face challenges with imbalanced building\ndistribution, resulting in poor detection of subtle changes and inaccurate edge\nextraction. Additionally, bi-temporal misalignment in change detection,\ntypically addressed by optical flow, remains vulnerable to background noises.\nThis affects the detection of building changes and compromises both detection\naccuracy and edge recognition. To tackle these challenges, we propose a new\nSAM-Based Network with Distribution-Aware Fourier Adaptation and\nEdge-Constrained Warping (FAEWNet) for building change detection. FAEWNet\nutilizes the SAM encoder to extract rich visual features from remote sensing\nimages. To guide SAM in focusing on specific ground objects in remote sensing\nscenes, we propose a Distribution-Aware Fourier Aggregated Adapter to aggregate\ntask-oriented changed information. This adapter not only effectively addresses\nthe domain gap issue, but also pays attention to the distribution of changed\nbuildings. Furthermore, to mitigate noise interference and misalignment in\nheight offset estimation, we design a novel flow module that refines building\nedge extraction and enhances the perception of changed buildings. Our\nstate-of-the-art results on the LEVIR-CD, S2Looking and WHU-CD datasets\nhighlight the effectiveness of FAEWNet. The code is available at\nhttps://github.com/SUPERMAN123000/FAEWNet.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning",
    "url": "http://arxiv.org/abs/2504.11999v1",
    "authors": [
      "Mengyu Wang",
      "Hanbo Bi",
      "Yingchao Feng",
      "Linlin Xin",
      "Shuo Gong",
      "Tianqi Wang",
      "Zhiyuan Yan",
      "Peijin Wang",
      "Wenhui Diao",
      "Xian Sun"
    ],
    "published": "2025-04-16",
    "abstract": "Vision foundation models in remote sensing have been extensively studied due\nto their superior generalization on various downstream tasks. Synthetic\nAperture Radar (SAR) offers all-day, all-weather imaging capabilities,\nproviding significant advantages for Earth observation. However, establishing a\nfoundation model for SAR image interpretation inevitably encounters the\nchallenges of insufficient information utilization and poor interpretability.\nIn this paper, we propose a remote sensing foundation model based on\ncomplex-valued SAR data, which simulates the polarimetric decomposition process\nfor pre-training, i.e., characterizing pixel scattering intensity as a weighted\ncombination of scattering bases and scattering coefficients, thereby endowing\nthe foundation model with physical interpretability. Specifically, we construct\na series of scattering queries, each representing an independent and meaningful\nscattering basis, which interact with SAR features in the scattering query\ndecoder and output the corresponding scattering coefficient. To guide the\npre-training process, polarimetric decomposition loss and power\nself-supervision loss are constructed. The former aligns the predicted\ncoefficients with Yamaguchi coefficients, while the latter reconstructs power\nfrom the predicted coefficients and compares it to the input image's power. The\nperformance of our foundation model is validated on six typical downstream\ntasks, achieving state-of-the-art results. Notably, the foundation model can\nextract stable feature representations and exhibits strong generalization, even\nin data-scarce conditions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization",
    "url": "http://arxiv.org/abs/2504.10727v1",
    "authors": [
      "Darryl Hannan",
      "John Cooper",
      "Dylan White",
      "Timothy Doster",
      "Henry Kvinge",
      "Yijing Watkins"
    ],
    "published": "2025-04-14",
    "abstract": "Multimodal large language models (MLLMs) have altered the landscape of\ncomputer vision, obtaining impressive results across a wide range of tasks,\nespecially in zero-shot settings. Unfortunately, their strong performance does\nnot always transfer to out-of-distribution domains, such as earth observation\n(EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks,\nsuch as image captioning and scene understanding, while failing at tasks that\nrequire more fine-grained spatial reasoning, such as object localization.\nHowever, MLLMs are advancing rapidly and insights quickly become out-dated. In\nthis work, we analyze more recent MLLMs that have been explicitly trained to\ninclude fine-grained spatial reasoning capabilities, benchmarking them on EO\nobject localization tasks. We demonstrate that these models are performant in\ncertain settings, making them well suited for zero-shot scenarios.\nAdditionally, we provide a detailed discussion focused on prompt selection,\nground sample distance (GSD) optimization, and analyzing failure cases. We hope\nthat this work will prove valuable as others evaluate whether an MLLM is well\nsuited for a given EO localization task and how to optimize it.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks",
    "url": "http://arxiv.org/abs/2504.10403v2",
    "authors": [
      "Yan Zhu",
      "Jingyang Zhu",
      "Ting Wang",
      "Yuanming Shi",
      "Chunxiao Jiang",
      "Khaled Ben Letaief"
    ],
    "published": "2025-04-14",
    "abstract": "Advancements in artificial intelligence (AI) and low-earth orbit (LEO)\nsatellites have promoted the application of large remote sensing foundation\nmodels for various downstream tasks. However, direct downloading of these\nmodels for fine-tuning on the ground is impeded by privacy concerns and limited\nbandwidth. Satellite federated learning (FL) offers a solution by enabling\nmodel fine-tuning directly on-board satellites and aggregating model updates\nwithout data downloading. Nevertheless, for large foundation models, the\ncomputational capacity of satellites is insufficient to support effective\non-board fine-tuning in traditional satellite FL frameworks. To address these\nchallenges, we propose a satellite-ground collaborative federated fine-tuning\nframework. The key of the framework lies in how to reasonably decompose and\nallocate model components to alleviate insufficient on-board computation\ncapabilities. During fine-tuning, satellites exchange intermediate results with\nground stations or other satellites for forward propagation and back\npropagation, which brings communication challenges due to the special\ncommunication topology of space transmission networks, such as intermittent\nsatellite-ground communication, short duration of satellite-ground\ncommunication windows, and unstable inter-orbit inter-satellite links (ISLs).\nTo reduce transmission delays, we further introduce tailored communication\nstrategies that integrate both communication and computing resources.\nSpecifically, we propose a parallel intra-orbit communication strategy, a\ntopology-aware satellite-ground communication strategy, and a\nlatency-minimalization inter-orbit communication strategy to reduce space\ncommunication costs. Simulation results demonstrate significant reductions in\ntraining time with improvements of approximately 33%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation",
    "url": "http://arxiv.org/abs/2504.06962v2",
    "authors": [
      "Thomas Kerdreux",
      "Alexandre Tuel",
      "Quentin Febvre",
      "Alexis Mouche",
      "Bertrand Chapron"
    ],
    "published": "2025-04-09",
    "abstract": "Self-supervised learning (SSL) has enabled the development of vision\nfoundation models for Earth Observation (EO), demonstrating strong\ntransferability across diverse remote sensing tasks. While prior work has\nfocused on network architectures and training strategies, the role of dataset\ncuration, especially in balancing and diversifying pre-training datasets,\nremains underexplored. In EO, this challenge is amplified by the redundancy and\nheavy-tailed distributions common in satellite imagery, which can lead to\nbiased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to\nimprove SSL pre-training by maximizing dataset diversity and balance. Our\nmethod iteratively refines the training set without requiring a pre-existing\nfeature extractor, making it well-suited for domains where curated datasets are\nlimited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode\n(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by\nocean observations. We train models from scratch on the entire Sentinel-1 WV\narchive spanning 10 years. Across three downstream tasks, our results show that\ndynamic pruning improves both computational efficiency and representation\nquality, leading to stronger transferability.\n  We also release the weights of OceanSAR-1, the first model in the OceanSAR\nfamily, a series of foundation models for ocean observation and analysis using\nSAR imagery, at github.com/galeio-research/OceanSAR-models/.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation",
    "url": "http://arxiv.org/abs/2504.06220v3",
    "authors": [
      "Xiaoxing Hu",
      "Ziyang Gong",
      "Yupei Wang",
      "Yuru Jia",
      "Gen Luo",
      "Xue Yang"
    ],
    "published": "2025-04-08",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt\npowerful Foundation Models (FMs) to diverse downstream tasks while preserving\nand unleashing their inherent capabilities. However, we have observed that\nexisting PEFT methods, which are often designed with natural imagery in mind,\nstruggle when applied to Remote Sensing (RS) scenarios. This is primarily due\nto their inability to handle artifact influences, a problem particularly severe\nin RS image features. To tackle this challenge, we introduce Earth-Adapter, the\nfirst PEFT method specifically designed for RS artifacts conquering.\nEarth-Adapter introduces a novel Mixture of Frequency Adaptation process that\ncombines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT).\nBy utilizing DFT, Earth-Adapter can decompose features into different frequency\ncomponents, precisely separating artifacts from original features. The MoA then\ndynamically assigns weights to each adapter expert, allowing for the\ncombination of features across various frequency domains. These\nsimple-yet-effective approaches enable Earth-Adapter to more efficiently\novercome the disturbances caused by artifacts than previous PEFT methods,\nsignificantly enhancing the FMs' performance on RS scenarios. Experiments on\nDomain Adaptation (DA), and Domain Generalization (DG) semantic segmentation\nbenchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline\nRein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG\nbenchmarks. Our code will be released at\nhttps://github.com/VisionXLab/Earth-Adapter.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via Eliminate Before Align and Keyword Explicit Reasoning",
    "url": "http://arxiv.org/abs/2504.05644v1",
    "authors": [
      "Yan Zhang",
      "Zhong Ji",
      "Changxu Meng",
      "Yanwei Pang",
      "Jungong Han"
    ],
    "published": "2025-04-08",
    "abstract": "Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR),\nwhich aims at searching for the corresponding targets based on the given query.\nAmong these efforts, the application of Foundation Models (FMs), such as CLIP,\nto the domain of remote sensing has yielded encouraging outcomes. However,\nexisting FM based methodologies neglect the negative impact of weakly\ncorrelated sample pairs and fail to account for the key distinctions among\nremote sensing texts, leading to biased and superficial exploration of sample\npairs. To address these challenges, we propose an approach named iEBAKER (an\nImproved Eliminate Before Align strategy with Keyword Explicit Reasoning\nframework) for RSITR. Specifically, we propose an innovative Eliminate Before\nAlign (EBA) strategy to filter out the weakly correlated sample pairs, thereby\nmitigating their deviations from optimal embedding space during\nalignment.Further, two specific schemes are introduced from the perspective of\nwhether local similarity and global similarity affect each other. On this\nbasis, we introduce an alternative Sort After Reversed Retrieval (SAR)\nstrategy, aims at optimizing the similarity matrix via reverse retrieval.\nAdditionally, we incorporate a Keyword Explicit Reasoning (KER) module to\nfacilitate the beneficial impact of subtle key concept distinctions. Without\nbells and whistles, our approach enables a direct transition from FM to RSITR\ntask, eliminating the need for additional pretraining on remote sensing data.\nExtensive experiments conducted on three popular benchmark datasets demonstrate\nthat our proposed iEBAKER method surpasses the state-of-the-art models while\nrequiring less training data. Our source code will be released at\nhttps://github.com/zhangy0822/iEBAKER.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": []
  },
  {
    "title": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation",
    "url": "http://arxiv.org/abs/2504.03166v1",
    "authors": [
      "Hanbo Bi",
      "Yingchao Feng",
      "Boyuan Tong",
      "Mengyu Wang",
      "Haichen Yu",
      "Yongqiang Mao",
      "Hao Chang",
      "Wenhui Diao",
      "Peijin Wang",
      "Yue Yu",
      "Hanyang Peng",
      "Yehong Zhang",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2025-04-04",
    "abstract": "The rapid advancement of foundation models has revolutionized visual\nrepresentation learning in a self-supervised manner. However, their application\nin remote sensing (RS) remains constrained by a fundamental gap: existing\nmodels predominantly handle single or limited modalities, overlooking the\ninherently multi-modal nature of RS observations. Optical, synthetic aperture\nradar (SAR), and multi-spectral data offer complementary insights that\nsignificantly reduce the inherent ambiguity and uncertainty in single-source\nanalysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS\nfoundation model with 14.7 billion parameters, pre-trained on 400 million\nmulti-modal RS images from nine satellites. RingMoE incorporates three key\ninnovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture\ncomprising modal-specialized, collaborative, and shared experts, effectively\nmodeling intra-modal knowledge while capturing cross-modal dependencies to\nmitigate conflicts between modal representations; (2) Physics-informed\nself-supervised learning, explicitly embedding sensor-specific radiometric\ncharacteristics into the pre-training objectives; (3) Dynamic expert pruning,\nenabling adaptive model compression from 14.7B to 1B parameters while\nmaintaining performance, facilitating efficient deployment in Earth observation\napplications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e.,\nclassification, detection, segmentation, tracking, change detection, and depth\nestimation), RingMoE outperforms existing foundation models and sets new SOTAs,\ndemonstrating remarkable adaptability from single-modal to multi-modal\nscenarios. Beyond theoretical progress, it has been deployed and trialed in\nmultiple sectors, including emergency response, land management, marine\nsciences, and urban planning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Tracking"
    ]
  },
  {
    "title": "FlexiMo: A Flexible Remote Sensing Foundation Model",
    "url": "http://arxiv.org/abs/2503.23844v1",
    "authors": [
      "Xuyang Li",
      "Chenyu Li",
      "Pedram Ghamisi",
      "Danfeng Hong"
    ],
    "published": "2025-03-31",
    "abstract": "The rapid expansion of multi-source satellite imagery drives innovation in\nEarth observation, opening unprecedented opportunities for Remote Sensing\nFoundation Models to harness diverse data. However, many existing models remain\nconstrained by fixed spatial resolutions and patch sizes, limiting their\nability to fully exploit the heterogeneous spatial characteristics inherent in\nsatellite imagery. To address these challenges, we propose FlexiMo, a flexible\nremote sensing foundation model that endows the pre-trained model with the\nflexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a\nspatial resolution-aware module that employs a parameter-free alignment\nembedding mechanism to dynamically recalibrate patch embeddings based on the\ninput image's resolution and dimensions. This design not only preserves\ncritical token characteristics and ensures multi-scale feature fidelity but\nalso enables efficient feature extraction without requiring modifications to\nthe underlying network architecture. In addition, FlexiMo incorporates a\nlightweight channel adaptation module that leverages prior spectral information\nfrom sensors. This mechanism allows the model to process images with varying\nnumbers of channels while maintaining the data's intrinsic physical properties.\nExtensive experiments on diverse multimodal, multi-resolution, and multi-scale\ndatasets demonstrate that FlexiMo significantly enhances model generalization\nand robustness. In particular, our method achieves outstanding performance\nacross a range of downstream tasks, including scene classification, land cover\nclassification, urban building segmentation, and cloud detection. By enabling\nparameter-efficient and physically consistent adaptation, FlexiMo paves the way\nfor more adaptable and effective foundation models in real-world remote sensing\napplications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Efficient Adaptation For Remote Sensing Visual Grounding",
    "url": "http://arxiv.org/abs/2503.23083v1",
    "authors": [
      "Hasan Moughnieh",
      "Mohamad Chalhoub",
      "Hasan Nasrallah",
      "Cristiano Nattero",
      "Paolo Campanella",
      "Ali J. Ghandour"
    ],
    "published": "2025-03-29",
    "abstract": "Foundation models have revolutionized artificial intelligence (AI), offering\nremarkable capabilities across multi-modal domains. Their ability to precisely\nlocate objects in complex aerial and satellite images, using rich contextual\ninformation and detailed object descriptions, is essential for remote sensing\n(RS). These models can associate textual descriptions with object positions\nthrough the Visual Grounding (VG) task, but due to domain-specific challenges,\ntheir direct application to RS produces sub-optimal results. To address this,\nwe applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these\nmodels for RS-specific VG tasks. Specifically, we evaluated LoRA placement\nacross different modules in Grounding DINO and used BitFit and adapters to\nfine-tune the OFA foundation model pre-trained on general-purpose VG datasets.\nThis approach achieved performance comparable to or surpassing current State Of\nThe Art (SOTA) models while significantly reducing computational costs. This\nstudy highlights the potential of PEFT techniques to advance efficient and\nprecise multi-modal analysis in RS, offering a practical and cost-effective\nalternative to full model training.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery",
    "url": "http://arxiv.org/abs/2503.22516v1",
    "authors": [
      "Samira Alkaee Taleghan",
      "Morteza Karimzadeh",
      "Andrew P. Barrett",
      "Walter N. Meier",
      "Farnoush Banaei-Kashani"
    ],
    "published": "2025-03-28",
    "abstract": "Accurate segmentation of sea ice types is essential for mapping and\noperational forecasting of sea ice conditions for safe navigation and resource\nextraction in ice-covered waters, as well as for understanding polar climate\nprocesses. While deep learning methods have shown promise in automating sea ice\nsegmentation, they often rely on extensive labeled datasets which require\nexpert knowledge and are time-consuming to create. Recently, foundation models\n(FMs) have shown excellent results for segmenting remote sensing images by\nutilizing pre-training on large datasets using self-supervised techniques.\nHowever, their effectiveness for sea ice segmentation remains unexplored,\nespecially given sea ice's complex structures, seasonal changes, and unique\nspectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagery\ncharacteristics including banding and scalloping noise, and varying ice\nbackscatter characteristics, which are often missing in standard remote sensing\npre-training datasets. In particular, SAR images over polar regions are\nacquired using different modes than used to capture the images at lower\nlatitudes by the same sensors that form training datasets for FMs. This study\nevaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1\nSAR imagery, focusing on their seasonal and spatial generalization. Among the\nselected models, Prithvi-600M outperforms the baseline models, while CROMA\nachieves a very similar performance in F1-score. Our contributions include\noffering a systematic methodology for selecting FMs for sea ice data analysis,\na comprehensive benchmarking study on performances of FMs for sea ice\nsegmentation with tailored performance metrics, and insights into existing gaps\nand future directions for improving domain-specific models in polar\napplications using SAR data.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "A Survey on Remote Sensing Foundation Models: From Vision to Multimodality",
    "url": "http://arxiv.org/abs/2503.22081v1",
    "authors": [
      "Ziyue Huang",
      "Hongxi Yan",
      "Qiqi Zhan",
      "Shuai Yang",
      "Mingming Zhang",
      "Chenkai Zhang",
      "YiMing Lei",
      "Zeming Liu",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "published": "2025-03-28",
    "abstract": "The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2503.21841v1",
    "authors": [
      "Jingtao Li",
      "Yingyi Liu",
      "Xinyu Wang",
      "Yunning Peng",
      "Chen Sun",
      "Shaoyu Wang",
      "Zhendong Sun",
      "Tian Ke",
      "Xiao Jiang",
      "Tangwei Lu",
      "Anran Zhao",
      "Yanfei Zhong"
    ],
    "published": "2025-03-27",
    "abstract": "Advanced interpretation of hyperspectral remote sensing images benefits many\nprecise Earth observation tasks. Recently, visual foundation models have\npromoted the remote sensing interpretation but concentrating on RGB and\nmultispectral images. Due to the varied hyperspectral channels,existing\nfoundation models would face image-by-image tuning situation, imposing great\npressure on hardware and time resources. In this paper, we propose a\ntuning-free hyperspectral foundation model called HyperFree, by adapting the\nexisting visual prompt engineering. To process varied channel numbers, we\ndesign a learned weight dictionary covering full-spectrum from $0.4 \\sim 2.5 \\,\n\\mu\\text{m}$, supporting to build the embedding layer dynamically. To make the\nprompt design more tractable, HyperFree can generate multiple semantic-aware\nmasks for one prompt by treating feature distance as semantic-similarity. After\npre-training HyperFree on constructed large-scale high-resolution hyperspectral\nimages, HyperFree (1 prompt) has shown comparable results with specialized\nmodels (5 shots) on 5 tasks and 11 datasets.Code and dataset are accessible at\nhttps://rsidea.whu.edu.cn/hyperfree.htm.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text",
    "url": "http://arxiv.org/abs/2503.19311v1",
    "authors": [
      "Weizhi Chen",
      "Jingbo Chen",
      "Yupeng Deng",
      "Jiansheng Chen",
      "Yuman Feng",
      "Zhihao Xi",
      "Diyou Liu",
      "Kai Li",
      "Yu Meng"
    ],
    "published": "2025-03-25",
    "abstract": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "CLIP"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2503.18540v1",
    "authors": [
      "Guneet Mutreja",
      "Philipp Schuegraf",
      "Ksenia Bittner"
    ],
    "published": "2025-03-24",
    "abstract": "Recent advances in self-supervised learning have led to the development of\nfoundation models that have significantly advanced performance in various\ncomputer vision tasks. However, despite their potential, these models often\noverlook the crucial role of high-resolution digital surface models (DSMs) in\nunderstanding urban environments, particularly for building-level analysis,\nwhich is essential for applications like digital twins. To address this gap, we\nintroduce HiRes-FusedMIM, a novel pre-trained model specifically designed to\nleverage the rich information contained within high-resolution RGB and DSM\ndata. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling\n(SimMIM) architecture with a multi-objective loss function that combines\nreconstruction and contrastive objectives, enabling it to learn powerful, joint\nrepresentations from both modalities. We conducted a comprehensive evaluation\nof HiRes-FusedMIM on a diverse set of downstream tasks, including\nclassification, semantic segmentation, and instance segmentation. Our results\ndemonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art\ngeospatial methods on several building-related datasets, including WHU Aerial\nand LoveDA, demonstrating its effectiveness in capturing and leveraging\nfine-grained building information; 2) Incorporating DSMs during pre-training\nconsistently improves performance compared to using RGB data alone,\nhighlighting the value of elevation information for building-level analysis; 3)\nThe dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB\nand DSM data, significantly outperforms a single-encoder model on the Vaihingen\nsegmentation task, indicating the benefits of learning specialized\nrepresentations for each modality. To facilitate further research and\napplications in this direction, we will publicly release the trained model\nweights.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations",
    "url": "http://arxiv.org/abs/2503.16683v1",
    "authors": [
      "Zeping Liu",
      "Fan Zhang",
      "Junfeng Jiao",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published": "2025-03-20",
    "abstract": "Advancements in vision and language foundation models have inspired the\ndevelopment of geo-foundation models (GeoFMs), enhancing performance across\ndiverse geospatial tasks. However, many existing GeoFMs primarily focus on\noverhead remote sensing (RS) data while neglecting other data modalities such\nas ground-level imagery. A key challenge in multimodal GeoFM development is to\nexplicitly model geospatial relationships across modalities, which enables\ngeneralizability across tasks, spatial scales, and temporal contexts. To\naddress these limitations, we propose GAIR, a novel multimodal GeoFM\narchitecture integrating overhead RS data, street view (SV) imagery, and their\ngeolocation metadata. We utilize three factorized neural encoders to project an\nSV image, its geolocation, and an RS image into the embedding space. The SV\nimage needs to be located within the RS image's spatial footprint but does not\nneed to be at its geographic center. In order to geographically align the SV\nimage and RS image, we propose a novel implicit neural representations (INR)\nmodule that learns a continuous RS image representation and looks up the RS\nembedding at the SV image's geolocation. Next, these geographically aligned SV\nembedding, RS embedding, and location embedding are trained with contrastive\nlearning objectives from unlabeled data. We evaluate GAIR across 10 geospatial\ntasks spanning RS image-based, SV image-based, and location embedding-based\nbenchmarks. Experimental results demonstrate that GAIR outperforms\nstate-of-the-art GeoFMs and other strong baselines, highlighting its\neffectiveness in learning generalizable and transferable geospatial\nrepresentations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding",
    "url": "http://arxiv.org/abs/2503.16426v1",
    "authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Bowen Chen",
      "Wenyuan Li",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-03-20",
    "abstract": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Falcon: A Remote Sensing Vision-Language Foundation Model",
    "url": "http://arxiv.org/abs/2503.11070v1",
    "authors": [
      "Kelu Yao",
      "Nuo Xu",
      "Rong Yang",
      "Yingying Xu",
      "Zhuoyan Gao",
      "Titinunt Kitrungrotsakul",
      "Yi Ren",
      "Pu Zhang",
      "Jin Wang",
      "Ning Wei",
      "Chao Li"
    ],
    "published": "2025-03-14",
    "abstract": "This paper introduces a holistic vision-language foundation model tailored\nfor remote sensing, named Falcon. Falcon offers a unified, prompt-based\nparadigm that effectively executes comprehensive and complex remote sensing\ntasks. Falcon demonstrates powerful understanding and reasoning abilities at\nthe image, region, and pixel levels. Specifically, given simple natural\nlanguage instructions and remote sensing images, Falcon can produce impressive\nresults in text form across 14 distinct tasks, i.e., image classification,\nobject detection, segmentation, image captioning, and etc. To facilitate\nFalcon's training and empower its representation capacity to encode rich\nspatial and semantic information, we developed Falcon_SFT, a large-scale,\nmulti-task, instruction-tuning dataset in the field of remote sensing. The\nFalcon_SFT dataset consists of approximately 78 million high-quality data\nsamples, covering 5.6 million multi-spatial resolution and multi-view remote\nsensing images with diverse instructions. It features hierarchical annotations\nand undergoes manual sampling verification to ensure high data quality and\nreliability. Extensive comparative experiments are conducted, which verify that\nFalcon achieves remarkable performance over 67 datasets and 14 tasks, despite\nhaving only 0.7B parameters. We release the complete dataset, code, and model\nweights at https://github.com/TianHuiLab/Falcon, hoping to help further develop\nthe open-source community.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Towards Privacy-preserved Pre-training of Remote Sensing Foundation Models with Federated Mutual-guidance Learning",
    "url": "http://arxiv.org/abs/2503.11051v1",
    "authors": [
      "Jieyi Tan",
      "Chengwei Zhang",
      "Bo Dang",
      "Yansheng Li"
    ],
    "published": "2025-03-14",
    "abstract": "Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with a\ndata-centralized paradigm, through self-supervision on large-scale curated\nremote sensing data. For each institution, however, pre-training RSFMs with\nlimited data in a standalone manner may lead to suboptimal performance, while\naggregating remote sensing data from multiple institutions for centralized\npre-training raises privacy concerns. Seeking for collaboration is a promising\nsolution to resolve this dilemma, where multiple institutions can\ncollaboratively train RSFMs without sharing private data. In this paper, we\npropose a novel privacy-preserved pre-training framework (FedSense), which\nenables multiple institutions to collaboratively train RSFMs without sharing\nprivate data. However, it is a non-trivial task hindered by a vicious cycle,\nwhich results from model drift by remote sensing data heterogeneity and high\ncommunication overhead. To break this vicious cycle, we introduce Federated\nMutual-guidance Learning. Specifically, we propose a Server-to-Clients Guidance\n(SCG) mechanism to guide clients updates towards global-flatness optimal\nsolutions. Additionally, we propose a Clients-to-Server Guidance (CSG)\nmechanism to inject local knowledge into the server by low-bit communication.\nExtensive experiments on four downstream tasks demonstrate the effectiveness of\nour FedSense in both full-precision and communication-reduced scenarios,\nshowcasing remarkable communication efficiency and performance gains.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing",
    "url": "http://arxiv.org/abs/2503.10392v1",
    "authors": [
      "Fengxiang Wang",
      "Hongzhen Wang",
      "Yulin Wang",
      "Di Wang",
      "Mingshuo Chen",
      "Haiyan Zhao",
      "Yangang Sun",
      "Shuo Wang",
      "Long Lan",
      "Wenjing Yang",
      "Jing Zhang"
    ],
    "published": "2025-03-13",
    "abstract": "Recent advances in self-supervised learning for Vision Transformers (ViTs)\nhave fueled breakthroughs in remote sensing (RS) foundation models. However,\nthe quadratic complexity of self-attention poses a significant barrier to\nscalability, particularly for large models and high-resolution images. While\nthe linear-complexity Mamba architecture offers a promising alternative,\nexisting RS applications of Mamba remain limited to supervised tasks on small,\ndomain-specific datasets. To address these challenges, we propose RoMA, a\nframework that enables scalable self-supervised pretraining of Mamba-based RS\nfoundation models using large-scale, diverse, unlabeled data. RoMA enhances\nscalability for high-resolution images through a tailored auto-regressive\nlearning strategy, incorporating two key innovations: 1) a rotation-aware\npretraining mechanism combining adaptive cropping with angular embeddings to\nhandle sparsely distributed objects with arbitrary orientations, and 2)\nmulti-scale token prediction objectives that address the extreme variations in\nobject scales inherent to RS imagery. Systematic empirical studies validate\nthat Mamba adheres to RS data and parameter scaling laws, with performance\nscaling reliably as model and data size increase. Furthermore, experiments\nacross scene classification, object detection, and semantic segmentation tasks\ndemonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based\ncounterparts in both accuracy and computational efficiency. The source code and\npretrained models will be released at https://github.com/MiliLab/RoMA.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Light-weighted foundation model for seismic data processing based on representative and non-redundant pre-training dataset",
    "url": "http://arxiv.org/abs/2503.10092v1",
    "authors": [
      "Xintong Dong",
      "Wenshuo Yu",
      "Jun Lin",
      "Zhenbo Guo",
      "Hongzhou Wang",
      "Jianhao Yang"
    ],
    "published": "2025-03-13",
    "abstract": "In the fields of computer vision (CV) and remote sensing (RS), foundational\nmodels typically follow the \"big data + large model parameters\" paradigm.\nHowever, the application of this strategy in seismic data processing faces\nseveral challenges: seismic data is difficult to obtain and the scarcity of\npublicly available datasets make it difficult to construct large-scale\ndatasets. Additionally, the high computational cost associated with a large\nnumber of model parameters restricts widespread research in this domain.\nTherefore, we propose a lightweight seismic processing foundational model\nparadigm (SPFM), which aims to overcome the limitations of traditional methods\nby data engineering and network architecture innovation. Specifically, we\npropose an innovative dataset construction strategy that generates more seismic\ndata by data augmentation techniques, including collecting publicly available\nfield data and using generative diffusion models (GDM) for data enhancement.\nFurthermore, we optimize the data distribution by employing dimensionality\nreduction, cluster analysis, and stratified sampling methods, reducing\nredundant information while preserving important seismic features, thus\nconstructing a comprehensive dataset. In terms of network architecture design,\nwe introduce the selective structured state-space model (Mamba) structure,\nwhich effectively captures global features of seismic data and alleviates the\nquadratic growth of computational complexity inherent in Transformer-based\nmodels, thereby improving computational efficiency. This model, pre-trained\nwith only four A800 GPUs, outperforms traditional methods across multiple\ntasks, including denoising, interpolation, frequency-band extrapolation, and\nresolution enhancement. The lightweight paradigm provides an solution for\nseismic data processing, advancing the generalization and accessibility of\nseismic data processing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Diffusion Models"
    ],
    "applications": []
  },
  {
    "title": "Isolated Channel Vision Transformers: From Single-Channel Pretraining to Multi-Channel Finetuning",
    "url": "http://arxiv.org/abs/2503.09826v1",
    "authors": [
      "Wenyi Lian",
      "Joakim Lindblad",
      "Patrick Micke",
      "Nata\u0161a Sladoje"
    ],
    "published": "2025-03-12",
    "abstract": "Vision Transformers (ViTs) have achieved remarkable success in standard RGB\nimage processing tasks. However, applying ViTs to multi-channel imaging (MCI)\ndata, e.g., for medical and remote sensing applications, remains a challenge.\nIn particular, MCI data often consist of layers acquired from different\nmodalities. Directly training ViTs on such data can obscure complementary\ninformation and impair the performance. In this paper, we introduce a simple\nyet effective pretraining framework for large-scale MCI datasets. Our method,\nnamed Isolated Channel ViT (IC-ViT), patchifies image channels individually and\nthereby enables pretraining for multimodal multi-channel tasks. We show that\nthis channel-wise patchifying is a key technique for MCI processing. More\nimportantly, one can pretrain the IC-ViT on single channels and finetune it on\ndownstream multi-channel datasets. This pretraining framework captures\ndependencies between patches as well as channels and produces robust feature\nrepresentation. Experiments on various tasks and benchmarks, including JUMP-CP\nand CHAMMI for cell microscopy imaging, and So2Sat-LCZ42 for satellite imaging,\nshow that the proposed IC-ViT delivers 4-14 percentage points of performance\nimprovement over existing channel-adaptive approaches. Further, its efficient\ntraining makes it a suitable candidate for large-scale pretraining of\nfoundation models on heterogeneous data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Visual and Text Prompt Segmentation: A Novel Multi-Model Framework for Remote Sensing",
    "url": "http://arxiv.org/abs/2503.07911v1",
    "authors": [
      "Xing Zi",
      "Kairui Jin",
      "Xian Tao",
      "Jun Li",
      "Ali Braytee",
      "Rajiv Ratn Shah",
      "Mukesh Prasad"
    ],
    "published": "2025-03-10",
    "abstract": "Pixel-level segmentation is essential in remote sensing, where foundational\nvision models like CLIP and Segment Anything Model(SAM) have demonstrated\nsignificant capabilities in zero-shot segmentation tasks. Despite their\nadvances, challenges specific to remote sensing remain substantial. Firstly,\nThe SAM without clear prompt constraints, often generates redundant masks, and\nmaking post-processing more complex. Secondly, the CLIP model, mainly designed\nfor global feature alignment in foundational models, often overlooks local\nobjects crucial to remote sensing. This oversight leads to inaccurate\nrecognition or misplaced focus in multi-target remote sensing imagery. Thirdly,\nboth models have not been pre-trained on multi-scale aerial views, increasing\nthe likelihood of detection failures. To tackle these challenges, we introduce\nthe innovative VTPSeg pipeline, utilizing the strengths of Grounding DINO,\nCLIP, and SAM for enhanced open-vocabulary image segmentation. The Grounding\nDINO+(GD+) module generates initial candidate bounding boxes, while the CLIP\nFilter++(CLIP++) module uses a combination of visual and textual prompts to\nrefine and filter out irrelevant object bounding boxes, ensuring that only\npertinent objects are considered. Subsequently, these refined bounding boxes\nserve as specific prompts for the FastSAM model, which executes precise\nsegmentation. Our VTPSeg is validated by experimental and ablation study\nresults on five popular remote sensing image segmentation datasets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?",
    "url": "http://arxiv.org/abs/2503.07890v1",
    "authors": [
      "Yuru Jia",
      "Valerio Marsocci",
      "Ziyang Gong",
      "Xue Yang",
      "Maarten Vergauwen",
      "Andrea Nascetti"
    ],
    "published": "2025-03-10",
    "abstract": "Self-supervised learning (SSL) has revolutionized representation learning in\nRemote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage\nvast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs\nprimarily focus on discriminative objectives, such as contrastive learning or\nmasked image modeling, owing to their proven success in learning transferable\nrepresentations. However, generative diffusion models--which demonstrate the\npotential to capture multi-grained semantics essential for RS tasks during\nimage generation--remain underexplored for discriminative applications. This\nprompts the question: can generative diffusion models also excel and serve as\nGFMs with sufficient discriminative power? In this work, we answer this\nquestion with SatDiFuser, a framework that transforms a diffusion-based\ngenerative geospatial foundation model into a powerful pretraining tool for\ndiscriminative RS. By systematically analyzing multi-stage, noise-dependent\ndiffusion features, we develop three fusion strategies to effectively leverage\nthese diverse representations. Extensive experiments on remote sensing\nbenchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving\ngains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in\nclassification, demonstrating the capacity of diffusion-based generative\nfoundation models to rival or exceed discriminative GFMs. Code will be\nreleased.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Image Generation"
    ]
  },
  {
    "title": "A Recipe for Improving Remote Sensing VLM Zero Shot Generalization",
    "url": "http://arxiv.org/abs/2503.08722v2",
    "authors": [
      "Aviad Barzilai",
      "Yotam Gigi",
      "Amr Helmy",
      "Vered Silverman",
      "Yehonathan Refael",
      "Bolous Jaber",
      "Tomer Shekel",
      "George Leifman",
      "Genady Beryozkin"
    ],
    "published": "2025-03-10",
    "abstract": "Foundation models have had a significant impact across various AI\napplications, enabling use cases that were previously impossible. Contrastive\nVisual Language Models (VLMs), in particular, have outperformed other\ntechniques in many tasks. However, their prevalence in remote sensing (RS) is\nstill limited, due to the scarcity of diverse remote-sensing visual-language\ndatasets. In this work we introduce two novel image-caption datasets for\ntraining of remote sensing foundation models. The first dataset pairs aerial\nand satellite imagery with captions generated by Gemini using landmarks\nextracted from Google Maps. The second dataset utilizes public web images and\ntheir corresponding alt-text, filtered for the remote sensing domain, resulting\nin a diverse dataset with greater breadth in image styles and subject matter.\nThese datasets are used to pre-train the\nMaMMUT~\\citep{kuo2023mammutsimplearchitecturejoint} VLM architecture, resulting\nin state-of-the-art generalization performance in zero-shot cross-modal\nretrieval on well-known public benchmarks. Finally, we present our ongoing\nresearch to distill image-level knowledge gained in the VLM contrastive\ntraining procedure to enhance the model's localization ability. Specifically,\nwe iteratively generate pseudo-labels for image regions based on the model's\nattention maps and use these labels for further training. To mitigate noisy\nattention maps and create robust segmentation masks, we introduce a novel\nattention-pooling mechanism called the Smooth-Attention-Operation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A General Purpose Spectral Foundational Model for Both Proximal and Remote Sensing Spectral Imaging",
    "url": "http://arxiv.org/abs/2503.01628v1",
    "authors": [
      "William Michael Laprade",
      "Jesper Cairo Westergaard",
      "Svend Christensen",
      "Mads Nielsen",
      "Anders Bjorholm Dahl"
    ],
    "published": "2025-03-03",
    "abstract": "Spectral imaging data acquired via multispectral and hyperspectral cameras\ncan have hundreds of channels, where each channel records the reflectance at a\nspecific wavelength and bandwidth. Time and resource constraints limit our\nability to collect large spectral datasets, making it difficult to build and\ntrain predictive models from scratch. In the RGB domain, we can often alleviate\nsome of the limitations of smaller datasets by using pretrained foundational\nmodels as a starting point. However, most existing foundation models are\npretrained on large datasets of 3-channel RGB images, severely limiting their\neffectiveness when used with spectral imaging data. The few spectral foundation\nmodels that do exist usually have one of two limitations: (1) they are built\nand trained only on remote sensing data limiting their application in proximal\nspectral imaging, (2) they utilize the more widely available multispectral\nimaging datasets with less than 15 channels restricting their use with\nhundred-channel hyperspectral images. To alleviate these issues, we propose a\nlarge-scale foundational model and dataset built upon the masked autoencoder\narchitecture that takes advantage of spectral channel encoding,\nspatial-spectral masking and ImageNet pretraining for an adaptable and robust\nmodel for downstream spectral imaging tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking",
    "url": "http://arxiv.org/abs/2502.18748v1",
    "authors": [
      "Shaheer Mohamed",
      "Tharindu Fernando",
      "Sridha Sridharan",
      "Peyman Moghadam",
      "Clinton Fookes"
    ],
    "published": "2025-02-26",
    "abstract": "Hyperspectral object tracking using snapshot mosaic cameras is emerging as it\nprovides enhanced spectral information alongside spatial data, contributing to\na more comprehensive understanding of material properties. Using transformers,\nwhich have consistently outperformed convolutional neural networks (CNNs) in\nlearning better feature representations, would be expected to be effective for\nHyperspectral object tracking. However, training large transformers\nnecessitates extensive datasets and prolonged training periods. This is\nparticularly critical for complex tasks like object tracking, and the scarcity\nof large datasets in the hyperspectral domain acts as a bottleneck in achieving\nthe full potential of powerful transformer models. This paper proposes an\neffective methodology that adapts large pretrained transformer-based foundation\nmodels for hyperspectral object tracking. We propose an adaptive, learnable\nspatial-spectral token fusion module that can be extended to any\ntransformer-based backbone for learning inherent spatial-spectral features in\nhyperspectral data. Furthermore, our model incorporates a cross-modality\ntraining pipeline that facilitates effective learning across hyperspectral\ndatasets collected with different sensor modalities. This enables the\nextraction of complementary knowledge from additional modalities, whether or\nnot they are present during testing. Our proposed model also achieves superior\nperformance with minimal training iterations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching",
    "url": "http://arxiv.org/abs/2502.18104v1",
    "authors": [
      "Han Nie",
      "Bin Luo",
      "Jun Liu",
      "Zhitao Fu",
      "Huan Zhou",
      "Shuo Zhang",
      "Weixing Liu"
    ],
    "published": "2025-02-25",
    "abstract": "The ideal goal of image matching is to achieve stable and efficient\nperformance in unseen domains. However, many existing learning-based\noptical-SAR image matching methods, despite their effectiveness in specific\nscenarios, exhibit limited generalization and struggle to adapt to practical\napplications. Repeatedly training or fine-tuning matching models to address\ndomain differences is not only not elegant enough but also introduces\nadditional computational overhead and data production costs. In recent years,\ngeneral foundation models have shown great potential for enhancing\ngeneralization. However, the disparity in visual domains between natural and\nremote sensing images poses challenges for their direct application. Therefore,\neffectively leveraging foundation models to improve the generalization of\noptical-SAR image matching remains challenge. To address the above challenges,\nwe propose PromptMID, a novel approach that constructs modality-invariant\ndescriptors using text prompts based on land use classification as priors\ninformation for optical and SAR image matching. PromptMID extracts multi-scale\nmodality-invariant features by leveraging pre-trained diffusion models and\nvisual foundation models (VFMs), while specially designed feature aggregation\nmodules effectively fuse features across different granularities. Extensive\nexperiments on optical-SAR image datasets from four diverse regions demonstrate\nthat PromptMID outperforms state-of-the-art matching methods, achieving\nsuperior results in both seen and unseen domains and exhibiting strong\ncross-domain generalization capabilities. The source code will be made publicly\navailable https://github.com/HanNieWHU/PromptMID.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "CARE: Confidence-Aware Regression Estimation of building density fine-tuning EO Foundation Models",
    "url": "http://arxiv.org/abs/2502.13734v2",
    "authors": [
      "Nikolaos Dionelis",
      "Jente Bosmans",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-02-19",
    "abstract": "Performing accurate confidence quantification and assessment in pixel-wise\nregression tasks, which are downstream applications of AI Foundation Models for\nEarth Observation (EO), is important for deep neural networks to predict their\nfailures, improve their performance and enhance their capabilities in\nreal-world applications, for their practical deployment. For pixel-wise\nregression tasks, specifically utilizing remote sensing data from satellite\nimagery in EO Foundation Models, confidence quantification is a critical\nchallenge. The focus of this research work is on developing a Foundation Model\nusing EO satellite data that computes and assigns a confidence metric alongside\nregression outputs to improve the reliability and interpretability of\npredictions generated by deep neural networks. To this end, we develop, train\nand evaluate the proposed Confidence-Aware Regression Estimation (CARE)\nFoundation Model. Our model CARE computes and assigns confidence to regression\nresults as downstream tasks of a Foundation Model for EO data, and performs a\nconfidence-aware self-corrective learning method for the low-confidence\nregions. We evaluate the model CARE, and experimental results on multi-spectral\ndata from the Copernicus Sentinel-2 satellite constellation to estimate the\nbuilding density (i.e. monitoring urban growth), show that the proposed method\ncan be successfully applied to important regression problems in EO and remote\nsensing. We also show that our model CARE outperforms other baseline methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "S2C: Learning Noise-Resistant Differences for Unsupervised Change Detection in Multimodal Remote Sensing Images",
    "url": "http://arxiv.org/abs/2502.12604v1",
    "authors": [
      "Lei Ding",
      "Xibing Zuo",
      "Danfeng Hong",
      "Haitao Guo",
      "Jun Lu",
      "Zhihui Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-02-18",
    "abstract": "Unsupervised Change Detection (UCD) in multimodal Remote Sensing (RS) images\nremains a difficult challenge due to the inherent spatio-temporal complexity\nwithin data, and the heterogeneity arising from different imaging sensors.\nInspired by recent advancements in Visual Foundation Models (VFMs) and\nContrastive Learning (CL) methodologies, this research aims to develop CL\nmethodologies to translate implicit knowledge in VFM into change\nrepresentations, thus eliminating the need for explicit supervision. To this\nend, we introduce a Semantic-to-Change (S2C) learning framework for UCD in both\nhomogeneous and multimodal RS images. Differently from existing CL\nmethodologies that typically focus on learning multi-temporal similarities, we\nintroduce a novel triplet learning strategy that explicitly models temporal\ndifferences, which are crucial to the CD task. Furthermore, random spatial and\nspectral perturbations are introduced during the training to enhance robustness\nto temporal noise. In addition, a grid sparsity regularization is defined to\nsuppress insignificant changes, and an IoU-matching algorithm is developed to\nrefine the CD results. Experiments on four benchmark CD datasets demonstrate\nthat the proposed S2C learning framework achieves significant improvements in\naccuracy, surpassing current state-of-the-art by over 31\\%, 9\\%, 23\\%, and\n15\\%, respectively. It also demonstrates robustness and sample efficiency,\nsuitable for training and adaptation of various Visual Foundation Models (VFMs)\nor backbone neural networks. The relevant code will be available at:\ngithub.com/DingLei14/S2C.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A Survey of Sample-Efficient Deep Learning for Change Detection in Remote Sensing: Tasks, Strategies, and Challenges",
    "url": "http://arxiv.org/abs/2502.02835v1",
    "authors": [
      "Lei Ding",
      "Danfeng Hong",
      "Maofan Zhao",
      "Hongruixuan Chen",
      "Chenyu Li",
      "Jie Deng",
      "Naoto Yokoya",
      "Lorenzo Bruzzone",
      "Jocelyn Chanussot"
    ],
    "published": "2025-02-05",
    "abstract": "In the last decade, the rapid development of deep learning (DL) has made it\npossible to perform automatic, accurate, and robust Change Detection (CD) on\nlarge volumes of Remote Sensing Images (RSIs). However, despite advances in CD\nmethods, their practical application in real-world contexts remains limited due\nto the diverse input data and the applicational context. For example, the\ncollected RSIs can be time-series observations, and more informative results\nare required to indicate the time of change or the specific change category.\nMoreover, training a Deep Neural Network (DNN) requires a massive amount of\ntraining samples, whereas in many cases these samples are difficult to collect.\nTo address these challenges, various specific CD methods have been developed\nconsidering different application scenarios and training resources.\nAdditionally, recent advancements in image generation, self-supervision, and\nvisual foundation models (VFMs) have opened up new approaches to address the\n'data-hungry' issue of DL-based CD. The development of these methods in broader\napplication scenarios requires further investigation and discussion. Therefore,\nthis article summarizes the literature methods for different CD tasks and the\navailable strategies and techniques to train and deploy DL-based CD methods in\nsample-limited scenarios. We expect that this survey can provide new insights\nand inspiration for researchers in this field to develop more effective CD\nmethods that can be applied in a wider range of contexts.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Detection",
      "Image Generation"
    ]
  },
  {
    "title": "SatMamba: Development of Foundation Models for Remote Sensing Imagery Using State Space Models",
    "url": "http://arxiv.org/abs/2502.00435v1",
    "authors": [
      "Chuc Man Duc",
      "Hiromichi Fukui"
    ],
    "published": "2025-02-01",
    "abstract": "Foundation models refer to deep learning models pretrained on large unlabeled\ndatasets through self-supervised algorithms. In the Earth science and remote\nsensing communities, there is growing interest in transforming the use of Earth\nobservation data, including satellite and aerial imagery, through foundation\nmodels. Various foundation models have been developed for remote sensing, such\nas those for multispectral, high-resolution, and hyperspectral images, and have\ndemonstrated superior performance on various downstream tasks compared to\ntraditional supervised models. These models are evolving rapidly, with\ncapabilities to handle multispectral, multitemporal, and multisensor data. Most\nstudies use masked autoencoders in combination with Vision Transformers (ViTs)\nas the backbone for pretraining. While the models showed promising performance,\nViTs face challenges, such as quadratic computational scaling with input\nlength, which may limit performance on multiband and multitemporal data with\nlong sequences. This research aims to address these challenges by proposing\nSatMamba, a new pretraining framework that combines masked autoencoders with\nState Space Model, offering linear computational scaling. Experiments on\nhigh-resolution imagery across various downstream tasks show promising results,\npaving the way for more efficient foundation models and unlocking the full\npotential of Earth observation data. The source code is available in\nhttps://github.com/mdchuc/HRSFM.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "A Simple Aerial Detection Baseline of Multimodal Language Models",
    "url": "http://arxiv.org/abs/2501.09720v3",
    "authors": [
      "Qingyun Li",
      "Yushi Chen",
      "Xinya Shu",
      "Dong Chen",
      "Xin He",
      "Yi Yu",
      "Xue Yang"
    ],
    "published": "2025-01-16",
    "abstract": "The multimodal language models (MLMs) based on generative pre-trained\nTransformer are considered powerful candidates for unifying various domains and\ntasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\nperformance in multiple tasks, such as visual question answering and visual\ngrounding. In addition to visual grounding that detects specific objects\ncorresponded to given instruction, aerial detection, which detects all objects\nof multiple categories, is also a valuable and challenging task for RS\nfoundation models. However, aerial detection has not been explored by existing\nRS MLMs because the autoregressive prediction mechanism of MLMs differs\nsignificantly from the detection outputs. In this paper, we present a simple\nbaseline for applying MLMs to aerial detection for the first time, named\nLMMRotate. Specifically, we first introduce a normalization method to transform\ndetection outputs into textual outputs to be compatible with the MLM framework.\nThen, we propose a evaluation method, which ensures a fair comparison between\nMLMs and conventional object detection models. We construct the baseline by\nfine-tuning open-source general-purpose MLMs and achieve impressive detection\nperformance comparable to conventional detector. We hope that this baseline\nwill serve as a reference for future MLM development, enabling more\ncomprehensive capabilities for understanding RS images. Code is available at\nhttps://github.com/Li-Qingyun/mllm-mmrotate.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation Models",
    "url": "http://arxiv.org/abs/2501.06809v1",
    "authors": [
      "Keyan Chen",
      "Jiafan Zhang",
      "Chenyang Liu",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-12",
    "abstract": "Referring remote sensing image segmentation is crucial for achieving\nfine-grained visual understanding through free-format textual input, enabling\nenhanced scene and object extraction in remote sensing applications. Current\nresearch primarily utilizes pre-trained language models to encode textual\ndescriptions and align them with visual modalities, thereby facilitating the\nexpression of relevant visual features. However, these approaches often\nstruggle to establish robust alignments between fine-grained semantic concepts,\nleading to inconsistent representations across textual and visual information.\nTo address these limitations, we introduce a referring remote sensing image\nsegmentation foundational model, RSRefSeg. RSRefSeg leverages CLIP for visual\nand textual encoding, employing both global and local textual semantics as\nfilters to generate referring-related visual activation features in the latent\nspace. These activated features then serve as input prompts for SAM, which\nrefines the segmentation masks through its robust visual generalization\ncapabilities. Experimental results on the RRSIS-D dataset demonstrate that\nRSRefSeg outperforms existing methods, underscoring the effectiveness of\nfoundational models in enhancing multimodal task comprehension. The code is\navailable at \\url{https://github.com/KyanChen/RSRefSeg}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Semantic-CD: Remote Sensing Image Semantic Change Detection towards Open-vocabulary Setting",
    "url": "http://arxiv.org/abs/2501.06808v1",
    "authors": [
      "Yongshuo Zhu",
      "Lu Li",
      "Keyan Chen",
      "Chenyang Liu",
      "Fugen Zhou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-12",
    "abstract": "Remote sensing image semantic change detection is a method used to analyze\nremote sensing images, aiming to identify areas of change as well as categorize\nthese changes within images of the same location taken at different times.\nTraditional change detection methods often face challenges in generalizing\nacross semantic categories in practical scenarios. To address this issue, we\nintroduce a novel approach called Semantic-CD, specifically designed for\nsemantic change detection in remote sensing images. This method incorporates\nthe open vocabulary semantics from the vision-language foundation model, CLIP.\nBy utilizing CLIP's extensive vocabulary knowledge, our model enhances its\nability to generalize across categories and improves segmentation through fully\ndecoupled multi-task learning, which includes both binary change detection and\nsemantic change detection tasks. Semantic-CD consists of four main components:\na bi-temporal CLIP visual encoder for extracting features from bi-temporal\nimages, an open semantic prompter for creating semantic cost volume maps with\nopen vocabulary, a binary change detection decoder for generating binary change\ndetection masks, and a semantic change detection decoder for producing semantic\nlabels. Experimental results on the SECOND dataset demonstrate that Semantic-CD\nachieves more accurate masks and reduces semantic classification errors,\nillustrating its effectiveness in applying semantic priors from vision-language\nfoundation models to SCD tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model",
    "url": "http://arxiv.org/abs/2501.00895v2",
    "authors": [
      "Chenyang Liu",
      "Keyan Chen",
      "Rui Zhao",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-01",
    "abstract": "Generative foundation models have advanced large-scale text-driven natural\nimage generation, becoming a prominent research trend across various vertical\ndomains. However, in the remote sensing field, there is still a lack of\nresearch on large-scale text-to-image (text2image) generation technology.\nExisting remote sensing image-text datasets are small in scale and confined to\nspecific geographic areas and scene types. Besides, existing text2image methods\nhave struggled to achieve global-scale, multi-resolution controllable, and\nunbounded image generation. To address these challenges, this paper presents\ntwo key contributions: the Git-10M dataset and the Text2Earth foundation model.\nGit-10M is a global-scale image-text dataset comprising 10.5 million image-text\npairs, 5 times larger than the previous largest one. The dataset covers a wide\nrange of geographic scenes and contains resolution information, significantly\nsurpassing existing datasets in both size and diversity. Building on Git-10M,\nwe propose Text2Earth, a 1.3 billion parameter generative foundation model\nbased on the diffusion framework to model global-scale remote sensing scenes.\nText2Earth integrates a resolution guidance mechanism, enabling users to\nspecify image resolutions. A dynamic condition adaptation strategy is proposed\nfor training and inference to improve image quality. Text2Earth excels in\nzero-shot text2image generation and demonstrates robust generalization and\nflexibility across multiple tasks, including unbounded scene construction,\nimage editing, and cross-modal image generation. This robust capability\nsurpasses previous models restricted to the basic fixed size and limited scene\ntypes. On the previous benchmark dataset, Text2Earth outperforms previous\nmodels with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA\nmetric.Our project page is https://chen-yang-liu.github.io/Text2Earth",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "SeaMo: A Season-Aware Multimodal Foundation Model for Remote Sensing",
    "url": "http://arxiv.org/abs/2412.19237v2",
    "authors": [
      "Xuyang Li",
      "Chenyu Li",
      "Gemine Vivone",
      "Danfeng Hong"
    ],
    "published": "2024-12-26",
    "abstract": "Remote Sensing (RS) data encapsulates rich multi-dimensional information\nessential for Earth observation. Its vast volume, diverse sources, and temporal\ncontinuity make it particularly well-suited for developing large Visual\nFoundation Models (VFMs). These models serve as powerful feature extractors,\nleveraging extensive RS data for pretraining and subsequent fine-tuning in\nvarious geoscientific applications. However, existing VFMs in the RS domain\noften concentrate on specific image characteristics, neglecting the full\nseason-aware potential of RS data. To bridge this gap, we introduce SeaMo, a\nnovel VFM that effectively integrates multimodal and multi-seasonal RS\ninformation. SeaMo leverages a masked image modeling framework to fully exploit\nthe spatial, spectral, and seasonal dimensions of RS data. Specifically, we\nemploy unaligned spatial region selection to capture spatial heterogeneity,\nincorporate multi-source inputs for enhanced multimodal integration, and\nintroduce temporal-multimodal fusion blocks to assimilate seasonal variations\neffectively. By explicitly modeling the complex, season-dependent attributes of\nRS data, SeaMo enhances generalization, robustness, and adaptability across\ngeoscientific tasks. Extensive experiments and ablation studies demonstrate its\nsuperior performance, underscoring its potential as a foundational model for\nEarth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Detect Changes like Humans: Incorporating Semantic Priors for Improved Change Detection",
    "url": "http://arxiv.org/abs/2412.16918v1",
    "authors": [
      "Yuhang Gan",
      "Wenjie Xuan",
      "Zhiming Luo",
      "Lei Fang",
      "Zengmao Wang",
      "Juhua Liu",
      "Bo Du"
    ],
    "published": "2024-12-22",
    "abstract": "When given two similar images, humans identify their differences by comparing\nthe appearance ({\\it e.g., color, texture}) with the help of semantics ({\\it\ne.g., objects, relations}). However, mainstream change detection models adopt a\nsupervised training paradigm, where the annotated binary change map is the main\nconstraint. Thus, these methods primarily emphasize the difference-aware\nfeatures between bi-temporal images and neglect the semantic understanding of\nthe changed landscapes, which undermines the accuracy in the presence of noise\nand illumination variations. To this end, this paper explores incorporating\nsemantic priors to improve the ability to detect changes. Firstly, we propose a\nSemantic-Aware Change Detection network, namely SA-CDNet, which transfers the\ncommon knowledge of the visual foundation models ({\\it i.e., FastSAM}) to\nchange detection. Inspired by the human visual paradigm, a novel dual-stream\nfeature decoder is derived to distinguish changes by combining semantic-aware\nfeatures and difference-aware features. Secondly, we design a single-temporal\nsemantic pre-training strategy to enhance the semantic understanding of\nlandscapes, which brings further increments. Specifically, we construct\npseudo-change detection data from public single-temporal remote sensing\nsegmentation datasets for large-scale pre-training, where an extra branch is\nalso introduced for the proxy semantic segmentation task. Experimental results\non five challenging benchmarks demonstrate the superiority of our method over\nthe existing state-of-the-art methods. The code is available at\n\\href{https://github.com/thislzm/SA-CD}{SA-CD}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "SAModified: A Foundation Model-Based Zero-Shot Approach for Refining Noisy Land-Use Land-Cover Maps",
    "url": "http://arxiv.org/abs/2412.12552v1",
    "authors": [
      "Sparsh Pekhale",
      "Rakshith Sathish",
      "Sathisha Basavaraju",
      "Divya Sharma"
    ],
    "published": "2024-12-17",
    "abstract": "Land-use and land cover (LULC) analysis is critical in remote sensing, with\nwide-ranging applications across diverse fields such as agriculture, utilities,\nand urban planning. However, automating LULC map generation using machine\nlearning is rendered challenging due to noisy labels. Typically, the ground\ntruths (e.g. ESRI LULC, MapBioMass) have noisy labels that hamper the model's\nability to learn to accurately classify the pixels. Further, these erroneous\nlabels can significantly distort the performance metrics of a model, leading to\nmisleading evaluations. Traditionally, the ambiguous labels are rectified using\nunsupervised algorithms. These algorithms struggle not only with scalability\nbut also with generalization across different geographies. To overcome these\nchallenges, we propose a zero-shot approach using the foundation model, Segment\nAnything Model (SAM), to automatically delineate different land parcels/regions\nand leverage them to relabel the unsure pixels by using the local label\nstatistics within each detected region. We achieve a significant reduction in\nlabel noise and an improvement in the performance of the downstream\nsegmentation model by $\\approx 5\\%$ when trained with denoised labels.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications",
    "url": "http://arxiv.org/abs/2412.02732v2",
    "authors": [
      "Daniela Szwarcman",
      "Sujit Roy",
      "Paolo Fraccaro",
      "\u00deorsteinn El\u00ed G\u00edslason",
      "Benedikt Blumenstiel",
      "Rinki Ghosal",
      "Pedro Henrique de Oliveira",
      "Joao Lucas de Sousa Almeida",
      "Rocco Sedona",
      "Yanghui Kang",
      "Srija Chakraborty",
      "Sizhe Wang",
      "Carlos Gomes",
      "Ankur Kumar",
      "Myscon Truong",
      "Denys Godwin",
      "Hyunho Lee",
      "Chia-Yu Hsu",
      "Ata Akbari Asanjan",
      "Besart Mujeci",
      "Disha Shidham",
      "Trevor Keenan",
      "Paulo Arevalo",
      "Wenwen Li",
      "Hamed Alemohammad",
      "Pontus Olofsson",
      "Christopher Hain",
      "Robert Kennedy",
      "Bianca Zadrozny",
      "David Bell",
      "Gabriele Cavallaro",
      "Campbell Watson",
      "Manil Maskey",
      "Rahul Ramachandran",
      "Juan Bernabe Moreno"
    ],
    "published": "2024-12-03",
    "abstract": "This technical report presents Prithvi-EO-2.0, a new geospatial foundation\nmodel that offers significant improvements over its predecessor,\nPrithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's\nHarmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M\nand 600M parameter models incorporate temporal and location embeddings for\nenhanced performance across various geospatial tasks. Through extensive\nbenchmarking with GEO-Bench, the 600M version outperforms the previous\nPrithvi-EO model by 8\\% across a range of tasks. It also outperforms six other\ngeospatial foundation models when benchmarked on remote sensing tasks from\ndifferent domains and resolutions (i.e. from 0.1m to 15m). The results\ndemonstrate the versatility of the model in both classical earth observation\nand high-resolution applications. Early involvement of end-users and subject\nmatter experts (SMEs) are among the key factors that contributed to the\nproject's success. In particular, SME involvement allowed for constant feedback\non model and dataset design, as well as successful customization for diverse\nSME-led applications in disaster response, land use and crop mapping, and\necosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and\nIBM terratorch, with additional resources on GitHub. The project exemplifies\nthe Trusted Open Science approach embraced by all involved organizations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model",
    "url": "http://arxiv.org/abs/2411.17984v2",
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Hanbo Bi",
      "Boyuan Tong",
      "Zhaozhi Wang",
      "Wenhui Diao",
      "Hao Chang",
      "Yingchao Feng",
      "Ziqi Zhang",
      "Yaowei Wang",
      "Qixiang Ye",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2024-11-27",
    "abstract": "Remote sensing foundation models largely break away from the traditional\nparadigm of designing task-specific models, offering greater scalability across\nmultiple tasks. However, they face challenges such as low computational\nefficiency and limited interpretability, especially when dealing with\nlarge-scale remote sensing images. To overcome these, we draw inspiration from\nheat conduction, a physical process modeling local heat diffusion. Building on\nthis idea, we are the first to explore the potential of using the parallel\ncomputing model of heat conduction to simulate the local region correlations in\nhigh-resolution remote sensing images, and introduce RS-vHeat, an efficient\nmulti-modal remote sensing foundation model. Specifically, RS-vHeat 1) applies\nthe Heat Conduction Operator (HCO) with a complexity of $O(N^{1.5})$ and a\nglobal receptive field, reducing computational overhead while capturing remote\nsensing object structure information to guide heat diffusion; 2) learns the\nfrequency distribution representations of various scenes through a\nself-supervised strategy based on frequency domain hierarchical masking and\nmulti-domain reconstruction; 3) significantly improves efficiency and\nperformance over state-of-the-art techniques across 4 tasks and 10 datasets.\nCompared to attention-based remote sensing foundation models, we reduce memory\nusage by 84\\%, FLOPs by 24\\% and improves throughput by 2.7 times. The code\nwill be made publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatVision-TOA: A Geospatial Foundation Model for Coarse-Resolution All-Sky Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2411.17000v1",
    "authors": [
      "Caleb S. Spradlin",
      "Jordan A. Caraballo-Vega",
      "Jian Li",
      "Mark L. Carroll",
      "Jie Gong",
      "Paul M. Montesano"
    ],
    "published": "2024-11-26",
    "abstract": "Foundation models have the potential to transform the landscape of remote\nsensing (RS) data analysis by enabling large computer vision models to be\npre-trained on vast amounts of remote sensing data. These models can then be\nfine-tuned with small amounts of labeled training and applied to a variety of\napplications. Most existing foundation models are designed for high spatial\nresolution, cloud-free satellite imagery or photos, limiting their\napplicability in scenarios that require frequent temporal monitoring or broad\nspectral profiles. As a result, foundation models trained solely on cloud-free\nimages have limited utility for applications that involve atmospheric variables\nor require atmospheric corrections. We introduce SatVision-TOA, a novel\nfoundation model pre-trained on 14-band MODIS L1B Top-Of-Atmosphere (TOA)\nradiance imagery, addressing the need for models pre-trained to handle\nmoderate- and coarse-resolution all-sky remote sensing data. The SatVision-TOA\nmodel is pre-trained using a Masked-Image-Modeling (MIM) framework and the\nSwinV2 architecture, and learns detailed contextual representations through\nself-supervised learning without the need for labels. It is a 3 billion\nparameter model that is trained on 100 million images. To our knowledge this is\nthe largest foundation model trained solely on satellite RS imagery. Results\nshow that SatVision-TOA achieves superior performance over baseline methods on\ndownstream tasks such as 3D cloud retrieval. Notably, the model achieves a mean\nintersection over union (mIOU) of 0.46, a substantial improvement over the\nbaseline mIOU of 0.22. Additionally, the rate of false negative results in the\nfine-tuning task were reduced by over 50% compared to the baseline. Our work\nadvances pre-trained vision modeling for multispectral RS by learning from a\nvariety of atmospheric and aerosol conditions to improve cloud and land surface\nmonitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2411.13127v2",
    "authors": [
      "Xuechao Zou",
      "Shun Zhang",
      "Kai Li",
      "Shiying Wang",
      "Junliang Xing",
      "Lei Jin",
      "Congyan Lang",
      "Pin Tao"
    ],
    "published": "2024-11-20",
    "abstract": "Cloud segmentation is a critical challenge in remote sensing image\ninterpretation, as its accuracy directly impacts the effectiveness of\nsubsequent data processing and analysis. Recently, vision foundation models\n(VFM) have demonstrated powerful generalization capabilities across various\nvisual tasks. In this paper, we present a parameter-efficient adaptive\napproach, termed Cloud-Adapter, designed to enhance the accuracy and robustness\nof cloud segmentation. Our method leverages a VFM pretrained on general domain\ndata, which remains frozen, eliminating the need for additional training.\nCloud-Adapter incorporates a lightweight spatial perception module that\ninitially utilizes a convolutional neural network (ConvNet) to extract dense\nspatial representations. These multi-scale features are then aggregated and\nserve as contextual inputs to an adapting module, which modulates the frozen\ntransformer layers within the VFM. Experimental results demonstrate that the\nCloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the\nfrozen backbone, achieves substantial performance gains. Cloud-Adapter\nconsistently achieves state-of-the-art performance across various cloud\nsegmentation datasets from multiple satellite sources, sensor series, data\nprocessing levels, land cover scenarios, and annotation granularities. We have\nreleased the code and model checkpoints at\nhttps://xavierjiezou.github.io/Cloud-Adapter/ to support further research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Aquila: A Hierarchically Aligned Visual-Language Model for Enhanced Remote Sensing Image Comprehension",
    "url": "http://arxiv.org/abs/2411.06074v1",
    "authors": [
      "Kaixuan Lu",
      "Ruiqian Zhang",
      "Xiao Huang",
      "Yuxing Xie"
    ],
    "published": "2024-11-09",
    "abstract": "Recently, large vision language models (VLMs) have made significant strides\nin visual language capabilities through visual instruction tuning, showing\ngreat promise in the field of remote sensing image interpretation. However,\nexisting remote sensing vision language models (RSVLMs) often fall short in\ncapturing the complex characteristics of remote sensing scenes, as they\ntypically rely on low resolution, single scale visual features and simplistic\nmethods to map visual features to language features. In this paper, we present\nAquila, an advanced visual language foundation model designed to enable richer\nvisual feature representation and more precise visual-language feature\nalignment for remote sensing images. Our approach introduces a learnable\nHierarchical Spatial Feature Integration (SFI) module that supports high\nresolution image inputs and aggregates multi scale visual features, allowing\nfor the detailed representation of complex visual information. Additionally,\nthe SFI module is repeatedly integrated into the layers of the large language\nmodel (LLM) to achieve deep visual language feature alignment, without\ncompromising the model's performance in natural language processing tasks.\nThese innovations, capturing detailed visual effects through higher resolution\nand multi scale input, and enhancing feature alignment significantly improve\nthe model's ability to learn from image text data. We validate the\neffectiveness of Aquila through extensive quantitative experiments and\nqualitative analyses, demonstrating its superior performance.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "STARS: Sensor-agnostic Transformer Architecture for Remote Sensing",
    "url": "http://arxiv.org/abs/2411.05714v1",
    "authors": [
      "Ethan King",
      "Jaime Rodriguez",
      "Diego Llanes",
      "Timothy Doster",
      "Tegan Emerson",
      "James Koch"
    ],
    "published": "2024-11-08",
    "abstract": "We present a sensor-agnostic spectral transformer as the basis for spectral\nfoundation models. To that end, we introduce a Universal Spectral\nRepresentation (USR) that leverages sensor meta-data, such as sensing kernel\nspecifications and sensing wavelengths, to encode spectra obtained from any\nspectral instrument into a common representation, such that a single model can\ningest data from any sensor. Furthermore, we develop a methodology for\npre-training such models in a self-supervised manner using a novel random\nsensor-augmentation and reconstruction pipeline to learn spectral features\nindependent of the sensing paradigm. We demonstrate that our architecture can\nlearn sensor independent spectral features that generalize effectively to\nsensors not seen during training. This work sets the stage for training\nfoundation models that can both leverage and be effective for the growing\ndiversity of spectral data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Joint-Optimized Unsupervised Adversarial Domain Adaptation in Remote Sensing Segmentation with Prompted Foundation Model",
    "url": "http://arxiv.org/abs/2411.05878v2",
    "authors": [
      "Shuchang Lyu",
      "Qi Zhao",
      "Guangliang Cheng",
      "Yiwei He",
      "Zheng Zhou",
      "Guangbiao Wang",
      "Zhenwei Shi"
    ],
    "published": "2024-11-08",
    "abstract": "Unsupervised Domain Adaptation for Remote Sensing Semantic Segmentation\n(UDA-RSSeg) addresses the challenge of adapting a model trained on source\ndomain data to target domain samples, thereby minimizing the need for annotated\ndata across diverse remote sensing scenes. This task presents two principal\nchallenges: (1) severe inconsistencies in feature representation across\ndifferent remote sensing domains, and (2) a domain gap that emerges due to the\nrepresentation bias of source domain patterns when translating features to\npredictive logits. To tackle these issues, we propose a joint-optimized\nadversarial network incorporating the \"Segment Anything Model (SAM)\n(SAM-JOANet)\" for UDA-RSSeg. Our approach integrates SAM to leverage its robust\ngeneralized representation capabilities, thereby alleviating feature\ninconsistencies. We introduce a finetuning decoder designed to convert\nSAM-Encoder features into predictive logits. Additionally, a feature-level\nadversarial-based prompted segmentor is employed to generate class-agnostic\nmaps, which guide the finetuning decoder's feature representations. The network\nis optimized end-to-end, combining the prompted segmentor and the finetuning\ndecoder. Extensive evaluations on benchmark datasets, including ISPRS\n(Potsdam/Vaihingen) and CITY-OSM (Paris/Chicago), demonstrate the effectiveness\nof our method. The results, supported by visualization and analysis, confirm\nthe method's interpretability and robustness. The code of this paper is\navailable at https://github.com/CV-ShuchangLyu/SAM-JOANet.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "In the Era of Prompt Learning with Vision-Language Models",
    "url": "http://arxiv.org/abs/2411.04892v1",
    "authors": [
      "Ankit Jha"
    ],
    "published": "2024-11-07",
    "abstract": "Large-scale foundation models like CLIP have shown strong zero-shot\ngeneralization but struggle with domain shifts, limiting their adaptability. In\nour work, we introduce \\textsc{StyLIP}, a novel domain-agnostic prompt learning\nstrategy for Domain Generalization (DG). StyLIP disentangles visual style and\ncontent in CLIP`s vision encoder by using style projectors to learn\ndomain-specific prompt tokens and combining them with content features. Trained\ncontrastively, this approach enables seamless adaptation across domains,\noutperforming state-of-the-art methods on multiple DG benchmarks. Additionally,\nwe propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s\nfrozen vision backbone to learn domain-invariant prompts through image style\nand content features. By aligning domains in embedding space with entropy\nminimization, AD-CLIP effectively handles domain shifts, even when only target\ndomain samples are available. Lastly, we outline future work on class discovery\nusing prompt learning for semantic segmentation in remote sensing, focusing on\nidentifying novel or rare classes in unstructured environments. This paves the\nway for more adaptive and generalizable models in complex, real-world\nscenarios.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2410.22629v2",
    "authors": [
      "Ziyang Gong",
      "Zhixiang Wei",
      "Di Wang",
      "Xianzheng Ma",
      "Hongruixuan Chen",
      "Yuru Jia",
      "Yupeng Deng",
      "Zhenming Ji",
      "Xiangwei Zhu",
      "Naoto Yokoya",
      "Jing Zhang",
      "Bo Du",
      "Liangpei Zhang"
    ],
    "published": "2024-10-30",
    "abstract": "The field of Remote Sensing Domain Generalization (RSDG) has emerged as a\ncritical and valuable research frontier, focusing on developing models that\ngeneralize effectively across diverse scenarios. Despite the substantial domain\ngaps in RS images that are characterized by variabilities such as location,\nwavelength, and sensor type, research in this area remains underexplored: (1)\nCurrent cross-domain methods primarily focus on Domain Adaptation (DA), which\nadapts models to predefined domains rather than to unseen ones; (2) Few studies\ntargeting the RSDG issue, especially for semantic segmentation tasks, where\nexisting models are developed for specific unknown domains, struggling with\nissues of underfitting on other unknown scenarios; (3) Existing RS foundation\nmodels tend to prioritize in-domain performance over cross-domain\ngeneralization. To this end, we introduce the first vision foundation model for\nRSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong\ncross-domain generalization through a specially designed data-level Earth-Style\nInjection pipeline and a model-level Multi-Task Training pipeline. In addition,\nfor the semantic segmentation task, we have curated an RSDG benchmark\ncomprising 28 cross-domain settings across various regions, spectral bands,\nplatforms, and climates, providing a comprehensive framework for testing the\ngeneralizability of future RSDG models. Extensive experiments on this benchmark\ndemonstrate the superiority of CrossEarth over existing state-of-the-art\nmethods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery",
    "url": "http://arxiv.org/abs/2410.19965v1",
    "authors": [
      "Philipe Dias",
      "Aristeidis Tsaris",
      "Jordan Bowman",
      "Abhishek Potnis",
      "Jacob Arndt",
      "H. Lexie Yang",
      "Dalton Lunga"
    ],
    "published": "2024-10-25",
    "abstract": "While the pretraining of Foundation Models (FMs) for remote sensing (RS)\nimagery is on the rise, models remain restricted to a few hundred million\nparameters. Scaling models to billions of parameters has been shown to yield\nunprecedented benefits including emergent abilities, but requires data scaling\nand computing resources typically not available outside industry R&D labs. In\nthis work, we pair high-performance computing resources including Frontier\nsupercomputer, America's first exascale system, and high-resolution optical RS\ndata to pretrain billion-scale FMs. Our study assesses performance of different\npretrained variants of vision Transformers across image classification,\nsemantic segmentation and object detection benchmarks, which highlight the\nimportance of data scaling for effective model scaling. Moreover, we discuss\nconstruction of a novel TIU pretraining dataset, model initialization, with\ndata and pretrained models intended for public release. By discussing technical\nchallenges and details often lacking in the related literature, this work is\nintended to offer best practices to the geospatial community toward efficient\ntraining and benchmarking of larger FMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Foundation Models for Remote Sensing and Earth Observation: A Survey",
    "url": "http://arxiv.org/abs/2410.16602v2",
    "authors": [
      "Aoran Xiao",
      "Weihao Xuan",
      "Junjue Wang",
      "Jiaxing Huang",
      "Dacheng Tao",
      "Shijian Lu",
      "Naoto Yokoya"
    ],
    "published": "2024-10-22",
    "abstract": "Remote Sensing (RS) is a crucial technology for observing, monitoring, and\ninterpreting our planet, with broad applications across geoscience, economics,\nhumanitarian fields, etc. While artificial intelligence (AI), particularly deep\nlearning, has achieved significant advances in RS, unique challenges persist in\ndeveloping more intelligent RS systems, including the complexity of Earth's\nenvironments, diverse sensor modalities, distinctive feature patterns, varying\nspatial and spectral resolutions, and temporal dynamics. Meanwhile, recent\nbreakthroughs in large Foundation Models (FMs) have expanded AI's potential\nacross many domains due to their exceptional generalizability and zero-shot\ntransfer capabilities. However, their success has largely been confined to\nnatural data like images and video, with degraded performance and even failures\nfor RS data of various non-optical modalities. This has inspired growing\ninterest in developing Remote Sensing Foundation Models (RSFMs) to address the\ncomplex demands of Earth Observation (EO) tasks, spanning the surface,\natmosphere, and oceans. This survey systematically reviews the emerging field\nof RSFMs. It begins with an outline of their motivation and background,\nfollowed by an introduction of their foundational concepts. It then categorizes\nand reviews existing RSFM studies including their datasets and technical\ncontributions across Visual Foundation Models (VFMs), Visual-Language Models\n(VLMs), Large Language Models (LLMs), and beyond. In addition, we benchmark\nthese models against publicly available datasets, discuss existing challenges,\nand propose future research directions in this rapidly evolving field. A\nproject associated with this survey has been built at\nhttps://github.com/xiaoaoran/awesome-RSFMs .",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "MANet: Fine-Tuning Segment Anything Model for Multimodal Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2410.11160v1",
    "authors": [
      "Xianping Ma",
      "Xiaokang Zhang",
      "Man-On Pun",
      "Bo Huang"
    ],
    "published": "2024-10-15",
    "abstract": "Multimodal remote sensing data, collected from a variety of sensors, provide\na comprehensive and integrated perspective of the Earth's surface. By employing\nmultimodal fusion techniques, semantic segmentation offers more detailed\ninsights into geographic scenes compared to single-modality approaches.\nBuilding upon recent advancements in vision foundation models, particularly the\nSegment Anything Model (SAM), this study introduces a novel Multimodal\nAdapter-based Network (MANet) for multimodal remote sensing semantic\nsegmentation. At the core of this approach is the development of a Multimodal\nAdapter (MMAdapter), which fine-tunes SAM's image encoder to effectively\nleverage the model's general knowledge for multimodal data. In addition, a\npyramid-based Deep Fusion Module (DFM) is incorporated to further integrate\nhigh-level geographic features across multiple scales before decoding. This\nwork not only introduces a novel network for multimodal fusion, but also\ndemonstrates, for the first time, SAM's powerful generalization capabilities\nwith Digital Surface Model (DSM) data. Experimental results on two\nwell-established fine-resolution multimodal remote sensing datasets, ISPRS\nVaihingen and ISPRS Potsdam, confirm that the proposed MANet significantly\nsurpasses current models in the task of multimodal semantic segmentation. The\nsource code for this work will be accessible at https://github.com/sstary/SSRS.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Exploring Foundation Models in Remote Sensing Image Change Detection: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2410.07824v1",
    "authors": [
      "Zihan Yu",
      "Tianxiao Li",
      "Yuxin Zhu",
      "Rongze Pan"
    ],
    "published": "2024-10-10",
    "abstract": "Change detection, as an important and widely applied technique in the field\nof remote sensing, aims to analyze changes in surface areas over time and has\nbroad applications in areas such as environmental monitoring, urban\ndevelopment, and land use analysis.In recent years, deep learning, especially\nthe development of foundation models, has provided more powerful solutions for\nfeature extraction and data fusion, effectively addressing these complexities.\nThis paper systematically reviews the latest advancements in the field of\nchange detection, with a focus on the application of foundation models in\nremote sensing tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2410.06194v1",
    "authors": [
      "Shiyu Miao",
      "Delong Chen",
      "Fan Liu",
      "Chuanyi Zhang",
      "Yanhui Gu",
      "Shengjie Guo",
      "Jun Zhou"
    ],
    "published": "2024-10-08",
    "abstract": "The Direct Segment Anything Model (DirectSAM) excels in class-agnostic\ncontour extraction. In this paper, we explore its use by applying it to optical\nremote sensing imagery, where semantic contour extraction-such as identifying\nbuildings, road networks, and coastlines-holds significant practical value.\nThose applications are currently handled via training specialized small models\nseparately on small datasets in each domain. We introduce a foundation model\nderived from DirectSAM, termed DirectSAM-RS, which not only inherits the strong\nsegmentation capability acquired from natural images, but also benefits from a\nlarge-scale dataset we created for remote sensing semantic contour extraction.\nThis dataset comprises over 34k image-text-contour triplets, making it at least\n30 times larger than individual dataset. DirectSAM-RS integrates a prompter\nmodule: a text encoder and cross-attention layers attached to the DirectSAM\narchitecture, which allows flexible conditioning on target class labels or\nreferring expressions. We evaluate the DirectSAM-RS in both zero-shot and\nfine-tuning setting, and demonstrate that it achieves state-of-the-art\nperformance across several downstream benchmarks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "PointSAM: Pointly-Supervised Segment Anything Model for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2409.13401v2",
    "authors": [
      "Nanqing Liu",
      "Xun Xu",
      "Yongyi Su",
      "Haojie Zhang",
      "Heng-Chao Li"
    ],
    "published": "2024-09-20",
    "abstract": "Segment Anything Model (SAM) is an advanced foundational model for image\nsegmentation, which is gradually being applied to remote sensing images (RSIs).\nDue to the domain gap between RSIs and natural images, traditional methods\ntypically use SAM as a source pre-trained model and fine-tune it with fully\nsupervised masks. Unlike these methods, our work focuses on fine-tuning SAM\nusing more convenient and challenging point annotations. Leveraging SAM's\nzero-shot capabilities, we adopt a self-training framework that iteratively\ngenerates pseudo-labels for training. However, if the pseudo-labels contain\nnoisy labels, there is a risk of error accumulation. To address this issue, we\nextract target prototypes from the target dataset and use the Hungarian\nalgorithm to match them with prediction prototypes, preventing the model from\nlearning in the wrong direction. Additionally, due to the complex backgrounds\nand dense distribution of objects in RSI, using point prompts may result in\nmultiple objects being recognized as one. To solve this problem, we propose a\nnegative prompt calibration method based on the non-overlapping nature of\ninstance masks. In brief, we use the prompts of overlapping masks as\ncorresponding negative signals, resulting in refined masks. Combining the above\nmethods, we propose a novel Pointly-supervised Segment Anything Model named\nPointSAM. We conduct experiments on RSI datasets, including WHU, HRSID, and\nNWPU VHR-10, and the results show that our method significantly outperforms\ndirect testing with SAM, SAM2, and other comparison methods. Furthermore, we\nintroduce PointSAM as a point-to-box converter and achieve encouraging results,\nsuggesting that this method can be extended to other point-supervised tasks.\nThe code is available at https://github.com/Lans1ng/PointSAM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "RingMo-Aerial: An Aerial Remote Sensing Foundation Model With A Affine Transformation Contrastive Learning",
    "url": "http://arxiv.org/abs/2409.13366v2",
    "authors": [
      "Wenhui Diao",
      "Haichen Yu",
      "Kaiyue Kang",
      "Tong Ling",
      "Di Liu",
      "Yingchao Feng",
      "Hanbo Bi",
      "Libo Ren",
      "Xuexue Li",
      "Yongqiang Mao",
      "Xian Sun"
    ],
    "published": "2024-09-20",
    "abstract": "Aerial Remote Sensing (ARS) vision tasks pose significant challenges due to\nthe unique characteristics of their viewing angles. Existing research has\nprimarily focused on algorithms for specific tasks, which have limited\napplicability in a broad range of ARS vision applications. This paper proposes\nthe RingMo-Aerial model, aiming to fill the gap in foundation model research in\nthe field of ARS vision. By introducing the Frequency-Enhanced Multi-Head\nSelf-Attention (FE-MSA) mechanism and an affine transformation-based\ncontrastive learning pre-training method, the model's detection capability for\nsmall targets is enhanced and optimized for the tilted viewing angles\ncharacteristic of ARS. Furthermore, the ARS-Adapter, an efficient parameter\nfine-tuning method, is proposed to improve the model's adaptability and\neffectiveness in various ARS vision tasks. Experimental results demonstrate\nthat RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This\nindicates the practicality and efficacy of RingMo-Aerial in enhancing the\nperformance of ARS vision tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Detecting Looted Archaeological Sites from Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2409.09432v1",
    "authors": [
      "Elliot Vincent",
      "Mehra\u00efl Saroufim",
      "Jonathan Chemla",
      "Yves Ubelmann",
      "Philippe Marquis",
      "Jean Ponce",
      "Mathieu Aubry"
    ],
    "published": "2024-09-14",
    "abstract": "Archaeological sites are the physical remains of past human activity and one\nof the main sources of information about past societies and cultures. However,\nthey are also the target of malevolent human actions, especially in countries\nhaving experienced inner turmoil and conflicts. Because monitoring these sites\nfrom space is a key step towards their preservation, we introduce the DAFA\nLooted Sites dataset, \\datasetname, a labeled multi-temporal remote sensing\ndataset containing 55,480 images acquired monthly over 8 years across 675\nAfghan archaeological sites, including 135 sites looted during the acquisition\nperiod. \\datasetname~is particularly challenging because of the limited number\nof training samples, the class imbalance, the weak binary annotations only\navailable at the level of the time series, and the subtlety of relevant changes\ncoupled with important irrelevant ones over a long time period. It is also an\ninteresting playground to assess the performance of satellite image time series\n(SITS) classification methods on a real and important use case. We evaluate a\nlarge set of baselines, outline the substantial benefits of using foundation\nmodels and show the additional boost that can be provided by using complete\ntime series instead of using a single image.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Pushing the Limits of Vision-Language Models in Remote Sensing without Human Annotations",
    "url": "http://arxiv.org/abs/2409.07048v1",
    "authors": [
      "Keumgang Cha",
      "Donggeun Yu",
      "Junghoon Seo"
    ],
    "published": "2024-09-11",
    "abstract": "The prominence of generalized foundation models in vision-language\nintegration has witnessed a surge, given their multifarious applications.\nWithin the natural domain, the procurement of vision-language datasets to\nconstruct these foundation models is facilitated by their abundant availability\nand the ease of web crawling. Conversely, in the remote sensing domain,\nalthough vision-language datasets exist, their volume is suboptimal for\nconstructing robust foundation models. This study introduces an approach to\ncurate vision-language datasets by employing an image decoding machine learning\nmodel, negating the need for human-annotated labels. Utilizing this\nmethodology, we amassed approximately 9.6 million vision-language paired\ndatasets in VHR imagery. The resultant model outperformed counterparts that did\nnot leverage publicly available vision-language datasets, particularly in\ndownstream tasks such as zero-shot classification, semantic localization, and\nimage-text retrieval. Moreover, in tasks exclusively employing vision encoders,\nsuch as linear probing and k-NN classification, our model demonstrated superior\nefficacy compared to those relying on domain-specific vision-language datasets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability",
    "url": "http://arxiv.org/abs/2409.00489v1",
    "authors": [
      "Chia-Yu Hsu",
      "Wenwen Li",
      "Sizhe Wang"
    ],
    "published": "2024-08-31",
    "abstract": "Research on geospatial foundation models (GFMs) has become a trending topic\nin geospatial artificial intelligence (AI) research due to their potential for\nachieving high generalizability and domain adaptability, reducing model\ntraining costs for individual researchers. Unlike large language models, such\nas ChatGPT, constructing visual foundation models for image analysis,\nparticularly in remote sensing, encountered significant challenges such as\nformulating diverse vision tasks into a general problem framework. This paper\nevaluates the recently released NASA-IBM GFM Prithvi for its predictive\nperformance on high-level image analysis tasks across multiple benchmark\ndatasets. Prithvi was selected because it is one of the first open-source GFMs\ntrained on time-series of high-resolution remote sensing imagery. A series of\nexperiments were designed to assess Prithvi's performance as compared to other\npre-trained task-specific AI models in geospatial image analysis. New\nstrategies, including band adaptation, multi-scale feature generation, and\nfine-tuning techniques, are introduced and integrated into an image analysis\npipeline to enhance Prithvi's domain adaptation capability and improve model\nperformance. In-depth analyses reveal Prithvi's strengths and weaknesses,\noffering insights for both improving Prithvi and developing future visual\nfoundation models for geospatial tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Tuning a SAM-Based Model with Multi-Cognitive Visual Adapter to Remote Sensing Instance Segmentation",
    "url": "http://arxiv.org/abs/2408.08576v1",
    "authors": [
      "Linghao Zheng",
      "Xinyang Pu",
      "Feng Xu"
    ],
    "published": "2024-08-16",
    "abstract": "The Segment Anything Model (SAM), a foundational model designed for\npromptable segmentation tasks, demonstrates exceptional generalization\ncapabilities, making it highly promising for natural scene image segmentation.\nHowever, SAM's lack of pretraining on massive remote sensing images and its\ninteractive structure limit its automatic mask prediction capabilities. In this\npaper, a Multi-Cognitive SAM-Based Instance Segmentation Model (MC-SAM SEG) is\nintroduced to employ SAM on remote sensing domain. The SAM-Mona encoder\nutilizing the Multi-cognitive Visual Adapter (Mona) is conducted to facilitate\nSAM's transfer learning in remote sensing applications. The proposed method\nnamed MC-SAM SEG extracts high-quality features by fine-tuning the SAM-Mona\nencoder along with a feature aggregator. Subsequently, a pixel decoder and\ntransformer decoder are designed for prompt-free mask generation and instance\nclassification. The comprehensive experiments are conducted on the HRSID and\nWHU datasets for instance segmentation tasks on Synthetic Aperture Radar (SAR)\nimages and optical remote sensing images respectively. The evaluation results\nindicate the proposed method surpasses other deep learning algorithms and\nverify its effectiveness and generalization.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "SpectralEarth: Training Hyperspectral Foundation Models at Scale",
    "url": "http://arxiv.org/abs/2408.08447v1",
    "authors": [
      "Nassim Ait Ali Braham",
      "Conrad M Albrecht",
      "Julien Mairal",
      "Jocelyn Chanussot",
      "Yi Wang",
      "Xiao Xiang Zhu"
    ],
    "published": "2024-08-15",
    "abstract": "Foundation models have triggered a paradigm shift in computer vision and are\nincreasingly being adopted in remote sensing, particularly for multispectral\nimagery. Yet, their potential in hyperspectral imaging (HSI) remains untapped\ndue to the absence of comprehensive and globally representative hyperspectral\ndatasets. To close this gap, we introduce SpectralEarth, a large-scale\nmulti-temporal dataset designed to pretrain hyperspectral foundation models\nleveraging data from the Environmental Mapping and Analysis Program (EnMAP).\nSpectralEarth comprises 538,974 image patches covering 415,153 unique locations\nfrom more than 11,636 globally distributed EnMAP scenes spanning two years of\narchive. Additionally, 17.5% of these locations include multiple timestamps,\nenabling multi-temporal HSI analysis. Utilizing state-of-the-art\nself-supervised learning (SSL) algorithms, we pretrain a series of foundation\nmodels on SpectralEarth. We integrate a spectral adapter into classical vision\nbackbones to accommodate the unique characteristics of HSI. In tandem, we\nconstruct four downstream datasets for land-cover and crop-type mapping,\nproviding benchmarks for model evaluation. Experimental results support the\nversatility of our models, showcasing their generalizability across different\ntasks and sensors. We also highlight computational efficiency during model\nfine-tuning. The dataset, models, and source code will be made publicly\navailable.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Seg-CycleGAN : SAR-to-optical image translation guided by a downstream task",
    "url": "http://arxiv.org/abs/2408.05777v1",
    "authors": [
      "Hannuo Zhang",
      "Huihui Li",
      "Jiarui Lin",
      "Yujie Zhang",
      "Jianghua Fan",
      "Hang Liu"
    ],
    "published": "2024-08-11",
    "abstract": "Optical remote sensing and Synthetic Aperture Radar(SAR) remote sensing are\ncrucial for earth observation, offering complementary capabilities. While\noptical sensors provide high-quality images, they are limited by weather and\nlighting conditions. In contrast, SAR sensors can operate effectively under\nadverse conditions. This letter proposes a GAN-based SAR-to-optical image\ntranslation method named Seg-CycleGAN, designed to enhance the accuracy of ship\ntarget translation by leveraging semantic information from a pre-trained\nsemantic segmentation model. Our method utilizes the downstream task of ship\ntarget semantic segmentation to guide the training of image translation\nnetwork, improving the quality of output Optical-styled images. The potential\nof foundation-model-annotated datasets in SAR-to-optical translation tasks is\nrevealed. This work suggests broader research and applications for\ndownstream-task-guided frameworks. The code will be available at\nhttps://github.com/NPULHH/",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height Estimation",
    "url": "http://arxiv.org/abs/2408.04523v1",
    "authors": [
      "Daniele Rege Cambrin",
      "Isaac Corley",
      "Paolo Garza"
    ],
    "published": "2024-08-08",
    "abstract": "Estimating global tree canopy height is crucial for forest conservation and\nclimate change applications. However, capturing high-resolution ground truth\ncanopy height using LiDAR is expensive and not available globally. An efficient\nalternative is to train a canopy height estimator to operate on single-view\nremotely sensed imagery. The primary obstacle to this approach is that these\nmethods require significant training data to generalize well globally and\nacross uncommon edge cases. Recent monocular depth estimation foundation models\nhave show strong zero-shot performance even for complex scenes. In this paper\nwe leverage the representations learned by these models to transfer to the\nremote sensing domain for measuring canopy height. Our findings suggest that\nour proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2\nmodel for canopy height estimation, provides a performant and efficient\nsolution, surpassing the current state-of-the-art with superior or comparable\nperformance using only a fraction of the computational resources and\nparameters. Furthermore, our approach requires less than \\$1.30 in compute and\nresults in an estimated carbon footprint of 0.14 kgCO2. Code, experimental\nresults, and model checkpoints are openly available at\nhttps://github.com/DarthReca/depth-any-canopy.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Vision Foundation Models in Remote Sensing: A Survey",
    "url": "http://arxiv.org/abs/2408.03464v2",
    "authors": [
      "Siqi Lu",
      "Junlin Guo",
      "James R Zimmer-Dauphinee",
      "Jordan M Nieusma",
      "Xiao Wang",
      "Parker VanValkenburgh",
      "Steven A Wernke",
      "Yuankai Huo"
    ],
    "published": "2024-08-06",
    "abstract": "Artificial Intelligence (AI) technologies have profoundly transformed the\nfield of remote sensing, revolutionizing data collection, processing, and\nanalysis. Traditionally reliant on manual interpretation and task-specific\nmodels, remote sensing research has been significantly enhanced by the advent\nof foundation models-large-scale, pre-trained AI models capable of performing a\nwide array of tasks with unprecedented accuracy and efficiency. This paper\nprovides a comprehensive survey of foundation models in the remote sensing\ndomain. We categorize these models based on their architectures, pre-training\ndatasets, and methodologies. Through detailed performance comparisons, we\nhighlight emerging trends and the significant advancements achieved by those\nfoundation models. Additionally, we discuss technical challenges, practical\nimplications, and future research directions, addressing the need for\nhigh-quality data, computational resources, and improved model generalization.\nOur research also finds that pre-training methods, particularly self-supervised\nlearning techniques like contrastive learning and masked autoencoders,\nremarkably enhance the performance and robustness of foundation models. This\nsurvey aims to serve as a resource for researchers and practitioners by\nproviding a panorama of advances and promising pathways for continued\ndevelopment and application of foundation models in remote sensing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "A Causally Informed Pretraining Approach for Multimodal Foundation Models: Applications in Remote Sensing",
    "url": "http://arxiv.org/abs/2407.19660v3",
    "authors": [
      "Praveen Ravirathinam",
      "Ankush Khandelwal",
      "Rahul Ghosh",
      "Vipin Kumar"
    ],
    "published": "2024-07-29",
    "abstract": "Self-supervised learning has emerged as a powerful paradigm for pretraining\nfoundation models using large-scale data. Existing pretraining approaches\npredominantly rely on masked reconstruction or next-token prediction\nstrategies, demonstrating strong performance across various downstream tasks,\nincluding geoscience applications. However, these approaches do not fully\ncapture the causal interplay between different geospatial and environmental\nvariables. To address this limitation, we propose Causally Informed\nVariable-Step Forecasting (CI-VSF), a novel pretraining task that models\nforecasting as a conditional generation task, where driver variables (e.g.,\nweather) inform the prediction of response variables (e.g., satellite imagery).\nWe demonstrate that pretraining in such a fashion leads to enhanced performance\nwhen finetuned on both prediction (e.g., crop mapping, missing image\nprediction, soil moisture estimation) and forecasting (e.g., future image\nforecasting, soil moisture forecasting) downstream tasks when compared to other\npretraining approaches. While we use remote sensing as our main application to\ndemonstrate the efficacy of our proposed pretraining strategy over existing\nparadigms, it is applicable to any domain that involves known causal\nrelationships amongst a set of variables.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Semantic-CC: Boosting Remote Sensing Image Change Captioning via Foundational Knowledge and Semantic Guidance",
    "url": "http://arxiv.org/abs/2407.14032v1",
    "authors": [
      "Yongshuo Zhu",
      "Lu Li",
      "Keyan Chen",
      "Chenyang Liu",
      "Fugen Zhou",
      "Zhenwei Shi"
    ],
    "published": "2024-07-19",
    "abstract": "Remote sensing image change captioning (RSICC) aims to articulate the changes\nin objects of interest within bi-temporal remote sensing images using natural\nlanguage. Given the limitations of current RSICC methods in expressing general\nfeatures across multi-temporal and spatial scenarios, and their deficiency in\nproviding granular, robust, and precise change descriptions, we introduce a\nnovel change captioning (CC) method based on the foundational knowledge and\nsemantic guidance, which we term Semantic-CC. Semantic-CC alleviates the\ndependency of high-generalization algorithms on extensive annotations by\nharnessing the latent knowledge of foundation models, and it generates more\ncomprehensive and accurate change descriptions guided by pixel-level semantics\nfrom change detection (CD). Specifically, we propose a bi-temporal SAM-based\nencoder for dual-image feature extraction; a multi-task semantic aggregation\nneck for facilitating information interaction between heterogeneous tasks; a\nstraightforward multi-scale change detection decoder to provide pixel-level\nsemantic guidance; and a change caption decoder based on the large language\nmodel (LLM) to generate change description sentences. Moreover, to ensure the\nstability of the joint training of CD and CC, we propose a three-stage training\nstrategy that supervises different tasks at various stages. We validate the\nproposed method on the LEVIR-CC and LEVIR-CD datasets. The experimental results\ncorroborate the complementarity of CD and CC, demonstrating that Semantic-CC\ncan generate more accurate change descriptions and achieve optimal performance\nacross both tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Paving the way toward foundation models for irregular and unaligned Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2407.08448v2",
    "authors": [
      "Iris Dumeur",
      "Silvia Valero",
      "Jordi Inglada"
    ],
    "published": "2024-07-11",
    "abstract": "Although recently several foundation models for satellite remote sensing\nimagery have been proposed, they fail to address major challenges of\nreal/operational applications. Indeed, embeddings that don't take into account\nthe spectral, spatial and temporal dimensions of the data as well as the\nirregular or unaligned temporal sampling are of little use for most real world\nuses. As a consequence, we propose an ALIgned Sits Encoder (ALISE), a novel\napproach that leverages the spatial, spectral, and temporal dimensions of\nirregular and unaligned SITS while producing aligned latent representations.\nUnlike SSL models currently available for SITS, ALISE incorporates a flexible\nquery mechanism to project the SITS into a common and learned temporal\nprojection space. Additionally, thanks to a multi-view framework, we explore\nintegration of instance discrimination along a masked autoencoding task to\nSITS. The quality of the produced representation is assessed through three\ndownstream tasks: crop segmentation (PASTIS), land cover segmentation\n(MultiSenGE), and a novel crop change detection dataset. Furthermore, the\nchange detection task is performed without supervision. The results suggest\nthat the use of aligned representations is more effective than previous SSL\nmethods for linear probing segmentation tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Evaluating and Benchmarking Foundation Models for Earth Observation and Geospatial AI",
    "url": "http://arxiv.org/abs/2406.18295v1",
    "authors": [
      "Nikolaos Dionelis",
      "Casper Fibaek",
      "Luke Camilleri",
      "Andreas Luyts",
      "Jente Bosmans",
      "Bertrand Le Saux"
    ],
    "published": "2024-06-26",
    "abstract": "When we are primarily interested in solving several problems jointly with a\ngiven prescribed high performance accuracy for each target application, then\nFoundation Models should for most cases be used rather than problem-specific\nmodels. We focus on the specific Computer Vision application of Foundation\nModels for Earth Observation (EO) and geospatial AI. These models can solve\nimportant problems we are tackling, including for example land cover\nclassification, crop type mapping, flood segmentation, building density\nestimation, and road regression segmentation. In this paper, we show that for a\nlimited number of labelled data, Foundation Models achieve improved performance\ncompared to problem-specific models. In this work, we also present our proposed\nevaluation benchmark for Foundation Models for EO. Benchmarking the\ngeneralization performance of Foundation Models is important as it has become\ndifficult to standardize a fair comparison across the many different models\nthat have been proposed recently. We present the results using our evaluation\nbenchmark for EO Foundation Models and show that Foundation Models are label\nefficient in the downstream tasks and help us solve problems we are tackling in\nEO and remote sensing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Changen2: Multi-Temporal Remote Sensing Generative Change Foundation Model",
    "url": "http://arxiv.org/abs/2406.17998v1",
    "authors": [
      "Zhuo Zheng",
      "Stefano Ermon",
      "Dongjun Kim",
      "Liangpei Zhang",
      "Yanfei Zhong"
    ],
    "published": "2024-06-26",
    "abstract": "Our understanding of the temporal dynamics of the Earth's surface has been\nadvanced by deep vision models, which often require lots of labeled\nmulti-temporal images for training. However, collecting, preprocessing, and\nannotating multi-temporal remote sensing images at scale is non-trivial since\nit is expensive and knowledge-intensive. In this paper, we present change data\ngenerators based on generative models, which are cheap and automatic,\nalleviating these data problems. Our main idea is to simulate a stochastic\nchange process over time. We describe the stochastic change process as a\nprobabilistic graphical model (GPCM), which factorizes the complex simulation\nproblem into two more tractable sub-problems, i.e., change event simulation and\nsemantic change synthesis. To solve these two problems, we present Changen2, a\nGPCM with a resolution-scalable diffusion transformer which can generate time\nseries of images and their semantic and change labels from labeled or unlabeled\nsingle-temporal images. Changen2 is a generative change foundation model that\ncan be trained at scale via self-supervision, and can produce change\nsupervisory signals from unlabeled single-temporal images. Unlike existing\nfoundation models, Changen2 synthesizes change data to train task-specific\nfoundation models for change detection. The resulting model possesses inherent\nzero-shot change detection capabilities and excellent transferability.\nExperiments suggest Changen2 has superior spatiotemporal scalability, e.g.,\nChangen2 model trained on 256$^2$ pixel single-temporal images can yield time\nseries of any length and resolutions of 1,024$^2$ pixels. Changen2 pre-trained\nmodels exhibit superior zero-shot performance (narrowing the performance gap to\n3% on LEVIR-CD and approximately 10% on both S2Looking and SECOND, compared to\nfully supervised counterparts) and transferability across multiple types of\nchange tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Towards a multimodal framework for remote sensing image change retrieval and captioning",
    "url": "http://arxiv.org/abs/2406.13424v1",
    "authors": [
      "Roger Ferrod",
      "Luigi Di Caro",
      "Dino Ienco"
    ],
    "published": "2024-06-19",
    "abstract": "Recently, there has been increasing interest in multimodal applications that\nintegrate text with other modalities, such as images, audio and video, to\nfacilitate natural language interactions with multimodal AI systems. While\napplications involving standard modalities have been extensively explored,\nthere is still a lack of investigation into specific data modalities such as\nremote sensing (RS) data. Despite the numerous potential applications of RS\ndata, including environmental protection, disaster monitoring and land\nplanning, available solutions are predominantly focused on specific tasks like\nclassification, captioning and retrieval. These solutions often overlook the\nunique characteristics of RS data, such as its capability to systematically\nprovide information on the same geographical areas over time. This ability\nenables continuous monitoring of changes in the underlying landscape. To\naddress this gap, we propose a novel foundation model for bi-temporal RS image\npairs, in the context of change detection analysis, leveraging Contrastive\nLearning and the LEVIR-CC dataset for both captioning and text-image retrieval.\nBy jointly training a contrastive encoder and captioning decoder, our model add\ntext-image retrieval capabilities, in the context of bi-temporal change\ndetection, while maintaining captioning performances that are comparable to the\nstate of the art. We release the source code and pretrained weights at:\nhttps://github.com/rogerferrod/RSICRC.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding",
    "url": "http://arxiv.org/abs/2406.12479v1",
    "authors": [
      "Linrui Xu",
      "Ling Zhao",
      "Wang Guo",
      "Qiujun Li",
      "Kewang Long",
      "Kaiqi Zou",
      "Yuhan Wang",
      "Haifeng Li"
    ],
    "published": "2024-06-18",
    "abstract": "The remote sensing image intelligence understanding model is undergoing a new\nprofound paradigm shift which has been promoted by multi-modal large language\nmodel (MLLM), i.e. from the paradigm learning a domain model (LaDM) shifts to\nparadigm learning a pre-trained general foundation model followed by an\nadaptive domain model (LaGD). Under the new LaGD paradigm, the old datasets,\nwhich have led to advances in RSI intelligence understanding in the last\ndecade, are no longer suitable for fire-new tasks. We argued that a new dataset\nmust be designed to lighten tasks with the following features: 1)\nGeneralization: training model to learn shared knowledge among tasks and to\nadapt to different tasks; 2) Understanding complex scenes: training model to\nunderstand the fine-grained attribute of the objects of interest, and to be\nable to describe the scene with natural language; 3) Reasoning: training model\nto be able to realize high-level visual reasoning. In this paper, we designed a\nhigh-quality, diversified, and unified multimodal instruction-following dataset\nfor RSI understanding produced by GPT-4V and existing datasets, which we called\nRS-GPT4V. To achieve generalization, we used a (Question, Answer) which was\ndeduced from GPT-4V via instruction-following to unify the tasks such as\ncaptioning and localization; To achieve complex scene, we proposed a\nhierarchical instruction description with local strategy in which the\nfine-grained attributes of the objects and their spatial relationships are\ndescribed and global strategy in which all the local information are integrated\nto yield detailed instruction descript; To achieve reasoning, we designed\nmultiple-turn QA pair to provide the reasoning ability for a model. The\nempirical results show that the fine-tuned MLLMs by RS-GPT4V can describe\nfine-grained information. The dataset is available at:\nhttps://github.com/GeoX-Lab/RS-GPT4V.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Scaling Efficient Masked Image Modeling on Large Remote Sensing Dataset",
    "url": "http://arxiv.org/abs/2406.11933v4",
    "authors": [
      "Fengxiang Wang",
      "Hongzhen Wang",
      "Di Wang",
      "Zonghao Guo",
      "Zhenyu Zhong",
      "Long Lan",
      "Jing Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "published": "2024-06-17",
    "abstract": "Masked Image Modeling (MIM) has become an essential method for building\nfoundational visual models in remote sensing (RS). However, the limitations in\nsize and diversity of existing RS datasets restrict the ability of MIM methods\nto learn generalizable representations. Additionally, conventional MIM\ntechniques, which require reconstructing all tokens, introduce unnecessary\ncomputational overhead. To address these issues, we present a new pre-training\npipeline for RS models, featuring the creation of a large-scale RS dataset and\nan efficient MIM approach. We curated a high-quality dataset named\nOpticalRS-13M by collecting publicly available RS datasets and processing them\nthrough exclusion, slicing, and deduplication. OpticalRS-13M comprises 13\nmillion optical images covering various RS tasks, such as object detection and\npixel segmentation. To enhance efficiency, we propose SelectiveMAE, a\npre-training method that dynamically encodes and reconstructs semantically rich\npatch tokens, thereby reducing the inefficiencies of traditional MIM models\ncaused by redundant background pixels in RS images. Extensive experiments\ndemonstrate that OpticalRS-13M significantly improves classification,\ndetection, and segmentation performance, while SelectiveMAE increases training\nefficiency over 2 times. This highlights the effectiveness and scalability of\nour pipeline in developing RS foundational models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "RS-DFM: A Remote Sensing Distributed Foundation Model for Diverse Downstream Tasks",
    "url": "http://arxiv.org/abs/2406.07032v1",
    "authors": [
      "Zhechao Wang",
      "Peirui Cheng",
      "Pengju Tian",
      "Yuchao Wang",
      "Mingxin Chen",
      "Shujing Duan",
      "Zhirui Wang",
      "Xinming Li",
      "Xian Sun"
    ],
    "published": "2024-06-11",
    "abstract": "Remote sensing lightweight foundation models have achieved notable success in\nonline perception within remote sensing. However, their capabilities are\nrestricted to performing online inference solely based on their own\nobservations and models, thus lacking a comprehensive understanding of\nlarge-scale remote sensing scenarios. To overcome this limitation, we propose a\nRemote Sensing Distributed Foundation Model (RS-DFM) based on generalized\ninformation mapping and interaction. This model can realize online\ncollaborative perception across multiple platforms and various downstream tasks\nby mapping observations into a unified space and implementing a task-agnostic\ninformation interaction strategy. Specifically, we leverage the ground-based\ngeometric prior of remote sensing oblique observations to transform the feature\nmapping from absolute depth estimation to relative depth estimation, thereby\nenhancing the model's ability to extract generalized features across diverse\nheights and perspectives. Additionally, we present a dual-branch information\ncompression module to decouple high-frequency and low-frequency feature\ninformation, achieving feature-level compression while preserving essential\ntask-agnostic details. In support of our research, we create a multi-task\nsimulation dataset named AirCo-MultiTasks for multi-UAV collaborative\nobservation. We also conduct extensive experiments, including 3D object\ndetection, instance segmentation, and trajectory prediction. The numerous\nresults demonstrate that our RS-DFM achieves state-of-the-art performance\nacross various downstream tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Multi-Label Guided Soft Contrastive Learning for Efficient Earth Observation Pretraining",
    "url": "http://arxiv.org/abs/2405.20462v2",
    "authors": [
      "Yi Wang",
      "Conrad M Albrecht",
      "Xiao Xiang Zhu"
    ],
    "published": "2024-05-30",
    "abstract": "Self-supervised pretraining on large-scale satellite data has raised great\ninterest in building Earth observation (EO) foundation models. However, many\nimportant resources beyond pure satellite imagery, such as land-cover-land-use\nproducts that provide free global semantic information, as well as vision\nfoundation models that hold strong knowledge of the natural world, are not\nwidely studied. In this work, we show these free additional resources not only\nhelp resolve common contrastive learning bottlenecks, but also significantly\nboost the efficiency and effectiveness of EO pretraining. Specifically, we\nfirst propose soft contrastive learning that optimizes cross-scene soft\nsimilarity based on land-cover-generated multi-label supervision, naturally\nsolving the issue of multiple positive samples and too strict positive matching\nin complex scenes. Second, we revisit and explore cross-domain continual\npretraining for both multispectral and SAR imagery, building efficient EO\nfoundation models from strongest vision models such as DINOv2. Adapting simple\nweight-initialization and Siamese masking strategies into our soft contrastive\nlearning framework, we demonstrate impressive continual pretraining performance\neven when the input modalities are not aligned. Without prohibitive training,\nwe produce multispectral and SAR foundation models that achieve significantly\nbetter results in 10 out of 11 downstream tasks than most existing SOTA models.\nFor example, our ResNet50/ViT-S achieve 84.8/85.0 linear probing mAP scores on\nBigEarthNet-10\\% which are better than most existing ViT-L models; under the\nsame setting, our ViT-B sets a new record of 86.8 in multispectral, and 82.5 in\nSAR, the latter even better than many multispectral models. Dataset and models\nare available at \\url{https://github.com/zhu-xlab/softcon}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "FMARS: Annotating Remote Sensing Images for Disaster Management using Foundation Models",
    "url": "http://arxiv.org/abs/2405.20109v2",
    "authors": [
      "Edoardo Arnaudo",
      "Jacopo Lungo Vaschetti",
      "Lorenzo Innocenti",
      "Luca Barco",
      "Davide Lisi",
      "Vanina Fissore",
      "Claudio Rossi"
    ],
    "published": "2024-05-30",
    "abstract": "Very-High Resolution (VHR) remote sensing imagery is increasingly accessible,\nbut often lacks annotations for effective machine learning applications. Recent\nfoundation models like GroundingDINO and Segment Anything (SAM) provide\nopportunities to automatically generate annotations. This study introduces\nFMARS (Foundation Model Annotations in Remote Sensing), a methodology\nleveraging VHR imagery and foundation models for fast and robust annotation. We\nfocus on disaster management and provide a large-scale dataset with labels\nobtained from pre-event imagery over 19 disaster events, derived from the Maxar\nOpen Data initiative. We train segmentation models on the generated labels,\nusing Unsupervised Domain Adaptation (UDA) techniques to increase\ntransferability to real-world scenarios. Our results demonstrate the\neffectiveness of leveraging foundation models to automatically annotate remote\nsensing data at scale, enabling robust downstream models for critical\napplications. Code and dataset are available at\n\\url{https://github.com/links-ads/igarss-fmars}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Research on the Spatial Data Intelligent Foundation Model",
    "url": "http://arxiv.org/abs/2405.19730v5",
    "authors": [
      "Shaohua Wang",
      "Xing Xie",
      "Yong Li",
      "Danhuai Guo",
      "Zhi Cai",
      "Yu Liu",
      "Yang Yue",
      "Xiao Pan",
      "Feng Lu",
      "Huayi Wu",
      "Zhipeng Gui",
      "Zhiming Ding",
      "Bolong Zheng",
      "Fuzheng Zhang",
      "Jingyuan Wang",
      "Zhengchao Chen",
      "Hao Lu",
      "Jiayi Li",
      "Peng Yue",
      "Wenhao Yu",
      "Yao Yao",
      "Leilei Sun",
      "Yong Zhang",
      "Longbiao Chen",
      "Xiaoping Du",
      "Xiang Li",
      "Xueying Zhang",
      "Kun Qin",
      "Zhaoya Gong",
      "Weihua Dong",
      "Xiaofeng Meng"
    ],
    "published": "2024-05-30",
    "abstract": "This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-view Remote Sensing Image Segmentation With SAM priors",
    "url": "http://arxiv.org/abs/2405.14171v1",
    "authors": [
      "Zipeng Qi",
      "Chenyang Liu",
      "Zili Liu",
      "Hao Chen",
      "Yongchang Wu",
      "Zhengxia Zou",
      "Zhenwei Sh"
    ],
    "published": "2024-05-23",
    "abstract": "Multi-view segmentation in Remote Sensing (RS) seeks to segment images from\ndiverse perspectives within a scene. Recent methods leverage 3D information\nextracted from an Implicit Neural Field (INF), bolstering result consistency\nacross multiple views while using limited accounts of labels (even within 3-5\nlabels) to streamline labor. Nonetheless, achieving superior performance within\nthe constraints of limited-view labels remains challenging due to inadequate\nscene-wide supervision and insufficient semantic features within the INF. To\naddress these. we propose to inject the prior of the visual foundation\nmodel-Segment Anything(SAM), to the INF to obtain better results under the\nlimited number of training data. Specifically, we contrast SAM features between\ntesting and training views to derive pseudo labels for each testing view,\naugmenting scene-wide labeling information. Subsequently, we introduce SAM\nfeatures via a transformer into the INF of the scene, supplementing the\nsemantic information. The experimental results demonstrate that our method\noutperforms the mainstream method, confirming the efficacy of SAM as a\nsupplement to the INF for this task.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing Image Generation",
    "url": "http://arxiv.org/abs/2405.13570v3",
    "authors": [
      "Zhiping Yu",
      "Chenyang Liu",
      "Liqin Liu",
      "Zhenwei Shi",
      "Zhengxia Zou"
    ],
    "published": "2024-05-22",
    "abstract": "The recent advancement of generative foundational models has ushered in a new\nera of image generation in the realm of natural images, revolutionizing art\ndesign, entertainment, environment simulation, and beyond. Despite producing\nhigh-quality samples, existing methods are constrained to generating images of\nscenes at a limited scale. In this paper, we present MetaEarth, a generative\nfoundation model that breaks the barrier by scaling image generation to a\nglobal level, exploring the creation of worldwide, multi-resolution, unbounded,\nand virtually limitless remote sensing images. In MetaEarth, we propose a\nresolution-guided self-cascading generative framework, which enables the\ngenerating of images at any region with a wide range of geographical\nresolutions. To achieve unbounded and arbitrary-sized image generation, we\ndesign a novel noise sampling strategy for denoising diffusion models by\nanalyzing the generation conditions and initial noise. To train MetaEarth, we\nconstruct a large dataset comprising multi-resolution optical remote sensing\nimages with geographical information. Experiments have demonstrated the\npowerful capabilities of our method in generating global-scale images.\nAdditionally, the MetaEarth serves as a data engine that can provide\nhigh-quality and rich training data for downstream tasks. Our model opens up\nnew possibilities for constructing generative world models by simulating Earth\nvisuals from an innovative overhead perspective.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "PIR: Remote Sensing Image-Text Retrieval with Prior Instruction Representation Learning",
    "url": "http://arxiv.org/abs/2405.10160v2",
    "authors": [
      "Jiancheng Pan",
      "Muyuan Ma",
      "Qing Ma",
      "Cong Bai",
      "Shengyong Chen"
    ],
    "published": "2024-05-16",
    "abstract": "Remote sensing image-text retrieval constitutes a foundational aspect of\nremote sensing interpretation tasks, facilitating the alignment of vision and\nlanguage representations. This paper introduces a prior instruction\nrepresentation (PIR) learning paradigm that draws on prior knowledge to\ninstruct adaptive learning of vision and text representations. Based on PIR, a\ndomain-adapted remote sensing image-text retrieval framework PIR-ITR is\ndesigned to address semantic noise issues in vision-language understanding\ntasks. However, with massive additional data for pre-training the\nvision-language foundation model, remote sensing image-text retrieval is\nfurther developed into an open-domain retrieval task. Continuing with the\nabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote\nsensing image-text retrieval, to address semantic noise in remote sensing\nvision-language representations and further improve open-domain retrieval\nperformance. In vision representation, we utilize the prior-guided knowledge of\nthe remote sensing scene recognition by building a belief matrix to select key\nfeatures for reducing the impact of semantic noise. In text representation, we\nuse the previous time step to cyclically activate the current time step to\nenhance text representation capability. A cluster-wise Affiliation Loss (AL) is\nproposed to constrain the inter-classes and to reduce the semantic confusion\nzones in the common subspace. Comprehensive experiments demonstrate that PIR\ncould enhance vision and text representations and outperform the\nstate-of-the-art methods of closed-domain and open-domain retrieval on two\nbenchmark datasets, RSICD and RSITMD.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Cross-sensor self-supervised training and alignment for remote sensing",
    "url": "http://arxiv.org/abs/2405.09922v1",
    "authors": [
      "Valerio Marsocci",
      "Nicolas Audebert"
    ],
    "published": "2024-05-16",
    "abstract": "Large-scale \"foundation models\" have gained traction as a way to leverage the\nvast amounts of unlabeled remote sensing data collected every day. However, due\nto the multiplicity of Earth Observation satellites, these models should learn\n\"sensor agnostic\" representations, that generalize across sensor\ncharacteristics with minimal fine-tuning. This is complicated by data\navailability, as low-resolution imagery, such as Sentinel-2 and Landsat-8 data,\nare available in large amounts, while very high-resolution aerial or satellite\ndata is less common. To tackle these challenges, we introduce cross-sensor\nself-supervised training and alignment for remote sensing (X-STARS). We design\na self-supervised training loss, the Multi-Sensor Alignment Dense loss (MSAD),\nto align representations across sensors, even with vastly different\nresolutions. Our X-STARS can be applied to train models from scratch, or to\nadapt large models pretrained on e.g low-resolution EO data to new\nhigh-resolution sensors, in a continual pretraining framework. We collect and\nrelease MSC-France, a new multi-sensor dataset, on which we train our X-STARS\nmodels, then evaluated on seven downstream classification and segmentation\ntasks. We demonstrate that X-STARS outperforms the state-of-the-art by a\nsignificant margin with less data across various conditions of data\navailability and resolutions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Many-Shot In-Context Learning in Multimodal Foundation Models",
    "url": "http://arxiv.org/abs/2405.09798v2",
    "authors": [
      "Yixing Jiang",
      "Jeremy Irvin",
      "Ji Hun Wang",
      "Muhammad Ahmed Chaudhry",
      "Jonathan H. Chen",
      "Andrew Y. Ng"
    ],
    "published": "2024-05-16",
    "abstract": "Large language models are effective at few-shot in-context learning (ICL).\nRecent advancements in multimodal foundation models have enabled\nunprecedentedly long context windows, presenting an opportunity to explore\ntheir capability to perform ICL with many more demonstrating examples. In this\nwork, we evaluate the performance of multimodal foundation models scaling from\nfew-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14\ndatasets spanning multiple domains (natural imagery, medical imagery, remote\nsensing, and molecular imagery) and tasks (image classification, visual QA, and\nobject localization). We observe that many-shot ICL, including up to almost\n2,000 demonstrating examples, leads to substantial improvements compared to\nfew-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5\nPro performance continues to improve log-linearly up to the maximum number of\ntested examples on many datasets. We also find open-weights multimodal\nfoundation models like Llama 3.2-Vision do not benefit from the demonstrating\nexamples, highlighting an important gap between open and closed multimodal\nfoundation models. Given the high inference costs required for many-shot ICL,\nwe also explore the impact of batching multiple queries in a single API call.\nWe show that batching up to 50 queries can lead to performance improvements\nunder zero-shot and many-shot ICL, with substantial gains in the zero-shot\nsetting on multiple datasets, while drastically reducing per-query cost and\nlatency. Finally, while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot\nperformance across the datasets, Gemini 1.5 Pro learns more quickly than GPT-4o\non most datasets. Our results suggest that many-shot ICL could enable users to\nefficiently adapt multimodal foundation models to new applications and domains.\nOur codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL .",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research",
    "url": "http://arxiv.org/abs/2405.08668v1",
    "authors": [
      "Qinglong Cao",
      "Yuntian Chen",
      "Lu Lu",
      "Hao Sun",
      "Zhenzhong Zeng",
      "Xiaokang Yang",
      "Dongxiao Zhang"
    ],
    "published": "2024-05-14",
    "abstract": "Large-scale Vision-Language Models (VLMs) have demonstrated exceptional\nperformance in natural vision tasks, motivating researchers across domains to\nexplore domain-specific VLMs. However, the construction of powerful\ndomain-specific VLMs demands vast amounts of annotated data, substantial\nelectrical energy, and computing resources, primarily accessible to industry,\nyet hindering VLM research in academia. To address this challenge and foster\nsustainable and equitable VLM research, we present the Generalized Domain\nPrompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs' robust\nrecognition capabilities from natural vision to specialized domains, without\nthe need for extensive data or resources. By leveraging small-scale\ndomain-specific foundation models and minimal prompt samples, GDPL empowers the\nlanguage branch with domain knowledge through quaternion networks, uncovering\ncross-modal relationships between domain-specific vision features and natural\nvision-based contextual embeddings. Simultaneously, GDPL guides the vision\nbranch into specific domains through hierarchical propagation of generated\nvision prompt features, grounded in well-matched vision-language relations.\nFurthermore, to fully harness the domain adaptation potential of VLMs, we\nintroduce a novel low-rank adaptation approach. Extensive experiments across\ndiverse domains like remote sensing, medical imaging, geology, Synthetic\nAperture Radar, and fluid dynamics, validate the efficacy of GDPL,\ndemonstrating its ability to achieve state-of-the-art domain recognition\nperformance in a prompt learning paradigm. Our framework paves the way for\nsustainable and inclusive VLM research, transcending the barriers between\nacademia and industry.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "SOAR: Advancements in Small Body Object Detection for Aerial Imagery Using State Space Models and Programmable Gradients",
    "url": "http://arxiv.org/abs/2405.01699v2",
    "authors": [
      "Tushar Verma",
      "Jyotsna Singh",
      "Yash Bhartari",
      "Rishi Jarwal",
      "Suraj Singh",
      "Shubhkarman Singh"
    ],
    "published": "2024-05-02",
    "abstract": "Small object detection in aerial imagery presents significant challenges in\ncomputer vision due to the minimal data inherent in small-sized objects and\ntheir propensity to be obscured by larger objects and background noise.\nTraditional methods using transformer-based models often face limitations\nstemming from the lack of specialized databases, which adversely affect their\nperformance with objects of varying orientations and scales. This underscores\nthe need for more adaptable, lightweight models. In response, this paper\nintroduces two innovative approaches that significantly enhance detection and\nsegmentation capabilities for small aerial objects. Firstly, we explore the use\nof the SAHI framework on the newly introduced lightweight YOLO v9 architecture,\nwhich utilizes Programmable Gradient Information (PGI) to reduce the\nsubstantial information loss typically encountered in sequential feature\nextraction processes. The paper employs the Vision Mamba model, which\nincorporates position embeddings to facilitate precise location-aware visual\nunderstanding, combined with a novel bidirectional State Space Model (SSM) for\neffective visual context modeling. This State Space Model adeptly harnesses the\nlinear complexity of CNNs and the global receptive field of Transformers,\nmaking it particularly effective in remote sensing image classification. Our\nexperimental results demonstrate substantial improvements in detection accuracy\nand processing efficiency, validating the applicability of these approaches for\nreal-time small object detection across diverse aerial scenarios. This paper\nalso discusses how these methodologies could serve as foundational models for\nfuture advancements in aerial object recognition technologies. The source code\nwill be made accessible here.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "When are Foundation Models Effective? Understanding the Suitability for Pixel-Level Classification Using Multispectral Imagery",
    "url": "http://arxiv.org/abs/2404.11797v1",
    "authors": [
      "Yiqun Xie",
      "Zhihao Wang",
      "Weiye Chen",
      "Zhili Li",
      "Xiaowei Jia",
      "Yanhua Li",
      "Ruichen Wang",
      "Kangyang Chai",
      "Ruohan Li",
      "Sergii Skakun"
    ],
    "published": "2024-04-17",
    "abstract": "Foundation models, i.e., very large deep learning models, have demonstrated\nimpressive performances in various language and vision tasks that are otherwise\ndifficult to reach using smaller-size models. The major success of GPT-type of\nlanguage models is particularly exciting and raises expectations on the\npotential of foundation models in other domains including satellite remote\nsensing. In this context, great efforts have been made to build foundation\nmodels to test their capabilities in broader applications, and examples include\nPrithvi by NASA-IBM, Segment-Anything-Model, ViT, etc. This leads to an\nimportant question: Are foundation models always a suitable choice for\ndifferent remote sensing tasks, and when or when not? This work aims to enhance\nthe understanding of the status and suitability of foundation models for\npixel-level classification using multispectral imagery at moderate resolution,\nthrough comparisons with traditional machine learning (ML) and regular-size\ndeep learning models. Interestingly, the results reveal that in many scenarios\ntraditional ML models still have similar or better performance compared to\nfoundation models, especially for tasks where texture is less useful for\nclassification. On the other hand, deep learning models did show more promising\nresults for tasks where labels partially depend on texture (e.g., burn scar),\nwhile the difference in performance between foundation models and deep learning\nmodels is not obvious. The results conform with our analysis: The suitability\nof foundation models depend on the alignment between the self-supervised\nlearning tasks and the real downstream tasks, and the typical masked\nautoencoder paradigm is not necessarily suitable for many remote sensing\nproblems.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "RSMamba: Remote Sensing Image Classification with State Space Model",
    "url": "http://arxiv.org/abs/2403.19654v1",
    "authors": [
      "Keyan Chen",
      "Bowen Chen",
      "Chenyang Liu",
      "Wenyuan Li",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2024-03-28",
    "abstract": "Remote sensing image classification forms the foundation of various\nunderstanding tasks, serving a crucial function in remote sensing image\ninterpretation. The recent advancements of Convolutional Neural Networks (CNNs)\nand Transformers have markedly enhanced classification accuracy. Nonetheless,\nremote sensing scene classification remains a significant challenge, especially\ngiven the complexity and diversity of remote sensing scenarios and the\nvariability of spatiotemporal resolutions. The capacity for whole-image\nunderstanding can provide more precise semantic cues for scene discrimination.\nIn this paper, we introduce RSMamba, a novel architecture for remote sensing\nimage classification. RSMamba is based on the State Space Model (SSM) and\nincorporates an efficient, hardware-aware design known as the Mamba. It\nintegrates the advantages of both a global receptive field and linear modeling\ncomplexity. To overcome the limitation of the vanilla Mamba, which can only\nmodel causal sequences and is not adaptable to two-dimensional image data, we\npropose a dynamic multi-path activation mechanism to augment Mamba's capacity\nto model non-causal data. Notably, RSMamba maintains the inherent modeling\nmechanism of the vanilla Mamba, yet exhibits superior performance across\nmultiple remote sensing image classification datasets. This indicates that\nRSMamba holds significant potential to function as the backbone of future\nvisual foundation models. The code will be available at\n\\url{https://github.com/KyanChen/RSMamba}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining",
    "url": "http://arxiv.org/abs/2403.13430v2",
    "authors": [
      "Di Wang",
      "Jing Zhang",
      "Minqiang Xu",
      "Lin Liu",
      "Dongsheng Wang",
      "Erzhong Gao",
      "Chengxi Han",
      "Haonan Guo",
      "Bo Du",
      "Dacheng Tao",
      "Liangpei Zhang"
    ],
    "published": "2024-03-20",
    "abstract": "Foundation models have reshaped the landscape of Remote Sensing (RS) by\nenhancing various image interpretation tasks. Pretraining is an active research\ntopic, encompassing supervised and self-supervised learning methods to\ninitialize model weights effectively. However, transferring the pretrained\nmodels to downstream tasks may encounter task discrepancy due to their\nformulation of pretraining as image classification or object discrimination\ntasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for\nRS foundation models to address this issue. Using a shared encoder and\ntask-specific decoder architecture, we conduct multi-task supervised\npretraining on the SAMRS dataset, encompassing semantic segmentation, instance\nsegmentation, and rotated object detection. MTP supports both convolutional\nneural networks and vision transformer foundation models with over 300 million\nparameters. The pretrained models are finetuned on various RS downstream tasks,\nsuch as scene classification, horizontal and rotated object detection, semantic\nsegmentation, and change detection. Extensive experiments across 14 datasets\ndemonstrate the superiority of our models over existing ones of similar size\nand their competitive performance compared to larger state-of-the-art models,\nthus validating the effectiveness of MTP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model",
    "url": "http://arxiv.org/abs/2403.07564v2",
    "authors": [
      "Mingze Wang",
      "Lili Su",
      "Cilin Yan",
      "Sheng Xu",
      "Pengcheng Yuan",
      "Xiaolong Jiang",
      "Baochang Zhang"
    ],
    "published": "2024-03-12",
    "abstract": "The intelligent interpretation of buildings plays a significant role in urban\nplanning and management, macroeconomic analysis, population dynamics, etc.\nRemote sensing image building interpretation primarily encompasses building\nextraction and change detection. However, current methodologies often treat\nthese two tasks as separate entities, thereby failing to leverage shared\nknowledge. Moreover, the complexity and diversity of remote sensing image\nscenes pose additional challenges, as most algorithms are designed to model\nindividual small datasets, thus lacking cross-scene generalization. In this\npaper, we propose a comprehensive remote sensing image building understanding\nmodel, termed RSBuilding, developed from the perspective of the foundation\nmodel. RSBuilding is designed to enhance cross-scene generalization and task\nuniversality. Specifically, we extract image features based on the prior\nknowledge of the foundation model and devise a multi-level feature sampler to\naugment scale information. To unify task representation and integrate image\nspatiotemporal clues, we introduce a cross-attention decoder with task prompts.\nAddressing the current shortage of datasets that incorporate annotations for\nboth tasks, we have developed a federated training strategy to facilitate\nsmooth model convergence even when supervision for some tasks is missing,\nthereby bolstering the complementarity of different tasks. Our model was\ntrained on a dataset comprising up to 245,000 images and validated on multiple\nbuilding extraction and change detection datasets. The experimental results\nsubstantiate that RSBuilding can concurrently handle two structurally distinct\ntasks and exhibits robust zero-shot generalization capabilities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa",
    "url": "http://arxiv.org/abs/2403.06860v2",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Mukhtar Opeyemi Yusuf",
      "Kobby Panford-Quainoo",
      "Arnu Pretorius"
    ],
    "published": "2024-03-11",
    "abstract": "Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2403.02059v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Viktoria Moor",
      "Romeo Kienzler",
      "Thomas Brunschwiler"
    ],
    "published": "2024-03-04",
    "abstract": "Image retrieval enables an efficient search through vast amounts of satellite\nimagery and returns similar images to a query. Deep learning models can\nidentify images across various semantic concepts without the need for\nannotations. This work proposes to use Geospatial Foundation Models, like\nPrithvi, for remote sensing image retrieval with multiple benefits: i) the\nmodels encode multi-spectral satellite data and ii) generalize without further\nfine-tuning. We introduce two datasets to the retrieval task and observe a\nstrong performance: Prithvi processes six bands and achieves a mean Average\nPrecision of 97.62% on BigEarthNet-43 and 44.51% on ForestNet-12, outperforming\nother RGB-based models. Further, we evaluate three compression methods with\nbinarized embeddings balancing retrieval speed and accuracy. They match the\nretrieval speed of much shorter hash codes while maintaining the same accuracy\nas floating-point embeddings but with a 32-fold compression. The code is\navailable at https://github.com/IBM/remote-sensing-image-retrieval.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ChatEarthNet: A Global-Scale Image-Text Dataset Empowering Vision-Language Geo-Foundation Models",
    "url": "http://arxiv.org/abs/2402.11325v2",
    "authors": [
      "Zhenghang Yuan",
      "Zhitong Xiong",
      "Lichao Mou",
      "Xiao Xiang Zhu"
    ],
    "published": "2024-02-17",
    "abstract": "An in-depth comprehension of global land cover is essential in Earth\nobservation, forming the foundation for a multitude of applications. Although\nremote sensing technology has advanced rapidly, leading to a proliferation of\nsatellite imagery, the inherent complexity of these images often makes them\ndifficult for non-expert users to understand. Natural language, as a carrier of\nhuman knowledge, can be a bridge between common users and complicated satellite\nimagery. In this context, we introduce a global-scale, high-quality image-text\ndataset for remote sensing, providing natural language descriptions for\nSentinel-2 data to facilitate the understanding of satellite imagery for common\nusers. Specifically, we utilize Sentinel-2 data for its global coverage as the\nfoundational image source, employing semantic segmentation labels from the\nEuropean Space Agency's (ESA) WorldCover project to enrich the descriptions of\nland covers. By conducting in-depth semantic analysis, we formulate detailed\nprompts to elicit rich descriptions from ChatGPT. To enhance the dataset's\nquality, we introduce the manual verification process. This step involves\nmanual inspection and correction to refine the dataset, thus significantly\nimproving its accuracy and quality. Finally, we offer the community\nChatEarthNet, a large-scale image-text dataset characterized by global\ncoverage, high quality, wide-ranging diversity, and detailed descriptions.\nChatEarthNet consists of 163,488 image-text pairs with captions generated by\nChatGPT-3.5 and an additional 10,000 image-text pairs with captions generated\nby ChatGPT-4V(ision). This dataset has significant potential for training\nvision-language geo-foundation models and evaluating large vision-language\nmodels for remote sensing. The dataset will be made publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment",
    "url": "http://arxiv.org/abs/2402.09816v1",
    "authors": [
      "Angelos Zavras",
      "Dimitrios Michail",
      "Beg\u00fcm Demir",
      "Ioannis Papoutsis"
    ],
    "published": "2024-02-15",
    "abstract": "Deep Learning (DL) is undergoing a paradigm shift with the emergence of\nfoundation models, aptly named by their crucial, yet incomplete nature. In this\nwork, we focus on Contrastive Language-Image Pre-training (CLIP), an\nopen-vocabulary foundation model, which achieves high accuracy across many\nimage classification tasks and is often competitive with a fully supervised\nbaseline without being explicitly trained. Nevertheless, there are still\ndomains where zero-shot CLIP performance is far from optimal, such as Remote\nSensing (RS) and medical imagery. These domains do not only exhibit\nfundamentally different distributions compared to natural images, but also\ncommonly rely on complementary modalities, beyond RGB, to derive meaningful\ninsights. To this end, we propose a methodology for the purpose of aligning\ndistinct RS imagery modalities with the visual and textual modalities of CLIP.\nOur two-stage procedure, comprises of robust fine-tuning CLIP in order to deal\nwith the distribution shift, accompanied by the cross-modal alignment of a RS\nmodality encoder, in an effort to extend the zero-shot capabilities of CLIP. We\nultimately demonstrate our method on the tasks of RS imagery classification and\ncross-modal retrieval. We empirically show that both robust fine-tuning and\ncross-modal alignment translate to significant performance gains, across\nseveral RS benchmark datasets. Notably, these enhancements are achieved without\nthe reliance on textual descriptions, without introducing any task-specific\nparameters, without training from scratch and without catastrophic forgetting.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models",
    "url": "http://arxiv.org/abs/2401.09083v1",
    "authors": [
      "Haonan Guo",
      "Xin Su",
      "Chen Wu",
      "Bo Du",
      "Liangpei Zhang",
      "Deren Li"
    ],
    "published": "2024-01-17",
    "abstract": "Recently, the flourishing large language models(LLM), especially ChatGPT,\nhave shown exceptional performance in language understanding, reasoning, and\ninteraction, attracting users and researchers from multiple fields and domains.\nAlthough LLMs have shown great capacity to perform human-like task\naccomplishment in natural language and natural image, their potential in\nhandling remote sensing interpretation tasks has not yet been fully explored.\nMoreover, the lack of automation in remote sensing task planning hinders the\naccessibility of remote sensing interpretation techniques, especially to\nnon-remote sensing experts from multiple research fields. To this end, we\npresent Remote Sensing ChatGPT, an LLM-powered agent that utilizes ChatGPT to\nconnect various AI-based remote sensing models to solve complicated\ninterpretation tasks. More specifically, given a user request and a remote\nsensing image, we utilized ChatGPT to understand user requests, perform task\nplanning according to the tasks' functions, execute each subtask iteratively,\nand generate the final response according to the output of each subtask.\nConsidering that LLM is trained with natural language and is not capable of\ndirectly perceiving visual concepts as contained in remote sensing images, we\ndesigned visual cues that inject visual information into ChatGPT. With Remote\nSensing ChatGPT, users can simply send a remote sensing image with the\ncorresponding request, and get the interpretation results as well as language\nfeedback from Remote Sensing ChatGPT. Experiments and examples show that Remote\nSensing ChatGPT can tackle a wide range of remote sensing tasks and can be\nextended to more tasks with more sophisticated models such as the remote\nsensing foundation model. The code and demo of Remote Sensing ChatGPT is\npublicly available at https://github.com/HaonanGuo/Remote-Sensing-ChatGPT .",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM)",
    "url": "http://arxiv.org/abs/2401.09019v1",
    "authors": [
      "Hongruixuan Chen",
      "Jian Song",
      "Naoto Yokoya"
    ],
    "published": "2024-01-17",
    "abstract": "Unsupervised multimodal change detection is pivotal for time-sensitive tasks\nand comprehensive multi-temporal Earth monitoring. In this study, we explore\nunsupervised multimodal change detection between two key remote sensing data\nsources: optical high-resolution imagery and OpenStreetMap (OSM) data.\nSpecifically, we propose to utilize the vision foundation model Segmentation\nAnything Model (SAM), for addressing our task. Leveraging SAM's exceptional\nzero-shot transfer capability, high-quality segmentation maps of optical images\ncan be obtained. Thus, we can directly compare these two heterogeneous data\nforms in the so-called segmentation domain. We then introduce two strategies\nfor guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt'\nmethods. The two strategies are designed to detect land-cover changes in\ngeneral scenarios and to identify new land-cover objects within existing\nbackgrounds, respectively. Experimental results on three datasets indicate that\nthe proposed approach can achieve more competitive results compared to\nrepresentative unsupervised multimodal change detection methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "OBSeg: Accurate and Fast Instance Segmentation Framework Using Segmentation Foundation Models with Oriented Bounding Box Prompts",
    "url": "http://arxiv.org/abs/2401.08174v6",
    "authors": [
      "Zhen Zhou",
      "Junfeng Fan",
      "Yunkai Ma",
      "Sihan Zhao",
      "Fengshui Jing",
      "Min Tan"
    ],
    "published": "2024-01-16",
    "abstract": "Instance segmentation in remote sensing images is a long-standing challenge.\nSince horizontal bounding boxes introduce many interference objects, oriented\nbounding boxes (OBBs) are usually used for instance identification. However,\nbased on ``segmentation within bounding box'' paradigm, current instance\nsegmentation methods using OBBs are overly dependent on bounding box detection\nperformance. To tackle this problem, this paper proposes OBSeg, an accurate and\nfast instance segmentation framework using OBBs. OBSeg is based on box\nprompt-based segmentation foundation models (BSMs), e.g., Segment Anything\nModel. Specifically, OBSeg first detects OBBs to distinguish instances and\nprovide coarse localization information. Then, it predicts OBB prompt-related\nmasks for fine segmentation. Since OBBs only serve as prompts, OBSeg alleviates\nthe over-dependence on bounding box detection performance of current instance\nsegmentation methods using OBBs. Thanks to OBB prompts, OBSeg outperforms other\ncurrent BSM-based methods using HBBs. In addition, to enable BSMs to handle OBB\nprompts, we propose a novel OBB prompt encoder. To make OBSeg more lightweight\nand further improve the performance of lightweight distilled BSMs, a Gaussian\nsmoothing-based knowledge distillation method is introduced. Experiments\ndemonstrate that OBSeg outperforms current instance segmentation methods on\nmultiple datasets in terms of instance segmentation accuracy and has\ncompetitive inference speed. The code is available at\nhttps://github.com/zhen6618/OBBInstanceSegmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
    "url": "http://arxiv.org/abs/2504.11171v1",
    "authors": [
      "Johannes Jakubik",
      "Felix Yang",
      "Benedikt Blumenstiel",
      "Erik Scheurer",
      "Rocco Sedona",
      "Stefano Maurogiovanni",
      "Jente Bosmans",
      "Nikolaos Dionelis",
      "Valerio Marsocci",
      "Niklas Kopp",
      "Rahul Ramachandran",
      "Paolo Fraccaro",
      "Thomas Brunschwiler",
      "Gabriele Cavallaro",
      "Juan Bernabe-Moreno",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-04-15",
    "abstract": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "TerraTorch: The Geospatial Foundation Models Toolkit",
    "url": "http://arxiv.org/abs/2503.20563v1",
    "authors": [
      "Carlos Gomes",
      "Benedikt Blumenstiel",
      "Joao Lucas de Sousa Almeida",
      "Pedro Henrique de Oliveira",
      "Paolo Fraccaro",
      "Francesc Marti Escofet",
      "Daniela Szwarcman",
      "Naomi Simumba",
      "Romeo Kienzler",
      "Bianca Zadrozny"
    ],
    "published": "2025-03-26",
    "abstract": "TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial\nFoundation Models built on PyTorch Lightning and tailored for satellite,\nweather, and climate data. It integrates domain-specific data modules,\npre-defined tasks, and a modular model factory that pairs any backbone with\ndiverse decoder heads. These components allow researchers and practitioners to\nfine-tune supported models in a no-code fashion by simply editing a training\nconfiguration. By consolidating best practices for model development and\nincorporating the automated hyperparameter optimization extension Iterate,\nTerraTorch reduces the expertise and time required to fine-tune or benchmark\nmodels on new Earth Observation use cases. Furthermore, TerraTorch directly\nintegrates with GEO-Bench, allowing for systematic and reproducible\nbenchmarking of Geospatial Foundation Models. TerraTorch is open sourced under\nApache 2.0, available at https://github.com/IBM/terratorch, and can be\ninstalled via pip install terratorch.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data",
    "url": "http://arxiv.org/abs/2503.12843v3",
    "authors": [
      "Haozhe Si",
      "Yuxuan Wan",
      "Minh Do",
      "Deepak Vasisht",
      "Han Zhao",
      "Hendrik F. Hamann"
    ],
    "published": "2025-03-17",
    "abstract": "Geospatial raster data, such as that collected by satellite-based imaging\nsystems at different times and spectral bands, hold immense potential for\nenabling a wide range of high-impact applications. This potential stems from\nthe rich information that is spatially and temporally contextualized across\nmultiple channels and sensing modalities. Recent work has adapted existing\nself-supervised learning approaches for such geospatial data. However, they\nfall short of scalable model architectures, leading to inflexibility and\ncomputational inefficiencies when faced with an increasing number of channels\nand modalities. To address these limitations, we introduce Low-rank Efficient\nSpatial-Spectral Vision Transformer with three key innovations: i) the LESS\nAttention Block that approximates high-dimensional spatial-spectral attention\nthrough Kronecker's product of the low-dimensional spatial and spectral\nattention components; ii) the Continuous Positional-Channel Embedding Layer\nthat preserves both the continuity and physical characteristics of each\nspatial-spectral patch; and iii) the Perception Field Mask that exploits local\nspatial dependencies by constraining attention to neighboring patches. To\nevaluate the proposed innovations, we construct GFM-Bench, which serves as a\ncomprehensive benchmark for such geospatial raster data. We pretrain LESS ViT\nusing a Hyperspectral Masked Autoencoder framework with integrated positional\nand channel masking strategies. Experimental results demonstrate that our\nproposed method achieves competitive performance against state-of-the-art\nmulti-modal geospatial foundation models while outperforming them on\ncross-satellite generalization tasks with higher computational efficiency. The\nflexibility and extensibility of our framework make it a promising direction\nfor future geospatial data analysis tasks that involve a wide range of\nmodalities and channels.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection",
    "url": "http://arxiv.org/abs/2503.09493v1",
    "authors": [
      "Romain Thoreau",
      "Valerio Marsocci",
      "Dawa Derksen"
    ],
    "published": "2025-03-12",
    "abstract": "As large-scale heterogeneous data sets become increasingly available,\nadapting foundation models at low cost has become a key issue. Seminal works in\nnatural language processing, e.g. Low-Rank Adaptation (LoRA), leverage the low\n\"intrinsic rank\" of parameter updates during adaptation. In this paper, we\nargue that incorporating stronger inductive biases in both data and models can\nenhance the adaptation of Geospatial Foundation Models (GFMs), pretrained on\nRGB satellite images, to other types of optical satellite data. Specifically,\nthe pretrained parameters of GFMs serve as a strong prior for the spatial\nstructure of multispectral images. For this reason, we introduce DEFLECT\n(Deflecting Embeddings for Finetuning Latent representations for Earth and\nClimate Tasks), a novel strategy for adapting GFMs to multispectral satellite\nimagery with very few additional parameters. DEFLECT improves the\nrepresentation capabilities of the extracted features, particularly enhancing\nspectral information, which is essential for geoscience and\nenvironmental-related tasks. We demonstrate the effectiveness of our method\nacross three different GFMs and five diverse datasets, ranging from forest\nmonitoring to marine environment segmentation. Compared to competing methods,\nDEFLECT achieves on-par or higher accuracy with 5-10$\\times$ fewer parameters\nfor classification and segmentation tasks. The code will be made publicly\navailable.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Lossy Neural Compression for Geospatial Analytics: A Review",
    "url": "http://arxiv.org/abs/2503.01505v1",
    "authors": [
      "Carlos Gomes",
      "Isabelle Wittmann",
      "Damien Robert",
      "Johannes Jakubik",
      "Tim Reichelt",
      "Michele Martone",
      "Stefano Maurogiovanni",
      "Rikard Vinge",
      "Jonas Hurst",
      "Erik Scheurer",
      "Rocco Sedona",
      "Thomas Brunschwiler",
      "Stefan Kesselheim",
      "Matej Batic",
      "Philip Stier",
      "Jan Dirk Wegner",
      "Gabriele Cavallaro",
      "Edzer Pebesma",
      "Michael Marszalek",
      "Miguel A Belenguer-Plomer",
      "Kennedy Adriko",
      "Paolo Fraccaro",
      "Romeo Kienzler",
      "Rania Briq",
      "Sabrina Benassou",
      "Michele Lazzarini",
      "Conrad M Albrecht"
    ],
    "published": "2025-03-03",
    "abstract": "Over the past decades, there has been an explosion in the amount of available\nEarth Observation (EO) data. The unprecedented coverage of the Earth's surface\nand atmosphere by satellite imagery has resulted in large volumes of data that\nmust be transmitted to ground stations, stored in data centers, and distributed\nto end users. Modern Earth System Models (ESMs) face similar challenges,\noperating at high spatial and temporal resolutions, producing petabytes of data\nper simulated day. Data compression has gained relevance over the past decade,\nwith neural compression (NC) emerging from deep learning and information\ntheory, making EO data and ESM outputs ideal candidates due to their abundance\nof unlabeled data. In this review, we outline recent developments in NC applied\nto geospatial data. We introduce the fundamental concepts of NC including\nseminal works in its traditional applications to image and video compression\ndomains with focus on lossy compression. We discuss the unique characteristics\nof EO and ESM data, contrasting them with \"natural images\", and explain the\nadditional challenges and opportunities they present. Moreover, we review\ncurrent applications of NC across various EO modalities and explore the limited\nefforts in ESM compression to date. The advent of self-supervised learning\n(SSL) and foundation models (FM) has advanced methods to efficiently distill\nrepresentations from vast unlabeled data. We connect these developments to NC\nfor EO, highlighting the similarities between the two fields and elaborate on\nthe potential of transferring compressed feature representations for\nmachine--to--machine communication. Based on insights drawn from this review,\nwe devise future directions relevant to applications in EO and ESM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SSL4EO-S12 v1.1: A Multimodal, Multiseasonal Dataset for Pretraining, Updated",
    "url": "http://arxiv.org/abs/2503.00168v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Nassim Ait Ali Braham",
      "Conrad M Albrecht",
      "Stefano Maurogiovanni",
      "Paolo Fraccaro"
    ],
    "published": "2025-02-28",
    "abstract": "This technical report presents SSL4EO-S12 v1.1, a multimodal, multitemporal\nEarth Observation dataset designed for pretraining large-scale foundation\nmodels. Building on the success of SSL4EO-S12 v1.0, the new version addresses\nthe previous challenges of data misalignment and a limited data structure for\nlow-barrier, analysis-ready EO processing. SSL4EO-S12 v1.1 covers the world's\n10,000 largest cities and its surroundings within a 50 km radius across four\nseasons, resulting in a diverse collection of nearly one million patches.\nSSL4EO-S12 v1.1 packages the data in Zarr file format for cloud-efficient\nloading and representation of meta-information such as including cloud masks\nand geolocation. Released under the CC-BY-4.0 license, SSL4EO-S12 v1.1\nfacilitates open research and provides a robust foundation for future\nadvancements in self-supervised learning and geospatial analysis. The dataset\nis available online through https://datapub.fz-juelich.de/ssl4eo-s12, and we\nprovided additional resources at https://github.com/DLR-MF-DAS/SSL4EO-S12-v1.1.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "How Does the Spatial Distribution of Pre-training Data Affect Geospatial Foundation Models?",
    "url": "http://arxiv.org/abs/2501.12535v1",
    "authors": [
      "Mirali Purohit",
      "Gedeon Muhawenayo",
      "Esther Rolf",
      "Hannah Kerner"
    ],
    "published": "2025-01-21",
    "abstract": "Foundation models have made rapid advances in many domains including Earth\nobservation, where Geospatial Foundation Models (GFMs) can help address global\nchallenges such as climate change, agriculture, and disaster response. Previous\nwork on GFMs focused on tailoring model architecture and pre-text tasks, and\ndid not investigate the impact of pre-training data selection on model\nperformance. However, recent works from other domains show that the\npre-training data distribution is an important factor influencing the\nperformance of the foundation models. With this motivation, our research\nexplores how the geographic distribution of pre-training data affects the\nperformance of GFMs. We evaluated several pre-training data distributions by\nsampling different compositions from a global data pool. Our experiments with\ntwo GFMs on downstream tasks indicate that balanced and globally representative\ndata compositions often outperform region-specific sampling, highlighting the\nimportance of diversity and global coverage in pre-training data. Our results\nsuggest that the most appropriate data sampling technique may depend on the\nspecific GFM architecture. These findings will support the development of\nrobust GFMs by incorporating quality pre-training data distributions,\nultimately improving machine learning solutions for Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2412.04204v2",
    "authors": [
      "Valerio Marsocci",
      "Yuru Jia",
      "Georges Le Bellier",
      "David Kerekes",
      "Liang Zeng",
      "Sebastian Hafner",
      "Sebastian Gerard",
      "Eric Brune",
      "Ritu Yadav",
      "Ali Shibli",
      "Heng Fang",
      "Yifang Ban",
      "Maarten Vergauwen",
      "Nicolas Audebert",
      "Andrea Nascetti"
    ],
    "published": "2024-12-05",
    "abstract": "Geospatial Foundation Models (GFMs) have emerged as powerful tools for\nextracting representations from Earth observation data, but their evaluation\nremains inconsistent and narrow. Existing works often evaluate on suboptimal\ndownstream datasets and tasks, that are often too easy or too narrow, limiting\nthe usefulness of the evaluations to assess the real-world applicability of\nGFMs. Additionally, there is a distinct lack of diversity in current evaluation\nprotocols, which fail to account for the multiplicity of image resolutions,\nsensor types, and temporalities, which further complicates the assessment of\nGFM performance. In particular, most existing benchmarks are geographically\nbiased towards North America and Europe, questioning the global applicability\nof GFMs. To overcome these challenges, we introduce PANGAEA, a standardized\nevaluation protocol that covers a diverse set of datasets, tasks, resolutions,\nsensor modalities, and temporalities. It establishes a robust and widely\napplicable benchmark for GFMs. We evaluate the most popular GFMs openly\navailable on this benchmark and analyze their performance across several\ndomains. In particular, we compare these models to supervised baselines (e.g.\nUNet and vanilla ViT), and assess their effectiveness when faced with limited\nlabeled data. Our findings highlight the limitations of GFMs, under different\nscenarios, showing that they do not consistently outperform supervised models.\nPANGAEA is designed to be highly extensible, allowing for the seamless\ninclusion of new datasets, models, and tasks in future research. By releasing\nthe evaluation code and benchmark, we aim to enable other researchers to\nreplicate our experiments and build upon our work, fostering a more principled\nevaluation protocol for large pre-trained geospatial models. The code is\navailable at https://github.com/VMarsocci/pangaea-bench.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "General Geospatial Inference with a Population Dynamics Foundation Model",
    "url": "http://arxiv.org/abs/2411.07207v4",
    "authors": [
      "Mohit Agarwal",
      "Mimi Sun",
      "Chaitanya Kamath",
      "Arbaaz Muslim",
      "Prithul Sarker",
      "Joydeep Paul",
      "Hector Yee",
      "Marcin Sieniek",
      "Kim Jablonski",
      "Yael Mayer",
      "David Fork",
      "Sheila de Guia",
      "Jamie McPike",
      "Adam Boulanger",
      "Tomer Shekel",
      "David Schottlander",
      "Yao Xiao",
      "Manjit Chakravarthy Manukonda",
      "Yun Liu",
      "Neslihan Bulut",
      "Sami Abu-el-haija",
      "Bryan Perozzi",
      "Monica Bharel",
      "Von Nguyen",
      "Luke Barrington",
      "Niv Efron",
      "Yossi Matias",
      "Greg Corrado",
      "Krish Eswaran",
      "Shruthi Prabhakara",
      "Shravya Shetty",
      "Gautam Prasad"
    ],
    "published": "2024-11-11",
    "abstract": "Supporting the health and well-being of dynamic populations around the world\nrequires governmental agencies, organizations and researchers to understand and\nreason over complex relationships between human behavior and local contexts in\norder to identify high-risk groups and strategically allocate limited\nresources. Traditional approaches to these classes of problems often entail\ndeveloping manually curated, task-specific features and models to represent\nhuman behavior and the natural and built environment, which can be challenging\nto adapt to new, or even, related tasks. To address this, we introduce a\nPopulation Dynamics Foundation Model (PDFM) that aims to capture the\nrelationships between diverse data modalities and is applicable to a broad\nrange of geospatial tasks. We first construct a geo-indexed dataset for postal\ncodes and counties across the United States, capturing rich aggregated\ninformation on human behavior from maps, busyness, and aggregated search\ntrends, and environmental factors such as weather and air quality. We then\nmodel this data and the complex relationships between locations using a graph\nneural network, producing embeddings that can be adapted to a wide range of\ndownstream tasks using relatively simple models. We evaluate the effectiveness\nof our approach by benchmarking it on 27 downstream tasks spanning three\ndistinct domains: health indicators, socioeconomic factors, and environmental\nmeasurements. The approach achieves state-of-the-art performance on all 27\ngeospatial interpolation tasks, and on 25 out of the 27 extrapolation and\nsuper-resolution tasks. We combined the PDFM with a state-of-the-art\nforecasting foundation model, TimesFM, to predict unemployment and poverty,\nachieving performance that surpasses fully supervised forecasting. The full set\nof embeddings and sample code are publicly available for researchers.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "MapSAM: Adapting Segment Anything Model for Automated Feature Detection in Historical Maps",
    "url": "http://arxiv.org/abs/2411.06971v1",
    "authors": [
      "Xue Xia",
      "Daiwei Zhang",
      "Wenxuan Song",
      "Wei Huang",
      "Lorenz Hurni"
    ],
    "published": "2024-11-11",
    "abstract": "Automated feature detection in historical maps can significantly accelerate\nthe reconstruction of the geospatial past. However, this process is often\nconstrained by the time-consuming task of manually digitizing sufficient\nhigh-quality training data. The emergence of visual foundation models, such as\nthe Segment Anything Model (SAM), offers a promising solution due to their\nremarkable generalization capabilities and rapid adaptation to new data\ndistributions. Despite this, directly applying SAM in a zero-shot manner to\nhistorical map segmentation poses significant challenges, including poor\nrecognition of certain geospatial features and a reliance on input prompts,\nwhich limits its ability to be fully automated. To address these challenges, we\nintroduce MapSAM, a parameter-efficient fine-tuning strategy that adapts SAM\ninto a prompt-free and versatile solution for various downstream historical map\nsegmentation tasks. Specifically, we employ Weight-Decomposed Low-Rank\nAdaptation (DoRA) to integrate domain-specific knowledge into the image\nencoder. Additionally, we develop an automatic prompt generation process,\neliminating the need for manual input. We further enhance the positional prompt\nin SAM, transforming it into a higher-level positional-semantic prompt, and\nmodify the cross-attention mechanism in the mask decoder with masked attention\nfor more effective feature aggregation. The proposed MapSAM framework\ndemonstrates promising performance across two distinct historical map\nsegmentation tasks: one focused on linear features and the other on areal\nfeatures. Experimental results show that it adapts well to various features,\neven when fine-tuned with extremely limited data (e.g. 10 shots).",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Multimodal Contrastive Learning of Urban Space Representations from POI Data",
    "url": "http://arxiv.org/abs/2411.06229v1",
    "authors": [
      "Xinglei Wang",
      "Tao Cheng",
      "Stephen Law",
      "Zichao Zeng",
      "Lu Yin",
      "Junyuan Liu"
    ],
    "published": "2024-11-09",
    "abstract": "Existing methods for learning urban space representations from\nPoint-of-Interest (POI) data face several limitations, including issues with\ngeographical delineation, inadequate spatial information modelling,\nunderutilisation of POI semantic attributes, and computational inefficiencies.\nTo address these issues, we propose CaLLiPer (Contrastive Language-Location\nPre-training), a novel representation learning model that directly embeds\ncontinuous urban spaces into vector representations that can capture the\nspatial and semantic distribution of urban environment. This model leverages a\nmultimodal contrastive learning objective, aligning location embeddings with\ntextual POI descriptions, thereby bypassing the need for complex training\ncorpus construction and negative sampling. We validate CaLLiPer's effectiveness\nby applying it to learning urban space representations in London, UK, where it\ndemonstrates 5-15% improvement in predictive performance for land use\nclassification and socioeconomic mapping tasks compared to state-of-the-art\nmethods. Visualisations of the learned representations further illustrate our\nmodel's advantages in capturing spatial variations in urban semantics with high\naccuracy and fine resolution. Additionally, CaLLiPer achieves reduced training\ntime, showcasing its efficiency and scalability. This work provides a promising\npathway for scalable, semantically rich urban space representation learning\nthat can support the development of geospatial foundation models. The\nimplementation code is available at https://github.com/xlwang233/CaLLiPer.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "On the Generalizability of Foundation Models for Crop Type Mapping",
    "url": "http://arxiv.org/abs/2409.09451v3",
    "authors": [
      "Yi-Chia Chang",
      "Adam J. Stewart",
      "Favyen Bastani",
      "Piper Wolters",
      "Shreya Kannan",
      "George R. Huber",
      "Jingtong Wang",
      "Arindam Banerjee"
    ],
    "published": "2024-09-14",
    "abstract": "Foundation models pre-trained using self-supervised learning have shown\npowerful transfer learning capabilities on various downstream tasks, including\nlanguage understanding, text generation, and image recognition. The Earth\nobservation (EO) field has produced several foundation models pre-trained\ndirectly on multispectral satellite imagery for applications like precision\nagriculture, wildfire and drought monitoring, and natural disaster response.\nHowever, few studies have investigated the ability of these models to\ngeneralize to new geographic locations, and potential concerns of geospatial\nbias -- models trained on data-rich developed nations not transferring well to\ndata-scarce developing nations -- remain. We investigate the ability of popular\nEO foundation models to transfer to new geographic regions in the agricultural\ndomain, where differences in farming practices and class imbalance make\ntransfer learning particularly challenging. We first select five crop\nclassification datasets across five continents, normalizing for dataset size\nand harmonizing classes to focus on four major cereal grains: maize, soybean,\nrice, and wheat. We then compare three popular foundation models, pre-trained\non SSL4EO-S12, SatlasPretrain, and ImageNet, using in-distribution (ID) and\nout-of-distribution (OOD) evaluation. Experiments show that pre-trained weights\ndesigned explicitly for Sentinel-2, such as SSL4EO-S12, outperform general\npre-trained weights like ImageNet. Furthermore, while only 100 labeled images\nare sufficient for achieving high overall accuracy, 900 images are required to\nachieve high average accuracy due to class imbalance. All harmonized datasets\nand experimental code are open-source and available for download.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Evaluating the Effectiveness of Large Language Models in Representing and Understanding Movement Trajectories",
    "url": "http://arxiv.org/abs/2409.00335v1",
    "authors": [
      "Yuhan Ji",
      "Song Gao"
    ],
    "published": "2024-08-31",
    "abstract": "This research focuses on assessing the ability of AI foundation models in\nrepresenting the trajectories of movements. We utilize one of the large\nlanguage models (LLMs) (i.e., GPT-J) to encode the string format of\ntrajectories and then evaluate the effectiveness of the LLM-based\nrepresentation for trajectory data analysis. The experiments demonstrate that\nwhile the LLM-based embeddings can preserve certain trajectory distance metrics\n(i.e., the correlation coefficients exceed 0.74 between the Cosine distance\nderived from GPT-J embeddings and the Hausdorff and Dynamic Time Warping\ndistances on raw trajectories), challenges remain in restoring numeric values\nand retrieving spatial neighbors in movement trajectory analytics. In addition,\nthe LLMs can understand the spatiotemporal dependency contained in trajectories\nand have good accuracy in location prediction tasks. This research highlights\nthe need for improvement in terms of capturing the nuances and complexities of\nthe underlying geospatial data and integrating domain knowledge to support\nvarious GeoAI applications using LLMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Self-Supervised Representation Learning for Geospatial Objects: A Survey",
    "url": "http://arxiv.org/abs/2408.12133v2",
    "authors": [
      "Yile Chen",
      "Weiming Huang",
      "Kaiqi Zhao",
      "Yue Jiang",
      "Gao Cong"
    ],
    "published": "2024-08-22",
    "abstract": "The proliferation of various data sources in urban and territorial\nenvironments has significantly facilitated the development of geospatial\nartificial intelligence (GeoAI) across a wide range of geospatial applications.\nHowever, geospatial data, which is inherently linked to geospatial objects,\noften exhibits data heterogeneity that necessitates specialized fusion and\nrepresentation strategies while simultaneously being inherently sparse in\nlabels for downstream tasks. Consequently, there is a growing demand for\ntechniques that can effectively leverage geospatial data without heavy reliance\non task-specific labels and model designs. This need aligns with the principles\nof self-supervised learning (SSL), which has garnered increasing attention for\nits ability to learn effective and generalizable representations directly from\ndata without extensive labeled supervision. This paper presents a comprehensive\nand up-to-date survey of SSL techniques specifically applied to or developed\nfor geospatial objects in three primary vector geometric types: Point,\nPolyline, and Polygon. We systematically categorize various SSL techniques into\npredictive and contrastive methods, and analyze their adaptation to different\ndata types for representation learning across various downstream tasks.\nFurthermore, we examine the emerging trends in SSL for geospatial objects,\nparticularly the gradual advancements towards geospatial foundation models.\nFinally, we discuss key challenges in current research and outline promising\ndirections for future investigation. By offering a structured analysis of\nexisting studies, this paper aims to inspire continued progress in integrating\nSSL with geospatial objects, and the development of geospatial foundation\nmodels in a longer term.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Fine-tuning of Geospatial Foundation Models for Aboveground Biomass Estimation",
    "url": "http://arxiv.org/abs/2406.19888v1",
    "authors": [
      "Michal Muszynski",
      "Levente Klein",
      "Ademir Ferreira da Silva",
      "Anjani Prasad Atluri",
      "Carlos Gomes",
      "Daniela Szwarcman",
      "Gurkanwar Singh",
      "Kewen Gu",
      "Maciel Zortea",
      "Naomi Simumba",
      "Paolo Fraccaro",
      "Shraddha Singh",
      "Steve Meliksetian",
      "Campbell Watson",
      "Daiki Kimura",
      "Harini Srinivasan"
    ],
    "published": "2024-06-28",
    "abstract": "Global vegetation structure mapping is critical for understanding the global\ncarbon cycle and maximizing the efficacy of nature-based carbon sequestration\ninitiatives. Moreover, vegetation structure mapping can help reduce the impacts\nof climate change by, for example, guiding actions to improve water security,\nincrease biodiversity and reduce flood risk. Global satellite measurements\nprovide an important set of observations for monitoring and managing\ndeforestation and degradation of existing forests, natural forest regeneration,\nreforestation, biodiversity restoration, and the implementation of sustainable\nagricultural practices. In this paper, we explore the effectiveness of\nfine-tuning of a geospatial foundation model to estimate above-ground biomass\n(AGB) using space-borne data collected across different eco-regions in Brazil.\nThe fine-tuned model architecture consisted of a Swin-B transformer as the\nencoder (i.e., backbone) and a single convolutional layer for the decoder head.\nAll results were compared to a U-Net which was trained as the baseline model\nExperimental results of this sparse-label prediction task demonstrate that the\nfine-tuned geospatial foundation model with a frozen encoder has comparable\nperformance to a U-Net trained from scratch. This is despite the fine-tuned\nmodel having 13 times less parameters requiring optimization, which saves both\ntime and compute resources. Further, we explore the transfer-learning\ncapabilities of the geospatial foundation models by fine-tuning on satellite\nimagery with sparse labels from different eco-regions in Brazil.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Geode: A Zero-shot Geospatial Question-Answering Agent with Explicit Reasoning and Precise Spatio-Temporal Retrieval",
    "url": "http://arxiv.org/abs/2407.11014v1",
    "authors": [
      "Devashish Vikas Gupta",
      "Azeez Syed Ali Ishaqui",
      "Divya Kiran Kadiyala"
    ],
    "published": "2024-06-26",
    "abstract": "Large language models (LLMs) have shown promising results in learning and\ncontextualizing information from different forms of data. Recent advancements\nin foundational models, particularly those employing self-attention mechanisms,\nhave significantly enhanced our ability to comprehend the semantics of diverse\ndata types. One such area that could highly benefit from multi-modality is in\nunderstanding geospatial data, which inherently has multiple modalities.\nHowever, current Natural Language Processing (NLP) mechanisms struggle to\neffectively address geospatial queries. Existing pre-trained LLMs are\ninadequately equipped to meet the unique demands of geospatial data, lacking\nthe ability to retrieve precise spatio-temporal data in real-time, thus leading\nto significantly reduced accuracy in answering complex geospatial queries. To\naddress these limitations, we introduce Geode--a pioneering system designed to\ntackle zero-shot geospatial question-answering tasks with high precision using\nspatio-temporal data retrieval. Our approach represents a significant\nimprovement in addressing the limitations of current LLM models, demonstrating\nremarkable improvement in geospatial question-answering abilities compared to\nexisting state-of-the-art pre-trained models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GFM4MPM: Towards Geospatial Foundation Models for Mineral Prospectivity Mapping",
    "url": "http://arxiv.org/abs/2406.12756v1",
    "authors": [
      "Angel Daruna",
      "Vasily Zadorozhnyy",
      "Georgina Lukoczki",
      "Han-Pang Chiu"
    ],
    "published": "2024-06-18",
    "abstract": "Machine Learning (ML) for Mineral Prospectivity Mapping (MPM) remains a\nchallenging problem as it requires the analysis of associations between\nlarge-scale multi-modal geospatial data and few historical mineral commodity\nobservations (positive labels). Recent MPM works have explored Deep Learning\n(DL) as a modeling tool with more representation capacity. However, these\noverparameterized methods may be more prone to overfitting due to their\nreliance on scarce labeled data. While a large quantity of unlabeled geospatial\ndata exists, no prior MPM works have considered using such information in a\nself-supervised manner. Our MPM approach uses a masked image modeling framework\nto pretrain a backbone neural network in a self-supervised manner using\nunlabeled geospatial data alone. After pretraining, the backbone network\nprovides feature extraction for downstream MPM tasks. We evaluated our approach\nalongside existing methods to assess mineral prospectivity of Mississippi\nValley Type (MVT) and Clastic-Dominated (CD) Lead-Zinc deposits in North\nAmerica and Australia. Our results demonstrate that self-supervision promotes\nrobustness in learned features, improving prospectivity predictions.\nAdditionally, we leverage explainable artificial intelligence techniques to\ndemonstrate that individual predictions can be interpreted from a geological\nperspective.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Towards Vision-Language Geo-Foundation Model: A Survey",
    "url": "http://arxiv.org/abs/2406.09385v1",
    "authors": [
      "Yue Zhou",
      "Litong Feng",
      "Yiping Ke",
      "Xue Jiang",
      "Junchi Yan",
      "Xue Yang",
      "Wayne Zhang"
    ],
    "published": "2024-06-13",
    "abstract": "Vision-Language Foundation Models (VLFMs) have made remarkable progress on\nvarious multimodal tasks, such as image captioning, image-text retrieval,\nvisual question answering, and visual grounding. However, most methods rely on\ntraining with general image datasets, and the lack of geospatial data leads to\npoor performance on earth observation. Numerous geospatial image-text pair\ndatasets and VLFMs fine-tuned on them have been proposed recently. These new\napproaches aim to leverage large-scale, multimodal geospatial data to build\nversatile intelligent models with diverse geo-perceptive capabilities, which we\nrefer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper\nthoroughly reviews VLGFMs, summarizing and analyzing recent developments in the\nfield. In particular, we introduce the background and motivation behind the\nrise of VLGFMs, highlighting their unique research significance. Then, we\nsystematically summarize the core technologies employed in VLGFMs, including\ndata construction, model architectures, and applications of various multimodal\ngeospatial tasks. Finally, we conclude with insights, issues, and discussions\nregarding future research directions. To the best of our knowledge, this is the\nfirst comprehensive literature review of VLGFMs. We keep tracing related works\nat https://github.com/zytx121/Awesome-VLGFM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SeeFar: Satellite Agnostic Multi-Resolution Dataset for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2406.06776v1",
    "authors": [
      "James Lowman",
      "Kelly Liu Zheng",
      "Roydon Fraser",
      "Jesse Van Griensven The",
      "Mojtaba Valipour"
    ],
    "published": "2024-06-10",
    "abstract": "SeeFar is an evolving collection of multi-resolution satellite images from\npublic and commercial satellites. We specifically curated this dataset for\ntraining geospatial foundation models, unconstrained by satellite type. In\nrecent years, advances in technology have made satellite imagery more\naccessible than ever. More earth-observing satellites have been launched in the\nlast five years than in the previous fifty. Modern commercial satellites now\noffer up to 100 times the spatial resolution of public access satellites.\nHowever, the high cost and limited historical availability of commercial\nsatellite imagery is a barrier to the training of foundational models,\nimpacting what images can be used during inference. The SeeFar dataset\nrepresents a step towards training models that are satellite-agnostic by\ncombining multi-resolution commercial and public access pre-processed images.\nThis will enable users to utilize historical data alongside higher-resolution,\nmore expensive satellite imagery, offering greater flexibility during\ninference. To achieve this, we describe a process for standardizing data from\ndiverse satellite sources, normalizing different data formats, and aligning\nspectral bands to enhance interoperability. The SeeFar dataset includes images\nat a resolution of 384x384 pixels, spanning four spectral bands (Blue, Green,\nRed, and Near-Infrared) and expanding spatial resolutions (starting with 30,\n10, 1.5, and 1.0 meters), all in cloud-optimized GeoTIFF format. It also\nprovides consistent and comprehensive metadata to enhance data transparency and\nreliability. By aggregating data from multiple sources, SeeFar makes processed\nand consistent satellite data accessible to a wider range of users - from\nresearchers to policymakers - fostering competition and innovation in satellite\nimagery analysis. The dataset is available at \\url{coastalcarbon.ai/seefar}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery",
    "url": "http://arxiv.org/abs/2405.02512v2",
    "authors": [
      "Yohei Nakayama",
      "Jiawei Su",
      "Luis M. Pazos-Out\u00f3n"
    ],
    "published": "2024-05-03",
    "abstract": "Recent advancements in foundation models have significantly impacted various\nfields, including natural language processing, computer vision, and multi-modal\ntasks. One area that stands to benefit greatly is Earth observation, where\nthese models can efficiently process large-scale, unlabeled geospatial data. In\nthis work we extend the SwinMAE model to integrate temporal information for\nsatellite time-series data. The architecture employs a hierarchical 3D Masked\nAutoencoder (MAE) with Video Swin Transformer blocks to effectively capture\nmulti-scale spatio-temporal dependencies in satellite imagery. To enhance\ntransfer learning, we incorporate both encoder and decoder pretrained weights,\nalong with skip connections to preserve scale-specific information. This forms\nan architecture similar to SwinUNet with an additional temporal component. Our\napproach shows significant performance improvements over existing\nstate-of-the-art foundation models for all the evaluated downstream tasks: land\ncover segmentation, building density prediction, flood mapping, wildfire scar\nmapping and multi-temporal crop segmentation. Particularly, in the land cover\nsegmentation task of the PhilEO Bench dataset, it outperforms other geospatial\nfoundation models with a 10.4% higher accuracy.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "NLP-enabled Trajectory Map-matching in Urban Road Networks using a Transformer-based Encoder-decoder",
    "url": "http://arxiv.org/abs/2404.12460v4",
    "authors": [
      "Sevin Mohammadi",
      "Andrew W. Smyth"
    ],
    "published": "2024-04-18",
    "abstract": "Vehicular trajectory data from geolocation telematics is vital for analyzing\nurban mobility patterns. Map-matching aligns noisy, sparsely sampled GPS\ntrajectories with digital road maps to reconstruct accurate vehicle paths.\nTraditional methods rely on geometric proximity, topology, and shortest-path\nheuristics, but they overlook two key factors: (1) drivers may prefer routes\nbased on local road characteristics rather than shortest paths, revealing\nlearnable shared preferences, and (2) GPS noise varies spatially due to\nmultipath effects. These factors can reduce the effectiveness of conventional\nmethods in complex scenarios and increase the effort required for\nheuristic-based implementations. This study introduces a data-driven, deep\nlearning-based map-matching framework, formulating the task as machine\ntranslation, inspired by NLP. Specifically, a transformer-based encoder-decoder\nmodel learns contextual representations of noisy GPS points to infer trajectory\nbehavior and road structures in an end-to-end manner. Trained on large-scale\ntrajectory data, the method improves path estimation accuracy. Experiments on\nsynthetic trajectories show that this approach outperforms conventional methods\nby integrating contextual awareness. Evaluation on real-world GPS traces from\nManhattan, New York, achieves 75% accuracy in reconstructing navigated routes.\nThese results highlight the effectiveness of transformers in capturing drivers'\ntrajectory behaviors, spatial dependencies, and noise patterns, offering a\nscalable, robust solution for map-matching. This work contributes to advancing\ntrajectory-driven foundation models for geospatial modeling and urban mobility\napplications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Pretraining Billion-scale Geospatial Foundational Models on Frontier",
    "url": "http://arxiv.org/abs/2404.11706v1",
    "authors": [
      "Aristeidis Tsaris",
      "Philipe Ambrozio Dias",
      "Abhishek Potnis",
      "Junqi Yin",
      "Feiyi Wang",
      "Dalton Lunga"
    ],
    "published": "2024-04-17",
    "abstract": "As AI workloads increase in scope, generalization capability becomes\nchallenging for small task-specific models and their demand for large amounts\nof labeled training samples increases. On the contrary, Foundation Models (FMs)\nare trained with internet-scale unlabeled data via self-supervised learning and\nhave been shown to adapt to various tasks with minimal fine-tuning. Although\nlarge FMs have demonstrated significant impact in natural language processing\nand computer vision, efforts toward FMs for geospatial applications have been\nrestricted to smaller size models, as pretraining larger models requires very\nlarge computing resources equipped with state-of-the-art hardware accelerators.\nCurrent satellite constellations collect 100+TBs of data a day, resulting in\nimages that are billions of pixels and multimodal in nature. Such geospatial\ndata poses unique challenges opening up new opportunities to develop FMs. We\ninvestigate billion scale FMs and HPC training profiles for geospatial\napplications by pretraining on publicly available data. We studied from\nend-to-end the performance and impact in the solution by scaling the model\nsize. Our larger 3B parameter size model achieves up to 30% improvement in top1\nscene classification accuracy when comparing a 100M parameter model. Moreover,\nwe detail performance experiments on the Frontier supercomputer, America's\nfirst exascale system, where we study different model and data parallel\napproaches using PyTorch's Fully Sharded Data Parallel library. Specifically,\nwe study variants of the Vision Transformer architecture (ViT), conducting\nperformance analysis for ViT models with size up to 15B parameters. By\ndiscussing throughput and performance bottlenecks under different parallelism\nconfigurations, we offer insights on how to leverage such leadership-class HPC\nresources when developing large models for geospatial imagery applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2404.01260v1",
    "authors": [
      "Boran Han",
      "Shuai Zhang",
      "Xingjian Shi",
      "Markus Reichstein"
    ],
    "published": "2024-04-01",
    "abstract": "In the realm of geospatial analysis, the diversity of remote sensors,\nencompassing both optical and microwave technologies, offers a wealth of\ndistinct observational capabilities. Recognizing this, we present msGFM, a\nmultisensor geospatial foundation model that effectively unifies data from four\nkey sensor modalities. This integration spans an expansive dataset of two\nmillion multisensor images. msGFM is uniquely adept at handling both paired and\nunpaired sensor data. For data originating from identical geolocations, our\nmodel employs an innovative cross-sensor pretraining approach in masked image\nmodeling, enabling the synthesis of joint representations from diverse sensors.\nmsGFM, incorporating four remote sensors, upholds strong performance, forming a\ncomprehensive model adaptable to various sensor types. msGFM has demonstrated\nenhanced proficiency in a range of both single-sensor and multisensor\ndownstream tasks. These include scene classification, segmentation, cloud\nremoval, and pan-sharpening. A key discovery of our research is that\nrepresentations derived from natural images are not always compatible with the\ndistinct characteristics of geospatial remote sensors, underscoring the\nlimitations of existing representations in this field. Our work can serve as a\nguide for developing multisensor geospatial pretraining models, paving the way\nfor more advanced geospatial capabilities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Large Language Models are Geographically Biased",
    "url": "http://arxiv.org/abs/2402.02680v2",
    "authors": [
      "Rohin Manvi",
      "Samar Khanna",
      "Marshall Burke",
      "David Lobell",
      "Stefano Ermon"
    ],
    "published": "2024-02-05",
    "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their\ntraining corpora, which can lead to the perpetuation of societal harm. As the\nimpact of these foundation models grows, understanding and evaluating their\nbiases becomes crucial to achieving fairness and accuracy. We propose to study\nwhat LLMs know about the world we live in through the lens of geography. This\napproach is particularly powerful as there is ground truth for the numerous\naspects of human life that are meaningfully projected onto geographic space\nsuch as culture, race, language, politics, and religion. We show various\nproblematic geographic biases, which we define as systemic errors in geospatial\npredictions. Initially, we demonstrate that LLMs are capable of making accurate\nzero-shot geospatial predictions in the form of ratings that show strong\nmonotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We\nthen show that LLMs exhibit common biases across a range of objective and\nsubjective topics. In particular, LLMs are clearly biased against locations\nwith lower socioeconomic conditions (e.g. most of Africa) on a variety of\nsensitive subjective topics such as attractiveness, morality, and intelligence\n(Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to\nquantify this and find that there is significant variation in the magnitude of\nbias across existing LLMs. Code is available on the project website:\nhttps://rohinmanvi.github.io/GeoLLM",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping",
    "url": "http://arxiv.org/abs/2401.08787v1",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Sizhe Wang",
      "Yezhou Yang",
      "Hyunho Lee",
      "Anna Liljedahl",
      "Chandi Witharana",
      "Yili Yang",
      "Brendan M. Rogers",
      "Samantha T. Arundel",
      "Matthew B. Jones",
      "Kenton McHenry",
      "Patricia Solis"
    ],
    "published": "2024-01-16",
    "abstract": "This paper assesses trending AI foundation models, especially emerging\ncomputer vision foundation models and their performance in natural landscape\nfeature segmentation. While the term foundation model has quickly garnered\ninterest from the geospatial domain, its definition remains vague. Hence, this\npaper will first introduce AI foundation models and their defining\ncharacteristics. Built upon the tremendous success achieved by Large Language\nModels (LLMs) as the foundation models for language tasks, this paper discusses\nthe challenges of building foundation models for geospatial artificial\nintelligence (GeoAI) vision tasks. To evaluate the performance of large AI\nvision models, especially Meta's Segment Anything Model (SAM), we implemented\ndifferent instance segmentation pipelines that minimize the changes to SAM to\nleverage its power as a foundation model. A series of prompt strategies was\ndeveloped to test SAM's performance regarding its theoretical upper bound of\npredictive accuracy, zero-shot performance, and domain adaptability through\nfine-tuning. The analysis used two permafrost feature datasets, ice-wedge\npolygons and retrogressive thaw slumps because (1) these landform features are\nmore challenging to segment than manmade features due to their complicated\nformation mechanisms, diverse forms, and vague boundaries; (2) their presence\nand changes are important indicators for Arctic warming and climate change. The\nresults show that although promising, SAM still has room for improvement to\nsupport AI-augmented terrain mapping. The spatial and domain generalizability\nof this finding is further validated using a more general dataset EuroCrop for\nagricultural field mapping. Finally, we discuss future research directions that\nstrengthen SAM's applicability in challenging geospatial domains.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies",
    "url": "http://arxiv.org/abs/2504.20203v1",
    "authors": [
      "Vladyslav Polushko",
      "Damjan Hatic",
      "Ronald R\u00f6sch",
      "Thomas M\u00e4rz",
      "Markus Rauhut",
      "Andreas Weinmann"
    ],
    "published": "2025-04-28",
    "abstract": "Floods cause serious problems around the world. Responding quickly and\neffectively requires accurate and timely information about the affected areas.\nThe effective use of Remote Sensing images for accurate flood detection\nrequires specific detection methods. Typically, Deep Neural Networks are\nemployed, which are trained on specific datasets. For the purpose of river\nflood detection in RGB imagery, we use the BlessemFlood21 dataset. We here\nexplore the use of different augmentation strategies, ranging from basic\napproaches to more complex techniques, including optical distortion. By\nidentifying effective strategies, we aim to refine the training process of\nstate-of-the-art Deep Learning segmentation networks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2504.19598v1",
    "authors": [
      "Dou Quan",
      "Rufan Zhou",
      "Shuang Wang",
      "Ning Huyan",
      "Dong Zhao",
      "Yunan Li",
      "Licheng Jiao"
    ],
    "published": "2025-04-28",
    "abstract": "Deep learning methods have shown promising performances in remote sensing\nimage change detection (CD). However, existing methods usually train a\ndataset-specific deep network for each dataset. Due to the significant\ndifferences in the data distribution and labeling between various datasets, the\ntrained dataset-specific deep network has poor generalization performances on\nother datasets. To solve this problem, this paper proposes a change adapter\nnetwork (CANet) for a more universal and generalized CD. CANet contains\ndataset-shared and dataset-specific learning modules. The former explores the\ndiscriminative features of images, and the latter designs a lightweight adapter\nmodel, to deal with the characteristics of different datasets in data\ndistribution and labeling. The lightweight adapter can quickly generalize the\ndeep network for new CD tasks with a small computation cost. Specifically, this\npaper proposes an interesting change region mask (ICM) in the adapter, which\ncan adaptively focus on interested change objects and decrease the influence of\nlabeling differences in various datasets. Moreover, CANet adopts a unique batch\nnormalization layer for each dataset to deal with data distribution\ndifferences. Compared with existing deep learning methods, CANet can achieve\nsatisfactory CD performances on various datasets simultaneously. Experimental\nresults on several public datasets have verified the effectiveness and\nadvantages of the proposed CANet on CD. CANet has a stronger generalization\nability, smaller training costs (merely updating 4.1%-7.7% parameters), and\nbetter performances under limited training datasets than other deep learning\nmethods, which also can be flexibly inserted with existing deep models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Photon Absorption Remote Sensing Virtual Histopathology: Diagnostic Equivalence to Gold-Standard H&E Staining in Skin Cancer Excisional Biopsies",
    "url": "http://arxiv.org/abs/2504.18737v1",
    "authors": [
      "Benjamin R. Ecclestone",
      "James E. D. Tweel",
      "Marie Abi Daoud",
      "Hager Gaouda",
      "Deepak Dinakaran",
      "Michael P. Wallace",
      "Ally Khan Somani",
      "Gilbert Bigras",
      "John R. Mackey",
      "Parsin Haji Reza"
    ],
    "published": "2025-04-25",
    "abstract": "Photon Absorption Remote Sensing (PARS) enables label-free imaging of\nsubcellular morphology by observing biomolecule specific absorption\ninteractions. Coupled with deep-learning, PARS produces label-free virtual\nHematoxylin and Eosin (H&E) stained images in unprocessed tissues. This study\nevaluates the diagnostic performance of these PARS-derived virtual H&E images\nin benign and malignant excisional skin biopsies, including Squamous (SCC),\nBasal (BCC) Cell Carcinoma, and normal skin. Sixteen unstained formalin-fixed\nparaffin-embedded skin excisions were PARS imaged, virtually H&E stained, then\nchemically stained and imaged at 40x. Seven fellowship trained\ndermatopathologists assessed all 32 images in a masked randomized fashion.\nConcordance analysis indicates 95.5% agreement between primary diagnoses\nrendered on PARS versus H&E images (Cohen's k=0.93). Inter-rater reliability\nwas near-perfect for both image types (Fleiss' k=0.89 for PARS, k=0.80 for\nH&E). For subtype classification, agreement was near-perfect 91% (k=0.73) for\nSCC and was perfect for BCC. When assessing malignancy confinement (e.g.,\ncancer margins), agreement was 92% between PARS and H&E (k=0.718). During\nassessment dermatopathologists could not reliably distinguish image origin\n(PARS vs. H&E), and diagnostic confidence was equivalent between the\nmodalities. Inter-rater reliability for PARS virtual H&E was consistent with\nreported benchmarks for histologic evaluation. These results indicate that PARS\nvirtual histology may be diagnostically equivalent to traditional H&E staining\nin dermatopathology diagnostics, while enabling assessment directly from\nunlabeled, or unprocessed slides. In turn, the label-free PARS virtual H&E\nimaging workflow may preserve tissue for downstream analysis while producing\ndata well-suited for AI integration potentially accelerating and enhancing the\naccuracy of skin cancer diagnostics.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition",
    "url": "http://arxiv.org/abs/2504.16467v1",
    "authors": [
      "Qishan He",
      "Lingjun Zhao",
      "Ru Luo",
      "Siqian Zhang",
      "Lin Lei",
      "Kefeng Ji",
      "Gangyao Kuang"
    ],
    "published": "2025-04-23",
    "abstract": "Aircraft recognition in synthetic aperture radar (SAR) imagery is a\nfundamental mission in both military and civilian applications. Recently deep\nlearning (DL) has emerged a dominant paradigm for its explosive performance on\nextracting discriminative features. However, current classification algorithms\nfocus primarily on learning decision hyperplane without enough comprehension on\naircraft structural knowledge. Inspired by the fined aircraft annotation\nmethods for optical remote sensing images (RSI), we first introduce a\nstructure-based SAR aircraft annotations approach to provide structural and\ncompositional supplement information. On this basis, we propose a multi-task\nstructure guided learning (MTSGL) network for robust and interpretable SAR\naircraft recognition. Besides the classification task, MTSGL includes a\nstructural semantic awareness (SSA) module and a structural consistency\nregularization (SCR) module. The SSA is designed to capture structure semantic\ninformation, which is conducive to gain human-like comprehension of aircraft\nknowledge. The SCR helps maintain the geometric consistency between the\naircraft structure in SAR imagery and the proposed annotation. In this process,\nthe structural attribute can be disentangled in a geometrically meaningful\nmanner. In conclusion, the MTSGL is presented with the expert-level aircraft\nprior knowledge and structure guided learning paradigm, aiming to comprehend\nthe aircraft concept in a way analogous to the human cognitive process.\nExtensive experiments are conducted on a self-constructed multi-task SAR\naircraft recognition dataset (MT-SARD) and the effective results illustrate the\nsuperiority of robustness and interpretation ability of the proposed MTSGL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "DAM-Net: Domain Adaptation Network with Micro-Labeled Fine-Tuning for Change Detection",
    "url": "http://arxiv.org/abs/2504.13748v1",
    "authors": [
      "Hongjia Chen",
      "Xin Xu",
      "Fangling Pu"
    ],
    "published": "2025-04-18",
    "abstract": "Change detection (CD) in remote sensing imagery plays a crucial role in\nvarious applications such as urban planning, damage assessment, and resource\nmanagement. While deep learning approaches have significantly advanced CD\nperformance, current methods suffer from poor domain adaptability, requiring\nextensive labeled data for retraining when applied to new scenarios. This\nlimitation severely restricts their practical applications across different\ndatasets. In this work, we propose DAM-Net: a Domain Adaptation Network with\nMicro-Labeled Fine-Tuning for CD. Our network introduces adversarial domain\nadaptation to CD for, utilizing a specially designed segmentation-discriminator\nand alternating training strategy to enable effective transfer between domains.\nAdditionally, we propose a novel Micro-Labeled Fine-Tuning approach that\nstrategically selects and labels a minimal amount of samples (less than 1%) to\nenhance domain adaptation. The network incorporates a Multi-Temporal\nTransformer for feature fusion and optimized backbone structure based on\nprevious research. Experiments conducted on the LEVIR-CD and WHU-CD datasets\ndemonstrate that DAM-Net significantly outperforms existing domain adaptation\nmethods, achieving comparable performance to semi-supervised approaches that\nrequire 10% labeled data while using only 0.3% labeled samples. Our approach\nsignificantly advances cross-dataset CD applications and provides a new\nparadigm for efficient domain adaptation in remote sensing. The source code of\nDAM-Net will be made publicly available upon publication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation",
    "url": "http://arxiv.org/abs/2504.11949v1",
    "authors": [
      "Jie Wang",
      "Chen Ye Gan",
      "Caoqi Wei",
      "Jiangtao Wen",
      "Yuxing Han"
    ],
    "published": "2025-04-16",
    "abstract": "Feature matching across video streams remains a cornerstone challenge in\ncomputer vision. Increasingly, robust multimodal matching has garnered interest\nin robotics, surveillance, remote sensing, and medical imaging. While\ntraditional rely on detecting and matching spatial features, they break down\nwhen faced with noisy, misaligned, or cross-modal data. Recent deep learning\nmethods have improved robustness through learned representations, but remain\nconstrained by their dependence on extensive training data and computational\ndemands. We present Flow Intelligence, a paradigm-shifting approach that moves\nbeyond spatial features by focusing on temporal motion patterns exclusively.\nInstead of detecting traditional keypoints, our method extracts motion\nsignatures from pixel blocks across consecutive frames and extract temporal\nmotion signatures between videos. These motion-based descriptors achieve\nnatural invariance to translation, rotation, and scale variations while\nremaining robust across different imaging modalities. This novel approach also\nrequires no pretraining data, eliminates the need for spatial feature\ndetection, enables cross-modal matching using only temporal motion, and it\noutperforms existing methods in challenging scenarios where traditional\napproaches fail. By leveraging motion rather than appearance, Flow Intelligence\nenables robust, real-time video feature matching in diverse environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps",
    "url": "http://arxiv.org/abs/2504.11416v1",
    "authors": [
      "Panagiotis Agrafiotis",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-04-15",
    "abstract": "Accurate, detailed, and high-frequent bathymetry is crucial for shallow\nseabed areas facing intense climatological and anthropogenic pressures. Current\nmethods utilizing airborne or satellite optical imagery to derive bathymetry\nprimarily rely on either SfM-MVS with refraction correction or Spectrally\nDerived Bathymetry (SDB). However, SDB methods often require extensive manual\nfieldwork or costly reference data, while SfM-MVS approaches face challenges\neven after refraction correction. These include depth data gaps and noise in\nenvironments with homogeneous visual textures, which hinder the creation of\naccurate and complete Digital Surface Models (DSMs) of the seabed. To address\nthese challenges, this work introduces a methodology that combines the\nhigh-fidelity 3D reconstruction capabilities of the SfM-MVS methods with\nstate-of-the-art refraction correction techniques, along with the spectral\nanalysis capabilities of a new deep learning-based method for bathymetry\nprediction. This integration enables a synergistic approach where SfM-MVS\nderived DSMs with data gaps are used as training data to generate complete\nbathymetric maps. In this context, we propose Swin-BathyUNet that combines\nU-Net with Swin Transformer self-attention layers and a cross-attention\nmechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve\nbathymetric accuracy by capturing long-range spatial relationships and can also\nfunction as a standalone solution for standard SDB with various training depth\ndata, independent of the SfM-MVS output. Experimental results in two completely\ndifferent test sites in the Mediterranean and Baltic Seas demonstrate the\neffectiveness of the proposed approach through extensive experiments that\ndemonstrate improvements in bathymetric accuracy, detail, coverage, and noise\nreduction in the predicted DSM. The code is available at\nhttps://github.com/pagraf/Swin-BathyUNet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "YOLO-RS: Remote Sensing Enhanced Crop Detection Methods",
    "url": "http://arxiv.org/abs/2504.11165v1",
    "authors": [
      "Linlin Xiao",
      "Zhang Tiancong",
      "Yutong Jia",
      "Xinyu Nie",
      "Mengyao Wang",
      "Xiaohang Shao"
    ],
    "published": "2025-04-15",
    "abstract": "With the rapid development of remote sensing technology, crop classification\nand health detection based on deep learning have gradually become a research\nhotspot. However, the existing target detection methods show poor performance\nwhen dealing with small targets in remote sensing images, especially in the\ncase of complex background and image mixing, which is difficult to meet the\npractical application requirementsite. To address this problem, a novel target\ndetection model YOLO-RS is proposed in this paper. The model is based on the\nlatest Yolov11 which significantly enhances the detection of small targets by\nintroducing the Context Anchor Attention (CAA) mechanism and an efficient\nmulti-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional\nfeature fusion strategy in the feature fusion process, which effectively\nenhances the model's performance in the detection of small targets. Small\ntarget detection. Meanwhile, the ACmix module at the end of the model backbone\nnetwork solves the category imbalance problem by adaptively adjusting the\ncontrast and sample mixing, thus enhancing the detection accuracy in complex\nscenes. In the experiments on the PDT remote sensing crop health detection\ndataset and the CWC crop classification dataset, YOLO-RS improves both the\nrecall and the mean average precision (mAP) by about 2-3\\% or so compared with\nthe existing state-of-the-art methods, while the F1-score is also significantly\nimproved. Moreover, the computational complexity of the model only increases by\nabout 5.2 GFLOPs, indicating its significant advantages in both performance and\nefficiency. The experimental results validate the effectiveness and application\npotential of YOLO-RS in the task of detecting small targets in remote sensing\nimages.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion",
    "url": "http://arxiv.org/abs/2504.10974v2",
    "authors": [
      "Zhisheng Zhang",
      "Peng Zhang",
      "Fengxiang Wang",
      "Liangli Ma",
      "Fuchun Sun"
    ],
    "published": "2025-04-15",
    "abstract": "Enhancing forward-looking sonar images is critical for accurate underwater\ntarget detection. Current deep learning methods mainly rely on supervised\ntraining with simulated data, but the difficulty in obtaining high-quality\nreal-world paired data limits their practical use and generalization. Although\nself-supervised approaches from remote sensing partially alleviate data\nshortages, they neglect the cross-modal degradation gap between sonar and\nremote sensing images. Directly transferring pretrained weights often leads to\noverly smooth sonar images, detail loss, and insufficient brightness. To\naddress this, we propose a feature-space transformation that maps sonar images\nfrom the pixel domain to a robust feature domain, effectively bridging the\ndegradation gap. Additionally, our self-supervised multi-frame fusion strategy\nleverages complementary inter-frame information to naturally remove speckle\nnoise and enhance target-region brightness. Experiments on three self-collected\nreal-world forward-looking sonar datasets show that our method significantly\noutperforms existing approaches, effectively suppressing noise, preserving\ndetailed edges, and substantially improving brightness, demonstrating strong\npotential for underwater target detection applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "LightFormer: A lightweight and efficient decoder for remote sensing image segmentation",
    "url": "http://arxiv.org/abs/2504.10834v1",
    "authors": [
      "Sihang Chen",
      "Lijun Yun",
      "Ze Liu",
      "JianFeng Zhu",
      "Jie Chen",
      "Hui Wang",
      "Yueping Nie"
    ],
    "published": "2025-04-15",
    "abstract": "Deep learning techniques have achieved remarkable success in the semantic\nsegmentation of remote sensing images and in land-use change detection.\nNevertheless, their real-time deployment on edge platforms remains constrained\nby decoder complexity. Herein, we introduce LightFormer, a lightweight decoder\nfor time-critical tasks that involve unstructured targets, such as disaster\nassessment, unmanned aerial vehicle search-and-rescue, and cultural heritage\nmonitoring. LightFormer employs a feature-fusion and refinement module built on\nchannel processing and a learnable gating mechanism to aggregate multi-scale,\nmulti-range information efficiently, which drastically curtails model\ncomplexity. Furthermore, we propose a spatial information selection module\n(SISM) that integrates long-range attention with a detail preservation branch\nto capture spatial dependencies across multiple scales, thereby substantially\nimproving the recognition of unstructured targets in complex scenes. On the\nISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9%\nvs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters,\nthus achieving an excellent accuracy-efficiency trade-off. Consistent results\non LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its\nrobustness and superior perception of unstructured objects. These findings\nhighlight LightFormer as a practical solution for remote sensing applications\nwhere both computational economy and high-precision segmentation are\nimperative.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Rainy: Unlocking Satellite Calibration for Deep Learning in Precipitation",
    "url": "http://arxiv.org/abs/2504.10776v1",
    "authors": [
      "Zhenyu Yu",
      "Hanqing Chen",
      "Mohd Yamani Idna Idris",
      "Pei Wang"
    ],
    "published": "2025-04-15",
    "abstract": "Precipitation plays a critical role in the Earth's hydrological cycle,\ndirectly affecting ecosystems, agriculture, and water resource management.\nAccurate precipitation estimation and prediction are crucial for understanding\nclimate dynamics, disaster preparedness, and environmental monitoring. In\nrecent years, artificial intelligence (AI) has gained increasing attention in\nquantitative remote sensing (QRS), enabling more advanced data analysis and\nimproving precipitation estimation accuracy. Although traditional methods have\nbeen widely used for precipitation estimation, they face limitations due to the\ndifficulty of data acquisition and the challenge of capturing complex feature\nrelationships. Furthermore, the lack of standardized multi-source satellite\ndatasets, and in most cases, the exclusive reliance on station data,\nsignificantly hinders the effective application of advanced AI models. To\naddress these challenges, we propose the Rainy dataset, a multi-source\nspatio-temporal dataset that integrates pure satellite data with station data,\nand propose Taper Loss, designed to fill the gap in tasks where only in-situ\ndata is available without area-wide support. The Rainy dataset supports five\nmain tasks: (1) satellite calibration, (2) precipitation event prediction, (3)\nprecipitation level prediction, (4) spatiotemporal prediction, and (5)\nprecipitation downscaling. For each task, we selected benchmark models and\nevaluation metrics to provide valuable references for researchers. Using\nprecipitation as an example, the Rainy dataset and Taper Loss demonstrate the\nseamless collaboration between QRS and computer vision, offering data support\nfor AI for Science in the field of QRS and providing valuable insights for\ninterdisciplinary collaboration and integration.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "DiffMOD: Progressive Diffusion Point Denoising for Moving Object Detection in Remote Sensing",
    "url": "http://arxiv.org/abs/2504.10278v1",
    "authors": [
      "Jinyue Zhang",
      "Xiangrong Zhang",
      "Zhongjian Huang",
      "Tianyang Zhang",
      "Yifei Jiang",
      "Licheng Jiao"
    ],
    "published": "2025-04-14",
    "abstract": "Moving object detection (MOD) in remote sensing is significantly challenged\nby low resolution, extremely small object sizes, and complex noise\ninterference. Current deep learning-based MOD methods rely on probability\ndensity estimation, which restricts flexible information interaction between\nobjects and across temporal frames. To flexibly capture high-order inter-object\nand temporal relationships, we propose a point-based MOD in remote sensing.\nInspired by diffusion models, the network optimization is formulated as a\nprogressive denoising process that iteratively recovers moving object centers\nfrom sparse noisy points. Specifically, we sample scattered features from the\nbackbone outputs as atomic units for subsequent processing, while global\nfeature embeddings are aggregated to compensate for the limited coverage of\nsparse point features. By modeling spatial relative positions and semantic\naffinities, Spatial Relation Aggregation Attention is designed to enable\nhigh-order interactions among point-level features for enhanced object\nrepresentation. To enhance temporal consistency, the Temporal Propagation and\nGlobal Fusion module is designed, which leverages an implicit memory reasoning\nmechanism for robust cross-frame feature integration. To align with the\nprogressive denoising process, we propose a progressive MinK optimal transport\nassignment strategy that establishes specialized learning objectives at each\ndenoising level. Additionally, we introduce a missing loss function to\ncounteract the clustering tendency of denoised points around salient objects.\nExperiments on the RsData remote sensing MOD dataset show that our MOD method\nbased on scattered point denoising can more effectively explore potential\nrelationships between sparse moving objects and improve the detection\ncapability and temporal consistency.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective Instance-Specific Pansharpening on Real-World Data",
    "url": "http://arxiv.org/abs/2504.10242v1",
    "authors": [
      "Tianyu Xin",
      "Jin-Liang Xiao",
      "Zeyu Xia",
      "Shan Yin",
      "Liang-Jian Deng"
    ],
    "published": "2025-04-14",
    "abstract": "Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images",
    "url": "http://arxiv.org/abs/2504.09621v1",
    "authors": [
      "Jiuchen Chen",
      "Xinyu Yan",
      "Qizhi Xu",
      "Kaiqi Li"
    ],
    "published": "2025-04-13",
    "abstract": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 $\\times$ 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 $\\times$\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Remote Sensing Based Crop Health Classification Using NDVI and Fully Connected Neural Networks",
    "url": "http://arxiv.org/abs/2504.10522v1",
    "authors": [
      "J. Judith",
      "R. Tamilselvi",
      "M. Parisa Beham",
      "S. Sathiya Pandiya Lakshmi",
      "Alavikunhu Panthakkan",
      "Saeed Al Mansoori",
      "Hussain Al Ahmad"
    ],
    "published": "2025-04-11",
    "abstract": "Accurate crop health monitoring is not only essential for improving\nagricultural efficiency but also for ensuring sustainable food production in\nthe face of environmental challenges. Traditional approaches often rely on\nvisual inspection or simple NDVI measurements, which, though useful, fall short\nin detecting nuanced variations in crop stress and disease conditions. In this\nresearch, we propose a more sophisticated method that leverages NDVI data\ncombined with a Fully Connected Neural Network (FCNN) to classify crop health\nwith greater precision. The FCNN, trained using satellite imagery from various\nagricultural regions, is capable of identifying subtle distinctions between\nhealthy crops, rust-affected plants, and other stressed conditions. Our\napproach not only achieved a remarkable classification accuracy of 97.80% but\nit also significantly outperformed conventional models in terms of precision,\nrecall, and F1-scores. The ability to map the relationship between NDVI values\nand crop health using deep learning presents new opportunities for real-time,\nlarge-scale monitoring of agricultural fields, reducing manual efforts, and\noffering a scalable solution to address global food security.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Few-Shot Adaptation of Grounding DINO for Agricultural Domain",
    "url": "http://arxiv.org/abs/2504.07252v1",
    "authors": [
      "Rajhans Singh",
      "Rafael Bidese Puhl",
      "Kshitiz Dhakal",
      "Sudhir Sornapudi"
    ],
    "published": "2025-04-09",
    "abstract": "Deep learning models are transforming agricultural applications by enabling\nautomated phenotyping, monitoring, and yield estimation. However, their\neffectiveness heavily depends on large amounts of annotated training data,\nwhich can be labor and time intensive. Recent advances in open-set object\ndetection, particularly with models like Grounding-DINO, offer a potential\nsolution to detect regions of interests based on text prompt input. Initial\nzero-shot experiments revealed challenges in crafting effective text prompts,\nespecially for complex objects like individual leaves and visually similar\nclasses. To address these limitations, we propose an efficient few-shot\nadaptation method that simplifies the Grounding-DINO architecture by removing\nthe text encoder module (BERT) and introducing a randomly initialized trainable\ntext embedding. This method achieves superior performance across multiple\nagricultural datasets, including plant-weed detection, plant counting, insect\nidentification, fruit counting, and remote sensing tasks. Specifically, it\ndemonstrates up to a $\\sim24\\%$ higher mAP than fully fine-tuned YOLO models on\nagricultural datasets and outperforms previous state-of-the-art methods by\n$\\sim10\\%$ in remote sensing, under few-shot learning conditions. Our method\noffers a promising solution for automating annotation and accelerating the\ndevelopment of specialized agricultural AI solutions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2504.05062v2",
    "authors": [
      "Chenfeng Xu"
    ],
    "published": "2025-04-07",
    "abstract": "With the rapid advancement of deep learning, the field of change detection\n(CD) in remote sensing imagery has achieved remarkable progress. Existing\nchange detection methods primarily focus on achieving higher accuracy with\nincreased computational costs and parameter sizes, leaving development of\nlightweight methods for rapid real-world processing an underexplored challenge.\nTo address this challenge, we propose a Lightweight Difference Guiding Network\n(LDGNet), leveraging absolute difference image to guide optical remote sensing\nchange detection. First, to enhance the feature representation capability of\nthe lightweight backbone network, we propose the Difference Guiding Module\n(DGM), which leverages multi-scale features extracted from the absolute\ndifference image to progressively influence the original image encoder at each\nlayer, thereby reinforcing feature extraction. Second, we propose the\nDifference-Aware Dynamic Fusion (DADF) module with Visual State Space Model\n(VSSM) for lightweight long-range dependency modeling. The module first uses\nfeature absolute differences to guide VSSM's global contextual modeling of\nchange regions, then employs difference attention to dynamically fuse these\nlong-range features with feature differences, enhancing change semantics while\nsuppressing noise and background. Extensive experiments on multiple datasets\ndemonstrate that our method achieves comparable or superior performance to\ncurrent state-of-the-art (SOTA) methods requiring several times more\ncomputation, while maintaining only 3.43M parameters and 1.12G FLOPs.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Joint Retrieval of Cloud properties using Attention-based Deep Learning Models",
    "url": "http://arxiv.org/abs/2504.03133v2",
    "authors": [
      "Zahid Hassan Tushar",
      "Adeleke Ademakinwa",
      "Jianwu Wang",
      "Zhibo Zhang",
      "Sanjay Purushotham"
    ],
    "published": "2025-04-04",
    "abstract": "Accurate cloud property retrieval is vital for understanding cloud behavior\nand its impact on climate, including applications in weather forecasting,\nclimate modeling, and estimating Earth's radiation balance. The Independent\nPixel Approximation (IPA), a widely used physics-based approach, simplifies\nradiative transfer calculations by assuming each pixel is independent of its\nneighbors. While computationally efficient, IPA has significant limitations,\nsuch as inaccuracies from 3D radiative effects, errors at cloud edges, and\nineffectiveness for overlapping or heterogeneous cloud fields. Recent\nAI/ML-based deep learning models have improved retrieval accuracy by leveraging\nspatial relationships across pixels. However, these models are often\nmemory-intensive, retrieve only a single cloud property, or struggle with joint\nproperty retrievals. To overcome these challenges, we introduce CloudUNet with\nAttention Module (CAM), a compact UNet-based model that employs attention\nmechanisms to reduce errors in thick, overlapping cloud regions and a\nspecialized loss function for joint retrieval of Cloud Optical Thickness (COT)\nand Cloud Effective Radius (CER). Experiments on a Large Eddy Simulation (LES)\ndataset show that our CAM model outperforms state-of-the-art deep learning\nmethods, reducing mean absolute errors (MAE) by 34% for COT and 42% for CER,\nand achieving 76% and 86% lower MAE for COT and CER retrievals compared to the\nIPA method.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Adaptive Frequency Enhancement Network for Remote Sensing Image Semantic Segmentation",
    "url": "http://arxiv.org/abs/2504.02647v1",
    "authors": [
      "Feng Gao",
      "Miao Fu",
      "Jingchao Cao",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2025-04-03",
    "abstract": "Semantic segmentation of high-resolution remote sensing images plays a\ncrucial role in land-use monitoring and urban planning. Recent remarkable\nprogress in deep learning-based methods makes it possible to generate\nsatisfactory segmentation results. However, existing methods still face\nchallenges in adapting network parameters to various land cover distributions\nand enhancing the interaction between spatial and frequency domain features. To\naddress these challenges, we propose the Adaptive Frequency Enhancement Network\n(AFENet), which integrates two key components: the Adaptive Frequency and\nSpatial feature Interaction Module (AFSIM) and the Selective feature Fusion\nModule (SFM). AFSIM dynamically separates and modulates high- and low-frequency\nfeatures according to the content of the input image. It adaptively generates\ntwo masks to separate high- and low-frequency components, therefore providing\noptimal details and contextual supplementary information for ground object\nfeature representation. SFM selectively fuses global context and local detailed\nfeatures to enhance the network's representation capability. Hence, the\ninteractions between frequency and spatial features are further enhanced.\nExtensive experiments on three publicly available datasets demonstrate that the\nproposed AFENet outperforms state-of-the-art methods. In addition, we also\nvalidate the effectiveness of AFSIM and SFM in managing diverse land cover\ntypes and complex scenarios. Our codes are available at\nhttps://github.com/oucailab/AFENet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A Decade of Deep Learning for Remote Sensing Spatiotemporal Fusion: Advances, Challenges, and Opportunities",
    "url": "http://arxiv.org/abs/2504.00901v1",
    "authors": [
      "Enzhe Sun",
      "Yongchuan Cui",
      "Peng Liu",
      "Jining Yan"
    ],
    "published": "2025-04-01",
    "abstract": "Hardware limitations and satellite launch costs make direct acquisition of\nhigh temporal-spatial resolution remote sensing imagery challenging. Remote\nsensing spatiotemporal fusion (STF) technology addresses this problem by\nmerging high temporal but low spatial resolution imagery with high spatial but\nlow temporal resolution imagery to efficiently generate high spatiotemporal\nresolution satellite images. STF provides unprecedented observational\ncapabilities for land surface change monitoring, agricultural management, and\nenvironmental research. Deep learning (DL) methods have revolutionized the\nremote sensing spatiotemporal fusion field over the past decade through\npowerful automatic feature extraction and nonlinear modeling capabilities,\nsignificantly outperforming traditional methods in handling complex\nspatiotemporal data. Despite the rapid development of DL-based remote sensing\nSTF, the community lacks a systematic review of this quickly evolving field.\nThis paper comprehensively reviews DL developments in remote sensing STF over\nthe last decade, analyzing key research trends, method classifications,\ncommonly used datasets, and evaluation metrics. It discusses major challenges\nin existing research and identifies promising future research directions as\nreferences for researchers in this field to inspire new ideas. The specific\nmodels, datasets, and other information mentioned in this article have been\ncollected in:\nhttps://github.com/yc-cui/Deep-Learning-Spatiotemporal-Fusion-Survey.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
    "url": "http://arxiv.org/abs/2503.23793v1",
    "authors": [
      "Zhongnan Cai",
      "Yingying Wang",
      "Yunlong Lin",
      "Hui Zheng",
      "Ge Meng",
      "Zixu Lin",
      "Jiaxin Xie",
      "Junbin Lu",
      "Yue Huang",
      "Xinghao Ding"
    ],
    "published": "2025-03-31",
    "abstract": "Recently, deep learning-based pan-sharpening algorithms have achieved notable\nadvancements over traditional methods. However, many deep learning-based\napproaches incur substantial computational overhead during inference,\nespecially with high-resolution images. This excessive computational demand\nlimits the applicability of these methods in real-world scenarios, particularly\nin the absence of dedicated computing devices such as GPUs and TPUs. To address\nthese challenges, we propose Pan-LUT, a novel learnable look-up table (LUT)\nframework for pan-sharpening that strikes a balance between performance and\ncomputational efficiency for high-resolution remote sensing images. To finely\ncontrol the spectral transformation, we devise the PAN-guided look-up table\n(PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained\nspatial details and adaptively learn local contexts, we introduce the spatial\ndetails look-up table (SDLUT) and adaptive aggregation look-up table (AALUT).\nOur proposed method contains fewer than 300K parameters and processes a 8K\nresolution image in under 1 ms using a single NVIDIA GeForce RTX 2080 Ti GPU,\ndemonstrating significantly faster performance compared to other methods.\nExperiments reveal that Pan-LUT efficiently processes large remote sensing\nimages in a lightweight manner, bridging the gap to real-world applications.\nFurthermore, our model surpasses SOTA methods in full-resolution scenes under\nreal-world conditions, highlighting its effectiveness and efficiency.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A large-scale image-text dataset benchmark for farmland segmentation",
    "url": "http://arxiv.org/abs/2503.23106v1",
    "authors": [
      "Chao Tao",
      "Dandan Zhong",
      "Weiliang Mu",
      "Zhuofei Du",
      "Haiyang Wu"
    ],
    "published": "2025-03-29",
    "abstract": "The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A Deep Learning Framework for Boundary-Aware Semantic Segmentation",
    "url": "http://arxiv.org/abs/2503.22050v1",
    "authors": [
      "Tai An",
      "Weiqiang Huang",
      "Da Xu",
      "Qingyuan He",
      "Jiacheng Hu",
      "Yujia Lou"
    ],
    "published": "2025-03-28",
    "abstract": "As a fundamental task in computer vision, semantic segmentation is widely\napplied in fields such as autonomous driving, remote sensing image analysis,\nand medical image processing. In recent years, Transformer-based segmentation\nmethods have demonstrated strong performance in global feature modeling.\nHowever, they still struggle with blurred target boundaries and insufficient\nrecognition of small targets. To address these issues, this study proposes a\nMask2Former-based semantic segmentation algorithm incorporating a boundary\nenhancement feature bridging module (BEFBM). The goal is to improve target\nboundary accuracy and segmentation consistency. Built upon the Mask2Former\nframework, this method constructs a boundary-aware feature map and introduces a\nfeature bridging mechanism. This enables effective cross-scale feature fusion,\nenhancing the model's ability to focus on target boundaries. Experiments on the\nCityscapes dataset demonstrate that, compared to mainstream segmentation\nmethods, the proposed approach achieves significant improvements in metrics\nsuch as mIOU, mDICE, and mRecall. It also exhibits superior boundary retention\nin complex scenes. Visual analysis further confirms the model's advantages in\nfine-grained regions. Future research will focus on optimizing computational\nefficiency and exploring its potential in other high-precision segmentation\ntasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Recognition"
    ]
  },
  {
    "title": "Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid Self-Attention U-Nets in Aerial Imagery",
    "url": "http://arxiv.org/abs/2503.21438v1",
    "authors": [
      "Anis Ur Rahman",
      "Einari Heinaro",
      "Mete Ahishali",
      "Samuli Junttila"
    ],
    "published": "2025-03-27",
    "abstract": "Mapping standing dead trees is critical for assessing forest health,\nmonitoring biodiversity, and mitigating wildfire risks, for which aerial\nimagery has proven useful. However, dense canopy structures, spectral overlaps\nbetween living and dead vegetation, and over-segmentation errors limit the\nreliability of existing methods. This study introduces a hybrid postprocessing\nframework that refines deep learning-based tree segmentation by integrating\nwatershed algorithms with adaptive filtering, enhancing boundary delineation,\nand reducing false positives in complex forest environments. Tested on\nhigh-resolution aerial imagery from boreal forests, the framework improved\ninstance-level segmentation accuracy by 41.5% and reduced positional errors by\n57%, demonstrating robust performance in densely vegetated regions. By\nbalancing detection accuracy and over-segmentation artifacts, the method\nenabled the precise identification of individual dead trees, which is critical\nfor ecological monitoring. The framework's computational efficiency supports\nscalable applications, such as wall-to-wall tree mortality mapping over large\ngeographic regions using aerial or satellite imagery. These capabilities\ndirectly benefit wildfire risk assessment (identifying fuel accumulations),\ncarbon stock estimation (tracking emissions from decaying biomass), and\nprecision forestry (targeting salvage loggings). By bridging advanced remote\nsensing techniques with practical forest management needs, this work advances\ntools for large-scale ecological conservation and climate resilience planning.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "SChanger: Change Detection from a Semantic Change and Spatial Consistency Perspective",
    "url": "http://arxiv.org/abs/2503.20734v1",
    "authors": [
      "Ziyu Zhou",
      "Keyan Hu",
      "Yutian Fang",
      "Xiaoping Rui"
    ],
    "published": "2025-03-26",
    "abstract": "Change detection is a key task in Earth observation applications. Recently,\ndeep learning methods have demonstrated strong performance and widespread\napplication. However, change detection faces data scarcity due to the\nlabor-intensive process of accurately aligning remote sensing images of the\nsame area, which limits the performance of deep learning algorithms. To address\nthe data scarcity issue, we develop a fine-tuning strategy called the Semantic\nChange Network (SCN). We initially pre-train the model on single-temporal\nsupervised tasks to acquire prior knowledge of instance feature extraction. The\nmodel then employs a shared-weight Siamese architecture and extended Temporal\nFusion Module (TFM) to preserve this prior knowledge and is fine-tuned on\nchange detection tasks. The learned semantics for identifying all instances is\nchanged to focus on identifying only the changes. Meanwhile, we observe that\nthe locations of changes between the two images are spatially identical, a\nconcept we refer to as spatial consistency. We introduce this inductive bias\nthrough an attention map that is generated by large-kernel convolutions and\napplied to the features from both time points. This enhances the modeling of\nmulti-scale changes and helps capture underlying relationships in change\ndetection semantics. We develop a binary change detection model utilizing these\ntwo strategies. The model is validated against state-of-the-art methods on six\ndatasets, surpassing all benchmark methods and achieving F1 scores of 92.87%,\n86.43%, 68.95%, 97.62%, 84.58%, and 93.20% on the LEVIR-CD, LEVIR-CD+,\nS2Looking, CDD, SYSU-CD, and WHU-CD datasets, respectively.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Small Object Detection: A Comprehensive Survey on Challenges, Techniques and Real-World Applications",
    "url": "http://arxiv.org/abs/2503.20516v1",
    "authors": [
      "Mahya Nikouei",
      "Bita Baroutian",
      "Shahabedin Nabavi",
      "Fateme Taraghi",
      "Atefe Aghaei",
      "Ayoob Sajedi",
      "Mohsen Ebrahimi Moghaddam"
    ],
    "published": "2025-03-26",
    "abstract": "Small object detection (SOD) is a critical yet challenging task in computer\nvision, with applications like spanning surveillance, autonomous systems,\nmedical imaging, and remote sensing. Unlike larger objects, small objects\ncontain limited spatial and contextual information, making accurate detection\ndifficult. Challenges such as low resolution, occlusion, background\ninterference, and class imbalance further complicate the problem. This survey\nprovides a comprehensive review of recent advancements in SOD using deep\nlearning, focusing on articles published in Q1 journals during 2024-2025. We\nanalyzed challenges, state-of-the-art techniques, datasets, evaluation metrics,\nand real-world applications. Recent advancements in deep learning have\nintroduced innovative solutions, including multi-scale feature extraction,\nSuper-Resolution (SR) techniques, attention mechanisms, and transformer-based\narchitectures. Additionally, improvements in data augmentation, synthetic data\ngeneration, and transfer learning have addressed data scarcity and domain\nadaptation issues. Furthermore, emerging trends such as lightweight neural\nnetworks, knowledge distillation (KD), and self-supervised learning offer\npromising directions for improving detection efficiency, particularly in\nresource-constrained environments like Unmanned Aerial Vehicles (UAV)-based\nsurveillance and edge computing. We also review widely used datasets, along\nwith standard evaluation metrics such as mean Average Precision (mAP) and\nsize-specific AP scores. The survey highlights real-world applications,\nincluding traffic monitoring, maritime surveillance, industrial defect\ndetection, and precision agriculture. Finally, we discuss open research\nchallenges and future directions, emphasizing the need for robust domain\nadaptation techniques, better feature fusion strategies, and real-time\nperformance optimization.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Super-Resolution"
    ]
  },
  {
    "title": "Leveraging Land Cover Priors for Isoprene Emission Super-Resolution",
    "url": "http://arxiv.org/abs/2503.18658v1",
    "authors": [
      "Christopher Ummerle",
      "Antonio Giganti",
      "Sara Mandelli",
      "Paolo Bestagini",
      "Stefano Tubaro"
    ],
    "published": "2025-03-24",
    "abstract": "Remote sensing plays a crucial role in monitoring Earth's ecosystems, yet\nsatellite-derived data often suffer from limited spatial resolution,\nrestricting their applicability in atmospheric modeling and climate research.\nIn this work, we propose a deep learning-based Super-Resolution (SR) framework\nthat leverages land cover information to enhance the spatial accuracy of\nBiogenic Volatile Organic Compounds (BVOCs) emissions, with a particular focus\non isoprene. Our approach integrates land cover priors as emission drivers,\ncapturing spatial patterns more effectively than traditional methods. We\nevaluate the model's performance across various climate conditions and analyze\nstatistical correlations between isoprene emissions and key environmental\ninformation such as cropland and tree cover data. Additionally, we assess the\ngeneralization capabilities of our SR model by applying it to unseen climate\nzones and geographical regions. Experimental results demonstrate that\nincorporating land cover data significantly improves emission SR accuracy,\nparticularly in heterogeneous landscapes. This study contributes to atmospheric\nchemistry and climate modeling by providing a cost-effective, data-driven\napproach to refining BVOC emission maps. The proposed method enhances the\nusability of satellite-based emissions data, supporting applications in air\nquality forecasting, climate impact assessments, and environmental studies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "MapGlue: Multimodal Remote Sensing Image Matching",
    "url": "http://arxiv.org/abs/2503.16185v1",
    "authors": [
      "Peihao Wu",
      "Yongxiang Yao",
      "Wenfei Zhang",
      "Dong Wei",
      "Yi Wan",
      "Yansheng Li",
      "Yongjun Zhang"
    ],
    "published": "2025-03-20",
    "abstract": "Multimodal remote sensing image (MRSI) matching is pivotal for cross-modal\nfusion, localization, and object detection, but it faces severe challenges due\nto geometric, radiometric, and viewpoint discrepancies across imaging\nmodalities. Existing unimodal datasets lack scale and diversity, limiting deep\nlearning solutions. This paper proposes MapGlue, a universal MRSI matching\nframework, and MapData, a large-scale multimodal dataset addressing these gaps.\nOur contributions are twofold. MapData, a globally diverse dataset spanning 233\nsampling points, offers original images (7,000x5,000 to 20,000x15,000 pixels).\nAfter rigorous cleaning, it provides 121,781 aligned electronic map-visible\nimage pairs (512x512 pixels) with hybrid manual-automated ground truth,\naddressing the scarcity of scalable multimodal benchmarks. MapGlue integrates\nsemantic context with a dual graph-guided mechanism to extract cross-modal\ninvariant features. This structure enables global-to-local interaction,\nenhancing descriptor robustness against modality-specific distortions.\nExtensive evaluations on MapData and five public datasets demonstrate MapGlue's\nsuperiority in matching accuracy under complex conditions, outperforming\nstate-of-the-art methods. Notably, MapGlue generalizes effectively to unseen\nmodalities without retraining, highlighting its adaptability. This work\naddresses longstanding challenges in MRSI matching by combining scalable\ndataset construction with a robust, semantics-driven framework. Furthermore,\nMapGlue shows strong generalization capabilities on other modality matching\ntasks for which it was not specifically trained. The dataset and code are\navailable at https://github.com/PeihaoWu/MapGlue.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Toward task-driven satellite image super-resolution",
    "url": "http://arxiv.org/abs/2503.15474v1",
    "authors": [
      "Maciej Ziaja",
      "Pawel Kowaleczko",
      "Daniel Kostrzewa",
      "Nicolas Long\u00e9p\u00e9",
      "Michal Kawulok"
    ],
    "published": "2025-03-19",
    "abstract": "Super-resolution is aimed at reconstructing high-resolution images from\nlow-resolution observations. State-of-the-art approaches underpinned with deep\nlearning allow for obtaining outstanding results, generating images of high\nperceptual quality. However, it often remains unclear whether the reconstructed\ndetails are close to the actual ground-truth information and whether they\nconstitute a more valuable source for image analysis algorithms. In the\nreported work, we address the latter problem, and we present our efforts toward\nlearning super-resolution algorithms in a task-driven way to make them suitable\nfor generating high-resolution images that can be exploited for automated image\nanalysis. In the reported initial research, we propose a methodological\napproach for assessing the existing models that perform computer vision tasks\nin terms of whether they can be used for evaluating super-resolution\nreconstruction algorithms, as well as training them in a task-driven way. We\nsupport our analysis with experimental study and we expect it to establish a\nsolid foundation for selecting appropriate computer vision tasks that will\nadvance the capabilities of real-world super-resolution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation Performance on RGB Images of Closed Canopy: Validation Using TLS",
    "url": "http://arxiv.org/abs/2503.14273v2",
    "authors": [
      "Matthew J. Allen",
      "Harry J. F. Owen",
      "Stuart W. D. Grieve",
      "Emily R. Lines"
    ],
    "published": "2025-03-18",
    "abstract": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Comparative and Interpretative Analysis of CNN and Transformer Models in Predicting Wildfire Spread Using Remote Sensing Data",
    "url": "http://arxiv.org/abs/2503.14150v1",
    "authors": [
      "Yihang Zhou",
      "Ruige Kong",
      "Zhengsen Xu",
      "Linlin Xu",
      "Sibo Cheng"
    ],
    "published": "2025-03-18",
    "abstract": "Facing the escalating threat of global wildfires, numerous computer vision\ntechniques using remote sensing data have been applied in this area. However,\nthe selection of deep learning methods for wildfire prediction remains\nuncertain due to the lack of comparative analysis in a quantitative and\nexplainable manner, crucial for improving prevention measures and refining\nmodels. This study aims to thoroughly compare the performance, efficiency, and\nexplainability of four prevalent deep learning architectures: Autoencoder,\nResNet, UNet, and Transformer-based Swin-UNet. Employing a real-world dataset\nthat includes nearly a decade of remote sensing data from California, U.S.,\nthese models predict the spread of wildfires for the following day. Through\ndetailed quantitative comparison analysis, we discovered that Transformer-based\nSwin-UNet and UNet generally outperform Autoencoder and ResNet, particularly\ndue to the advanced attention mechanisms in Transformer-based Swin-UNet and the\nefficient use of skip connections in both UNet and Transformer-based Swin-UNet,\nwhich contribute to superior predictive accuracy and model interpretability.\nThen we applied XAI techniques on all four models, this not only enhances the\nclarity and trustworthiness of models but also promotes focused improvements in\nwildfire prediction capabilities. The XAI analysis reveals that UNet and\nTransformer-based Swin-UNet are able to focus on critical features such as\n'Previous Fire Mask', 'Drought', and 'Vegetation' more effectively than the\nother two models, while also maintaining balanced attention to the remaining\nfeatures, leading to their superior performance. The insights from our thorough\ncomparative analysis offer substantial implications for future model design and\nalso provide guidance for model selection in different scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "ResNet",
      "Transformer",
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening",
    "url": "http://arxiv.org/abs/2503.13214v3",
    "authors": [
      "Jie Huang",
      "Haorui Chen",
      "Jiaxuan Ren",
      "Siran Peng",
      "Liangjian Deng"
    ],
    "published": "2025-03-17",
    "abstract": "Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Ship Detection in Remote Sensing Imagery for Arbitrarily Oriented Object Detection",
    "url": "http://arxiv.org/abs/2503.14534v1",
    "authors": [
      "Bibi Erum Ayesha",
      "T. Satyanarayana Murthy",
      "Palamakula Ramesh Babu",
      "Ramu Kuchipudi"
    ],
    "published": "2025-03-17",
    "abstract": "This research paper presents an innovative ship detection system tailored for\napplications like maritime surveillance and ecological monitoring. The study\nemploys YOLOv8 and repurposed U-Net, two advanced deep learning models, to\nsignificantly enhance ship detection accuracy. Evaluation metrics include Mean\nAverage Precision (mAP), processing speed, and overall accuracy. The research\nutilizes the \"Airbus Ship Detection\" dataset, featuring diverse remote sensing\nimages, to assess the models' versatility in detecting ships with varying\norientations and environmental contexts. Conventional ship detection faces\nchallenges with arbitrary orientations, complex backgrounds, and obscured\nperspectives. Our approach incorporates YOLOv8 for real-time processing and\nU-Net for ship instance segmentation. Evaluation focuses on mAP, processing\nspeed, and overall accuracy. The dataset is chosen for its diverse images,\nmaking it an ideal benchmark. Results demonstrate significant progress in ship\ndetection. YOLOv8 achieves an 88% mAP, excelling in accurate and rapid ship\ndetection. U Net, adapted for ship instance segmentation, attains an 89% mAP,\nimproving boundary delineation and handling occlusions. This research enhances\nmaritime surveillance, disaster response, and ecological monitoring,\nexemplifying the potential of deep learning models in ship detection.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Early Detection of Forest Calamities in Homogeneous Stands -- Deep Learning Applied to Bark-Beetle Outbreaks",
    "url": "http://arxiv.org/abs/2503.12883v1",
    "authors": [
      "Maximilian Kirsch",
      "Jakob Wernicke",
      "Pawan Datta",
      "Christine Preisach"
    ],
    "published": "2025-03-17",
    "abstract": "Climate change has increased the vulnerability of forests to insect-related\ndamage, resulting in widespread forest loss in Central Europe and highlighting\nthe need for effective, continuous monitoring systems. Remote sensing based\nforest health monitoring, oftentimes, relies on supervised machine learning\nalgorithms that require labeled training data. Monitoring temporal patterns\nthrough time series analysis offers a potential alternative for earlier\ndetection of disturbance but requires substantial storage resources. This study\ninvestigates the potential of a Deep Learning algorithm based on a Long Short\nTerm Memory (LSTM) Autoencoder for the detection of anomalies in forest health\n(e.g. bark beetle outbreaks), utilizing Sentinel-2 time series data. This\napproach is an alternative to supervised machine learning methods, avoiding the\nnecessity for labeled training data. Furthermore, it is more memory-efficient\nthan other time series analysis approaches, as a robust model can be created\nusing only a 26-week-long time series as input. In this study, we monitored\npure stands of spruce in Thuringia, Germany, over a 7-year period from 2018 to\nthe end of 2024. Our best model achieved a detection accuracy of 87% on test\ndata and was able to detect 61% of all anomalies at a very early stage (more\nthan a month before visible signs of forest degradation). Compared to another\nwidely used time series break detection algorithm - BFAST (Breaks For Additive\nSeason and Trend), our approach consistently detected higher percentage of\nanomalies at an earlier stage. These findings suggest that LSTM-based\nAutoencoders could provide a promising, resource-efficient approach to forest\nhealth monitoring, enabling more timely responses to emerging threats.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LSTM",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Time Series Analysis"
    ]
  },
  {
    "title": "Observation-only learning of neural mapping schemes for gappy satellite-derived ocean colour parameters",
    "url": "http://arxiv.org/abs/2503.11532v1",
    "authors": [
      "Cl\u00e9ment Dorffer",
      "Fr\u00e9d\u00e9ric Jourdin",
      "Thi Thuy Nga Nguyen",
      "Rodolphe Devillers",
      "David Mouillot",
      "Ronan Fablet"
    ],
    "published": "2025-03-14",
    "abstract": "Monitoring optical properties of coastal and open ocean waters is crucial to\nassessing the health of marine ecosystems. Deep learning offers a promising\napproach to address these ecosystem dynamics, especially in scenarios where\ngap-free ground-truth data is lacking, which poses a challenge for designing\neffective training frameworks. Using an advanced neural variational data\nassimilation scheme (called 4DVarNet), we introduce a comprehensive training\nframework designed to effectively train directly on gappy data sets. Using the\nMediterranean Sea as a case study, our experiments not only highlight the high\nperformance of the chosen neural network in reconstructing gap-free images from\ngappy datasets but also demonstrate its superior performance over\nstate-of-the-art algorithms such as DInEOF and Direct Inversion, whether using\nCNN or UNet architectures.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "ChromaFormer: A Scalable and Accurate Transformer Architecture for Land Cover Classification",
    "url": "http://arxiv.org/abs/2503.08534v1",
    "authors": [
      "Mingshi Li",
      "Dusan Grujicic",
      "Ben Somers",
      "Stien Heremans",
      "Steven De Saeger",
      "Matthew B. Blaschko"
    ],
    "published": "2025-03-11",
    "abstract": "Remote sensing imagery from systems such as Sentinel provides full coverage\nof the Earth's surface at around 10-meter resolution. The remote sensing\ncommunity has transitioned to extensive use of deep learning models due to\ntheir high performance on benchmarks such as the UCMerced and ISPRS Vaihingen\ndatasets. Convolutional models such as UNet and ResNet variations are commonly\nemployed for remote sensing but typically only accept three channels, as they\nwere developed for RGB imagery, while satellite systems provide more than ten.\nRecently, several transformer architectures have been proposed for remote\nsensing, but they have not been extensively benchmarked and are typically used\non small datasets such as Salinas Valley. Meanwhile, it is becoming feasible to\nobtain dense spatial land-use labels for entire first-level administrative\ndivisions of some countries. Scaling law observations suggest that\nsubstantially larger multi-spectral transformer models could provide a\nsignificant leap in remote sensing performance in these settings.\n  In this work, we propose ChromaFormer, a family of multi-spectral transformer\nmodels, which we evaluate across orders of magnitude differences in model\nparameters to assess their performance and scaling effectiveness on a densely\nlabeled imagery dataset of Flanders, Belgium, covering more than 13,500 km^2\nand containing 15 classes. We propose a novel multi-spectral attention strategy\nand demonstrate its effectiveness through ablations. Furthermore, we show that\nmodels many orders of magnitude larger than conventional architectures, such as\nUNet, lead to substantial accuracy improvements: a UNet++ model with 23M\nparameters achieves less than 65% accuracy, while a multi-spectral transformer\nwith 655M parameters achieves over 95% accuracy on the Biological Valuation Map\nof Flanders.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "ResNet",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "i-WiViG: Interpretable Window Vision GNN",
    "url": "http://arxiv.org/abs/2503.08321v1",
    "authors": [
      "Ivica Obadic",
      "Dmitry Kangin",
      "Dario Oliveira",
      "Plamen P Angelov",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-03-11",
    "abstract": "Deep learning models based on graph neural networks have emerged as a popular\napproach for solving computer vision problems. They encode the image into a\ngraph structure and can be beneficial for efficiently capturing the long-range\ndependencies typically present in remote sensing imagery. However, an important\ndrawback of these methods is their black-box nature which may hamper their\nwider usage in critical applications. In this work, we tackle the\nself-interpretability of the graph-based vision models by proposing our\nInterpretable Window Vision GNN (i-WiViG) approach, which provides explanations\nby automatically identifying the relevant subgraphs for the model prediction.\nThis is achieved with window-based image graph processing that constrains the\nnode receptive field to a local image region and by using a self-interpretable\ngraph bottleneck that ranks the importance of the long-range relations between\nthe image regions. We evaluate our approach to remote sensing classification\nand regression tasks, showing it achieves competitive performance while\nproviding inherent and faithful explanations through the identified relations.\nFurther, the quantitative evaluation reveals that our model reduces the\ninfidelity of post-hoc explanations compared to other Vision GNN models,\nwithout sacrificing explanation sparsity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "A Deep Learning Architecture for Land Cover Mapping Using Spatio-Temporal Sentinel-1 Features",
    "url": "http://arxiv.org/abs/2503.07230v1",
    "authors": [
      "Luigi Russo",
      "Antonietta Sorriso",
      "Silvia Liberata Ullo",
      "Paolo Gamba"
    ],
    "published": "2025-03-10",
    "abstract": "Land Cover (LC) mapping using satellite imagery is critical for environmental\nmonitoring and management. Deep Learning (DL), particularly Convolutional\nNeural Networks (CNNs) and Vision Transformers (ViTs), have revolutionized this\nfield by enhancing the accuracy of classification tasks. In this work, a novel\napproach combining a transformer-based Swin-Unet architecture with seasonal\nsynthesized spatio-temporal images has been employed to classify LC types using\nspatio-temporal features extracted from Sentinel-1 (S1) Synthetic Aperture\nRadar (SAR) data, organized into seasonal clusters. The study focuses on three\ndistinct regions - Amazonia, Africa, and Siberia - and evaluates the model\nperformance across diverse ecoregions within these areas. By utilizing seasonal\nfeature sequences instead of dense temporal sequences, notable performance\nimprovements have been achieved, especially in regions with temporal data gaps\nlike Siberia, where S1 data distribution is uneven and non-uniform. The results\ndemonstrate the effectiveness and the generalization capabilities of the\nproposed methodology in achieving high overall accuracy (O.A.) values, even in\nregions with limited training data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Spatiotemporal Deep Learning Network for Photon-Level Block Compressed Sensing Imaging",
    "url": "http://arxiv.org/abs/2503.07143v2",
    "authors": [
      "Changzhi Yu",
      "Shuangping Han",
      "Kai Song",
      "Liantuan Xiao"
    ],
    "published": "2025-03-10",
    "abstract": "In this paper, we propose a spatiotemporal deep learning network for\nphoton-level Block Compressed Sensing Imaging, aimed to address challenges such\nas signal loss, artifacts, and noise interference in large-pixel dynamic\nimaging and tracking at the photon level. This approach combines information in\nthe time and frequency domains with a U-Net-LSTM deep learning model,\nsignificantly improving the restoration quality of dynamic target images at\nhigh frame rates. The experimental results demonstrate that dynamic target\nimaging with an average photon number of less than 10 per pixel can be achieved\nusing 16-channel parallel detection, where the pixel size is 256*256 and the\nframe rate is 200 fps.. Compared to conventional Block Compressed Sensing\nImaging, this method increases the peak signal-to-noise ratio to 38.66 dB and\nimproves the structural similarity index to 0.96. In the presence of a dynamic\nscattering medium and a static complex background, we successfully achieved\nimaging and tracking of two targets undergoing complex motion. Even in\nscenarios where the targets overlap or obstruct each other, we can still\nreconstruct clear images of each individual target separately.. This method\nprovides an effective solution for large-pixel dynamic target recognition,\ntracking, and real-time imaging in complex environments, offering promising\napplications in remote sensing, military reconnaissance, and beyond.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "LSTM"
    ],
    "applications": [
      "Detection",
      "Recognition",
      "Tracking"
    ]
  },
  {
    "title": "Exploring FMCW Radars and Feature Maps for Activity Recognition: A Benchmark Study",
    "url": "http://arxiv.org/abs/2503.05629v1",
    "authors": [
      "Ali Samimi Fard",
      "Mohammadreza Mashhadigholamali",
      "Samaneh Zolfaghari",
      "Hajar Abedi",
      "Mainak Chakraborty",
      "Luigi Borz\u00ec",
      "Masoud Daneshtalab",
      "George Shaker"
    ],
    "published": "2025-03-07",
    "abstract": "Human Activity Recognition has gained significant attention due to its\ndiverse applications, including ambient assisted living and remote sensing.\nWearable sensor-based solutions often suffer from user discomfort and\nreliability issues, while video-based methods raise privacy concerns and\nperform poorly in low-light conditions or long ranges. This study introduces a\nFrequency-Modulated Continuous Wave radar-based framework for human activity\nrecognition, leveraging a 60 GHz radar and multi-dimensional feature maps.\nUnlike conventional approaches that process feature maps as images, this study\nfeeds multi-dimensional feature maps -- Range-Doppler, Range-Azimuth, and\nRange-Elevation -- as data vectors directly into the machine learning (SVM,\nMLP) and deep learning (CNN, LSTM, ConvLSTM) models, preserving the spatial and\ntemporal structures of the data. These features were extracted from a novel\ndataset with seven activity classes and validated using two different\nvalidation approaches. The ConvLSTM model outperformed conventional machine\nlearning and deep learning models, achieving an accuracy of 90.51% and an\nF1-score of 87.31% on cross-scene validation and an accuracy of 89.56% and an\nF1-score of 87.15% on leave-one-person-out cross-validation. The results\nhighlight the approach's potential for scalable, non-intrusive, and\nprivacy-preserving activity monitoring in real-world scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "HoloMine: A Synthetic Dataset for Buried Landmines Recognition using Microwave Holographic Imaging",
    "url": "http://arxiv.org/abs/2502.21054v1",
    "authors": [
      "Emanuele Vivoli",
      "Lorenzo Capineri",
      "Marco Bertini"
    ],
    "published": "2025-02-28",
    "abstract": "The detection and removal of landmines is a complex and risky task that\nrequires advanced remote sensing techniques to reduce the risk for the\nprofessionals involved in this task. In this paper, we propose a novel\nsynthetic dataset for buried landmine detection to provide researchers with a\nvaluable resource to observe, measure, locate, and address issues in landmine\ndetection. The dataset consists of 41,800 microwave holographic images (2D) and\ntheir holographic inverted scans (3D) of different types of buried objects,\nincluding landmines, clutter, and pottery objects, and is collected by means of\na microwave holography sensor.\n  We evaluate the performance of several state-of-the-art deep learning models\ntrained on our synthetic dataset for various classification tasks. While the\nresults do not yield yet high performances, showing the difficulty of the\nproposed task, we believe that our dataset has significant potential to drive\nprogress in the field of landmine detection thanks to the accuracy and\nresolution obtainable using holographic radars.\n  To the best of our knowledge, our dataset is the first of its kind and will\nhelp drive further research on computer vision methods to automatize mine\ndetection, with the overall goal of reducing the risks and the costs of the\ndemining process.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "LMHLD: A Large-scale Multi-source High-resolution Landslide Dataset for Landslide Detection based on Deep Learning",
    "url": "http://arxiv.org/abs/2502.19866v1",
    "authors": [
      "Guanting Liu",
      "Yi Wang",
      "Xi Chen",
      "Baoyu Du",
      "Penglei Li",
      "Yuan Wu",
      "Zhice Fang"
    ],
    "published": "2025-02-27",
    "abstract": "Landslides are among the most common natural disasters globally, posing\nsignificant threats to human society. Deep learning (DL) has proven to be an\neffective method for rapidly generating landslide inventories in large-scale\ndisaster areas. However, DL models rely heavily on high-quality labeled\nlandslide data for strong feature extraction capabilities. And landslide\ndetection using DL urgently needs a benchmark dataset to evaluate the\ngeneralization ability of the latest models. To solve the above problems, we\nconstruct a Large-scale Multi-source High-resolution Landslide Dataset (LMHLD)\nfor Landslide Detection based on DL. LMHLD collects remote sensing images from\nfive different satellite sensors across seven study areas worldwide: Wenchuan,\nChina (2008); Rio de Janeiro, Brazil (2011); Gorkha, Nepal (2015); Jiuzhaigou,\nChina (2015); Taiwan, China (2018); Hokkaido, Japan (2018); Emilia-Romagna,\nItaly (2023). The dataset includes a total of 25,365 patches, with different\npatch sizes to accommodate different landslide scales. Additionally, a training\nmodule, LMHLDpart, is designed to accommodate landslide detection tasks at\nvarying scales and to alleviate the issue of catastrophic forgetting in\nmulti-task learning. Furthermore, the models trained by LMHLD is applied in\nother datasets to highlight the robustness of LMHLD. Five dataset quality\nevaluation experiments designed by using seven DL models from the U-Net family\ndemonstrate that LMHLD has the potential to become a benchmark dataset for\nlandslide detection. LMHLD is open access and can be accessed through the link:\nhttps://doi.org/10.5281/zenodo.11424988. This dataset provides a strong\nfoundation for DL models, accelerates the development of DL in landslide\ndetection, and serves as a valuable resource for landslide prevention and\nmitigation efforts.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "From underwater to aerial: a novel multi-scale knowledge distillation approach for coral reef monitoring",
    "url": "http://arxiv.org/abs/2502.17883v1",
    "authors": [
      "Matteo Contini",
      "Victor Illien",
      "Julien Barde",
      "Sylvain Poulain",
      "Serge Bernard",
      "Alexis Joly",
      "Sylvain Bonhommeau"
    ],
    "published": "2025-02-25",
    "abstract": "Drone-based remote sensing combined with AI-driven methodologies has shown\ngreat potential for accurate mapping and monitoring of coral reef ecosystems.\nThis study presents a novel multi-scale approach to coral reef monitoring,\nintegrating fine-scale underwater imagery with medium-scale aerial imagery.\nUnderwater images are captured using an Autonomous Surface Vehicle (ASV), while\naerial images are acquired with an aerial drone. A transformer-based\ndeep-learning model is trained on underwater images to detect the presence of\n31 classes covering various coral morphotypes, associated fauna, and habitats.\nThese predictions serve as annotations for training a second model applied to\naerial images. The transfer of information across scales is achieved through a\nweighted footprint method that accounts for partial overlaps between underwater\nimage footprints and aerial image tiles. The results show that the multi-scale\nmethodology successfully extends fine-scale classification to larger reef\nareas, achieving a high degree of accuracy in predicting coral morphotypes and\nassociated habitats. The method showed a strong alignment between\nunderwater-derived annotations and ground truth data, reflected by an AUC (Area\nUnder the Curve) score of 0.9251. This shows that the integration of underwater\nand aerial imagery, supported by deep-learning models, can facilitate scalable\nand accurate reef assessments. This study demonstrates the potential of\ncombining multi-scale imaging and AI to facilitate the monitoring and\nconservation of coral reefs. Our approach leverages the strengths of underwater\nand aerial imagery, ensuring the precision of fine-scale analysis while\nextending it to cover a broader reef area.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Convolutional neural networks for mineral prospecting through alteration mapping with remote sensing data",
    "url": "http://arxiv.org/abs/2502.18533v1",
    "authors": [
      "Ehsan Farahbakhsh",
      "Dakshi Goel",
      "Dhiraj Pimparkar",
      "R. Dietmar Muller",
      "Rohitash Chandra"
    ],
    "published": "2025-02-25",
    "abstract": "Traditional geological mapping, based on field observations and rock sample\nanalysis, is inefficient for continuous spatial mapping of features like\nalteration zones. Deep learning models, such as convolutional neural networks\n(CNNs), have revolutionised remote sensing data analysis by automatically\nextracting features for classification and regression tasks. CNNs can detect\nspecific mineralogical changes linked to mineralisation by identifying subtle\nfeatures in remote sensing data. This study uses CNNs with Landsat 8, Landsat\n9, and ASTER data to map alteration zones north of Broken Hill, New South\nWales, Australia. The model is trained using ground truth data and an automated\napproach with selective principal component analysis (PCA). We compare CNNs\nwith traditional machine learning models, including k-nearest neighbours,\nsupport vector machines, and multilayer perceptron. Results show that ground\ntruth-based training yields more reliable maps, with CNNs slightly\noutperforming conventional models in capturing spatial patterns. Landsat 9\noutperforms Landsat 8 in mapping iron oxide areas using ground truth-trained\nCNNs, while ASTER data provides the most accurate argillic and propylitic\nalteration maps. This highlights CNNs' effectiveness in improving geological\nmapping precision, especially for identifying subtle mineralisation-related\nalterations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "A Deep Learning Framework with Geographic Information Adaptive Loss for Remote Sensing Images based UAV Self-Positioning",
    "url": "http://arxiv.org/abs/2502.16164v1",
    "authors": [
      "Mingkun Li",
      "Ziming Wang",
      "Guang Huo",
      "Wei Chen",
      "Xiaoning Zhao"
    ],
    "published": "2025-02-22",
    "abstract": "With the expanding application scope of unmanned aerial vehicles (UAVs), the\ndemand for stable UAV control has significantly increased. However, in complex\nenvironments, GPS signals are prone to interference, resulting in ineffective\nUAV positioning. Therefore, self-positioning of UAVs in GPS-denied environments\nhas become a critical objective. Some methods obtain geolocation information in\nGPS-denied environments by matching ground objects in the UAV viewpoint with\nremote sensing images. However, most of these methods only provide coarse-level\npositioning, which satisfies cross-view geo-localization but cannot support\nprecise UAV positioning tasks. Consequently, this paper focuses on a newer and\nmore challenging task: precise UAV self-positioning based on remote sensing\nimages. This approach not only considers the features of ground objects but\nalso accounts for the spatial distribution of objects in the images. To address\nthis challenge, we present a deep learning framework with geographic\ninformation adaptive loss, which achieves precise localization by aligning UAV\nimages with corresponding satellite imagery in fine detail through the\nintegration of geographic information from multiple perspectives. To validate\nthe effectiveness of the proposed method, we conducted a series of experiments.\nThe results demonstrate the method's efficacy in enabling UAVs to achieve\nprecise self-positioning using remote sensing imagery.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
    "url": "http://arxiv.org/abs/2502.13407v2",
    "authors": [
      "Ziyuan Liu",
      "Ruifei Zhu",
      "Long Gao",
      "Yuanxiu Zhou",
      "Jingyu Ma",
      "Yuantao Gu"
    ],
    "published": "2025-02-19",
    "abstract": "Deep learning has achieved significant success in the field of remote sensing\nimage change detection (CD), yet two major challenges remain: the scarcity of\nsub-meter, comprehensive open-source CD datasets, and the difficulty of\nachieving consistent and satisfactory detection results across images with\nvarying change areas. To address these issues, we introduce the JL1-CD dataset,\nwhich consists of 5,000 pairs of 512 x 512 pixel images with a resolution of\n0.5 to 0.75 meters. This all-inclusive dataset covers a wide range of\nhuman-induced and natural changes, including buildings, roads, hardened\nsurfaces, woodlands, grasslands, croplands, water bodies, and photovoltaic\npanels, among others. Additionally, we propose a novel multi-teacher knowledge\ndistillation (MTKD) framework that leverages the Origin-Partition (O-P)\nstrategy to enhance CD performance. In the O-P strategy, we partition the\ntraining data based on the Change Area Ratio (CAR) to train separate models for\nsmall, medium, and large CAR values, alleviating the learning burden on each\nmodel and improving their performance within their respective partitions.\nBuilding upon this, our MTKD framework distills knowledge from multiple teacher\nmodels trained on different CAR partitions into a single student model,enabling\nthe student model to achieve superior detection results across diverse CAR\nscenarios without incurring additional computational or time overhead during\nthe inference phase. Experimental results on the JL1-CD and SYSU-CD datasets\ndemonstrate that the MTKD framework significantly improves the performance of\nCD models with various network architectures and parameter sizes, achieving new\nstate-of-the-art results. The JL1-CD dataset and code are available at\nhttps://github.com/circleLZY/MTKD-CD.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "End-to-end pipeline for simultaneous temperature estimation and super resolution of low-cost uncooled infrared camera frames for precision agriculture applications",
    "url": "http://arxiv.org/abs/2502.13985v1",
    "authors": [
      "Navot Oz",
      "Nir Sochen",
      "David Mendlovic",
      "Iftach Klapp"
    ],
    "published": "2025-02-18",
    "abstract": "Radiometric infrared (IR) imaging is a valuable technique for remote-sensing\napplications in precision agriculture, such as irrigation monitoring, crop\nhealth assessment, and yield estimation. Low-cost uncooled non-radiometric IR\ncameras offer new implementations in agricultural monitoring. However, these\ncameras have inherent drawbacks that limit their usability, such as low spatial\nresolution, spatially variant nonuniformity, and lack of radiometric\ncalibration. In this article, we present an end-to-end pipeline for temperature\nestimation and super resolution of frames captured by a low-cost uncooled IR\ncamera. The pipeline consists of two main components: a deep-learning-based\ntemperature-estimation module, and a deep-learning-based super-resolution\nmodule. The temperature-estimation module learns to map the raw gray level IR\nimages to the corresponding temperature maps while also correcting for\nnonuniformity. The super-resolution module uses a deep-learning network to\nenhance the spatial resolution of the IR images by scale factors of x2 and x4.\nWe evaluated the performance of the pipeline on both simulated and real-world\nagricultural datasets composing of roughly 20,000 frames of various crops. For\nthe simulated data, the results were on par with the real-world data with\nsub-degree accuracy. For the real data, the proposed pipeline was compared to a\nhigh-end radiometric thermal camera, and achieved sub-degree accuracy. The\nresults of the real data are on par with the simulated data. The proposed\npipeline can enable various applications in precision agriculture that require\nhigh quality thermal information from low-cost IR cameras.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "When Segmentation Meets Hyperspectral Image: New Paradigm for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2502.12541v1",
    "authors": [
      "Weilian Zhou",
      "Weixuan Xie",
      "Sei-ichiro Kamata",
      "Man Sing Wong",
      "Huiying",
      "Hou",
      "Haipeng Wang"
    ],
    "published": "2025-02-18",
    "abstract": "Hyperspectral image (HSI) classification is a cornerstone of remote sensing,\nenabling precise material and land-cover identification through rich spectral\ninformation. While deep learning has driven significant progress in this task,\nsmall patch-based classifiers, which account for over 90% of the progress, face\nlimitations: (1) the small patch (e.g., 7x7, 9x9)-based sampling approach\nconsiders a limited receptive field, resulting in insufficient spatial\nstructural information critical for object-level identification and noise-like\nmisclassifications even within uniform regions; (2) undefined optimal patch\nsizes lead to coarse label predictions, which degrade performance; and (3) a\nlack of multi-shape awareness around objects. To address these challenges, we\ndraw inspiration from large-scale image segmentation techniques, which excel at\nhandling object boundaries-a capability essential for semantic labeling in HSI\nclassification. However, their application remains under-explored in this task\ndue to (1) the prevailing notion that larger patch sizes degrade performance,\n(2) the extensive unlabeled regions in HSI groundtruth, and (3) the\nmisalignment of input shapes between HSI data and segmentation models. Thus, in\nthis study, we propose a novel paradigm and baseline, HSIseg, for HSI\nclassification that leverages segmentation techniques combined with a novel\nDynamic Shifted Regional Transformer (DSRT) to overcome these challenges. We\nalso introduce an intuitive progressive learning framework with adaptive\npseudo-labeling to iteratively incorporate unlabeled regions into the training\nprocess, thereby advancing the application of segmentation techniques.\nAdditionally, we incorporate auxiliary data through multi-source data\ncollaboration, promoting better feature interaction. Validated on five public\nHSI datasets, our proposal outperforms state-of-the-art methods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Multispectral Remote Sensing for Weed Detection in West Australian Agricultural Lands",
    "url": "http://arxiv.org/abs/2502.08678v1",
    "authors": [
      "Haitian Wang",
      "Muhammad Ibrahim",
      "Yumeng Miao",
      "D ustin Severtson",
      "Atif Mansoor",
      "Ajmal S. Mian"
    ],
    "published": "2025-02-12",
    "abstract": "The Kondinin region in Western Australia faces significant agricultural\nchallenges due to pervasive weed infestations, causing economic losses and\necological impacts. This study constructs a tailored multispectral remote\nsensing dataset and an end-to-end framework for weed detection to advance\nprecision agriculture practices. Unmanned aerial vehicles were used to collect\nraw multispectral data from two experimental areas (E2 and E8) over four years,\ncovering 0.6046 km^{2} and ground truth annotations were created with\nGPS-enabled vehicles to manually label weeds and crops. The dataset is\nspecifically designed for agricultural applications in Western Australia. We\npropose an end-to-end framework for weed detection that includes extensive\npreprocessing steps, such as denoising, radiometric calibration, image\nalignment, orthorectification, and stitching. The proposed method combines\nvegetation indices (NDVI, GNDVI, EVI, SAVI, MSAVI) with multispectral channels\nto form classification features, and employs several deep learning models to\nidentify weeds based on the input features. Among these models, ResNet achieves\nthe highest performance, with a weed detection accuracy of 0.9213, an F1-Score\nof 0.8735, an mIOU of 0.7888, and an mDC of 0.8865, validating the efficacy of\nthe dataset and the proposed weed detection method.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Simultaneous Automatic Picking and Manual Picking Refinement for First-Break",
    "url": "http://arxiv.org/abs/2502.01474v1",
    "authors": [
      "Haowen Bai",
      "Zixiang Zhao",
      "Jiangshe Zhang",
      "Yukun Cui",
      "Chunxia Zhang",
      "Zhenbo Guo",
      "Yongjun Wang"
    ],
    "published": "2025-02-03",
    "abstract": "First-break picking is a pivotal procedure in processing microseismic data\nfor geophysics and resource exploration. Recent advancements in deep learning\nhave catalyzed the evolution of automated methods for identifying first-break.\nNevertheless, the complexity of seismic data acquisition and the requirement\nfor detailed, expert-driven labeling often result in outliers and potential\nmislabeling within manually labeled datasets. These issues can negatively\naffect the training of neural networks, necessitating algorithms that handle\noutliers or mislabeled data effectively. We introduce the Simultaneous Picking\nand Refinement (SPR) algorithm, designed to handle datasets plagued by outlier\nsamples or even noisy labels. Unlike conventional approaches that regard manual\npicks as ground truth, our method treats the true first-break as a latent\nvariable within a probabilistic model that includes a first-break labeling\nprior. SPR aims to uncover this variable, enabling dynamic adjustments and\nimproved accuracy across the dataset. This strategy mitigates the impact of\noutliers or inaccuracies in manual labels. Intra-site picking experiments and\ncross-site generalization experiments on publicly available data confirm our\nmethod's performance in identifying first-break and its generalization across\ndifferent sites. Additionally, our investigations into noisy signals and labels\nunderscore SPR's resilience to both types of noise and its capability to refine\nmisaligned manual annotations. Moreover, the flexibility of SPR, not being\nlimited to any single network architecture, enhances its adaptability across\nvarious deep learning-based picking methods. Focusing on learning from data\nthat may contain outliers or partial inaccuracies, SPR provides a robust\nsolution to some of the principal obstacles in automatic first-break picking.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "General Feature Extraction In SAR Target Classification: A Contrastive Learning Approach Across Sensor Types",
    "url": "http://arxiv.org/abs/2502.01162v1",
    "authors": [
      "M. Muzeau",
      "J. Frontera-Pons",
      "Chengfang Ren",
      "J. -P. Ovarlez"
    ],
    "published": "2025-02-03",
    "abstract": "The increased availability of SAR data has raised a growing interest in\napplying deep learning algorithms. However, the limited availability of labeled\ndata poses a significant challenge for supervised training. This article\nintroduces a new method for classifying SAR data with minimal labeled images.\nThe method is based on a feature extractor Vit trained with contrastive\nlearning. It is trained on a dataset completely different from the one on which\nclassification is made. The effectiveness of the method is assessed through 2D\nvisualization using t-SNE for qualitative evaluation and k-NN classification\nwith a small number of labeled data for quantitative evaluation. Notably, our\nresults outperform a k-NN on data processed with PCA and a ResNet-34\nspecifically trained for the task, achieving a 95.9% accuracy on the MSTAR\ndataset with just ten labeled images per class.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "ResNet",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Multi-Resolution SAR and Optical Remote Sensing Image Registration Methods: A Review, Datasets, and Future Perspectives",
    "url": "http://arxiv.org/abs/2502.01002v1",
    "authors": [
      "Wenfei Zhang",
      "Ruipeng Zhao",
      "Yongxiang Yao",
      "Yi Wan",
      "Peihao Wu",
      "Jiayuan Li",
      "Yansheng Li",
      "Yongjun Zhang"
    ],
    "published": "2025-02-03",
    "abstract": "Synthetic Aperture Radar (SAR) and optical image registration is essential\nfor remote sensing data fusion, with applications in military reconnaissance,\nenvironmental monitoring, and disaster management. However, challenges arise\nfrom differences in imaging mechanisms, geometric distortions, and radiometric\nproperties between SAR and optical images. As image resolution increases, fine\nSAR textures become more significant, leading to alignment issues and 3D\nspatial discrepancies. Two major gaps exist: the lack of a publicly available\nmulti-resolution, multi-scene registration dataset and the absence of\nsystematic analysis of current methods. To address this, the MultiResSAR\ndataset was created, containing over 10k pairs of multi-source,\nmulti-resolution, and multi-scene SAR and optical images. Sixteen\nstate-of-the-art algorithms were tested. Results show no algorithm achieves\n100% success, and performance decreases as resolution increases, with most\nfailing on sub-meter data. XoFTR performs best among deep learning methods\n(40.58%), while RIFT performs best among traditional methods (66.51%). Future\nresearch should focus on noise suppression, 3D geometric fusion, cross-view\ntransformation modeling, and deep learning optimization for robust registration\nof high-resolution SAR and optical images. The dataset is available at\nhttps://github.com/betterlll/Multi-Resolution-SAR-dataset-.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Exploring Linear Attention Alternative for Single Image Super-Resolution",
    "url": "http://arxiv.org/abs/2502.00404v1",
    "authors": [
      "Rongchang Lu",
      "Changyu Li",
      "Donghang Li",
      "Guojing Zhang",
      "Jianqiang Huang",
      "Xilai Li"
    ],
    "published": "2025-02-01",
    "abstract": "Deep learning-based single-image super-resolution (SISR) technology focuses\non enhancing low-resolution (LR) images into high-resolution (HR) ones.\nAlthough significant progress has been made, challenges remain in computational\ncomplexity and quality, particularly in remote sensing image processing. To\naddress these issues, we propose our Omni-Scale RWKV Super-Resolution\n(OmniRWKVSR) model which presents a novel approach that combines the Receptance\nWeighted Key Value (RWKV) architecture with feature extraction techniques such\nas Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM),\naiming to overcome the limitations of existing methods and achieve superior\nSISR performance. This work has proved able to provide effective solutions for\nhigh-quality image reconstruction. Under the 4x Super-Resolution tasks,\ncompared to the MambaIR model, we achieved an average improvement of 0.26% in\nPSNR and 0.16% in SSIM.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Ground Awareness in Deep Learning for Large Outdoor Point Cloud Segmentation",
    "url": "http://arxiv.org/abs/2501.18246v1",
    "authors": [
      "Kevin Qiu",
      "Dimitri Bulatov",
      "Dorota Iwaszczuk"
    ],
    "published": "2025-01-30",
    "abstract": "This paper presents an analysis of utilizing elevation data to aid outdoor\npoint cloud semantic segmentation through existing machine-learning networks in\nremote sensing, specifically in urban, built-up areas. In dense outdoor point\nclouds, the receptive field of a machine learning model may be too small to\naccurately determine the surroundings and context of a point. By computing\nDigital Terrain Models (DTMs) from the point clouds, we extract the relative\nelevation feature, which is the vertical distance from the terrain to a point.\nRandLA-Net is employed for efficient semantic segmentation of large-scale point\nclouds. We assess its performance across three diverse outdoor datasets\ncaptured with varying sensor technologies and sensor locations. Integration of\nrelative elevation data leads to consistent performance improvements across all\nthree datasets, most notably in the Hessigheim dataset, with an increase of 3.7\npercentage points in average F1 score from 72.35% to 76.01%, by establishing\nlong-range dependencies between ground and objects. We also explore additional\nlocal features such as planarity, normal vectors, and 2D features, but their\nefficacy varied based on the characteristics of the point cloud. Ultimately,\nthis study underscores the important role of the non-local relative elevation\nfeature for semantic segmentation of point clouds in remote sensing\napplications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "AdaSemSeg: An Adaptive Few-shot Semantic Segmentation of Seismic Facies",
    "url": "http://arxiv.org/abs/2501.16760v1",
    "authors": [
      "Surojit Saha",
      "Ross Whitaker"
    ],
    "published": "2025-01-28",
    "abstract": "Automated interpretation of seismic images using deep learning methods is\nchallenging because of the limited availability of training data. Few-shot\nlearning is a suitable learning paradigm in such scenarios due to its ability\nto adapt to a new task with limited supervision (small training budget).\nExisting few-shot semantic segmentation (FSSS) methods fix the number of target\nclasses. Therefore, they do not support joint training on multiple datasets\nvarying in the number of classes. In the context of the interpretation of\nseismic facies, fixing the number of target classes inhibits the generalization\ncapability of a model trained on one facies dataset to another, which is likely\nto have a different number of facies. To address this shortcoming, we propose a\nfew-shot semantic segmentation method for interpreting seismic facies that can\nadapt to the varying number of facies across the dataset, dubbed the AdaSemSeg.\nIn general, the backbone network of FSSS methods is initialized with the\nstatistics learned from the ImageNet dataset for better performance. The lack\nof such a huge annotated dataset for seismic images motivates using a\nself-supervised algorithm on seismic datasets to initialize the backbone\nnetwork. We have trained the AdaSemSeg on three public seismic facies datasets\nwith different numbers of facies and evaluated the proposed method on multiple\nmetrics. The performance of the AdaSemSeg on unseen datasets (not used in\ntraining) is better than the prototype-based few-shot method and baselines.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP",
    "url": "http://arxiv.org/abs/2501.16222v2",
    "authors": [
      "Li Pang",
      "Jing Yao",
      "Kaiyu Li",
      "Xiangyong Cao"
    ],
    "published": "2025-01-27",
    "abstract": "Hyperspectral image (HSI) classification aims at categorizing each pixel in\nan HSI into a specific land cover class, which is crucial for applications like\nremote sensing, environmental monitoring, and agriculture. Although deep\nlearning-based HSI classification methods have achieved significant\nadvancements, existing methods still rely on manually labeled data for\ntraining, which is both time-consuming and labor-intensive. To address this\nlimitation, we introduce a novel zero-shot hyperspectral image classification\nframework based on CLIP (SPECIAL), aiming to eliminate the need for manual\nannotations. The SPECIAL framework consists of two main stages: (1) CLIP-based\npseudo-label generation, and (2) noisy label learning. In the first stage, HSI\nis spectrally interpolated to produce RGB bands. These bands are subsequently\nclassified using CLIP, resulting in noisy pseudo-labels that are accompanied by\nconfidence scores. To improve the quality of these labels, we propose a scaling\nstrategy that fuses predictions from multiple spatial scales. In the second\nstage, spectral information and a label refinement technique are incorporated\nto mitigate label noise and further enhance classification accuracy.\nExperimental results on three benchmark datasets demonstrate that our SPECIAL\noutperforms existing methods in zero-shot HSI classification, showing its\npotential for more practical applications. The code is available at\nhttps://github.com/LiPang/SPECIAL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Semi-Supervised Learning for AVO Inversion with Strong Spatial Feature Constraints",
    "url": "http://arxiv.org/abs/2501.15473v2",
    "authors": [
      "Yingtian Liu",
      "Yong Li",
      "Junheng Peng",
      "Mingwei Wang"
    ],
    "published": "2025-01-26",
    "abstract": "One-dimensional convolution is a widely used deep learning technique in\nprestack amplitude variation with offset (AVO) inversion; however, it lacks\nlateral continuity. Although two-dimensional convolution improves lateral\ncontinuity, due to the sparsity of well-log data, the model only learns weak\nspatial features and fails to explore the spatial correlations in seismic data\nfully. To overcome these challenges, we propose a novel AVO inversion method\nbased on semi-supervised learning with strong spatial feature constraints\n(SSFC-SSL). First, two-dimensional predicted values are obtained through the\ninversion network, and the predicted values at well locations are sparsely\nrepresented using well-log labels. Subsequently, a label-annihilation operator\nis introduced, enabling the predicted values at non-well locations to learn the\nspatial features of well locations through the neural network. Ultimately, a\ntwo-way strong spatial feature mapping between non-well locations and well\nlocations is achieved. Additionally, to reduce the dependence on well-log\nlabels, we combine the semi-supervised learning strategy with a low-frequency\nmodel, further enhancing the robustness of the method. Experimental results on\nboth synthetic example and field data demonstrate that the proposed method\nsignificantly improves lateral continuity and inversion accuracy compared to\none- and two-dimensional deep learning techniques.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void Filling",
    "url": "http://arxiv.org/abs/2501.15440v2",
    "authors": [
      "Daniel Panangian",
      "Ksenia Bittner"
    ],
    "published": "2025-01-26",
    "abstract": "Digital Surface Models (DSMs) are essential for accurately representing\nEarth's topography in geospatial analyses. DSMs capture detailed elevations of\nnatural and manmade features, crucial for applications like urban planning,\nvegetation studies, and 3D reconstruction. However, DSMs derived from stereo\nsatellite imagery often contain voids or missing data due to occlusions,\nshadows, and lowsignal areas. Previous studies have primarily focused on void\nfilling for digital elevation models (DEMs) and Digital Terrain Models (DTMs),\nemploying methods such as inverse distance weighting (IDW), kriging, and spline\ninterpolation. While effective for simpler terrains, these approaches often\nfail to handle the intricate structures present in DSMs. To overcome these\nlimitations, we introduce Dfilled, a guided DSM void filling method that\nleverages optical remote sensing images through edge-enhancing diffusion.\nDfilled repurposes deep anisotropic diffusion models, which originally designed\nfor super-resolution tasks, to inpaint DSMs. Additionally, we utilize Perlin\nnoise to create inpainting masks that mimic natural void patterns in DSMs.\nExperimental evaluations demonstrate that Dfilled surpasses traditional\ninterpolation methods and deep learning approaches in DSM void filling tasks.\nBoth quantitative and qualitative assessments highlight the method's ability to\nmanage complex features and deliver accurate, visually coherent results.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "CP2M: Clustered-Patch-Mixed Mosaic Augmentation for Aerial Image Segmentation",
    "url": "http://arxiv.org/abs/2501.15389v1",
    "authors": [
      "Yijie Li",
      "Hewei Wang",
      "Jinfeng Xu",
      "Zixiao Ma",
      "Puzhen Wu",
      "Shaofan Wang",
      "Soumyabrata Dev"
    ],
    "published": "2025-01-26",
    "abstract": "Remote sensing image segmentation is pivotal for earth observation,\nunderpinning applications such as environmental monitoring and urban planning.\nDue to the limited annotation data available in remote sensing images, numerous\nstudies have focused on data augmentation as a means to alleviate overfitting\nin deep learning networks. However, some existing data augmentation strategies\nrely on simple transformations that may not sufficiently enhance data diversity\nor model generalization capabilities. This paper proposes a novel augmentation\nstrategy, Clustered-Patch-Mixed Mosaic (CP2M), designed to address these\nlimitations. CP2M integrates a Mosaic augmentation phase with a clustered patch\nmix phase. The former stage constructs a new sample from four random samples,\nwhile the latter phase uses the connected component labeling algorithm to\nensure the augmented data maintains spatial coherence and avoids introducing\nirrelevant semantics when pasting random patches. Our experiments on the ISPRS\nPotsdam dataset demonstrate that CP2M substantially mitigates overfitting,\nsetting new benchmarks for segmentation accuracy and model robustness in remote\nsensing tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Snapshot multi-spectral imaging through defocusing and a Fourier imager network",
    "url": "http://arxiv.org/abs/2501.14287v1",
    "authors": [
      "Xilin Yang",
      "Michael John Fanous",
      "Hanlong Chen",
      "Ryan Lee",
      "Paloma Casteleiro Costa",
      "Yuhang Li",
      "Luzhe Huang",
      "Yijie Zhang",
      "Aydogan Ozcan"
    ],
    "published": "2025-01-24",
    "abstract": "Multi-spectral imaging, which simultaneously captures the spatial and\nspectral information of a scene, is widely used across diverse fields,\nincluding remote sensing, biomedical imaging, and agricultural monitoring.\nHere, we introduce a snapshot multi-spectral imaging approach employing a\nstandard monochrome image sensor with no additional spectral filters or\ncustomized components. Our system leverages the inherent chromatic aberration\nof wavelength-dependent defocusing as a natural source of physical encoding of\nmulti-spectral information; this encoded image information is rapidly decoded\nvia a deep learning-based multi-spectral Fourier Imager Network (mFIN). We\nexperimentally tested our method with six illumination bands and demonstrated\nan overall accuracy of 92.98% for predicting the illumination channels at the\ninput and achieved a robust multi-spectral image reconstruction on various test\nobjects. This deep learning-powered framework achieves high-quality\nmulti-spectral image reconstruction using snapshot image acquisition with a\nmonochrome image sensor and could be useful for applications in biomedicine,\nindustrial quality control, and agriculture, among others.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ATRNet-STAR: A Large Dataset and Benchmark Towards Remote Sensing Object Recognition in the Wild",
    "url": "http://arxiv.org/abs/2501.13354v4",
    "authors": [
      "Yongxiang Liu",
      "Weijie Li",
      "Li Liu",
      "Jie Zhou",
      "Bowen Peng",
      "Yafei Song",
      "Xuying Xiong",
      "Wei Yang",
      "Tianpeng Liu",
      "Zhen Liu",
      "Xiang Li"
    ],
    "published": "2025-01-23",
    "abstract": "The absence of publicly available, large-scale, high-quality datasets for\nSynthetic Aperture Radar Automatic Target Recognition (SAR ATR) has\nsignificantly hindered the application of rapidly advancing deep learning\ntechniques, which hold huge potential to unlock new capabilities in this field.\nThis is primarily because collecting large volumes of diverse target samples\nfrom SAR images is prohibitively expensive, largely due to privacy concerns,\nthe characteristics of microwave radar imagery perception, and the need for\nspecialized expertise in data annotation. Throughout the history of SAR ATR\nresearch, there have been only a number of small datasets, mainly including\ntargets like ships, airplanes, buildings, etc. There is only one vehicle\ndataset MSTAR collected in the 1990s, which has been a valuable source for SAR\nATR. To fill this gap, this paper introduces a large-scale, new dataset named\nATRNet-STAR with 40 different vehicle categories collected under various\nrealistic imaging conditions and scenes. It marks a substantial advancement in\ndataset scale and diversity, comprising over 190,000 well-annotated samples, 10\ntimes larger than its predecessor, the famous MSTAR. Building such a large\ndataset is a challenging task, and the data collection scheme will be detailed.\nSecondly, we illustrate the value of ATRNet-STAR via extensively evaluating the\nperformance of 15 representative methods with 7 different experimental settings\non challenging classification and detection benchmarks derived from the\ndataset. Finally, based on our extensive experiments, we identify valuable\ninsights for SAR ATR and discuss potential future research directions in this\nfield. We hope that the scale, diversity, and benchmark of ATRNet-STAR can\nsignificantly facilitate the advancement of SAR ATR.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Progressive Cross Attention Network for Flood Segmentation using Multispectral Satellite Imagery",
    "url": "http://arxiv.org/abs/2501.11923v1",
    "authors": [
      "Vicky Feliren",
      "Fithrothul Khikmah",
      "Irfan Dwiki Bhaswara",
      "Bahrul I. Nasution",
      "Alex M. Lechner",
      "Muhamad Risqi U. Saputra"
    ],
    "published": "2025-01-21",
    "abstract": "In recent years, the integration of deep learning techniques with remote\nsensing technology has revolutionized the way natural hazards, such as floods,\nare monitored and managed. However, existing methods for flood segmentation\nusing remote sensing data often overlook the utility of correlative features\namong multispectral satellite information. In this study, we introduce a\nprogressive cross attention network (ProCANet), a deep learning model that\nprogressively applies both self- and cross-attention mechanisms to\nmultispectral features, generating optimal feature combinations for flood\nsegmentation. The proposed model was compared with state-of-the-art approaches\nusing Sen1Floods11 dataset and our bespoke flood data generated for the Citarum\nRiver basin, Indonesia. Our model demonstrated superior performance with the\nhighest Intersection over Union (IoU) score of 0.815. Our results in this\nstudy, coupled with the ablation assessment comparing scenarios with and\nwithout attention across various modalities, opens a promising path for\nenhancing the accuracy of flood analysis using remote sensing technology.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "On the Adversarial Vulnerabilities of Transfer Learning in Remote Sensing",
    "url": "http://arxiv.org/abs/2501.11462v1",
    "authors": [
      "Tao Bai",
      "Xingjian Tian",
      "Yonghao Xu",
      "Bihan Wen"
    ],
    "published": "2025-01-20",
    "abstract": "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Few-shot Human Motion Recognition through Multi-Aspect mmWave FMCW Radar Data",
    "url": "http://arxiv.org/abs/2501.11028v1",
    "authors": [
      "Hao Fan",
      "Lingfeng Chen",
      "Chengbai Xu",
      "Jiadong Zhou",
      "Yongpeng Dai",
      "Panhe HU"
    ],
    "published": "2025-01-19",
    "abstract": "Radar human motion recognition methods based on deep learning models has been\na heated spot of remote sensing in recent years, yet the existing methods are\nmostly radial-oriented. In practical application, the test data could be\nmulti-aspect and the sample number of each motion could be very limited,\ncausing model overfitting and reduced recognition accuracy. This paper proposed\nchannel-DN4, a multi-aspect few-shot human motion recognition method. First,\nlocal descriptors are introduced for a precise classification metric. Moreover,\nepisodic training strategy was adopted to reduce model overfitting. To utilize\nthe invariant sematic information in multi-aspect conditions, we considered\nchannel attention after the embedding network to obtain precise implicit\nhigh-dimensional representation of sematic information. We tested the\nperformance of channel-DN4 and methods for comparison on measured mmWave FMCW\nradar data. The proposed channel-DN4 produced competitive and convincing\nresults, reaching the highest 87.533% recognition accuracy in 3-way 10-shot\ncondition while other methods suffer from overfitting. Codes are available at:\nhttps://github.com/MountainChenCad/channel-DN4",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "A Remote Sensing Image Change Detection Method Integrating Layer Exchange and Channel-Spatial Differences",
    "url": "http://arxiv.org/abs/2501.10905v1",
    "authors": [
      "Sijun Dong",
      "Fangcheng Zuo",
      "Geng Chen",
      "Siming Fu",
      "Xiaoliang Meng"
    ],
    "published": "2025-01-19",
    "abstract": "Change detection in remote sensing imagery is a critical technique for Earth\nobservation, primarily focusing on pixel-level segmentation of change regions\nbetween bi-temporal images. The essence of pixel-level change detection lies in\ndetermining whether corresponding pixels in bi-temporal images have changed. In\ndeep learning, the spatial and channel dimensions of feature maps represent\ndifferent information from the original images. In this study, we found that in\nchange detection tasks, difference information can be computed not only from\nthe spatial dimension of bi-temporal features but also from the channel\ndimension. Therefore, we designed the Channel-Spatial Difference Weighting\n(CSDW) module as an aggregation-distribution mechanism for bi-temporal features\nin change detection. This module enhances the sensitivity of the change\ndetection model to difference features. Additionally, bi-temporal images share\nthe same geographic location and exhibit strong inter-image correlations. To\nconstruct the correlation between bi-temporal images, we designed a decoding\nstructure based on the Layer-Exchange (LE) method to enhance the interaction of\nbi-temporal features. Comprehensive experiments on the CLCD, PX-CLCD, LEVIR-CD,\nand S2Looking datasets demonstrate that the proposed LENet model significantly\nimproves change detection performance. The code and pre-trained models will be\navailable at: https://github.com/dyzy41/lenet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "EarthView: A Large Scale Remote Sensing Dataset for Self-Supervision",
    "url": "http://arxiv.org/abs/2501.08111v1",
    "authors": [
      "Diego Velazquez",
      "Pau Rodriguez L\u00f3pez",
      "Sergio Alonso",
      "Josep M. Gonfaus",
      "Jordi Gonzalez",
      "Gerardo Richarte",
      "Javier Marin",
      "Yoshua Bengio",
      "Alexandre Lacoste"
    ],
    "published": "2025-01-14",
    "abstract": "This paper presents EarthView, a comprehensive dataset specifically designed\nfor self-supervision on remote sensing data, intended to enhance deep learning\napplications on Earth monitoring tasks. The dataset spans 15 tera pixels of\nglobal remote-sensing data, combining imagery from a diverse range of sources,\nincluding NEON, Sentinel, and a novel release of 1m spatial resolution data\nfrom Satellogic. Our dataset provides a wide spectrum of image data with\nvarying resolutions, harnessed from different sensors and organized coherently\ninto an accessible HuggingFace dataset in parquet format. This data spans five\nyears, from 2017 to 2022. Accompanying the dataset, we introduce EarthMAE, a\ntailored Masked Autoencoder, developed to tackle the distinct challenges of\nremote sensing data. Trained in a self-supervised fashion, EarthMAE effectively\nprocesses different data modalities such as hyperspectral, multispectral,\ntopographical data, segmentation maps, and temporal structure. This model helps\nus show that pre-training on Satellogic data improves performance on downstream\ntasks. While there is still a gap to fill in MAE for heterogeneous data, we\nregard this innovative combination of an expansive, diverse dataset and a\nversatile model adapted for self-supervised learning as a stride forward in\ndeep learning for Earth monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "UCloudNet: A Residual U-Net with Deep Supervision for Cloud Image Segmentation",
    "url": "http://arxiv.org/abs/2501.06440v1",
    "authors": [
      "Yijie Li",
      "Hewei Wang",
      "Shaofan Wang",
      "Yee Hui Lee",
      "Muhammad Salman Pathan",
      "Soumyabrata Dev"
    ],
    "published": "2025-01-11",
    "abstract": "Recent advancements in meteorology involve the use of ground-based sky\ncameras for cloud observation. Analyzing images from these cameras helps in\ncalculating cloud coverage and understanding atmospheric phenomena.\nTraditionally, cloud image segmentation relied on conventional computer vision\ntechniques. However, with the advent of deep learning, convolutional neural\nnetworks (CNNs) are increasingly applied for this purpose. Despite their\neffectiveness, CNNs often require many epochs to converge, posing challenges\nfor real-time processing in sky camera systems. In this paper, we introduce a\nresidual U-Net with deep supervision for cloud segmentation which provides\nbetter accuracy than previous approaches, and with less training consumption.\nBy utilizing residual connection in encoders of UCloudNet, the feature\nextraction ability is further improved.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction",
    "url": "http://arxiv.org/abs/2501.04487v1",
    "authors": [
      "Guofeng Yang",
      "Nanfei Jin",
      "Wenjie Ai",
      "Zhonghua Zheng",
      "Yuhong He",
      "Yong He"
    ],
    "published": "2025-01-08",
    "abstract": "Yield is one of the core goals of crop breeding. By predicting the potential\nyield of different breeding materials, breeders can screen these materials at\nvarious growth stages to select the best performing. Based on unmanned aerial\nvehicle remote sensing technology, high-throughput crop phenotyping data in\nbreeding areas is collected to provide data support for the breeding decisions\nof breeders. However, the accuracy of current yield predictions still requires\nimprovement, and the usability and user-friendliness of yield forecasting tools\nremain suboptimal. To address these challenges, this study introduces a hybrid\nmethod and tool for crop yield prediction, designed to allow breeders to\ninteractively and accurately predict wheat yield by chatting with a large\nlanguage model (LLM). First, the newly designed data assimilation algorithm is\nused to assimilate the leaf area index into the WOFOST model. Then, selected\noutputs from the assimilation process, along with remote sensing inversion\nresults, are used to drive the time-series temporal fusion transformer model\nfor wheat yield prediction. Finally, based on this hybrid method and leveraging\nan LLM with retrieval augmented generation technology, we developed an\ninteractive yield prediction Web tool that is user-friendly and supports\nsustainable data updates. This tool integrates multi-source data to assist\nbreeding decision-making. This study aims to accelerate the identification of\nhigh-yield materials in the breeding process, enhance breeding efficiency, and\nenable more scientific and smart breeding decisions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Rapid Automated Mapping of Clouds on Titan With Instance Segmentation",
    "url": "http://arxiv.org/abs/2501.04459v1",
    "authors": [
      "Zachary Yahn",
      "Douglas M Trent",
      "Ethan Duncan",
      "Beno\u00eet Seignovert",
      "John Santerre",
      "Conor Nixon"
    ],
    "published": "2025-01-08",
    "abstract": "Despite widespread adoption of deep learning models to address a variety of\ncomputer vision tasks, planetary science has yet to see extensive utilization\nof such tools to address its unique problems. On Titan, the largest moon of\nSaturn, tracking seasonal trends and weather patterns of clouds provides\ncrucial insights into one of the most complex climates in the Solar System, yet\nmuch of the available image data are still analyzed in a conventional way. In\nthis work, we apply a Mask R-CNN trained via transfer learning to perform\ninstance segmentation of clouds in Titan images acquired by the Cassini\nspacecraft - a previously unexplored approach to a big data problem in\nplanetary science. We demonstrate that an automated technique can provide\nquantitative measures for clouds, such as areas and centroids, that may\notherwise be prohibitively time-intensive to produce by human mapping.\nFurthermore, despite Titan specific challenges, our approach yields accuracy\ncomparable to contemporary cloud identification studies on Earth and other\nworlds. We compare the efficiencies of human-driven versus algorithmic\napproaches, showing that transfer learning provides speed-ups that may open new\nhorizons for data investigation for Titan. Moreover, we suggest that such\napproaches have broad potential for application to similar problems in\nplanetary science where they are currently under-utilized. Future planned\nmissions to the planets and remote sensing initiatives for the Earth promise to\nprovide a deluge of image data in the coming years that will benefit strongly\nfrom leveraging machine learning approaches to perform the analysis.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Segmentation",
      "Tracking"
    ]
  },
  {
    "title": "Generalization-Enhanced Few-Shot Object Detection in Remote Sensing",
    "url": "http://arxiv.org/abs/2501.02474v1",
    "authors": [
      "Hui Lin",
      "Nan Li",
      "Pengjuan Yao",
      "Kexin Dong",
      "Yuhan Guo",
      "Danfeng Hong",
      "Ying Zhang",
      "Congcong Wen"
    ],
    "published": "2025-01-05",
    "abstract": "Remote sensing object detection is particularly challenging due to the high\nresolution, multi-scale features, and diverse ground object characteristics\ninherent in satellite and UAV imagery. These challenges necessitate more\nadvanced approaches for effective object detection in such environments. While\ndeep learning methods have achieved remarkable success in remote sensing object\ndetection, they typically rely on large amounts of labeled data. Acquiring\nsufficient labeled data, particularly for novel or rare objects, is both\nchallenging and time-consuming in remote sensing scenarios, limiting the\ngeneralization capabilities of existing models. To address these challenges,\nfew-shot learning (FSL) has emerged as a promising approach, aiming to enable\nmodels to learn new classes from limited labeled examples. Building on this\nconcept, few-shot object detection (FSOD) specifically targets object detection\nchallenges in data-limited conditions. However, the generalization capability\nof FSOD models, particularly in remote sensing, is often constrained by the\ncomplex and diverse characteristics of the objects present in such\nenvironments. In this paper, we propose the Generalization-Enhanced Few-Shot\nObject Detection (GE-FSOD) model to improve the generalization capability in\nremote sensing FSOD tasks. Our model introduces three key innovations: the\nCross-Level Fusion Pyramid Attention Network (CFPAN) for enhanced multi-scale\nfeature representation, the Multi-Stage Refinement Region Proposal Network\n(MRRPN) for more accurate region proposals, and the Generalized Classification\nLoss (GCL) for improved classification performance in few-shot scenarios.\nExtensive experiments on the DIOR and NWPU VHR-10 datasets show that our model\nachieves state-of-the-art performance for few-shot object detection in remote\nsensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Deep learning selection of analogues for Mars landing sites in the Qaidam Basin, Qinghai-Tibet Plateau",
    "url": "http://arxiv.org/abs/2501.08584v1",
    "authors": [
      "Fanwei Meng",
      "Xiaopeng Wang",
      "Andr\u00e9 Antunes",
      "Jie Zhao",
      "Guoliang Zhou",
      "Biqiong Wu",
      "Tianqi Hao"
    ],
    "published": "2024-12-31",
    "abstract": "Remote sensing observations and Mars rover missions have recorded the\npresence of beaches, salt lakes, and wind erosion landforms in Martian\nsediments. All these observations indicate that Mars was hydrated in its early\nhistory. There used to be oceans on Mars, but they have now dried up.\nTherefore, signs of previous life on Mars could be preserved in the evaporites\nformed during this process. The study of evaporite regions has thus become a\npriority area for Mars' life exploration. This study proposes a method for\ntraining similarity metrics from surface land image data of Earth and Mars,\nwhich can be used for recognition or validation applications. The method will\nbe applied in simulating tasks to select Mars landing sites using a selecting\nsmall-scale area of the Mars analaogue the evaporite region of Qaidam Basin,\nQinghai-Tibet Plateau. This learning process minimizes discriminative loss\nfunction, which makes the similarity measure smaller for images from the same\nlocation and larger for images from different locations. This study selected a\nConvolutional Neural Networks (CNN) based model, which has been trained to\nexplain various changes in image appearance and identify different landforms in\nMars. By identifying different landforms, priority landing sites on Mars can be\nselected.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation",
    "url": "http://arxiv.org/abs/2412.19492v1",
    "authors": [
      "Chengyang Ye",
      "Yunzhi Zhuge",
      "Pingping Zhang"
    ],
    "published": "2024-12-27",
    "abstract": "Recently, deep learning based methods have revolutionized remote sensing\nimage segmentation. However, these methods usually rely on a pre-defined\nsemantic class set, thus needing additional image annotation and model training\nwhen adapting to new classes. More importantly, they are unable to segment\narbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote\nSensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary\nsemantic classes in remote sensing images. To address the lack of OVRSISS\ndatasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images\ncovering 40 diverse semantic classes. In addition, we propose a novel framework\nnamed GSNet that integrates domain priors from special remote sensing models\nand versatile capabilities of general vision-language models. Technically,\nGSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature\nFusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE\nfirst captures comprehensive features from both special models and general\nmodels in dual streams. Then, with the guidance of variable vocabularies, QGFF\nintegrates specialist and generalist features, enabling them to complement each\nother. Finally, RIPD is proposed to aggregate multi-source features for more\naccurate mask predictions. Experiments show that our method outperforms other\nmethods by a large margin, and our proposed LandDiscover50K improves the\nperformance of OVRSISS methods. The proposed dataset and method will be made\npublicly available at https://github.com/yecy749/GSNet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Mask Approximation Net: A Novel Diffusion Model Approach for Remote Sensing Change Captioning",
    "url": "http://arxiv.org/abs/2412.19179v2",
    "authors": [
      "Dongwei Sun",
      "Jing Yao",
      "Changsheng Zhou",
      "Xiangyong Cao",
      "Pedram Ghamisi"
    ],
    "published": "2024-12-26",
    "abstract": "Remote sensing image change description represents an innovative multimodal\ntask within the realm of remote sensing processing. This task not only\nfacilitates the detection of alterations in surface conditions, but also\nprovides comprehensive descriptions of these changes, thereby improving human\ninterpretability and interactivity.Generally, existing deep-learning-based\nmethods predominantly utilized a three-stage framework that successively\nperform feature extraction, feature fusion, and localization from bitemporal\nimages before text generation. However, this reliance often leads to an\nexcessive focus on the design of specific network architectures and restricts\nthe feature distributions to the dataset at hand, which in turn results in\nlimited generalizability and robustness during application.To address these\nlimitations, this paper proposes a novel approach for remote sensing image\nchange detection and description that incorporates diffusion models, aiming to\ntransition the emphasis of modeling paradigms from conventional feature\nlearning to data distribution learning. The proposed method primarily includes\na simple multi-scale change detection module, whose output features are\nsubsequently refined by an well-designed diffusion model. Furthermore, we\nintroduce a frequency-guided complex filter module to boost the model\nperformance by managing high-frequency noise throughout the diffusion process.\nWe validate the effectiveness of our proposed method across several datasets\nfor remote sensing change detection and description, showcasing its superior\nperformance compared to existing techniques. The code will be available at\n\\href{https://github.com/sundongwei}{MaskApproxNet} after a possible\npublication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Fusion of Deep Learning and GIS for Advanced Remote Sensing Image Analysis",
    "url": "http://arxiv.org/abs/2412.19856v1",
    "authors": [
      "Sajjad Afroosheh",
      "Mohammadreza Askari"
    ],
    "published": "2024-12-25",
    "abstract": "This paper presents an innovative framework for remote sensing image analysis\nby fusing deep learning techniques, specifically Convolutional Neural Networks\n(CNNs) and Long Short-Term Memory (LSTM) networks, with Geographic Information\nSystems (GIS). The primary objective is to enhance the accuracy and efficiency\nof spatial data analysis by overcoming challenges associated with high\ndimensionality, complex patterns, and temporal data processing. We implemented\noptimization algorithms, namely Particle Swarm Optimization (PSO) and Genetic\nAlgorithms (GA), to fine-tune model parameters, resulting in improved\nperformance metrics. Our findings reveal a significant increase in\nclassification accuracy from 78% to 92% and a reduction in prediction error\nfrom 12% to 6% after optimization. Additionally, the temporal accuracy of the\nmodels improved from 75% to 88%, showcasing the frameworks capability to\nmonitor dynamic changes effectively. The integration of GIS not only enriched\nthe spatial analysis but also facilitated a deeper understanding of the\nrelationships between geographical features. This research demonstrates that\ncombining advanced deep learning methods with GIS and optimization strategies\ncan significantly advance remote sensing applications, paving the way for\nfuture developments in environmental monitoring, urban planning, and resource\nmanagement.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends",
    "url": "http://arxiv.org/abs/2412.16631v1",
    "authors": [
      "Sofiane Bouaziz",
      "Adel Hafiane",
      "Raphael Canals",
      "Rachid Nedjai"
    ],
    "published": "2024-12-21",
    "abstract": "The rapid advancements in satellite remote sensing have enhanced the\ncapability to monitor and analyze the Earth's surface. Among the many variables\ncaptured through satellite sensors, Land Surface Temperature (LST) plays a\ncritical role in understanding key environmental processes. However, obtaining\nhigh-resolution LST data remains a challenge, as satellite sensors often face a\ntrade-off between spatial and temporal resolutions. In response,\nSpatio-Temporal Fusion (STF) has emerged as a powerful method to integrate two\nsatellite data sources, one providing high spatial but low temporal resolution,\nand the other offering high temporal but low spatial resolution. Although a\nrange of STF techniques have been proposed, from traditional methods to\ncutting-edge deep learning (DL) models, most have focused on surface\nreflectance, with limited application to LST estimation. DL approaches, in\nparticular, show promise in improving the spatial and temporal resolutions of\nLST by capturing complex, non-linear relationships between input and output LST\ndata. This paper offers a comprehensive review of the latest advancements in\nDL-based STF techniques for LST estimation. We analyze key research\ndevelopments, mathematically formulate the STF problem, and introduce a novel\ntaxonomy for DL-based STF methods. Furthermore, we discuss the challenges faced\nby current methods and highlight future research directions. In addition, we\npresent the first open-source benchmark STF dataset for LST estimation,\nconsisting of 51 pairs of MODIS-Landsat images spanning from 2013 to 2024. To\nsupport our findings, we conduct extensive experiments on state-of-the-art\nmethods and present both quantitative and qualitative assessments. This is the\nfirst survey paper focused on DL-based STF for LST estimation. We hope it\nserves as a valuable reference for researchers and paves the way for future\nresearch in this field.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Segmentation of arbitrary features in very high resolution remote sensing imagery",
    "url": "http://arxiv.org/abs/2412.16046v1",
    "authors": [
      "Henry Cording",
      "Yves Plancherel",
      "Pablo Brito-Parada"
    ],
    "published": "2024-12-20",
    "abstract": "Very high resolution (VHR) mapping through remote sensing (RS) imagery\npresents a new opportunity to inform decision-making and sustainable practices\nin countless domains. Efficient processing of big VHR data requires automated\ntools applicable to numerous geographic regions and features. Contemporary RS\nstudies address this challenge by employing deep learning (DL) models for\nspecific datasets or features, which limits their applicability across\ncontexts.\n  The present research aims to overcome this limitation by introducing\nEcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.\nEcoMapper fully automates processing of geospatial data, DL model training, and\ninference. Models trained with EcoMapper successfully segmented two distinct\nfeatures in a real-world UAV dataset, achieving scores competitive with prior\nstudies which employed context-specific models.\n  To evaluate EcoMapper, many additional models were trained on permutations of\nprincipal field survey characteristics (FSCs). A relationship was discovered\nallowing derivation of optimal ground sampling distance from feature size,\ntermed Cording Index (CI). A comprehensive methodology for field surveys was\ndeveloped to ensure DL methods can be applied effectively to collected data.\n  The EcoMapper code accompanying this work is available at\nhttps://github.com/hcording/ecomapper .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DroughtSet: Understanding Drought Through Spatial-Temporal Learning",
    "url": "http://arxiv.org/abs/2412.15075v1",
    "authors": [
      "Xuwei Tan",
      "Qian Zhao",
      "Yanlan Liu",
      "Xueru Zhang"
    ],
    "published": "2024-12-19",
    "abstract": "Drought is one of the most destructive and expensive natural disasters,\nseverely impacting natural resources and risks by depleting water resources and\ndiminishing agricultural yields. Under climate change, accurately predicting\ndrought is critical for mitigating drought-induced risks. However, the\nintricate interplay among the physical and biological drivers that regulate\ndroughts limits the predictability and understanding of drought, particularly\nat a subseasonal to seasonal (S2S) time scale. While deep learning has been\ndemonstrated with potential in addressing climate forecasting challenges, its\napplication to drought prediction has received relatively less attention. In\nthis work, we propose a new dataset, DroughtSet, which integrates relevant\npredictive features and three drought indices from multiple remote sensing and\nreanalysis datasets across the contiguous United States (CONUS). DroughtSet\nspecifically provides the machine learning community with a new real-world\ndataset to benchmark drought prediction models and more generally, time-series\nforecasting methods. Furthermore, we propose a spatial-temporal model SPDrought\nto predict and interpret S2S droughts. Our model learns from the spatial and\ntemporal information of physical and biological features to predict three types\nof droughts simultaneously. Multiple strategies are employed to quantify the\nimportance of physical and biological features for drought prediction. Our\nresults provide insights for researchers to better understand the\npredictability and sensitivity of drought to biological and physical\nconditions. We aim to contribute to the climate field by proposing a new tool\nto predict and understand the occurrence of droughts and provide the AI\ncommunity with a new benchmark to study deep learning applications in climate\nscience.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "TS-SatFire: A Multi-Task Satellite Image Time-Series Dataset for Wildfire Detection and Prediction",
    "url": "http://arxiv.org/abs/2412.11555v1",
    "authors": [
      "Yu Zhao",
      "Sebastian Gerard",
      "Yifang Ban"
    ],
    "published": "2024-12-16",
    "abstract": "Wildfire monitoring and prediction are essential for understanding wildfire\nbehaviour. With extensive Earth observation data, these tasks can be integrated\nand enhanced through multi-task deep learning models. We present a\ncomprehensive multi-temporal remote sensing dataset for active fire detection,\ndaily wildfire monitoring, and next-day wildfire prediction. Covering wildfire\nevents in the contiguous U.S. from January 2017 to October 2021, the dataset\nincludes 3552 surface reflectance images and auxiliary data such as weather,\ntopography, land cover, and fuel information, totalling 71 GB. The lifecycle of\neach wildfire is documented, with labels for active fires (AF) and burned areas\n(BA), supported by manual quality assurance of AF and BA test labels. The\ndataset supports three tasks: a) active fire detection, b) daily burned area\nmapping, and c) wildfire progression prediction. Detection tasks use pixel-wise\nclassification of multi-spectral, multi-temporal images, while prediction tasks\nintegrate satellite and auxiliary data to model fire dynamics. This dataset and\nits benchmarks provide a foundation for advancing wildfire research using deep\nlearning.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Deep Random Features for Scalable Interpolation of Spatiotemporal Data",
    "url": "http://arxiv.org/abs/2412.11350v1",
    "authors": [
      "Weibin Chen",
      "Azhir Mahmood",
      "Michel Tsamados",
      "So Takao"
    ],
    "published": "2024-12-16",
    "abstract": "The rapid growth of earth observation systems calls for a scalable approach\nto interpolate remote-sensing observations. These methods in principle, should\nacquire more information about the observed field as data grows. Gaussian\nprocesses (GPs) are candidate model choices for interpolation. However, due to\ntheir poor scalability, they usually rely on inducing points for inference,\nwhich restricts their expressivity. Moreover, commonly imposed assumptions such\nas stationarity prevents them from capturing complex patterns in the data.\nWhile deep GPs can overcome this issue, training and making inference with them\nare difficult, again requiring crude approximations via inducing points. In\nthis work, we instead approach the problem through Bayesian deep learning,\nwhere spatiotemporal fields are represented by deep neural networks, whose\nlayers share the inductive bias of stationary GPs on the plane/sphere via\nrandom feature expansions. This allows one to (1) capture high frequency\npatterns in the data, and (2) use mini-batched gradient descent for large scale\ntraining. We experiment on various remote sensing data at local/global scales,\nshowing that our approach produce competitive or superior results to existing\nmethods, with well-calibrated uncertainties.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": []
  },
  {
    "title": "Super-Resolution for Remote Sensing Imagery via the Coupling of a Variational Model and Deep Learning",
    "url": "http://arxiv.org/abs/2412.09841v1",
    "authors": [
      "Jing Sun",
      "Huanfeng Shen",
      "Qiangqiang Yuan",
      "Liangpei Zhang"
    ],
    "published": "2024-12-13",
    "abstract": "Image super-resolution (SR) is an effective way to enhance the spatial\nresolution and detail information of remote sensing images, to obtain a\nsuperior visual quality. As SR is severely ill-conditioned, effective image\npriors are necessary to regularize the solution space and generate the\ncorresponding high-resolution (HR) image. In this paper, we propose a novel\ngradient-guided multi-frame super-resolution (MFSR) framework for remote\nsensing imagery reconstruction. The framework integrates a learned gradient\nprior as the regularization term into a model-based optimization method.\nSpecifically, the local gradient regularization (LGR) prior is derived from the\ndeep residual attention network (DRAN) through gradient profile transformation.\nThe non-local total variation (NLTV) prior is characterized using the spatial\nstructure similarity of the gradient patches with the maximum a posteriori\n(MAP) model. The modeled prior performs well in preserving edge smoothness and\nsuppressing visual artifacts, while the learned prior is effective in enhancing\nsharp edges and recovering fine structures. By incorporating the two\ncomplementary priors into an adaptive norm based reconstruction framework, the\nmixed L1 and L2 regularization minimization problem is optimized to achieve the\nrequired HR remote sensing image. Extensive experimental results on remote\nsensing data demonstrate that the proposed method can produce visually pleasant\nimages and is superior to several of the state-of-the-art SR algorithms in\nterms of the quantitative evaluation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "CrossVIT-augmented Geospatial-Intelligence Visualization System for Tracking Economic Development Dynamics",
    "url": "http://arxiv.org/abs/2412.10474v1",
    "authors": [
      "Yanbing Bai",
      "Jinhua Su",
      "Bin Qiao",
      "Xiaoran Ma"
    ],
    "published": "2024-12-13",
    "abstract": "Timely and accurate economic data is crucial for effective policymaking.\nCurrent challenges in data timeliness and spatial resolution can be addressed\nwith advancements in multimodal sensing and distributed computing. We introduce\nSenseconomic, a scalable system for tracking economic dynamics via multimodal\nimagery and deep learning. Built on the Transformer framework, it integrates\nremote sensing and street view images using cross-attention, with nighttime\nlight data as weak supervision. The system achieved an R-squared value of\n0.8363 in county-level economic predictions and halved processing time to 23\nminutes using distributed computing. Its user-friendly design includes a\nVue3-based front end with Baidu maps for visualization and a Python-based back\nend automating tasks like image downloads and preprocessing. Senseconomic\nempowers policymakers and researchers with efficient tools for resource\nallocation and economic planning.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "A Progressive Image Restoration Network for High-order Degradation Imaging in Remote Sensing",
    "url": "http://arxiv.org/abs/2412.07195v1",
    "authors": [
      "Yujie Feng",
      "Yin Yang",
      "Xiaohong Fan",
      "Zhengpeng Zhang",
      "Lijing Bu",
      "Jianping Zhang"
    ],
    "published": "2024-12-10",
    "abstract": "Recently, deep learning methods have gained remarkable achievements in the\nfield of image restoration for remote sensing (RS). However, most existing RS\nimage restoration methods focus mainly on conventional first-order degradation\nmodels, which may not effectively capture the imaging mechanisms of remote\nsensing images. Furthermore, many RS image restoration approaches that use deep\nlearning are often criticized for their lacks of architecture transparency and\nmodel interpretability. To address these problems, we propose a novel\nprogressive restoration network for high-order degradation imaging (HDI-PRNet),\nto progressively restore different image degradation. HDI-PRNet is developed\nbased on the theoretical framework of degradation imaging, offering the benefit\nof mathematical interpretability within the unfolding network. The framework is\ncomposed of three main components: a module for image denoising that relies on\nproximal mapping prior learning, a module for image deblurring that integrates\nNeumann series expansion with dual-domain degradation learning, and a module\nfor super-resolution. Extensive experiments demonstrate that our method\nachieves superior performance on both synthetic and real remote sensing images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Comprehensive Analysis and Improvements in Pansharpening Using Deep Learning",
    "url": "http://arxiv.org/abs/2412.04896v1",
    "authors": [
      "Mahek Kantharia",
      "Neeraj Badal",
      "Zankhana Shah"
    ],
    "published": "2024-12-06",
    "abstract": "Pansharpening is a crucial task in remote sensing, enabling the generation of\nhigh-resolution multispectral images by fusing low-resolution multispectral\ndata with high-resolution panchromatic images. This paper provides a\ncomprehensive analysis of traditional and deep learning-based pansharpening\nmethods. While state-of-the-art deep learning methods have significantly\nimproved image quality, issues like spectral distortions persist. To address\nthis, we propose enhancements to the PSGAN framework by introducing novel\nregularization techniques for the generator loss function. Experimental results\non images from the Worldview-3 dataset demonstrate that the proposed\nmodifications improve spectral fidelity and achieve superior performance across\nmultiple quantitative metrics while delivering visually superior results.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Dual-Branch Subpixel-Guided Network for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2412.03893v1",
    "authors": [
      "Zhu Han",
      "Jin Yang",
      "Lianru Gao",
      "Zhiqiang Zeng",
      "Bing Zhang",
      "Jocelyn Chanussot"
    ],
    "published": "2024-12-05",
    "abstract": "Deep learning (DL) has been widely applied into hyperspectral image (HSI)\nclassification owing to its promising feature learning and representation\ncapabilities. However, limited by the spatial resolution of sensors, existing\nDL-based classification approaches mainly focus on pixel-level spectral and\nspatial information extraction through complex network architecture design,\nwhile ignoring the existence of mixed pixels in actual scenarios. To tackle\nthis difficulty, we propose a novel dual-branch subpixel-guided network for HSI\nclassification, called DSNet, which automatically integrates subpixel\ninformation and convolutional class features by introducing a deep autoencoder\nunmixing architecture to enhance classification performance. DSNet is capable\nof fully considering physically nonlinear properties within subpixels and\nadaptively generating diagnostic abundances in an unsupervised manner to\nachieve more reliable decision boundaries for class label distributions. The\nsubpixel fusion module is designed to ensure high-quality information fusion\nacross pixel and subpixel features, further promoting stable joint\nclassification. Experimental results on three benchmark datasets demonstrate\nthe effectiveness and superiority of DSNet compared with state-of-the-art\nDL-based HSI classification approaches. The codes will be available at\nhttps://github.com/hanzhu97702/DSNet, contributing to the remote sensing\ncommunity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion",
    "url": "http://arxiv.org/abs/2412.03413v1",
    "authors": [
      "Andrea Asperti",
      "Ali Aydogdu",
      "Emanuela Clementi",
      "Angelo Greco",
      "Lorenzo Mentaschi",
      "Fabio Merizzi",
      "Pietro Miraglio",
      "Paolo Oddo",
      "Nadia Pinardi",
      "Alessandro Testa"
    ],
    "published": "2024-12-04",
    "abstract": "Sea Surface Temperature (SST) is crucial for understanding Earth's oceans and\nclimate, significantly influencing weather patterns, ocean currents, marine\necosystem health, and the global energy balance. Large-scale SST monitoring\nrelies on satellite infrared radiation detection, but cloud cover presents a\nmajor challenge, creating extensive observational gaps and hampering our\nability to fully capture large-scale ocean temperature patterns. Efforts to\naddress these gaps in existing L4 datasets have been made, but they often\nexhibit notable local and seasonal biases, compromising data reliability and\naccuracy. To tackle this challenge, we employed deep neural networks to\nreconstruct cloud-covered portions of satellite imagery while preserving the\nintegrity of observed values in cloud-free areas, using MODIS satellite derived\nobservations of SST. Our best-performing architecture showed significant skill\nimprovements over established methodologies, achieving substantial reductions\nin error metrics when benchmarked against widely used approaches and datasets.\nThese results underscore the potential of advanced AI techniques to enhance the\ncompleteness of satellite observations in Earth-science remote sensing,\nproviding more accurate and reliable datasets for environmental assessments,\ndata-driven model training, climate research, and seamless integration into\nmodel data assimilation workflows.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Fire-Image-DenseNet (FIDN) for predicting wildfire burnt area using remote sensing data",
    "url": "http://arxiv.org/abs/2412.01400v1",
    "authors": [
      "Bo Pang",
      "Sibo Cheng",
      "Yuhan Huang",
      "Yufang Jin",
      "Yike Guo",
      "I. Colin Prentice",
      "Sandy P. Harrison",
      "Rossella Arcucci"
    ],
    "published": "2024-12-02",
    "abstract": "Predicting the extent of massive wildfires once ignited is essential to\nreduce the subsequent socioeconomic losses and environmental damage, but\nchallenging because of the complexity of fire behaviour. Existing physics-based\nmodels are limited in predicting large or long-duration wildfire events. Here,\nwe develop a deep-learning-based predictive model, Fire-Image-DenseNet (FIDN),\nthat uses spatial features derived from both near real-time and reanalysis data\non the environmental and meteorological drivers of wildfire. We trained and\ntested this model using more than 300 individual wildfires that occurred\nbetween 2012 and 2019 in the western US. In contrast to existing models, the\nperformance of FIDN does not degrade with fire size or duration. Furthermore,\nit predicts final burnt area accurately even in very heterogeneous landscapes\nin terms of fuel density and flammability. The FIDN model showed higher\naccuracy, with a mean squared error (MSE) about 82% and 67% lower than those of\nthe predictive models based on cellular automata (CA) and the minimum travel\ntime (MTT) approaches, respectively. Its structural similarity index measure\n(SSIM) averages 97%, outperforming the CA and FlamMap MTT models by 6% and 2%,\nrespectively. Additionally, FIDN is approximately three orders of magnitude\nfaster than both CA and MTT models. The enhanced computational efficiency and\naccuracy advancements offer vital insights for strategic planning and resource\nallocation for firefighting operations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Tomographic SAR Reconstruction for Forest Height Estimation",
    "url": "http://arxiv.org/abs/2412.00903v2",
    "authors": [
      "Grace Colverd",
      "Jumpei Takami",
      "Laura Schade",
      "Karol Bot",
      "Joseph A. Gallego-Mejia"
    ],
    "published": "2024-12-01",
    "abstract": "Tree height estimation serves as an important proxy for biomass estimation in\necological and forestry applications. While traditional methods such as\nphotogrammetry and Light Detection and Ranging (LiDAR) offer accurate height\nmeasurements, their application on a global scale is often cost-prohibitive and\nlogistically challenging. In contrast, remote sensing techniques, particularly\n3D tomographic reconstruction from Synthetic Aperture Radar (SAR) imagery,\nprovide a scalable solution for global height estimation. SAR images have been\nused in earth observation contexts due to their ability to work in all\nweathers, unobscured by clouds. In this study, we use deep learning to estimate\nforest canopy height directly from 2D Single Look Complex (SLC) images, a\nderivative of SAR. Our method attempts to bypass traditional tomographic signal\nprocessing, potentially reducing latency from SAR capture to end product. We\nalso quantify the impact of varying numbers of SLC images on height estimation\naccuracy, aiming to inform future satellite operations and optimize data\ncollection strategies. Compared to full tomographic processing combined with\ndeep learning, our minimal method (partial processing + deep learning) falls\nshort, with an error 16-21\\% higher, highlighting the continuing relevance of\ngeometric signal processing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Gated-Attention Feature-Fusion Based Framework for Poverty Prediction",
    "url": "http://arxiv.org/abs/2411.19690v1",
    "authors": [
      "Muhammad Umer Ramzan",
      "Wahab Khaddim",
      "Muhammad Ehsan Rana",
      "Usman Ali",
      "Manohar Ali",
      "Fiaz ul Hassan",
      "Fatima Mehmood"
    ],
    "published": "2024-11-29",
    "abstract": "This research paper addresses the significant challenge of accurately\nestimating poverty levels using deep learning, particularly in developing\nregions where traditional methods like household surveys are often costly,\ninfrequent, and quickly become outdated. To address these issues, we propose a\nstate-of-the-art Convolutional Neural Network (CNN) architecture, extending the\nResNet50 model by incorporating a Gated-Attention Feature-Fusion Module (GAFM).\nOur architecture is designed to improve the model's ability to capture and\ncombine both global and local features from satellite images, leading to more\naccurate poverty estimates. The model achieves a 75% R2 score, significantly\noutperforming existing leading methods in poverty mapping. This improvement is\ndue to the model's capacity to focus on and refine the most relevant features,\nfiltering out unnecessary data, which makes it a powerful tool for remote\nsensing and poverty estimation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "Weakly Supervised Framework Considering Multi-temporal Information for Large-scale Cropland Mapping with Satellite Imagery",
    "url": "http://arxiv.org/abs/2411.18475v1",
    "authors": [
      "Yuze Wang",
      "Aoran Hu",
      "Ji Qi",
      "Yang Liu",
      "Chao Tao"
    ],
    "published": "2024-11-27",
    "abstract": "Accurately mapping large-scale cropland is crucial for agricultural\nproduction management and planning. Currently, the combination of remote\nsensing data and deep learning techniques has shown outstanding performance in\ncropland mapping. However, those approaches require massive precise labels,\nwhich are labor-intensive. To reduce the label cost, this study presented a\nweakly supervised framework considering multi-temporal information for\nlarge-scale cropland mapping. Specifically, we extract high-quality labels\naccording to their consistency among global land cover (GLC) products to\nconstruct the supervised learning signal. On the one hand, to alleviate the\noverfitting problem caused by the model's over-trust of remaining errors in\nhigh-quality labels, we encode the similarity/aggregation of cropland in the\nvisual/spatial domain to construct the unsupervised learning signal, and take\nit as the regularization term to constrain the supervised part. On the other\nhand, to sufficiently leverage the plentiful information in the samples without\nhigh-quality labels, we also incorporate the unsupervised learning signal in\nthese samples, enriching the diversity of the feature space. After that, to\ncapture the phenological features of croplands, we introduce dense satellite\nimage time series (SITS) to extend the proposed framework in the temporal\ndimension. We also visualized the high dimensional phenological features to\nuncover how multi-temporal information benefits cropland extraction, and\nassessed the method's robustness under conditions of data scarcity. The\nproposed framework has been experimentally validated for strong adaptability\nacross three study areas (Hunan Province, Southeast France, and Kansas) in\nlarge-scale cropland mapping, and the internal mechanism and temporal\ngeneralizability are also investigated.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "\u03a9SFormer: Dual-Modal \u03a9-like Super-Resolution Transformer Network for Cross-scale and High-accuracy Terraced Field Vectorization Extraction",
    "url": "http://arxiv.org/abs/2411.17088v1",
    "authors": [
      "Chang Li",
      "Yu Wang",
      "Ce Zhang",
      "Yongjun Zhang"
    ],
    "published": "2024-11-26",
    "abstract": "Terraced field is a significant engineering practice for soil and water\nconservation (SWC). Terraced field extraction from remotely sensed imagery is\nthe foundation for monitoring and evaluating SWC. This study is the first to\npropose a novel dual-modal {\\Omega}-like super-resolution Transformer network\nfor intelligent TFVE, offering the following advantages: (1) reducing edge\nsegmentation error from conventional multi-scale downsampling encoder, through\nfusing original high-resolution features with downsampling features at each\nstep of encoder and leveraging a multi-head attention mechanism; (2) improving\nthe accuracy of TFVE by proposing a {\\Omega}-like network structure, which\nfully integrates rich high-level features from both spectral and terrain data\nto form cross-scale super-resolution features; (3) validating an optimal fusion\nscheme for cross-modal and cross-scale (i.e., inconsistent spatial resolution\nbetween remotely sensed imagery and DEM) super-resolution feature extraction;\n(4) mitigating uncertainty between segmentation edge pixels by a coarse-to-fine\nand spatial topological semantic relationship optimization (STSRO) segmentation\nstrategy; (5) leveraging contour vibration neural network to continuously\noptimize parameters and iteratively vectorize terraced fields from semantic\nsegmentation results. Moreover, a DMRVD for deep-learning-based TFVE was\ncreated for the first time, which covers nine study areas in four provinces of\nChina, with a total coverage area of 22441 square kilometers. To assess the\nperformance of {\\Omega}SFormer, classic and SOTA networks were compared. The\nmIOU of {\\Omega}SFormer has improved by 0.165, 0.297 and 0.128 respectively,\nwhen compared with best accuracy single-modal remotely sensed imagery,\nsingle-modal DEM and dual-modal result.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Super-Resolution"
    ]
  },
  {
    "title": "CMAViT: Integrating Climate, Managment, and Remote Sensing Data for Crop Yield Estimation with Multimodel Vision Transformers",
    "url": "http://arxiv.org/abs/2411.16989v1",
    "authors": [
      "Hamid Kamangir",
      "Brent. S. Sams",
      "Nick Dokoozlian",
      "Luis Sanchez",
      "J. Mason. Earles"
    ],
    "published": "2024-11-25",
    "abstract": "Crop yield prediction is essential for agricultural planning but remains\nchallenging due to the complex interactions between weather, climate, and\nmanagement practices. To address these challenges, we introduce a deep\nlearning-based multi-model called Climate-Management Aware Vision Transformer\n(CMAViT), designed for pixel-level vineyard yield predictions. CMAViT\nintegrates both spatial and temporal data by leveraging remote sensing imagery\nand short-term meteorological data, capturing the effects of growing season\nvariations. Additionally, it incorporates management practices, which are\nrepresented in text form, using a cross-attention encoder to model their\ninteraction with time-series data. This innovative multi-modal transformer\ntested on a large dataset from 2016-2019 covering 2,200 hectares and eight\ngrape cultivars including more than 5 million vines, outperforms traditional\nmodels like UNet-ConvLSTM, excelling in spatial variability capture and yield\nprediction, particularly for extreme values in vineyards. CMAViT achieved an R2\nof 0.84 and a MAPE of 8.22% on an unseen test dataset. Masking specific\nmodalities lowered performance: excluding management practices, climate data,\nand both reduced R2 to 0.73, 0.70, and 0.72, respectively, and raised MAPE to\n11.92%, 12.66%, and 12.39%, highlighting each modality's importance for\naccurate yield prediction. Code is available at\nhttps://github.com/plant-ai-biophysics-lab/CMAViT.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Resolution-Adaptive Micro-Doppler Spectrogram for Human Activity Recognition",
    "url": "http://arxiv.org/abs/2411.15057v2",
    "authors": [
      "Do-Hyun Park",
      "Min-Wook Jeon",
      "Hyoung-Nam Kim"
    ],
    "published": "2024-11-22",
    "abstract": "The rising demand for remote-sensing systems for detecting hazardous\nsituations has led to increased interest in radar-based human activity\nrecognition (HAR). Conventional radar-based HAR methods predominantly rely on\nmicro-Doppler spectrograms for recognition tasks. However, spectrograms\nfrequently fail to effectively capture micro-Doppler signatures because of\ntheir limited linear resolution. To address this limitation, we propose a\ntime--frequency domain representation method that adaptively adjusts the\nresolution based on activity characteristics. This approach nonlinearly\ntransforms the resolution to focus on the most relevant frequency range for\nmicro-Doppler signatures. We validate the proposed method by training\ndeep-learning-based HAR models on datasets generated using the adaptive\nrepresentation method. Experimental results demonstrate that the models trained\nusing the proposed method achieve superior recognition accuracy compared with\nthose trained using conventional methods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Attentive Contextual Attention for Cloud Removal",
    "url": "http://arxiv.org/abs/2411.13042v1",
    "authors": [
      "Wenli Huang",
      "Ye Deng",
      "Yang Wu",
      "Jinjun Wang"
    ],
    "published": "2024-11-20",
    "abstract": "Cloud cover can significantly hinder the use of remote sensing images for\nEarth observation, prompting urgent advancements in cloud removal technology.\nRecently, deep learning strategies have shown strong potential in restoring\ncloud-obscured areas. These methods utilize convolution to extract intricate\nlocal features and attention mechanisms to gather long-range information,\nimproving the overall comprehension of the scene. However, a common drawback of\nthese approaches is that the resulting images often suffer from blurriness,\nartifacts, and inconsistencies. This is partly because attention mechanisms\napply weights to all features based on generalized similarity scores, which can\ninadvertently introduce noise and irrelevant details from cloud-covered areas.\nTo overcome this limitation and better capture relevant distant context, we\nintroduce a novel approach named Attentive Contextual Attention (AC-Attention).\nThis method enhances conventional attention mechanisms by dynamically learning\ndata-driven attentive selection scores, enabling it to filter out noise and\nirrelevant features effectively. By integrating the AC-Attention module into\nthe DSen2-CR cloud removal framework, we significantly improve the model's\nability to capture essential distant information, leading to more effective\ncloud removal. Our extensive evaluation of various datasets shows that our\nmethod outperforms existing ones regarding image reconstruction quality.\nAdditionally, we conducted ablation studies by integrating AC-Attention into\nmultiple existing methods and widely used network architectures. These studies\ndemonstrate the effectiveness and adaptability of AC-Attention and reveal its\nability to focus on relevant features, thereby improving the overall\nperformance of the networks. The code is available at\n\\url{https://github.com/huangwenwenlili/ACA-CRNet}.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Cuvis.Ai: An Open-Source, Low-Code Software Ecosystem for Hyperspectral Processing and Classification",
    "url": "http://arxiv.org/abs/2411.11324v1",
    "authors": [
      "Nathaniel Hanson",
      "Philip Manke",
      "Simon Birkholz",
      "Maximilian M\u00fchlbauer",
      "Rene Heine",
      "Arnd Brandes"
    ],
    "published": "2024-11-18",
    "abstract": "Machine learning is an important tool for analyzing high-dimension\nhyperspectral data; however, existing software solutions are either\nclosed-source or inextensible research products. In this paper, we present\ncuvis.ai, an open-source and low-code software ecosystem for data acquisition,\npreprocessing, and model training. The package is written in Python and\nprovides wrappers around common machine learning libraries, allowing both\nclassical and deep learning models to be trained on hyperspectral data. The\ncodebase abstracts processing interconnections and data dependencies between\noperations to minimize code complexity for users. This software package\ninstantiates nodes in a directed acyclic graph to handle all stages of a\nmachine learning ecosystem, from data acquisition, including live or static\ndata sources, to final class assignment or property prediction. User-created\nmodels contain convenient serialization methods to ensure portability and\nincrease sharing within the research community. All code and data are available\nonline: https://github.com/cubert-hyperspectral/cuvis.ai",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Program Evaluation with Remotely Sensed Outcomes",
    "url": "http://arxiv.org/abs/2411.10959v2",
    "authors": [
      "Ashesh Rambachan",
      "Rahul Singh",
      "Davide Viviano"
    ],
    "published": "2024-11-17",
    "abstract": "Economists often estimate treatment effects in experiments using remotely\nsensed variables (RSVs), e.g. satellite images or mobile phone activity, in\nplace of directly measured economic outcomes. A common practice is to use an\nobservational sample to train a predictor of the economic outcome from the RSV,\nand then to use its predictions as the outcomes in the experiment. We show that\nthis method is biased whenever the RSV is post-outcome, i.e. if variation in\nthe economic outcome causes variation in the RSV. In program evaluation,\nchanges in poverty or environmental quality cause changes in satellite images,\nbut not vice versa. As our main result, we nonparametrically identify the\ntreatment effect by formalizing the intuition that underlies common practice:\nthe conditional distribution of the RSV given the outcome and treatment is\nstable across the samples.Based on our identifying formula, we find that the\nefficient representation of RSVs for causal inference requires three\npredictions rather than one. Valid inference does not require any rate\nconditions on RSV predictions, justifying the use of complex deep learning\nalgorithms with unknown statistical properties. We re-analyze the effect of an\nanti-poverty program in India using satellite images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Slender Object Scene Segmentation in Remote Sensing Image Based on Learnable Morphological Skeleton with Segment Anything Model",
    "url": "http://arxiv.org/abs/2411.08592v2",
    "authors": [
      "Jun Xie",
      "Wenxiao Li",
      "Faqiang Wang",
      "Liqiang Zhang",
      "Zhengyang Hou",
      "Jun Liu"
    ],
    "published": "2024-11-13",
    "abstract": "Morphological methods play a crucial role in remote sensing image processing,\ndue to their ability to capture and preserve small structural details. However,\nmost of the existing deep learning models for semantic segmentation are based\non the encoder-decoder architecture including U-net and Segment Anything Model\n(SAM), where the downsampling process tends to discard fine details. In this\npaper, we propose a new approach that integrates learnable morphological\nskeleton prior into deep neural networks using the variational method. To\naddress the difficulty in backpropagation in neural networks caused by the\nnon-differentiability presented in classical morphological operations, we\nprovide a smooth representation of the morphological skeleton and design a\nvariational segmentation model integrating morphological skeleton prior by\nemploying operator splitting and dual methods. Then, we integrate this model\ninto the network architecture of SAM, which is achieved by adding a token to\nmask decoder and modifying the final sigmoid layer, ensuring the final\nsegmentation results preserve the skeleton structure as much as possible.\nExperimental results on remote sensing datasets, including buildings, roads and\nwater, demonstrate that our method outperforms the original SAM on slender\nobject segmentation and exhibits better generalization capability.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "Deep Neural Network"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal",
    "url": "http://arxiv.org/abs/2504.20620v1",
    "authors": [
      "Abhishek Pasula",
      "Deepak N. Subramani"
    ],
    "published": "2025-04-29",
    "abstract": "Climate change alters ocean conditions, notably temperature and sea level. In\nthe Bay of Bengal, these changes influence monsoon precipitation and marine\nproductivity, critical to the Indian economy. In Phase 6 of the Coupled Model\nIntercomparison Project (CMIP6), Global Climate Models (GCMs) use different\nshared socioeconomic pathways (SSPs) to obtain future climate projections.\nHowever, significant discrepancies are observed between these models and the\nreanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean\nsquare error (RMSE) between the climate model output and the Ocean Reanalysis\nSystem (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the\ndynamic sea level (DSL). We introduce a new data-driven deep learning model to\ncorrect for this bias. The deep neural model for each variable is trained using\npairs of climatology-removed monthly climate projections as input and the\ncorresponding month's ORAS5 as output. This model is trained with historical\ndata (1950 to 2014), validated with future projection data from 2015 to 2020,\nand tested with future projections from 2021 to 2023. Compared to the\nconventional EquiDistant Cumulative Distribution Function (EDCDF) statistical\nmethod for bias correction in climate models, our approach decreases RMSE by\n0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the\nprojections for 2024-2100. A detailed analysis of the monthly, seasonal, and\ndecadal means and variability is performed to underscore the implications of\nthe novel dynamics uncovered in our corrected projections.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Mj\u00f6lnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density",
    "url": "http://arxiv.org/abs/2504.19822v1",
    "authors": [
      "Minjong Cheon"
    ],
    "published": "2025-04-28",
    "abstract": "Recent advances in AI-based weather forecasting models, such as FourCastNet,\nPangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep\nlearning to emulate complex atmospheric dynamics. Building on this momentum, we\npropose Mj\\\"olnir, a novel deep learning-based framework for global lightning\nflash density parameterization. Trained on ERA5 atmospheric predictors and\nWorld Wide Lightning Location Network (WWLLN) observations at a daily temporal\nresolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear\nmapping between large-scale environmental conditions and lightning activity.\nThe model architecture is based on the InceptionNeXt backbone with SENet, and a\nmulti-task learning strategy to simultaneously predict lightning occurrence and\nmagnitude. Extensive evaluations yield that Mollnir accurately reproduces the\nglobal distribution, seasonal variability, and regional characteristics of\nlightning activity, achieving a global Pearson correlation coefficient of 0.96\nfor annual mean fields. These results suggest that Mj\\\"olnir serves not only as\nan effective data-driven global lightning parameterization but also as a\npromising AI-based scheme for next-generation Earth system models (AI-ESMs).",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Global Climate Model Bias Correction Using Deep Learning",
    "url": "http://arxiv.org/abs/2504.19145v1",
    "authors": [
      "Abhishek Pasula",
      "Deepak N. Subramani"
    ],
    "published": "2025-04-27",
    "abstract": "Climate change affects ocean temperature, salinity and sea level, impacting\nmonsoons and ocean productivity. Future projections by Global Climate Models\nbased on shared socioeconomic pathways from the Coupled Model Intercomparison\nProject (CMIP) are widely used to understand the effects of climate change.\nHowever, CMIP models have significant bias compared to reanalysis in the Bay of\nBengal for the time period when both projections and reanalysis are available.\nFor example, there is a 1.5C root mean square error (RMSE) in the sea surface\ntemperature (SST) projections of the climate model CNRM-CM6 compared to the\nOcean Reanalysis System (ORAS5). We develop a suite of data-driven deep\nlearning models for bias correction of climate model projections and apply it\nto correct SST projections of the Bay of Bengal. We propose the use of three\ndifferent deep neural network architectures: convolutional encoder-decoder\nUNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression\nmodel and the Equi-Distant Cumulative Density Function (EDCDF) bias correction\nmethod for comparison and evaluating the impact of the new deep learning\nmodels. All bias correction models are trained using pairs of monthly CMIP6\nprojections and the corresponding month's ORAS5 as input and output. Historical\ndata (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used\nfor training and validation, including hyperparameter tuning. Testing is\nperformed on future projection data from 2021 to 2024. Detailed analysis of the\nthree deep neural models has been completed. We found that the UNet\narchitecture trained using a climatology-removed CNRM-CM6 projection as input\nand climatology-removed ORAS5 as output gives the best bias-corrected\nprojections. Our novel deep learning-based method for correcting CNRM-CM6 data\nhas a 15% reduction in RMSE compared EDCDF.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "LSTM",
      "Deep Neural Network"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence",
    "url": "http://arxiv.org/abs/2504.19036v1",
    "authors": [
      "Henry Herzog",
      "Joshua Hansen",
      "Yawen Zhang",
      "Patrick Beukema"
    ],
    "published": "2025-04-26",
    "abstract": "Unsustainable exploitation of the oceans exacerbated by global warming is\nthreatening coastal communities worldwide. Accurate and timely monitoring of\nmaritime activity is an essential step to effective governance and to inform\nfuture policy. In support of this complex global-scale effort, we built\nAtlantes, a deep learning based system that provides the first-ever real-time\nview of vessel behavior at global scale. Atlantes leverages a series of bespoke\ntransformers to distill a high volume, continuous stream of GPS messages\nemitted by hundreds of thousands of vessels into easily quantifiable behaviors.\nThe combination of low latency and high performance enables operationally\nrelevant decision-making and successful interventions on the high seas where\nillegal and exploitative activity is too common. Atlantes is already in use by\nhundreds of organizations worldwide. Here we provide an overview of the model\nand infrastructure that enables this system to function efficiently and\ncost-effectively at global-scale and in real-time.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Appa: Bending Weather Dynamics with Latent Diffusion Models for Global Data Assimilation",
    "url": "http://arxiv.org/abs/2504.18720v1",
    "authors": [
      "G\u00e9r\u00f4me Andry",
      "Fran\u00e7ois Rozet",
      "Sacha Lewin",
      "Omer Rochman",
      "Victor Mangeleer",
      "Matthias Pirlet",
      "Elise Faulx",
      "Marilaure Gr\u00e9goire",
      "Gilles Louppe"
    ],
    "published": "2025-04-25",
    "abstract": "Deep learning has transformed weather forecasting by improving both its\naccuracy and computational efficiency. However, before any forecast can begin,\nweather centers must identify the current atmospheric state from vast amounts\nof observational data. To address this challenging problem, we introduce Appa,\na score-based data assimilation model producing global atmospheric trajectories\nat 0.25-degree resolution and 1-hour intervals. Powered by a 1.5B-parameter\nspatio-temporal latent diffusion model trained on ERA5 reanalysis data, Appa\ncan be conditioned on any type of observations to infer the posterior\ndistribution of plausible state trajectories, without retraining. Our unified\nprobabilistic framework flexibly tackles multiple inference tasks --\nreanalysis, filtering, and forecasting -- using the same model, eliminating the\nneed for task-specific architectures or training procedures. Experiments\ndemonstrate physical consistency on a global scale and good reconstructions\nfrom observations, while showing competitive forecasting skills. Our results\nestablish latent score-based data assimilation as a promising foundation for\nfuture global atmospheric modeling systems.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Last-layer committee machines for uncertainty estimations of benthic imagery",
    "url": "http://arxiv.org/abs/2504.16952v1",
    "authors": [
      "H. Martin Gillis",
      "Isaac Xu",
      "Benjamin Misiuk",
      "Craig J. Brown",
      "Thomas Trappenberg"
    ],
    "published": "2025-04-22",
    "abstract": "Automating the annotation of benthic imagery (i.e., images of the seafloor\nand its associated organisms, habitats, and geological features) is critical\nfor monitoring rapidly changing ocean ecosystems. Deep learning approaches have\nsucceeded in this purpose; however, consistent annotation remains challenging\ndue to ambiguous seafloor images, potential inter-user annotation\ndisagreements, and out-of-distribution samples. Marine scientists implementing\ndeep learning models often obtain predictions based on one-hot representations\ntrained using a cross-entropy loss objective with softmax normalization,\nresulting with a single set of model parameters. While efficient, this approach\nmay lead to overconfident predictions for context-challenging datasets, raising\nreliability concerns that present risks for downstream tasks such as benthic\nhabitat mapping and marine spatial planning. In this study, we investigated\nclassification uncertainty as a tool to improve the labeling of benthic habitat\nimagery. We developed a framework for two challenging sub-datasets of the\nrecently publicly available BenthicNet dataset using Bayesian neural networks,\nMonte Carlo dropout inference sampling, and a proposed single last-layer\ncommittee machine. This approach resulted with a > 95% reduction of network\nparameters to obtain per-sample uncertainties while obtaining near-identical\nperformance compared to computationally more expensive strategies such as\nBayesian neural networks, Monte Carlo dropout, and deep ensembles. The method\nproposed in this research provides a strategy for obtaining prioritized lists\nof uncertain samples for human-in-the-loop interventions to identify ambiguous,\nmislabeled, out-of-distribution, and/or difficult images for enhancing existing\nannotation tools for benthic mapping and other applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping",
    "url": "http://arxiv.org/abs/2504.14372v1",
    "authors": [
      "Jose Marie Antonio Minoza"
    ],
    "published": "2025-04-19",
    "abstract": "Accurate ocean modeling and coastal hazard prediction depend on\nhigh-resolution bathymetric data; yet, current worldwide datasets are too\ncoarse for exact numerical simulations. While recent deep learning advances\nhave improved earth observation data resolution, existing methods struggle with\nthe unique challenges of producing detailed ocean floor maps, especially in\nmaintaining physical structure consistency and quantifying uncertainties. This\nwork presents a novel uncertainty-aware mechanism using spatial blocks to\nefficiently capture local bathymetric complexity based on block-based conformal\nprediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)\narchitecture, the integration of this uncertainty quantification framework\nyields spatially adaptive confidence estimates while preserving topographical\nfeatures via discrete latent representations. With smaller uncertainty widths\nin well-characterized areas and appropriately larger bounds in areas of complex\nseafloor structures, the block-based design adapts uncertainty estimates to\nlocal bathymetric complexity. Compared to conventional techniques, experimental\nresults over several ocean regions show notable increases in both\nreconstruction quality and uncertainty estimation reliability. This framework\nincreases the reliability of bathymetric reconstructions by preserving\nstructural integrity while offering spatially adaptive uncertainty estimates,\nso opening the path for more solid climate modeling and coastal hazard\nassessment.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enhancing Deterministic Freezing Level Predictions in the Northern Sierra Nevada Through Deep Neural Networks",
    "url": "http://arxiv.org/abs/2504.11560v1",
    "authors": [
      "Vesta Afzali Gorooh",
      "Agniv Sengupta",
      "Shawn Roj",
      "Rachel Weihs",
      "Brian Kawzenuk",
      "Luca Delle Monache",
      "F. Martin Ralph"
    ],
    "published": "2025-04-15",
    "abstract": "Accurate prediction of the freezing level (FZL) is essential for\nhydrometeorological forecasting systems and precipitation phase estimation, and\nit influences runoff generation and reservoir management decisions. In this\nstudy, we develop a deep learning based postprocessing framework using the Unet\nconvolutional neural network (CNN) architecture to refine the FZL forecasts\nfrom the West Weather Research and Forecasting (West-WRF) model. The proposed\nframework leverages reforecast data from West WRF and FZL estimates from the\nCalifornia Nevada River Forecast Center (CNRFC) to train a deterministic Unet\nmodel over the Yuba-Feather watershed, a hydrologically critical basin in\nnorthern California. We introduce two variants of our model, Unet-Log and\nUnet-GMM, which utilize the logarithm of the hyperbolic cosine of Error and\nGaussian Mixture Model loss functions, respectively, to enhance FZL forecast\naccuracy beyond an RMSE based benchmark. Results indicate that the Unet based\npostprocessing framework significantly improves FZL forecast skill across\ndiverse atmospheric conditions and complex topography. Compared to the raw\nWest-WRF output, our model achieves reductions in RMSE of up to 25% and\nincreases the forecast observation correlation by about 10% over the\nYuba-Feather watershed. Furthermore, it effectively captures the spatiotemporal\nvariability of the FZL across different elevations, mitigating systematic\nbiases inherent in the West-WRF model. This novel deep learning based\npostprocessing approach demonstrates a promising pathway for integrating\nmachine learning into hydrometeorological forecasting and decision support\nwithin the Forecast Informed Reservoir Operations (FIRO) framework.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "NWP-based deep learning for tropical cyclone intensity prediction",
    "url": "http://arxiv.org/abs/2504.09143v1",
    "authors": [
      "Chanh Kieu",
      "Khanh Luong",
      "Tri Nguyen"
    ],
    "published": "2025-04-12",
    "abstract": "Global artificial intelligence (AI) models are rapidly advancing and\nbeginning to outperform traditional numerical weather prediction (NWP) models\nacross metrics, yet predicting regional extreme weather such as tropical\ncyclone (TC) intensity presents unique spatial and temporal challenges that\nglobal AI models cannot capture. This study presents a new approach to train\ndeep learning (DL) models specifically for regional extreme weather prediction.\nBy leveraging physics-based NWP models to generate high-resolution data, we\ndemonstrate that DL models can better predict or downscale TC intensity and\nstructure when fine-scale processes are properly accounted for. Furthermore, by\ntraining DL models on different resolution outputs from physics-based\nsimulations, we highlight the critical role of fine-scale processes in larger\nstorm-scale dynamics, an aspect that current climate datasets used to train\nmost global DL models cannot fully represent. These findings underscore the\nchallenges in predicting or downscaling extreme weather with data-driven\nmodels, thus proposing the new role of NWP models as data generators for\ntraining DL models in the future AI model development for weather applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A physics informed neural network approach to simulating ice dynamics governed by the shallow ice approximation",
    "url": "http://arxiv.org/abs/2504.08136v1",
    "authors": [
      "Kapil Chawla",
      "William Holmes"
    ],
    "published": "2025-04-10",
    "abstract": "In this article we develop a Physics Informed Neural Network (PINN) approach\nto simulate ice sheet dynamics governed by the Shallow Ice Approximation. This\nproblem takes the form of a time-dependent parabolic obstacle problem. Prior\nwork has used this approach to address the stationary obstacle problem and here\nwe extend it to the time dependent problem. Through comprehensive 1D and 2D\nsimulations, we validate the model's effectiveness in capturing complex\nfree-boundary conditions. By merging traditional mathematical modeling with\ncutting-edge deep learning methods, this approach provides a scalable and\nrobust solution for predicting temporal variations in ice thickness. To\nillustrate this approach in a real world setting, we simulate the dynamics of\nthe Devon Ice Cap, incorporating aerogeophysical data from 2000 and 2018.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Improving Predictions of Convective Storm Wind Gusts through Statistical Post-Processing of Neural Weather Models",
    "url": "http://arxiv.org/abs/2504.00128v1",
    "authors": [
      "Antoine Leclerc",
      "Erwan Koch",
      "Monika Feldmann",
      "Daniele Nerini",
      "Tom Beucler"
    ],
    "published": "2025-03-31",
    "abstract": "Issuing timely severe weather warnings helps mitigate potentially disastrous\nconsequences. Recent advancements in Neural Weather Models (NWMs) offer a\ncomputationally inexpensive and fast approach for forecasting atmospheric\nenvironments on a 0.25{\\deg} global grid. For thunderstorms, these environments\ncan be empirically post-processed to predict wind gust distributions at\nspecific locations. With the Pangu-Weather NWM, we apply a hierarchy of\nstatistical and deep learning post-processing methods to forecast hourly wind\ngusts up to three days ahead. To ensure statistical robustness, we constrain\nour probabilistic forecasts using generalised extreme-value distributions\nacross five regions in Switzerland. Using a convolutional neural network to\npost-process the predicted atmospheric environment's spatial patterns yields\nthe best results, outperforming direct forecasting approaches across lead times\nand wind gust speeds. Our results confirm the added value of NWMs for extreme\nwind forecasting, especially for designing more responsive early-warning\nsystems.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Joint Source-Environment Adaptation for Deep Learning-Based Underwater Acoustic Source Ranging",
    "url": "http://arxiv.org/abs/2503.23262v1",
    "authors": [
      "Dariush Kari",
      "Andrew C. Singer"
    ],
    "published": "2025-03-30",
    "abstract": "In this paper, we propose a method to adapt a pre-trained deep-learning-based\nmodel for underwater acoustic localization to a new environment. We use\nunsupervised domain adaptation to improve the generalization performance of the\nmodel, i.e., using an unsupervised loss, fine-tune the pre-trained network\nparameters without access to any labels of the target environment or any data\nused to pre-train the model. This method improves the pre-trained model\nprediction by coupling that with an almost independent estimation based on the\nreceived signal energy (that depends on the source). We show the effectiveness\nof this approach on Bellhop generated data in an environment similar to that of\nthe SWellEx-96 experiment contaminated with real ocean noise from the KAM11\nexperiment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Joint Source-Environment Adaptation of Data-Driven Underwater Acoustic Source Ranging Based on Model Uncertainty",
    "url": "http://arxiv.org/abs/2503.23258v1",
    "authors": [
      "Dariush Kari",
      "Hari Vishnu",
      "Andrew C. Singer"
    ],
    "published": "2025-03-30",
    "abstract": "Adapting pre-trained deep learning models to new and unknown environments is\na difficult challenge in underwater acoustic localization. We show that\nalthough pre-trained models have performance that suffers from mismatch between\nthe training and test data, they generally exhibit a higher ``implied\nuncertainty'' in environments where there is more mismatch. Leveraging this\nnotion of implied uncertainty, we partition the test samples into more certain\nand less certain sets, and implement an estimation method using the certain\nsamples to improve the labeling for uncertain samples, which helps to adapt the\nmodel. We use an efficient method to quantify model prediction uncertainty, and\nan innovative approach to adapt a pre-trained model to unseen underwater\nenvironments at test time. This eliminates the need for labeled data from the\ntarget environment or the original training data. This adaptation is enhanced\nby integrating an independent estimate based on the received signal energy. We\nvalidate the approach extensively using real experimental data, as well as\nsynthetic data consisting of model-generated signals with real ocean noise. The\nresults demonstrate significant improvements in model prediction accuracy,\nunderscoring the potential of the method to enhance underwater acoustic\nlocalization in diverse, noisy, and unknown environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Simulation-informed deep learning for enhanced SWOT observations of fine-scale ocean dynamics",
    "url": "http://arxiv.org/abs/2503.21303v1",
    "authors": [
      "Eugenio Cutolo",
      "Carlos Granero-Belinchon",
      "Ptashanna Thiraux",
      "Jinbo Wang",
      "Ronan Fablet"
    ],
    "published": "2025-03-27",
    "abstract": "Oceanic processes at fine scales are crucial yet difficult to observe\naccurately due to limitations in satellite and in-situ measurements. The\nSurface Water and Ocean Topography (SWOT) mission provides high-resolution Sea\nSurface Height (SSH) data, though noise patterns often obscure fine scale\nstructures. Current methods struggle with noisy data or require extensive\nsupervised training, limiting their effectiveness on real-world observations.\nWe introduce SIMPGEN (Simulation-Informed Metric and Prior for Generative\nEnsemble Networks), an unsupervised adversarial learning framework combining\nreal SWOT observations with simulated reference data. SIMPGEN leverages\nwavelet-informed neural metrics to distinguish noisy from clean fields, guiding\nrealistic SSH reconstructions. Applied to SWOT data, SIMPGEN effectively\nremoves noise, preserving fine-scale features better than existing neural\nmethods. This robust, unsupervised approach not only improves SWOT SSH data\ninterpretation but also demonstrates strong potential for broader oceanographic\napplications, including data assimilation and super-resolution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years",
    "url": "http://arxiv.org/abs/2503.21211v1",
    "authors": [
      "Rixu Hao",
      "Yuxin Zhao",
      "Shaoqing Zhang",
      "Guihua Wang",
      "Xiong Deng"
    ],
    "published": "2025-03-27",
    "abstract": "El Ni\\~no-Southern Oscillation (ENSO) exerts global climate and societal\nimpacts, but real-time prediction with lead times beyond one year remains\nchallenging. Dynamical models suffer from large biases and uncertainties, while\ndeep learning struggles with interpretability and multi-scale dynamics. Here,\nwe introduce PTSTnet, an interpretable model that unifies dynamical processes\nand cross-scale spatiotemporal learning in an innovative neural-network\nframework with physics-encoding learning. PTSTnet produces interpretable\npredictions significantly outperforming state-of-the-art benchmarks with lead\ntimes beyond 24 months, providing physical insights into error propagation in\nocean-atmosphere interactions. PTSTnet learns feature representations with\nphysical consistency from sparse data to tackle inherent multi-scale and\nmulti-physics challenges underlying ocean-atmosphere processes, thereby\ninherently enhancing long-term prediction skill. Our successful realizations\nmark substantial steps forward in interpretable insights into innovative neural\nocean modelling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
    "url": "http://arxiv.org/abs/2503.19502v1",
    "authors": [
      "Qi Chen",
      "Yinghao Cui",
      "Guobin Hong",
      "Karumuri Ashok",
      "Yuchun Pu",
      "Xiaogu Zheng",
      "Xuanze Zhang",
      "Wei Zhong",
      "Peng Zhan",
      "Zhonglei Wang"
    ],
    "published": "2025-03-25",
    "abstract": "El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling",
    "url": "http://arxiv.org/abs/2503.19940v1",
    "authors": [
      "Qiusheng Huang",
      "Xiaohui Zhong",
      "Xu Fan",
      "Lei Chen",
      "Hao Li"
    ],
    "published": "2025-03-25",
    "abstract": "Similar to conventional video generation, current deep learning-based weather\nprediction frameworks often lack explicit physical constraints, leading to\nunphysical outputs that limit their reliability for operational forecasting.\nAmong various physical processes requiring proper representation, radiation\nplays a fundamental role as it drives Earth's weather and climate systems.\nHowever, accurate simulation of radiative transfer processes remains\nchallenging for traditional numerical weather prediction (NWP) models due to\ntheir inherent complexity and high computational costs. Here, we propose\nFuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance\nweather forecast accuracy while enforcing physical consistency. FuXi-RTM\nintegrates a primary forecasting model (FuXi) with a fixed deep learning-based\nradiative transfer model (DLRTM) surrogate that efficiently replaces\nconventional radiation parameterization schemes. This represents the first deep\nlearning-based weather forecasting framework to explicitly incorporate physical\nprocess modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM\noutperforms its unconstrained counterpart in 88.51% of 3320 variable and lead\ntime combinations, with improvements in radiative flux predictions. By\nincorporating additional physical processes, FuXi-RTM paves the way for\nnext-generation weather forecasting systems that are both accurate and\nphysically consistent.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Deep learning in the abyss: a stratified Physics Informed Neural Network for data assimilation",
    "url": "http://arxiv.org/abs/2503.19160v1",
    "authors": [
      "Vadim Limousin",
      "Nelly Pustelnik",
      "Bruno Deremble",
      "Antoine Venaille"
    ],
    "published": "2025-03-24",
    "abstract": "The reconstruction of deep ocean currents is a major challenge in data\nassimilation due to the scarcity of interior data. In this work, we present a\nproof of concept for deep ocean flow reconstruction using a Physics-Informed\nNeural Network (PINN), a machine learning approach that offers an alternative\nto traditional data assimilation methods. We introduce an efficient algorithm\ncalled StrAssPINN (for Stratified Assimilation PINNs), which assigns a separate\nnetwork to each layer of the ocean model while allowing them to interact during\ntraining. The neural network takes spatiotemporal coordinates as input and\npredicts the velocity field at those points. Using a SIREN architecture (a\nmultilayer perceptron with sine activation functions), which has proven\neffective in various contexts, the network is trained using both available\nobservational data and dynamical priors enforced at several collocation points.\nWe apply this method to pseudo-observed ocean data generated from a 3-layer\nquasi-geostrophic model, where the pseudo-observations include surface-level\ndata akin to SWOT observations of sea surface height, interior data similar to\nARGO floats, and a limited number of deep ARGO-like measurements in the lower\nlayers. Our approach successfully reconstructs ocean flows in both the interior\nand surface layers, demonstrating a strong ability to resolve key ocean\nmesoscale features, including vortex rings, eastward jets associated with\npotential vorticity fronts, and smoother Rossby waves. This work serves as a\nprelude to applying StrAssPINN to real-world observational data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Towards Location-Specific Precipitation Projections Using Deep Neural Networks",
    "url": "http://arxiv.org/abs/2503.14095v1",
    "authors": [
      "Bipin Kumar",
      "Bhvisy Kumar Yadav",
      "Soumypdeep Mukhopadhyay",
      "Rakshit Rohan",
      "Bhupendra Bahadur Singh",
      "Rajib Chattopadhyay",
      "Nagraju Chilukoti",
      "Atul Kumar Sahai"
    ],
    "published": "2025-03-18",
    "abstract": "Accurate precipitation estimates at individual locations are crucial for\nweather forecasting and spatial analysis. This study presents a paradigm shift\nby leveraging Deep Neural Networks (DNNs) to surpass traditional methods like\nKriging for station-specific precipitation approximation. We propose two\ninnovative NN architectures: one utilizing precipitation, elevation, and\nlocation, and another incorporating additional meteorological parameters like\nhumidity, temperature, and wind speed. Trained on a vast dataset (1980-2019),\nthese models outperform Kriging across various evaluation metrics (correlation\ncoefficient, root mean square error, bias, and skill score) on a five-year\nvalidation set. This compelling evidence demonstrates the transformative power\nof deep learning for spatial prediction, offering a robust and precise\nalternative for station-specific precipitation estimation.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "COSMOS: Continuous Simplicial Neural Networks",
    "url": "http://arxiv.org/abs/2503.12919v1",
    "authors": [
      "Aref Einizade",
      "Dorina Thanou",
      "Fragkiskos D. Malliaros",
      "Jhony H. Giraldo"
    ],
    "published": "2025-03-17",
    "abstract": "Simplicial complexes provide a powerful framework for modeling high-order\ninteractions in structured data, making them particularly suitable for\napplications such as trajectory prediction and mesh processing. However,\nexisting simplicial neural networks (SNNs), whether convolutional or\nattention-based, rely primarily on discrete filtering techniques, which can be\nrestrictive. In contrast, partial differential equations (PDEs) on simplicial\ncomplexes offer a principled approach to capture continuous dynamics in such\nstructures. In this work, we introduce COntinuous SiMplicial neural netwOrkS\n(COSMOS), a novel SNN architecture derived from PDEs on simplicial complexes.\nWe provide theoretical and experimental justifications of COSMOS's stability\nunder simplicial perturbations. Furthermore, we investigate the over-smoothing\nphenomenon, a common issue in geometric deep learning, demonstrating that\nCOSMOS offers better control over this effect than discrete SNNs. Our\nexperiments on real-world datasets of ocean trajectory prediction and\nregression on partial deformable shapes demonstrate that COSMOS achieves\ncompetitive performance compared to state-of-the-art SNNs in complex and noisy\nenvironments.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Open-Set Plankton Recognition",
    "url": "http://arxiv.org/abs/2503.11318v1",
    "authors": [
      "Joona Kareinen",
      "Annaliina Skytt\u00e4",
      "Tuomas Eerola",
      "Kaisa Kraft",
      "Lasse Lensu",
      "Sanna Suikkanen",
      "Maiju Lehtiniemi",
      "Heikki K\u00e4lvi\u00e4inen"
    ],
    "published": "2025-03-14",
    "abstract": "This paper considers open-set recognition (OSR) of plankton images. Plankton\ninclude a diverse range of microscopic aquatic organisms that have an important\nrole in marine ecosystems as primary producers and as a base of food webs.\nGiven their sensitivity to environmental changes, fluctuations in plankton\npopulations offer valuable information about oceans' health and climate change\nmotivating their monitoring. Modern automatic plankton imaging devices enable\nthe collection of large-scale plankton image datasets, facilitating\nspecies-level analysis. Plankton species recognition can be seen as an image\nclassification task and is typically solved using deep learning-based image\nrecognition models. However, data collection in real aquatic environments\nresults in imaging devices capturing a variety of non-plankton particles and\nplankton species not present in the training set. This creates a challenging\nfine-grained OSR problem, characterized by subtle differences between\ntaxonomically close plankton species. We address this challenge by conducting\nextensive experiments on three OSR approaches using both phyto- and zooplankton\nimages analyzing also on the effect of the rejection thresholds for OSR. The\nresults demonstrate that high OSR accuracy can be obtained promoting the use of\nthese methods in operational plankton research. We have made the data publicly\navailable to the research community.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Physics-Informed Residual Neural Ordinary Differential Equations for Enhanced Tropical Cyclone Intensity Forecasting",
    "url": "http://arxiv.org/abs/2503.06436v1",
    "authors": [
      "Fan Meng"
    ],
    "published": "2025-03-09",
    "abstract": "Accurate tropical cyclone (TC) intensity prediction is crucial for mitigating\nstorm hazards, yet its complex dynamics pose challenges to traditional methods.\nHere, we introduce a Physics-Informed Residual Neural Ordinary Differential\nEquation (PIR-NODE) model to precisely forecast TC intensity evolution. This\nmodel leverages the powerful non-linear fitting capabilities of deep learning,\nintegrates residual connections to enhance model depth and training stability,\nand explicitly models the continuous temporal evolution of TC intensity using\nNeural ODEs. Experimental results in the SHIPS dataset demonstrate that the\nPIR-NODE model achieves a significant improvement in 24-hour intensity\nprediction accuracy compared to traditional statistical models and benchmark\ndeep learning methods, with a 25. 2\\% reduction in the root mean square error\n(RMSE) and a 19.5\\% increase in R-square (R2) relative to a baseline of neural\nnetwork. Crucially, the residual structure effectively preserves initial state\ninformation, and the model exhibits robust generalization capabilities. This\nstudy details the PIR-NODE model architecture, physics-informed integration\nstrategies, and comprehensive experimental validation, revealing the\nsubstantial potential of deep learning techniques in predicting complex\ngeophysical systems and laying the foundation for future refined TC forecasting\nresearch.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Correlation to Causation: A Causal Deep Learning Framework for Arctic Sea Ice Prediction",
    "url": "http://arxiv.org/abs/2503.02093v1",
    "authors": [
      "Emam Hossain",
      "Muhammad Hasan Ferdous",
      "Jianwu Wang",
      "Aneesh Subramanian",
      "Md Osman Gani"
    ],
    "published": "2025-03-03",
    "abstract": "Traditional machine learning and deep learning techniques rely on\ncorrelation-based learning, often failing to distinguish spurious associations\nfrom true causal relationships, which limits robustness, interpretability, and\ngeneralizability. To address these challenges, we propose a causality-driven\ndeep learning framework that integrates Multivariate Granger Causality (MVGC)\nand PCMCI+ causal discovery algorithms with a hybrid deep learning\narchitecture. Using 43 years (1979-2021) of daily and monthly Arctic Sea Ice\nExtent (SIE) and ocean-atmospheric datasets, our approach identifies causally\nsignificant factors, prioritizes features with direct influence, reduces\nfeature overhead, and improves computational efficiency. Experiments\ndemonstrate that integrating causal features enhances the deep learning model's\npredictive accuracy and interpretability across multiple lead times. Beyond SIE\nprediction, the proposed framework offers a scalable solution for dynamic,\nhigh-dimensional systems, advancing both theoretical understanding and\npractical applications in predictive modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "The Role, Trends, and Applications of Machine Learning in Undersea Communication: A Bangladesh Perspective",
    "url": "http://arxiv.org/abs/2503.00669v1",
    "authors": [
      "Yousuf Islam",
      "Sumon Chandra Das",
      "Md. Jalal Uddin Chowdhury"
    ],
    "published": "2025-03-01",
    "abstract": "The rapid evolution of machine learning (ML) has brought about groundbreaking\ndevelopments in numerous industries, not the least of which is in the area of\nundersea communication. This domain is critical for applications like ocean\nexploration, environmental monitoring, resource management, and national\nsecurity. Bangladesh, a maritime nation with abundant resources in the Bay of\nBengal, can harness the immense potential of ML to tackle the unprecedented\nchallenges associated with underwater communication. Beyond that, environmental\nconditions are unique to the region: in addition to signal attenuation,\nmultipath propagation, noise interference, and limited bandwidth. In this\nstudy, we address the necessity to bring ML into communication via undersea; it\ninvestigates the latest technologies under the domain of ML in that respect,\nsuch as deep learning and reinforcement learning, especially concentrating on\nBangladesh scenarios in the sense of implementation. This paper offers a\ncontextualized regional perspective by incorporating region-specific needs,\ncase studies, and recent research to propose a roadmap for deploying ML-driven\nsolutions to improve safety at sea, promote sustainable resource use, and\nenhance disaster response systems. This research ultimately highlights the\npromise of ML-powered solutions for transforming undersea communication,\nleading to more efficient and cost-effective technologies that subsequently\ncontribute to both economic growth and environmental sustainability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Integrating Weather Station Data and Radar for Precipitation Nowcasting: SmaAt-fUsion and SmaAt-Krige-GNet",
    "url": "http://arxiv.org/abs/2502.16116v1",
    "authors": [
      "Aleksej Cornelissen",
      "Jie Shi",
      "Siamak Mehrkanoon"
    ],
    "published": "2025-02-22",
    "abstract": "In recent years, data-driven, deep learning-based approaches for\nprecipitation nowcasting have attracted significant attention, showing\npromising results. However, many existing models fail to fully exploit the\nextensive atmospheric information available, relying primarily on precipitation\ndata alone. This study introduces two novel deep learning architectures,\nSmaAt-fUsion and SmaAt-Krige-GNet, specifically designed to enhance\nprecipitation nowcasting by integrating multi-variable weather station data\nwith radar datasets. By leveraging additional meteorological information, these\nmodels improve representation learning in the latent space, resulting in\nenhanced nowcasting performance. The SmaAt-fUsion model extends the SmaAt-UNet\nframework by incorporating weather station data through a convolutional layer,\nintegrating it into the bottleneck of the network. Conversely, the\nSmaAt-Krige-GNet model combines precipitation maps with weather station data\nprocessed using Kriging, a geo-statistical interpolation method, to generate\nvariable-specific maps. These maps are then utilized in a dual-encoder\narchitecture based on SmaAt-GNet, allowing multi-level data integration.\nExperimental evaluations were conducted using four years (2016--2019) of\nweather station and precipitation radar data from the Netherlands. Results\ndemonstrate that SmaAt-Krige-GNet outperforms the standard SmaAt-UNet, which\nrelies solely on precipitation radar data, in low precipitation scenarios,\nwhile SmaAt-fUsion surpasses SmaAt-UNet in both low and high precipitation\nscenarios. This highlights the potential of incorporating discrete weather\nstation data to enhance the performance of deep learning-based weather\nnowcasting models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "CondensNet: Enabling stable long-term climate simulations via hybrid deep learning models with adaptive physical constraints",
    "url": "http://arxiv.org/abs/2502.13185v1",
    "authors": [
      "Xin Wang",
      "Juntao Yang",
      "Jeff Adie",
      "Simon See",
      "Kalli Furtado",
      "Chen Chen",
      "Troy Arcomano",
      "Romit Maulik",
      "Gianmarco Mengaldo"
    ],
    "published": "2025-02-18",
    "abstract": "Accurate and efficient climate simulations are crucial for understanding\nEarth's evolving climate. However, current general circulation models (GCMs)\nface challenges in capturing unresolved physical processes, such as cloud and\nconvection. A common solution is to adopt cloud resolving models, that provide\nmore accurate results than the standard subgrid parametrisation schemes\ntypically used in GCMs. However, cloud resolving models, also referred to as\nsuper paramtetrizations, remain computationally prohibitive. Hybrid modeling,\nwhich integrates deep learning with equation-based GCMs, offers a promising\nalternative but often struggles with long-term stability and accuracy issues.\nIn this work, we find that water vapor oversaturation during condensation is a\nkey factor compromising the stability of hybrid models. To address this, we\nintroduce CondensNet, a novel neural network architecture that embeds a\nself-adaptive physical constraint to correct unphysical condensation processes.\nCondensNet effectively mitigates water vapor oversaturation, enhancing\nsimulation stability while maintaining accuracy and improving computational\nefficiency compared to super parameterization schemes.\n  We integrate CondensNet into a GCM to form PCNN-GCM (Physics-Constrained\nNeural Network GCM), a hybrid deep learning framework designed for long-term\nstable climate simulations in real-world conditions, including ocean and land.\nPCNN-GCM represents a significant milestone in hybrid climate modeling, as it\nshows a novel way to incorporate physical constraints adaptively, paving the\nway for accurate, lightweight, and stable long-term climate simulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Skillful Nowcasting of Convective Clouds With a Cascade Diffusion Model",
    "url": "http://arxiv.org/abs/2502.10957v1",
    "authors": [
      "Haoming Chen",
      "Xiaohui Zhong",
      "Qiang Zhai",
      "Xiaomeng Li",
      "Ying Wa Chan",
      "Pak Wai Chan",
      "Yuanyuan Huang",
      "Hao Li",
      "Xiaoming Shi"
    ],
    "published": "2025-02-16",
    "abstract": "Accurate nowcasting of convective clouds from satellite imagery is essential\nfor mitigating the impacts of meteorological disasters, especially in\ndeveloping countries and remote regions with limited ground-based observations.\nRecent advances in deep learning have shown promise in video prediction;\nhowever, existing models frequently produce blurry results and exhibit reduced\naccuracy when forecasting physical fields. Here, we introduce SATcast, a\ndiffusion model that leverages a cascade architecture and multimodal inputs for\nnowcasting cloud fields in satellite imagery. SATcast incorporates physical\nfields predicted by FuXi, a deep-learning weather model, alongside past\nsatellite observations as conditional inputs to generate high-quality future\ncloud fields. Through comprehensive evaluation, SATcast outperforms\nconventional methods on multiple metrics, demonstrating its superior accuracy\nand robustness. Ablation studies underscore the importance of its multimodal\ndesign and the cascade architecture in achieving reliable predictions. Notably,\nSATcast maintains predictive skill for up to 24 hours, underscoring its\npotential for operational nowcasting applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Learning to generate physical ocean states: Towards hybrid climate modeling",
    "url": "http://arxiv.org/abs/2502.02499v1",
    "authors": [
      "Etienne Meunier",
      "David Kamm",
      "Guillaume Gachon",
      "Redouane Lguensat",
      "Julie Deshayes"
    ],
    "published": "2025-02-04",
    "abstract": "Ocean General Circulation Models require extensive computational resources to\nreach equilibrium states, while deep learning emulators, despite offering fast\npredictions, lack the physical interpretability and long-term stability\nnecessary for climate scientists to understand climate sensitivity (to\ngreenhouse gas emissions) and mechanisms of abrupt % variability such as\ntipping points. We propose to take the best from both worlds by leveraging deep\ngenerative models to produce physically consistent oceanic states that can\nserve as initial conditions for climate projections. We assess the viability of\nthis hybrid approach through both physical metrics and numerical experiments,\nand highlight the benefits of enforcing physical constraints during generation.\nAlthough we train here on ocean variables from idealized numerical simulations,\nwe claim that this hybrid approach, combining the computational efficiency of\ndeep learning with the physical accuracy of numerical models, can effectively\nreduce the computational burden of running climate models to equilibrium, and\nreduce uncertainties in climate projections by minimizing drifts in baseline\nsimulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FireCastNet: Earth-as-a-Graph for Seasonal Fire Prediction",
    "url": "http://arxiv.org/abs/2502.01550v1",
    "authors": [
      "Dimitrios Michail",
      "Charalampos Davalas",
      "Lefki-Ioanna Panagiotou",
      "Ioannis Prapas",
      "Spyros Kondylatos",
      "Nikolaos Ioannis Bountos",
      "Ioannis Papoutsis"
    ],
    "published": "2025-02-03",
    "abstract": "With climate change expected to exacerbate fire weather conditions, the\naccurate and timely anticipation of wildfires becomes increasingly crucial for\ndisaster mitigation. In this study, we utilize SeasFire, a comprehensive global\nwildfire dataset with climate, vegetation, oceanic indices, and human-related\nvariables, to enable seasonal wildfire forecasting with machine learning. For\nthe predictive analysis, we present FireCastNet, a novel architecture which\ncombines a 3D convolutional encoder with GraphCast, originally developed for\nglobal short-term weather forecasting using graph neural networks. FireCastNet\nis trained to capture the context leading to wildfires, at different spatial\nand temporal scales. Our investigation focuses on assessing the effectiveness\nof our model in predicting the presence of burned areas at varying forecasting\ntime horizons globally, extending up to six months into the future, and on how\ndifferent spatial or/and temporal context affects the performance. Our findings\ndemonstrate the potential of deep learning models in seasonal fire forecasting;\nlonger input time-series leads to more robust predictions, while integrating\nspatial information to capture wildfire spatio-temporal dynamics boosts\nperformance. Finally, our results hint that in order to enhance performance at\nlonger forecasting horizons, a larger receptive field spatially needs to be\nconsidered.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "OneForecast: A Universal Framework for Global and Regional Weather Forecasting",
    "url": "http://arxiv.org/abs/2502.00338v1",
    "authors": [
      "Yuan Gao",
      "Hao Wu",
      "Ruiqi Shu",
      "Huanshuo Dong",
      "Fan Xu",
      "Rui Chen",
      "Yibo Yan",
      "Qingsong Wen",
      "Xuming Hu",
      "Kun Wang",
      "Jiahao Wu",
      "Qing Li",
      "Hui Xiong",
      "Xiaomeng Huang"
    ],
    "published": "2025-02-01",
    "abstract": "Accurate weather forecasts are important for disaster prevention,\nagricultural planning, and water resource management. Traditional numerical\nweather prediction (NWP) methods offer physically interpretable high-accuracy\npredictions but are computationally expensive and fail to fully leverage\nrapidly growing historical data. In recent years, deep learning methods have\nmade significant progress in weather forecasting, but challenges remain, such\nas balancing global and regional high-resolution forecasts, excessive smoothing\nin extreme event predictions, and insufficient dynamic system modeling. To\naddress these issues, this paper proposes a global-regional nested weather\nforecasting framework based on graph neural networks (GNNs). By combining a\ndynamic system perspective with multi-grid theory, we construct a multi-scale\ngraph structure and densify the target region to capture local high-frequency\nfeatures. We introduce an adaptive information propagation mechanism, using\ndynamic gating units to deeply integrate node and edge features for more\naccurate extreme event forecasting. For high-resolution regional forecasts, we\npropose a neural nested grid method to mitigate boundary information loss.\nExperimental results show that the proposed method performs excellently across\nglobal to regional scales and short-term to long-term forecasts, especially in\nextreme event predictions (e.g., typhoons), significantly improving forecast\naccuracy. Our codes are available at https://github.com/YuanGao-YG/OneForecast.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Improving Tropical Cyclone Forecasting With Video Diffusion Models",
    "url": "http://arxiv.org/abs/2501.16003v4",
    "authors": [
      "Zhibo Ren",
      "Pritthijit Nath",
      "Pancham Shukla"
    ],
    "published": "2025-01-27",
    "abstract": "Tropical cyclone (TC) forecasting is crucial for disaster preparedness and\nmitigation. While recent deep learning approaches have shown promise, existing\nmethods often treat TC evolution as a series of independent frame-to-frame\npredictions, limiting their ability to capture long-term dynamics. We present a\nnovel application of video diffusion models for TC forecasting that explicitly\nmodels temporal dependencies through additional temporal layers. Our approach\nenables the model to generate multiple frames simultaneously, better capturing\ncyclone evolution patterns. We introduce a two-stage training strategy that\nsignificantly improves individual-frame quality and performance in low-data\nregimes. Experimental results show our method outperforms the previous approach\nof Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably,\nwe extend the reliable forecasting horizon from 36 to 50 hours. Through\ncomprehensive evaluation using both traditional metrics and Fr\\'echet Video\nDistance (FVD), we demonstrate that our approach produces more temporally\ncoherent forecasts while maintaining competitive single-frame quality. Code\naccessible at https://github.com/Ren-creater/forecast-video-diffmodels.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Deep State Space Model for Rainfall-Runoff Simulations",
    "url": "http://arxiv.org/abs/2501.14980v1",
    "authors": [
      "Yihan Wang",
      "Lujun Zhang",
      "Annan Yu",
      "N. Benjamin Erichson",
      "Tiantian Yang"
    ],
    "published": "2025-01-24",
    "abstract": "The classical way of studying the rainfall-runoff processes in the water\ncycle relies on conceptual or physically-based hydrologic models. Deep learning\n(DL) has recently emerged as an alternative and blossomed in hydrology\ncommunity for rainfall-runoff simulations. However, the decades-old Long\nShort-Term Memory (LSTM) network remains the benchmark for this task,\noutperforming newer architectures like Transformers. In this work, we propose a\nState Space Model (SSM), specifically the Frequency Tuned Diagonal State Space\nSequence (S4D-FT) model, for rainfall-runoff simulations. The proposed S4D-FT\nis benchmarked against the established LSTM and a physically-based Sacramento\nSoil Moisture Accounting model across 531 watersheds in the contiguous United\nStates (CONUS). Results show that S4D-FT is able to outperform the LSTM model\nacross diverse regions. Our pioneering introduction of the S4D-FT for\nrainfall-runoff simulations challenges the dominance of LSTM in the hydrology\ncommunity and expands the arsenal of DL tools available for hydrological\nmodeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "Simultaneous emulation and downscaling with physically-consistent deep learning-based regional ocean emulators",
    "url": "http://arxiv.org/abs/2501.05058v1",
    "authors": [
      "Leonard Lupin-Jimenez",
      "Moein Darman",
      "Subhashis Hazarika",
      "Tianning Wu",
      "Michael Gray",
      "Ruyoing He",
      "Anthony Wong",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-01-09",
    "abstract": "Building on top of the success in AI-based atmospheric emulation, we propose\nan AI-based ocean emulation and downscaling framework focusing on the\nhigh-resolution regional ocean over Gulf of Mexico. Regional ocean emulation\npresents unique challenges owing to the complex bathymetry and lateral boundary\nconditions as well as from fundamental biases in deep learning-based\nframeworks, such as instability and hallucinations. In this paper, we develop a\ndeep learning-based framework to autoregressively integrate ocean-surface\nvariables over the Gulf of Mexico at $8$ Km spatial resolution without\nunphysical drifts over decadal time scales and simulataneously downscale and\nbias-correct it to $4$ Km resolution using a physics-constrained generative\nmodel. The framework shows both short-term skills as well as accurate long-term\nstatistics in terms of mean and variability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Analogue Forecast System for Daily Precipitation Prediction Using Autoencoder Feature Extraction: Application in Hong Kong",
    "url": "http://arxiv.org/abs/2501.02814v1",
    "authors": [
      "Yee Chun Tsoi",
      "Yu Ting Kwok",
      "Ming Chun Lam",
      "Wai Kin Wong"
    ],
    "published": "2025-01-06",
    "abstract": "In the Hong Kong Observatory, the Analogue Forecast System (AFS) for\nprecipitation has been providing useful reference in predicting possible daily\nrainfall scenarios for the next 9 days, by identifying historical cases with\nsimilar weather patterns to the latest output from the deterministic model of\nthe European Centre for Medium-Range Weather Forecasts (ECMWF). Recent advances\nin machine learning allow more sophisticated models to be trained using\nhistorical data and the patterns of high-impact weather events to be\nrepresented more effectively. As such, an enhanced AFS has been developed using\nthe deep learning technique autoencoder. The datasets of the fifth generation\nof the ECMWF Reanalysis (ERA5) are utilised where more meteorological elements\nin higher horizontal, vertical and temporal resolutions are available as\ncompared to the previous ECMWF reanalysis products used in the existing AFS.\nThe enhanced AFS features four major steps in generating the daily rain class\nforecasts: (1) preprocessing of gridded ERA5 and ECMWF model forecast, (2)\nfeature extraction by the pretrained autoencoder, (3) application of optimised\nfeature weightings based on historical cases, and (4) calculation of the final\nrain class from a weighted ensemble of top analogues. The enhanced AFS\ndemonstrates a consistent and superior performance over the existing AFS,\nespecially in capturing heavy rain cases, during the verification period from\n2019 to 2022. This paper presents the detailed formulation of the enhanced AFS\nand discusses its advantages and limitations in supporting precipitation\nforecasting in Hong Kong.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "LWFNet: Coherent Doppler Wind Lidar-Based Network for Wind Field Retrieval",
    "url": "http://arxiv.org/abs/2501.02613v1",
    "authors": [
      "Ran Tao",
      "Chong Wang",
      "Hao Chen",
      "Mingjiao Jia",
      "Xiang Shang",
      "Luoyuan Qu",
      "Guoliang Shentu",
      "Yanyu Lu",
      "Yanfeng Huo",
      "Lei Bai",
      "Xianghui Xue",
      "Xiankang Dou"
    ],
    "published": "2025-01-05",
    "abstract": "Accurate detection of wind fields within the troposphere is essential for\natmospheric dynamics research and plays a crucial role in extreme weather\nforecasting. Coherent Doppler wind lidar (CDWL) is widely regarded as the most\nsuitable technique for high spatial and temporal resolution wind field\ndetection. However, since coherent detection relies heavily on the\nconcentration of aerosol particles, which cause Mie scattering, the received\nbackscattering lidar signal exhibits significantly low intensity at high\naltitudes. As a result, conventional methods, such as spectral centroid\nestimation, often fail to produce credible and accurate wind retrieval results\nin these regions. To address this issue, we propose LWFNet, the first\nLidar-based Wind Field (WF) retrieval neural Network, built upon Transformer\nand the Kolmogorov-Arnold network. Our model is trained solely on targets\nderived from the traditional wind retrieval algorithm and utilizes radiosonde\nmeasurements as the ground truth for test results evaluation. Experimental\nresults demonstrate that LWFNet not only extends the maximum wind field\ndetection range but also produces more accurate results, exhibiting a level of\nprecision that surpasses the labeled targets. This phenomenon, which we refer\nto as super-accuracy, is explored by investigating the potential underlying\nfactors that contribute to this intriguing occurrence. In addition, we compare\nthe performance of LWFNet with other state-of-the-art (SOTA) models,\nhighlighting its superior effectiveness and capability in high-resolution wind\nretrieval. LWFNet demonstrates remarkable performance in lidar-based wind field\nretrieval, setting a benchmark for future research and advancing the\ndevelopment of deep learning models in this domain.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Paraformer: Parameterization of Sub-grid Scale Processes Using Transformers",
    "url": "http://arxiv.org/abs/2412.16763v1",
    "authors": [
      "Shuochen Wang",
      "Nishant Yadav",
      "Auroop R. Ganguly"
    ],
    "published": "2024-12-21",
    "abstract": "One of the major sources of uncertainty in the current generation of Global\nClimate Models (GCMs) is the representation of sub-grid scale physical\nprocesses. Over the years, a series of deep-learning-based parameterization\nschemes have been developed and tested on both idealized and real-geography\nGCMs. However, datasets on which previous deep-learning models were trained\neither contain limited variables or have low spatial-temporal coverage, which\ncan not fully simulate the parameterization process. Additionally, these\nschemes rely on classical architectures while the latest attention mechanism\nused in Transformer models remains unexplored in this field. In this paper, we\npropose Paraformer, a \"memory-aware\" Transformer-based model on ClimSim, the\nlargest dataset ever created for climate parameterization. Our results\ndemonstrate that the proposed model successfully captures the complex\nnon-linear dependencies in the sub-grid scale variables and outperforms\nclassical deep-learning architectures. This work highlights the applicability\nof the attenuation mechanism in this field and provides valuable insights for\ndeveloping future deep-learning-based climate parameterization schemes.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Improved Forecasts of Global Extreme Marine Heatwaves Through a Physics-guided Data-driven Approach",
    "url": "http://arxiv.org/abs/2412.15532v1",
    "authors": [
      "Ruiqi Shu",
      "Hao Wu",
      "Yuan Gao",
      "Fanghua Xu",
      "Ruijian Gou",
      "Xiaomeng Huang"
    ],
    "published": "2024-12-20",
    "abstract": "The unusually warm sea surface temperature events known as marine heatwaves\n(MHWs) have a profound impact on marine ecosystems. Accurate prediction of\nextreme MHWs has significant scientific and financial worth. However, existing\nmethods still have certain limitations, especially in the most extreme MHWs. In\nthis study, to address these issues, based on the physical nature of MHWs, we\ncreated a novel deep learning neural network that is capable of accurate 10-day\nMHW forecasting. Our framework significantly improves the forecast ability of\nextreme MHWs through two specially designed modules inspired by numerical\nmodels: a coupler and a probabilistic data argumentation. The coupler simulates\nthe driving effect of atmosphere on MHWs while the probabilistic data\nargumentation approaches significantly boost the forecast ability of extreme\nMHWs based on the idea of ensemble forecast. Compared with traditional\nnumerical prediction, our framework has significantly higher accuracy and\nrequires fewer computational resources. What's more, explainable AI methods\nshow that wind forcing is the primary driver of MHW evolution and reveal its\nrelation with air-sea heat exchange. Overall, our model provides a framework\nfor understanding MHWs' driving processes and operational forecasts in the\nfuture.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Downscaling Precipitation with Bias-informed Conditional Diffusion Model",
    "url": "http://arxiv.org/abs/2412.14539v1",
    "authors": [
      "Ran Lyu",
      "Linhan Wang",
      "Yanshen Sun",
      "Hedanqiu Bai",
      "Chang-Tien Lu"
    ],
    "published": "2024-12-19",
    "abstract": "Climate change is intensifying rainfall extremes, making high-resolution\nprecipitation projections crucial for society to better prepare for impacts\nsuch as flooding. However, current Global Climate Models (GCMs) operate at\nspatial resolutions too coarse for localized analyses. To address this\nlimitation, deep learning-based statistical downscaling methods offer promising\nsolutions, providing high-resolution precipitation projections with a moderate\ncomputational cost. In this work, we introduce a bias-informed conditional\ndiffusion model for statistical downscaling of precipitation. Specifically, our\nmodel leverages a conditional diffusion approach to learn distribution priors\nfrom large-scale, high-resolution precipitation datasets. The long-tail\ndistribution of precipitation poses a unique challenge for training diffusion\nmodels; to address this, we apply gamma correction during preprocessing.\nAdditionally, to correct biases in the downscaled results, we employ a\nguided-sampling strategy to enhance bias correction. Our experiments\ndemonstrate that the proposed model achieves highly accurate results in an 8\ntimes downscaling setting, outperforming previous deterministic methods. The\ncode and dataset are available at\nhttps://github.com/RoseLV/research_super-resolution",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Generating Unseen Nonlinear Evolution in Sea Surface Temperature Using a Deep Learning-Based Latent Space Data Assimilation Framework",
    "url": "http://arxiv.org/abs/2412.13477v1",
    "authors": [
      "Qingyu Zheng",
      "Guijun Han",
      "Wei Li",
      "Lige Cao",
      "Gongfu Zhou",
      "Haowen Wu",
      "Qi Shao",
      "Ru Wang",
      "Xiaobo Wu",
      "Xudong Cui",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2024-12-18",
    "abstract": "Advances in data assimilation (DA) methods have greatly improved the accuracy\nof Earth system predictions. To fuse multi-source data and reconstruct the\nnonlinear evolution missing from observations, geoscientists are developing\nfuture-oriented DA methods. In this paper, we redesign a purely data-driven\nlatent space DA framework (DeepDA) that employs a generative artificial\nintelligence model to capture the nonlinear evolution in sea surface\ntemperature. Under variational constraints, DeepDA embedded with nonlinear\nfeatures can effectively fuse heterogeneous data. The results show that DeepDA\nremains highly stable in capturing and generating nonlinear evolutions even\nwhen a large amount of observational information is missing. It can be found\nthat when only 10% of the observation information is available, the error\nincrease of DeepDA does not exceed 40%. Furthermore, DeepDA has been shown to\nbe robust in the fusion of real observations and ensemble simulations. In\nparticular, this paper provides a mechanism analysis of the nonlinear evolution\ngenerated by DeepDA from the perspective of physical patterns, which reveals\nthe inherent explainability of our DL model in capturing multi-scale ocean\nsignals.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Staged Deep Learning Approach to Spatial Refinement in 3D Temporal Atmospheric Transport",
    "url": "http://arxiv.org/abs/2412.10945v2",
    "authors": [
      "M. Giselle Fern\u00e1ndez-Godino",
      "Wai Tong Chung",
      "Akshay A. Gowardhan",
      "Matthias Ihme",
      "Qingkai Kong",
      "Donald D. Lucas",
      "Stephen C. Myers"
    ],
    "published": "2024-12-14",
    "abstract": "High-resolution spatiotemporal simulations effectively capture the\ncomplexities of atmospheric plume dispersion in complex terrain. However, their\nhigh computational cost makes them impractical for applications requiring rapid\nresponses or iterative processes, such as optimization, uncertainty\nquantification, or inverse modeling. To address this challenge, this work\nintroduces the Dual-Stage Temporal Three-dimensional UNet Super-resolution\n(DST3D-UNet-SR) model, a highly efficient deep learning model for plume\ndispersion prediction. DST3D-UNet-SR is composed of two sequential modules: the\ntemporal module (TM), which predicts the transient evolution of a plume in\ncomplex terrain from low-resolution temporal data, and the spatial refinement\nmodule (SRM), which subsequently enhances the spatial resolution of the TM\npredictions. We train DST3DUNet- SR using a comprehensive dataset derived from\nhigh-resolution large eddy simulations (LES) of plume transport. We propose the\nDST3D-UNet-SR model to significantly accelerate LES simulations of\nthree-dimensional plume dispersion by three orders of magnitude. Additionally,\nthe model demonstrates the ability to dynamically adapt to evolving conditions\nthrough the incorporation of new observational data, substantially improving\nprediction accuracy in high-concentration regions near the source.\n  Keywords: Atmospheric sciences, Geosciences, Plume transport,3D temporal\nsequences, Artificial intelligence, CNN, LSTM, Autoencoder, Autoregressive\nmodel, U-Net, Super-resolution, Spatial Refinement.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "LSTM",
      "Autoencoder"
    ],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "A Hybrid Deep-Learning Model for El Ni\u00f1o Southern Oscillation in the Low-Data Regime",
    "url": "http://arxiv.org/abs/2412.03743v2",
    "authors": [
      "Jakob Schloer",
      "Matthew Newman",
      "Jannik Thuemmel",
      "Antonietta Capotondi",
      "Bedartha Goswami"
    ],
    "published": "2024-12-04",
    "abstract": "While deep-learning models have demonstrated skillful El Ni\\~no Southern\nOscillation (ENSO) forecasts up to one year in advance, they are predominantly\ntrained on climate model simulations that provide thousands of years of\ntraining data at the expense of introducing climate model biases. Simpler\nLinear Inverse Models (LIMs) trained on the much shorter observational record\nalso make skillful ENSO predictions but do not capture predictable nonlinear\nprocesses. This motivates a hybrid approach, combining the LIMs modest data\nneeds with a deep-learning non-Markovian correction of the LIM. For O(100 yr)\ndatasets, our resulting Hybrid model is more skillful than the LIM while also\nexceeding the skill of a full deep-learning model. Additionally, while the most\npredictable ENSO events are still identified in advance by the LIM, they are\nbetter predicted by the Hybrid model, especially in the western tropical\nPacific for leads beyond about 9 months, by capturing the subsequent asymmetric\n(warm versus cold phases) evolution of ENSO.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Learning Surrogate Rainfall-driven Inundation Models with Few Data",
    "url": "http://arxiv.org/abs/2411.19323v1",
    "authors": [
      "Marzieh Alireza Mirhoseini"
    ],
    "published": "2024-11-28",
    "abstract": "Flood hazard assessment demands fast and accurate predictions. Hydrodynamic\nmodels are detailed but computationally intensive, making them impractical for\nquantifying uncertainty or identifying extremes. In contrast, machine learning\nsurrogates can be rapid, but training on scarce simulated or observed extreme\ndata can also be ineffective. This work demonstrates the development of an\neffective surrogate model for flood hazard prediction by initializing deep\nlearning (ResNet-18) with ensemble-approximated Conditional Gaussian Processes\n(EnsCGP) and finalizing it with a bias correction. The proposed methodology\ncouples EnsCGP with a ResNet-18 architecture to estimate flood depth and uses\nensemble optimal estimation for bias correction. The surrogate model was\ntrained and evaluated using rainfall data from Daymet and hydrodynamic\nsimulations from LISFLOOD-FP, spanning the period from 1981 to 2019. The\ntraining involved using data up to a certain year and testing on the subsequent\nyear, iteratively progressing through the dataset. This process required\napproximately 100 training iterations and extensive data. Inundation depths are\nestimated rapidly at runtime (approximately 0.006 seconds per event). Results\nover multiple years in the current climate over Chicago demonstrate an average\nR-squared greater than 0.96, with median relative errors in flood depth\nestimates of about 1 percent.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Maximizing the Impact of Deep Learning on Subseasonal-to-Seasonal Climate Forecasting: The Essential Role of Optimization",
    "url": "http://arxiv.org/abs/2411.16728v1",
    "authors": [
      "Yizhen Guo",
      "Tian Zhou",
      "Wanyi Jiang",
      "Bo Wu",
      "Liang Sun",
      "Rong Jin"
    ],
    "published": "2024-11-23",
    "abstract": "Weather and climate forecasting is vital for sectors such as agriculture and\ndisaster management. Although numerical weather prediction (NWP) systems have\nadvanced, forecasting at the subseasonal-to-seasonal (S2S) scale, spanning 2 to\n6 weeks, remains challenging due to the chaotic and sparse atmospheric signals\nat this interval. Even state-of-the-art deep learning models struggle to\noutperform simple climatology models in this domain. This paper identifies that\noptimization, instead of network structure, could be the root cause of this\nperformance gap, and then we develop a novel multi-stage optimization strategy\nto close the gap. Extensive empirical studies demonstrate that our multi-stage\noptimization approach significantly improves key skill metrics, PCC and TCC,\nwhile utilizing the same backbone structure, surpassing the state-of-the-art\nNWP systems (ECMWF-S2S) by over \\textbf{19-91\\%}. Our research contests the\nrecent study that direct forecasting outperforms rolling forecasting for S2S\ntasks. Through theoretical analysis, we propose that the underperformance of\nrolling forecasting may arise from the accumulation of Jacobian matrix products\nduring training. Our multi-stage framework can be viewed as a form of teacher\nforcing to address this issue. Code is available at\n\\url{https://anonymous.4open.science/r/Baguan-S2S-23E7/}",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Global spatio-temporal downscaling of ERA5 precipitation through generative AI",
    "url": "http://arxiv.org/abs/2411.16098v1",
    "authors": [
      "Luca Glawion",
      "Julius Polz",
      "Harald Kunstmann",
      "Benjamin Fersch",
      "Christian Chwala"
    ],
    "published": "2024-11-22",
    "abstract": "The spatial and temporal distribution of precipitation has a significant\nimpact on human lives by determining freshwater resources and agricultural\nyield, but also rainfall-driven hazards like flooding or landslides. While the\nERA5 reanalysis dataset provides consistent long-term global precipitation\ninformation that allows investigations of these impacts, it lacks the\nresolution to capture the high spatio-temporal variability of precipitation.\nERA5 misses intense local rainfall events that are crucial drivers of\ndevastating flooding - a critical limitation since extreme weather events\nbecome increasingly frequent. Here, we introduce spateGAN-ERA5, the first deep\nlearning based spatio-temporal downscaling of precipitation data on a global\nscale. SpateGAN-ERA5 uses a conditional generative adversarial neural network\n(cGAN) that enhances the resolution of ERA5 precipitation data from 24 km and 1\nhour to 2 km and 10 minutes, delivering high-resolution rainfall fields with\nrealistic spatio-temporal patterns and accurate rain rate distribution\nincluding extremes. Its computational efficiency enables the generation of a\nlarge ensemble of solutions, addressing uncertainties inherent to the\nchallenges of downscaling. Trained solely on data from Germany and validated in\nthe US and Australia considering diverse climate zones, spateGAN-ERA5\ndemonstrates strong generalization indicating a robust global applicability.\nSpateGAN-ERA5 fulfils a critical need for high-resolution precipitation data in\nhydrological and meteorological research, offering new capabilities for flood\nrisk assessment, AI-enhanced weather forecasting, and impact modelling to\naddress climate-driven challenges worldwide.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Leadsee-Precip: A Deep Learning Diagnostic Model for Precipitation",
    "url": "http://arxiv.org/abs/2411.12640v1",
    "authors": [
      "Weiwen Ji",
      "Jin Feng",
      "Yueqi Liu",
      "Yulu Qiu",
      "Hua Gao"
    ],
    "published": "2024-11-19",
    "abstract": "Recently, deep-learning weather forecasting models have surpassed traditional\nnumerical models in terms of the accuracy of meteorological variables. However,\nthere is considerable potential for improvements in precipitation forecasts,\nespecially for heavy precipitation events. To address this deficiency, we\npropose Leadsee-Precip, a global deep learning model to generate precipitation\nfrom meteorological circulation fields. The model utilizes an information\nbalance scheme to tackle the challenges of predicting heavy precipitation\ncaused by the long-tail distribution of precipitation data. Additionally, more\naccurate satellite and radar-based precipitation retrievals are used as\ntraining targets. Compared to artificial intelligence global weather models,\nthe heavy precipitation from Leadsee-Precip is more consistent with\nobservations and shows competitive performance against global numerical weather\nprediction models. Leadsee-Precip can be integrated with any global circulation\nmodel to generate precipitation forecasts. But the deviations between the\npredicted and the ground-truth circulation fields may lead to a weakened\nprecipitation forecast, which could potentially be mitigated by further\nfine-tuning based on the predicted circulation fields.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Advancing Marine Heatwave Forecasts: An Integrated Deep Learning Approach",
    "url": "http://arxiv.org/abs/2412.04475v1",
    "authors": [
      "Ding Ning",
      "Varvara Vetrova",
      "Yun Sing Koh",
      "Karin R. Bryan"
    ],
    "published": "2024-11-19",
    "abstract": "Marine heatwaves (MHWs), an extreme climate phenomenon, pose significant\nchallenges to marine ecosystems and industries, with their frequency and\nintensity increasing due to climate change. This study introduces an integrated\ndeep learning approach to forecast short-to-long-term MHWs on a global scale.\nThe approach combines graph representation for modeling spatial properties in\nclimate data, imbalanced regression to handle skewed data distributions, and\ntemporal diffusion to enhance forecast accuracy across various lead times. To\nthe best of our knowledge, this is the first study that synthesizes three\nspatiotemporal anomaly methodologies to predict MHWs. Additionally, we\nintroduce a method for constructing graphs that avoids isolated nodes and\nprovide a new publicly available sea surface temperature anomaly graph dataset.\nWe examine the trade-offs in the selection of loss functions and evaluation\nmetrics for MHWs. We analyze spatial patterns in global MHW predictability by\nfocusing on historical hotspots, and our approach demonstrates better\nperformance compared to traditional numerical models in regions such as the\nmiddle south Pacific, equatorial Atlantic near Africa, south Atlantic, and\nhigh-latitude Indian Ocean. We highlight the potential of temporal diffusion to\nreplace the conventional sliding window approach for long-term forecasts,\nachieving improved prediction up to six months in advance. These insights not\nonly establish benchmarks for machine learning applications in MHW forecasting\nbut also enhance understanding of general climate forecasting methodologies.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "FengWu-W2S: A deep learning model for seamless weather-to-subseasonal forecast of global atmosphere",
    "url": "http://arxiv.org/abs/2411.10191v2",
    "authors": [
      "Fenghua Ling",
      "Kang Chen",
      "Jiye Wu",
      "Tao Han",
      "Jing-Jia Luo",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "published": "2024-11-15",
    "abstract": "Seamless forecasting that produces warning information at continuum\ntimescales based on only one system is a long-standing pursuit for\nweather-climate service. While the rapid advancement of deep learning has\ninduced revolutionary changes in classical forecasting field, current efforts\nare still focused on building separate AI models for weather and climate\nforecasts. To explore the seamless forecasting ability based on one AI model,\nwe propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the\nFengWu global weather forecast model and incorporates an ocean-atmosphere-land\ncoupling structure along with a diverse perturbation strategy. FengWu-W2S can\ngenerate 6-hourly atmosphere forecasts extending up to 42 days through an\nautoregressive and seamless manner. Our hindcast results demonstrate that\nFengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead,\nenhancing predictive capabilities for global surface air temperature,\nprecipitation, geopotential height and intraseasonal signals such as the\nMadden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover,\nour ablation experiments on forecast error growth from daily to seasonal\ntimescales reveal potential pathways for developing AI-based integrated system\nfor seamless weather-climate forecasting in the future.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "An Analysis of Deep Learning Parameterizations for Ocean Subgrid Eddy Forcing",
    "url": "http://arxiv.org/abs/2411.06604v1",
    "authors": [
      "Cem Gultekin",
      "Adam Subel",
      "Cheng Zhang",
      "Matan Leibovich",
      "Pavel Perezhogin",
      "Alistair Adcroft",
      "Carlos Fernandez-Granda",
      "Laure Zanna"
    ],
    "published": "2024-11-10",
    "abstract": "Due to computational constraints, climate simulations cannot resolve a range\nof small-scale physical processes, which have a significant impact on the\nlarge-scale evolution of the climate system. Parameterization is an approach to\ncapture the effect of these processes, without resolving them explicitly. In\nrecent years, data-driven parameterizations based on convolutional neural\nnetworks have obtained promising results. In this work, we provide an in-depth\nanalysis of these parameterizations developed using data from ocean\nsimulations. The parametrizations account for the effect of mesoscale eddies\ntoward improving simulations of momentum, heat, and mass exchange in the ocean.\nOur results provide several insights into the properties of data-driven\nparameterizations based on neural networks. First, their performance can be\nsubstantially improved by increasing the geographic extent of the training\ndata. Second, they learn nonlinear structure, since they are able to outperform\na linear baseline. Third, they generalize robustly across different CO2\nforcings, but not necessarily across different ocean depths. Fourth, they\nexploit a relatively small region of their input to generate their output. Our\nresults will guide the further development of ocean mesoscale eddy\nparameterizations, and multiscale modeling more generally.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Are Deep Learning Methods Suitable for Downscaling Global Climate Projections? Review and Intercomparison of Existing Models",
    "url": "http://arxiv.org/abs/2411.05850v1",
    "authors": [
      "Jose Gonz\u00e1lez-Abad",
      "Jos\u00e9 Manuel Guti\u00e9rrez"
    ],
    "published": "2024-11-06",
    "abstract": "Deep Learning (DL) has shown promise for downscaling global climate change\nprojections under different approaches, including Perfect Prognosis (PP) and\nRegional Climate Model (RCM) emulation. Unlike emulators, PP downscaling models\nare trained on observational data, so it remains an open question whether they\ncan plausibly extrapolate unseen conditions and changes in future emissions\nscenarios. Here we focus on this problem as the main drawback for the\noperationalization of these methods and present the results of 1) a literature\nreview to identify state-of-the-art DL models for PP downscaling and 2) an\nintercomparison experiment to evaluate the performance of these models and to\nassess their extrapolation capability using a common experimental framework,\ntaking into account the sensitivity of results to different training replicas.\nWe focus on minimum and maximum temperatures and precipitation over Spain, a\nregion with a range of climatic conditions with different influential regional\nprocesses. We conclude with a discussion of the findings, limitations of\nexisting methods, and prospects for future development.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "PACER: Physics Informed Uncertainty Aware Climate Emulator",
    "url": "http://arxiv.org/abs/2410.21657v2",
    "authors": [
      "Hira Saleem",
      "Flora Salim",
      "Cormac Purcell"
    ],
    "published": "2024-10-29",
    "abstract": "Climate models serve as critical tools for evaluating the effects of climate\nchange and projecting future climate scenarios. However, the reliance on\nnumerical simulations of physical equations renders them computationally\nintensive and inefficient. While deep learning methodologies have made\nsignificant progress in weather forecasting, they are still unstable for\nclimate emulation tasks. Here, we propose PACER, a lightweight 684K parameter\nPhysics Informed Uncertainty Aware Climate Emulator. PACER emulates temperature\nand precipitation stably for 86 years while only being trained on greenhouse\ngas emissions data. We incorporate a fundamental physical law of\nadvection-diffusion in PACER accounting for boundary conditions and empirically\nestimating the diffusion co-efficient and flow velocities from emissions data.\nPACER has been trained on 15 climate models provided by ClimateSet\noutperforming baselines across most of the climate models and advancing a new\nstate of the art in a climate diagnostic task.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Modulated Adaptive Fourier Neural Operators for Temporal Interpolation of Weather Forecasts",
    "url": "http://arxiv.org/abs/2410.18904v1",
    "authors": [
      "Jussi Leinonen",
      "Boris Bonev",
      "Thorsten Kurth",
      "Yair Cohen"
    ],
    "published": "2024-10-24",
    "abstract": "Weather and climate data are often available at limited temporal resolution,\neither due to storage limitations, or in the case of weather forecast models\nbased on deep learning, their inherently long time steps. The coarse temporal\nresolution makes it difficult to capture rapidly evolving weather events. To\naddress this limitation, we introduce an interpolation model that reconstructs\nthe atmospheric state between two points in time for which the state is known.\nThe model makes use of a novel network layer that modifies the adaptive Fourier\nneural operator (AFNO), which has been previously used in weather prediction\nand other applications of machine learning to physics problems. The modulated\nAFNO (ModAFNO) layer takes an embedding, here computed from the interpolation\ntarget time, as an additional input and applies a learned shift-scale operation\ninside the AFNO layers to adapt them to the target time. Thus, one model can be\nused to produce all intermediate time steps. Trained to interpolate between two\ntime steps 6 h apart, the ModAFNO-based interpolation model produces 1 h\nresolution intermediate time steps that are visually nearly indistinguishable\nfrom the actual corresponding 1 h resolution data. The model reduces the RMSE\nloss of reconstructing the intermediate steps by approximately 50% compared to\nlinear interpolation. We also demonstrate its ability to reproduce the\nstatistics of extreme weather events such as hurricanes and heat waves better\nthan 6 h resolution data. The ModAFNO layer is generic and is expected to be\napplicable to other problems, including weather forecasting with tunable lead\ntime.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Evaluating Deep Learning Approaches for Predictions in Unmonitored Basins with Continental-scale Stream Temperature Models",
    "url": "http://arxiv.org/abs/2410.19865v1",
    "authors": [
      "Jared D. Willard",
      "Fabio Ciulla",
      "Helen Weierbach",
      "Vipin Kumar",
      "Charuleka Varadharajan"
    ],
    "published": "2024-10-23",
    "abstract": "The prediction of streamflows and other environmental variables in\nunmonitored basins is a grand challenge in hydrology. Recent machine learning\n(ML) models can harness vast datasets for accurate predictions at large spatial\nscales. However, there are open questions regarding model design and data\nneeded for inputs and training to improve performance. This study explores\nthese questions while demonstrating the ability of deep learning models to make\naccurate stream temperature predictions in unmonitored basins across the\nconterminous United States. First, we compare top-down models that utilize data\nfrom a large number of basins with bottom-up methods that transfer ML models\nbuilt on local sites, reflecting traditional regionalization techniques. We\nalso evaluate an intermediary grouped modeling approach that categorizes sites\nbased on regional co-location or similarity of catchment characteristics.\nSecond, we evaluate trade-offs between model complexity, prediction accuracy,\nand applicability for more target locations by systematically removing inputs.\nWe then examine model performance when additional training data becomes\navailable due to reductions in input requirements. Our results suggest that\ntop-down models significantly outperform bottom-up and grouped models.\nMoreover, it is possible to get acceptable accuracy by reducing both dynamic\nand static inputs enabling predictions for more sites with lower model\ncomplexity and computational needs. From detailed error analysis, we determined\nthat the models are more accurate for sites primarily controlled by air\ntemperatures compared to locations impacted by groundwater and dams. By\naddressing these questions, this research offers a comprehensive perspective on\noptimizing ML model design for accurate predictions in unmonitored regions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark",
    "url": "http://arxiv.org/abs/2410.16695v2",
    "authors": [
      "Yang Yu",
      "Yuezun Li",
      "Xin Sun",
      "Junyu Dong"
    ],
    "published": "2024-10-22",
    "abstract": "Phytoplankton are a crucial component of aquatic ecosystems, and effective\nmonitoring of them can provide valuable insights into ocean environments and\necosystem changes. Traditional phytoplankton monitoring methods are often\ncomplex and lack timely analysis. Therefore, deep learning algorithms offer a\npromising approach for automated phytoplankton monitoring. However, the lack of\nlarge-scale, high-quality training samples has become a major bottleneck in\nadvancing phytoplankton tracking. In this paper, we propose a challenging\nbenchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse\nbackground information and variations in motion during observation. The dataset\nincludes 27 species of phytoplankton and zooplankton, 14 different backgrounds\nto simulate diverse and complex underwater environments, and a total of 140\nvideos. To enable accurate real-time observation of phytoplankton, we introduce\na multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion\nTracker(DSFT), which addresses issues such as focus shifts during tracking and\nthe loss of small target information when computing frame-to-frame similarity.\nSpecifically, we introduce an additional feature extractor to predict the\nresiduals of the standard feature extractor's output, and compute multi-scale\nframe-to-frame similarity based on features from different layers of the\nextractor. Extensive experiments on the MPT have demonstrated the validity of\nthe dataset and the superiority of DSFT in tracking phytoplankton, providing an\neffective solution for phytoplankton monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Deep Learning for Weather Forecasting: A CNN-LSTM Hybrid Model for Predicting Historical Temperature Data",
    "url": "http://arxiv.org/abs/2410.14963v1",
    "authors": [
      "Yuhao Gong",
      "Yuchen Zhang",
      "Fei Wang",
      "Chi-Han Lee"
    ],
    "published": "2024-10-19",
    "abstract": "As global climate change intensifies, accurate weather forecasting has become\nincreasingly important, affecting agriculture, energy management, environmental\nprotection, and daily life. This study introduces a hybrid model combining\nConvolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks\nto predict historical temperature data. CNNs are utilized for spatial feature\nextraction, while LSTMs handle temporal dependencies, resulting in\nsignificantly improved prediction accuracy and stability. By using Mean\nAbsolute Error (MAE) as the loss function, the model demonstrates excellent\nperformance in processing complex meteorological data, addressing challenges\nsuch as missing data and high-dimensionality. The results show a strong\nalignment between the prediction curve and test data, validating the model's\npotential in climate prediction. This study offers valuable insights for fields\nsuch as agriculture, energy management, and urban planning, and lays the\ngroundwork for future applications in weather forecasting under the context of\nglobal climate change.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Accelerate Coastal Ocean Circulation Model with AI Surrogate",
    "url": "http://arxiv.org/abs/2410.14952v2",
    "authors": [
      "Zelin Xu",
      "Jie Ren",
      "Yupu Zhang",
      "Jose Maria Gonzalez Ondina",
      "Maitane Olabarrieta",
      "Tingsong Xiao",
      "Wenchong He",
      "Zibo Liu",
      "Shigang Chen",
      "Kaleb Smith",
      "Zhe Jiang"
    ],
    "published": "2024-10-19",
    "abstract": "Nearly 900 million people live in low-lying coastal zones around the world\nand bear the brunt of impacts from more frequent and severe hurricanes and\nstorm surges. Oceanographers simulate ocean current circulation along the\ncoasts to develop early warning systems that save lives and prevent loss and\ndamage to property from coastal hazards. Traditionally, such simulations are\nconducted using coastal ocean circulation models such as the Regional Ocean\nModeling System (ROMS), which usually runs on an HPC cluster with multiple CPU\ncores. However, the process is time-consuming and energy expensive. While\ncoarse-grained ROMS simulations offer faster alternatives, they sacrifice\ndetail and accuracy, particularly in complex coastal environments. Recent\nadvances in deep learning and GPU architecture have enabled the development of\nfaster AI (neural network) surrogates. This paper introduces an AI surrogate\nbased on a 4D Swin Transformer to simulate coastal tidal wave propagation in an\nestuary for both hindcast and forecast (up to 12 days). Our approach not only\naccelerates simulations but also incorporates a physics-based constraint to\ndetect and correct inaccurate results, ensuring reliability while minimizing\nmanual intervention. We develop a fully GPU-accelerated workflow, optimizing\nthe model training and inference pipeline on NVIDIA DGX-2 A100 GPUs. Our\nexperiments demonstrate that our AI surrogate reduces the time cost of 12-day\nforecasting of traditional ROMS simulations from 9,908 seconds (on 512 CPU\ncores) to 22 seconds (on one A100 GPU), achieving over 450$\\times$ speedup\nwhile maintaining high-quality simulation results. This work contributes to\noceanographic modeling by offering a fast, accurate, and physically consistent\nalternative to traditional simulation models, particularly for real-time\nforecasting in rapid disaster response.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "TCP-Diffusion: A Multi-modal Diffusion Model for Global Tropical Cyclone Precipitation Forecasting with Change Awareness",
    "url": "http://arxiv.org/abs/2410.13175v1",
    "authors": [
      "Cheng Huang",
      "Pan Mu",
      "Cong Bai",
      "Peter AG Watson"
    ],
    "published": "2024-10-17",
    "abstract": "Precipitation from tropical cyclones (TCs) can cause disasters such as\nflooding, mudslides, and landslides. Predicting such precipitation in advance\nis crucial, giving people time to prepare and defend against these\nprecipitation-induced disasters. Developing deep learning (DL) rainfall\nprediction methods offers a new way to predict potential disasters. However,\none problem is that most existing methods suffer from cumulative errors and\nlack physical consistency. Second, these methods overlook the importance of\nmeteorological factors in TC rainfall and their integration with the numerical\nweather prediction (NWP) model. Therefore, we propose Tropical Cyclone\nPrecipitation Diffusion (TCP-Diffusion), a multi-modal model for global\ntropical cyclone precipitation forecasting. It forecasts TC rainfall around the\nTC center for the next 12 hours at 3 hourly resolution based on past rainfall\nobservations and multi-modal environmental variables. Adjacent residual\nprediction (ARP) changes the training target from the absolute rainfall value\nto the rainfall trend and gives our model the ability of rainfall change\nawareness, reducing cumulative errors and ensuring physical consistency.\nConsidering the influence of TC-related meteorological factors and the useful\ninformation from NWP model forecasts, we propose a multi-model framework with\nspecialized encoders to extract richer information from environmental variables\nand results provided by NWP models. The results of extensive experiments show\nthat our method outperforms other DL methods and the NWP method from the\nEuropean Centre for Medium-Range Weather Forecasts (ECMWF).",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SIFM: A Foundation Model for Multi-granularity Arctic Sea Ice Forecasting",
    "url": "http://arxiv.org/abs/2410.14732v1",
    "authors": [
      "Jingyi Xu",
      "Yeqi Luo",
      "Weidong Yang",
      "Keyi Liu",
      "Shengnan Wang",
      "Ben Fei",
      "Lei Bai"
    ],
    "published": "2024-10-16",
    "abstract": "Arctic sea ice performs a vital role in global climate and has paramount\nimpacts on both polar ecosystems and coastal communities. In the last few\nyears, multiple deep learning based pan-Arctic sea ice concentration (SIC)\nforecasting methods have emerged and showcased superior performance over\nphysics-based dynamical models. However, previous methods forecast SIC at a\nfixed temporal granularity, e.g. sub-seasonal or seasonal, thus only leveraging\ninter-granularity information and overlooking the plentiful inter-granularity\ncorrelations. SIC at various temporal granularities exhibits cumulative effects\nand are naturally consistent, with short-term fluctuations potentially\nimpacting long-term trends and long-term trends provides effective hints for\nfacilitating short-term forecasts in Arctic sea ice. Therefore, in this study,\nwe propose to cultivate temporal multi-granularity that naturally derived from\nArctic sea ice reanalysis data and provide a unified perspective for modeling\nSIC via our Sea Ice Foundation Model. SIFM is delicately designed to leverage\nboth intra-granularity and inter-granularity information for capturing\ngranularity-consistent representations that promote forecasting skills. Our\nextensive experiments show that SIFM outperforms off-the-shelf deep learning\nmodels for their specific temporal granularity.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MAX: Masked Autoencoder for X-ray Fluorescence in Geological Investigation",
    "url": "http://arxiv.org/abs/2410.12330v1",
    "authors": [
      "An-Sheng Lee",
      "Yu-Wen Pao",
      "Hsuan-Tien Lin",
      "Sofia Ya Hsuan Liou"
    ],
    "published": "2024-10-16",
    "abstract": "Pre-training foundation models has become the de-facto procedure for deep\nlearning approaches, yet its application remains limited in the geological\nstudies, where in needs of the model transferability to break the shackle of\ndata scarcity. Here we target on the X-ray fluorescence (XRF) scanning data, a\nstandard high-resolution measurement in extensive scientific drilling projects.\nWe propose a scalable self-supervised learner, masked autoencoders on XRF\nspectra (MAX), to pre-train a foundation model covering geological records from\nmultiple regions of the Pacific and Southern Ocean. In pre-training, we find\nthat masking a high proportion of the input spectrum (50\\%) yields a nontrivial\nand meaningful self-supervisory task. For downstream tasks, we select the\nquantification of XRF spectra into two costly geochemical measurements,\nCaCO$_3$ and total organic carbon, due to their importance in understanding the\npaleo-oceanic carbon system. Our results show that MAX, requiring only\none-third of the data, outperforms models without pre-training in terms of\nquantification accuracy. Additionally, the model's generalizability improves by\nmore than 60\\% in zero-shot tests on new materials, with explainability further\nensuring its robustness. Thus, our approach offers a promising pathway to\novercome data scarcity in geological discovery by leveraging the\nself-supervised foundation model and fast-acquired XRF scanning data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "IceDiff: High Resolution and High-Quality Sea Ice Forecasting with Generative Diffusion Prior",
    "url": "http://arxiv.org/abs/2410.09111v1",
    "authors": [
      "Jingyi Xu",
      "Siwei Tu",
      "Weidong Yang",
      "Shuhao Li",
      "Keyi Liu",
      "Yeqi Luo",
      "Lipeng Ma",
      "Ben Fei",
      "Lei Bai"
    ],
    "published": "2024-10-10",
    "abstract": "Variation of Arctic sea ice has significant impacts on polar ecosystems,\ntransporting routes, coastal communities, and global climate. Tracing the\nchange of sea ice at a finer scale is paramount for both operational\napplications and scientific studies. Recent pan-Arctic sea ice forecasting\nmethods that leverage advances in artificial intelligence has made promising\nprogress over numerical models. However, forecasting sea ice at higher\nresolutions is still under-explored. To bridge the gap, we propose a two-staged\ndeep learning framework, IceDiff, to forecast sea ice concentration at finer\nscales. IceDiff first leverages an independently trained vision transformer to\ngenerate coarse yet superior forecasting over previous methods at a regular\n25km x 25km grid. This high-quality sea ice forecasting can be utilized as\nreliable guidance for the next stage. Subsequently, an unconditional diffusion\nmodel pre-trained on sea ice concentration maps is utilized for sampling\ndown-scaled sea ice forecasting via a zero-shot guided sampling strategy and a\npatch-based method. For the first time, IceDiff demonstrates sea ice\nforecasting with the 6.25km x 6.25km resolution. IceDiff extends the boundary\nof existing sea ice forecasting models and more importantly, its capability to\ngenerate high-resolution sea ice concentration data is vital for pragmatic\nusages and research.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Improved deep learning of chaotic dynamical systems with multistep penalty losses",
    "url": "http://arxiv.org/abs/2410.05572v1",
    "authors": [
      "Dibyajyoti Chakraborty",
      "Seung Whan Chung",
      "Ashesh Chattopadhyay",
      "Romit Maulik"
    ],
    "published": "2024-10-08",
    "abstract": "Predicting the long-term behavior of chaotic systems remains a formidable\nchallenge due to their extreme sensitivity to initial conditions and the\ninherent limitations of traditional data-driven modeling approaches. This paper\nintroduces a novel framework that addresses these challenges by leveraging the\nrecently proposed multi-step penalty (MP) optimization technique. Our approach\nextends the applicability of MP optimization to a wide range of deep learning\narchitectures, including Fourier Neural Operators and UNETs. By introducing\npenalized local discontinuities in the forecast trajectory, we effectively\nhandle the non-convexity of loss landscapes commonly encountered in training\nneural networks for chaotic systems. We demonstrate the effectiveness of our\nmethod through its application to two challenging use-cases: the prediction of\nflow velocity evolution in two-dimensional turbulence and ocean dynamics using\nreanalysis data. Our results highlight the potential of this approach for\naccurate and stable long-term prediction of chaotic dynamics, paving the way\nfor new advancements in data-driven modeling of complex natural phenomena.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Predictability of Global AI Weather Models",
    "url": "http://arxiv.org/abs/2410.03266v1",
    "authors": [
      "Chanh Kieu"
    ],
    "published": "2024-10-04",
    "abstract": "This study examines the predictability of artificial intelligence (AI) models\nfor weather prediction. Using a simple deep-learning architecture based on\nconvolutional long short-term memory and the ERA5 data for training, we show\nthat different time-stepping techniques can have a strong influence on the\nmodel performance and weather predictability. Specifically, a small-step\napproach for which the future state is predicted by recursively iterating an AI\nmodel over a small time increment displays strong sensitivity to the type of\ninput channels, the number of data frames, or forecast lead times. In contrast,\na big-step approach for which a current state is directly projected to a future\nstate at each corresponding lead time provides much better forecast skill and a\nlonger predictability range. In particular, the big-step approach is very\nresilient to different input channels, or data frames. In this regard, our\nresults present a different method for implementing global AI models for\nweather prediction, which can optimize the model performance even with minimum\ninput channels or data frames.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Anti-biofouling Lensless Camera System with Deep Learning based Image Reconstruction",
    "url": "http://arxiv.org/abs/2410.01365v1",
    "authors": [
      "Naoki Ide",
      "Tomohiro Kawahara",
      "Hiroshi Ueno",
      "Daiki Yanagidaira",
      "Susumu Takatsuka"
    ],
    "published": "2024-10-02",
    "abstract": "In recent years, there has been an increasing demand for underwater cameras\nthat monitor the condition of offshore structures and check the number of\nindividuals in aqua culture environments with long-period observation. One of\nthe significant issues with this observation is that biofouling sticks to the\naperture and lens densely and prevents cameras from capturing clear images.\nThis study examines an underwater camera that applies material technologies\nwith high inherent resistance to biofouling and computer vision technologies\nbased on image reconstruction by deep learning to lens-less cameras. For this\npurpose, our prototype camera uses a coded aperture with 1k rectangular shape\npinholes in a thin metal plate, such as copper, which hinder the growth of\nbiofouling and keep the surface clean. Although images taken by lens-less\ncameras are usually not well formed due to lack of the traditional glass-based\nlens, a deep learning approach using ViT (Vision Transformer) has recently\ndemonstrated reconstructing original photo images well and our study shows that\nusing gated MLP (Multilayer Perceptron) also yields good results. On the other\nhand, a certain degree of thickness for bio-repellence materials is required to\nexhibit their effect the thickness of aperture is necessary to use apertures\nsufficiently thinner than the size of the pinholes to avoid unintentional\nreflection and absorption on the sidewalls. Therefore, we prepared a\nsufficiently thin plate for image reconstruction and now currently we conduct\ntests of the lens-less camera of the bio-repellence aperture with actual\nseawater environments to determine whether it can sufficiently demonstrate the\nbiofouling effect compared with usual camera with only waterproof.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Pose Estimation of Buried Deep-Sea Objects using 3D Vision Deep Learning Models",
    "url": "http://arxiv.org/abs/2410.01061v2",
    "authors": [
      "Jerry Yan",
      "Chinmay Talegaonkar",
      "Nicholas Antipa",
      "Eric Terrill",
      "Sophia Merrifield"
    ],
    "published": "2024-10-01",
    "abstract": "We present an approach for pose and burial fraction estimation of debris\nfield barrels found on the seabed in the Southern California San Pedro Basin.\nOur computational workflow leverages recent advances in foundation models for\nsegmentation and a vision transformer-based approach to estimate the point\ncloud which defines the geometry of the barrel. We propose BarrelNet for\nestimating the 6-DOF pose and radius of buried barrels from the barrel point\nclouds as input. We train BarrelNet using synthetically generated barrel point\nclouds, and qualitatively demonstrate the potential of our approach using\nremotely operated vehicle (ROV) video footage of barrels found at a historic\ndump site. We compare our method to a traditional least squares fitting\napproach and show significant improvement according to our defined benchmarks.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Advancing Spatio-temporal Storm Surge Prediction with Hierarchical Deep Neural Networks",
    "url": "http://arxiv.org/abs/2410.12823v1",
    "authors": [
      "Saeed Saviz Naeini",
      "Reda Snaiki",
      "Teng Wu"
    ],
    "published": "2024-10-01",
    "abstract": "Coastal regions in North America face major threats from storm surges caused\nby hurricanes and nor'easters. Traditional numerical models, while accurate,\nare computationally expensive, limiting their practicality for real-time\npredictions. Recently, deep learning techniques have been developed for\nefficient simulation of time-dependent storm surge. To resolve the small scales\nof storm surge in both time and space over a long duration and a large area,\nthese simulations typically need to employ oversized neural networks that\nstruggle with the accumulation of prediction errors over successive time steps.\nTo address these challenges, this study introduces a hierarchical deep neural\nnetwork (HDNN) combined with a convolutional autoencoder (CAE) to accurately\nand efficiently predict storm surge time series. The CAE reduces the\ndimensionality of storm surge data, streamlining the learning process. HDNNs\nthen map storm parameters to the low-dimensional representation of storm surge,\nallowing for sequential predictions across different time scales. Specifically,\nthe current-level neural network is utilized to predict future states with a\nrelatively large time step, which are passed as inputs to the next-level neural\nnetwork for smaller time-step predictions. This process continues sequentially\nfor all time steps. The results from different-level neural networks across\nvarious time steps are then stacked to acquire the entire time series of storm\nsurge. The simulated low-dimensional representations are finally decoded back\ninto storm surge time series. The proposed model was trained and tested using\nsynthetic data from the North Atlantic Comprehensive Coastal Study. Results\ndemonstrate its excellent performance to effectively handle high-dimensional\nsurge data while mitigating the accumulation of prediction errors over time,\nmaking it a promising tool for advancing storm surge prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Inferring Thunderstorm Occurrence from Vertical Profiles of Convection-Permitting Simulations: Physical Insights from a Physical Deep Learning Model",
    "url": "http://arxiv.org/abs/2409.20087v3",
    "authors": [
      "Kianusch Vahid Yousefnia",
      "Christoph Metzl",
      "Tobias B\u00f6lle"
    ],
    "published": "2024-09-30",
    "abstract": "Thunderstorms have significant social and economic impacts due to heavy\nprecipitation, hail, lightning, and strong winds, necessitating reliable\nforecasts. Thunderstorm forecasts based on numerical weather prediction (NWP)\noften rely on single-level surrogate predictors, like convective available\npotential energy and convective inhibition, derived from vertical profiles of\nthree-dimensional atmospheric variables. In this study, we develop SALAMA 1D, a\ndeep neural network which directly infers the probability of thunderstorm\noccurrence from vertical profiles of ten atmospheric variables, bypassing\nsingle-level predictors. By training the model on convection-permitting NWP\nforecasts, we allow SALAMA 1D to flexibly identify convective patterns, with\nthe goal of enhancing forecast accuracy. The model's architecture is physically\nmotivated: sparse connections encourage interactions at similar height levels\nwhile keeping model size and inference times computationally efficient, whereas\na shuffling mechanism prevents the model from learning non-physical patterns\ntied to the vertical grid. SALAMA 1D is trained over Central Europe with\nlightning observations as the ground truth. Comparative analysis against a\nbaseline machine learning model that uses single-level predictors shows SALAMA\n1D's superior skill across various metrics and lead times of up to at least 11\nhours. Moreover, expanding the archive of forecasts from which training\nexamples are sampled improves skill, even when training set size remains\nconstant. Finally, a sensitivity analysis using saliency maps indicates that\nour model relies on physically interpretable patterns consistent with\nestablished theoretical understanding, such as ice particle content near the\ntropopause, cloud cover, conditional instability, and low-level moisture.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Center-fixing of tropical cyclones using uncertainty-aware deep learning applied to high-temporal-resolution geostationary satellite imagery",
    "url": "http://arxiv.org/abs/2409.16507v2",
    "authors": [
      "Ryan Lagerquist",
      "Galina Chirokova",
      "Robert DeMaria",
      "Mark DeMaria",
      "Imme Ebert-Uphoff"
    ],
    "published": "2024-09-24",
    "abstract": "Determining the location of a tropical cyclone's (TC) surface circulation\ncenter -- \"center-fixing\" -- is a critical first step in the TC-forecasting\nprocess, affecting current and future estimates of track, intensity, and\nstructure. Despite a recent increase in automated center-fixing methods, only\none such method (ARCHER-2) is operational, and its best performance is achieved\nwhen using microwave or scatterometer data, which are not available at every\nforecast cycle. We develop a deep-learning algorithm called GeoCenter; besides\na few scalars in the operational ATCF, it relies only on geostationary IR\nsatellite imagery, which is available for all TC basins at high frequency (10\nmin) and low latency (< 10 min) during both day and night. GeoCenter ingests an\nanimation (time series) of IR images, including 9 channels at lag times up to 4\nhours. The animation is centered at a \"first guess\" location, offset from the\ntrue TC-center location by 48 km on average and sometimes > 100 km; GeoCenter\nis tasked with correcting this offset. On an independent testing dataset,\nGeoCenter achieves a mean/median/RMS (root mean square) error of 26.6/22.2/32.4\nkm for all systems, 24.7/20.8/30.0 km for tropical systems, and 14.6/12.5/17.3\nkm for category-2--5 hurricanes. These values are similar to ARCHER-2 errors\nwith microwave or scatterometer data, and better than ARCHER-2 errors when only\nIR data are available. GeoCenter also performs skillful uncertainty\nquantification, producing a well calibrated ensemble of 150 TC-center\nlocations. Furthermore, all predictors used by GeoCenter are available in real\ntime, which would make GeoCenter easy to implement operationally every 10 min.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Deep Learning Earth System Model for Efficient Simulation of the Observed Climate",
    "url": "http://arxiv.org/abs/2409.16247v2",
    "authors": [
      "Nathaniel Cresswell-Clay",
      "Bowen Liu",
      "Dale Durran",
      "Zihui Liu",
      "Zachary I. Espinosa",
      "Raul Moreno",
      "Matthias Karlbauer"
    ],
    "published": "2024-09-24",
    "abstract": "A key challenge for computationally intensive state-of-the-art Earth System\nmodels is to distinguish global warming signals from interannual variability.\nHere we introduce DLESyM, a parsimonious deep learning model that accurately\nsimulates the Earth's current climate over 1000-year periods with no smoothing\nor drift. DLESyM simulations equal or exceed key metrics of seasonal and\ninterannual variability--such as tropical cyclogenesis over the range of\nobserved intensities, the cycle of the Indian Summer monsoon, and the\nclimatology of mid-latitude blocking events--when compared to historical\nsimulations from four leading models from the 6th Climate Model Intercomparison\nProject. DLESyM, trained on both historical reanalysis data and satellite\nobservations, is an accurate, highly efficient model of the coupled Earth\nsystem, empowering long-range sub-seasonal and seasonal forecasts while using a\nfraction of the energy and computational time required by traditional models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A competitive baseline for deep learning enhanced data assimilation using conditional Gaussian ensemble Kalman filtering",
    "url": "http://arxiv.org/abs/2409.14300v1",
    "authors": [
      "Zachariah Malik",
      "Romit Maulik"
    ],
    "published": "2024-09-22",
    "abstract": "Ensemble Kalman Filtering (EnKF) is a popular technique for data\nassimilation, with far ranging applications. However, the vanilla EnKF\nframework is not well-defined when perturbations are nonlinear. We study two\nnon-linear extensions of the vanilla EnKF - dubbed the conditional-Gaussian\nEnKF (CG-EnKF) and the normal score EnKF (NS-EnKF) - which sidestep assumptions\nof linearity by constructing the Kalman gain matrix with the `conditional\nGaussian' update formula in place of the traditional one. We then compare these\nmodels against a state-of-the-art deep learning based particle filter called\nthe score filter (SF). This model uses an expensive score diffusion model for\nestimating densities and also requires a strong assumption on the perturbation\noperator for validity. In our comparison, we find that CG-EnKF and NS-EnKF\ndramatically outperform SF for a canonical problem in high-dimensional\nmultiscale data assimilation given by the Lorenz-96 system. Our analysis also\ndemonstrates that the CG-EnKF and NS-EnKF can handle highly non-Gaussian\nadditive noise perturbations, with the latter typically outperforming the\nformer.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "WeatherFormer: Empowering Global Numerical Weather Forecasting with Space-Time Transformer",
    "url": "http://arxiv.org/abs/2409.16321v1",
    "authors": [
      "Junchao Gong",
      "Tao Han",
      "Kang Chen",
      "Lei Bai"
    ],
    "published": "2024-09-21",
    "abstract": "Numerical Weather Prediction (NWP) system is an infrastructure that exerts\nconsiderable impacts on modern society.Traditional NWP system, however,\nresolves it by solving complex partial differential equations with a huge\ncomputing cluster, resulting in tons of carbon emission. Exploring efficient\nand eco-friendly solutions for NWP attracts interest from Artificial\nIntelligence (AI) and earth science communities. To narrow the performance gap\nbetween the AI-based methods and physic predictor, this work proposes a new\ntransformer-based NWP framework, termed as WeatherFormer, to model the complex\nspatio-temporal atmosphere dynamics and empowering the capability of\ndata-driven NWP. WeatherFormer innovatively introduces the space-time\nfactorized transformer blocks to decrease the parameters and memory\nconsumption, in which Position-aware Adaptive Fourier Neural Operator (PAFNO)\nis proposed for location sensible token mixing. Besides, two data augmentation\nstrategies are utilized to boost the performance and decrease training\nconsumption. Extensive experiments on WeatherBench dataset show WeatherFormer\nachieves superior performance over existing deep learning methods and further\napproaches the most advanced physical model.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Developing a Thailand solar irradiance map using Himawari-8 satellite imageries and deep learning models",
    "url": "http://arxiv.org/abs/2409.16320v3",
    "authors": [
      "Suwichaya Suwanwimolkul",
      "Natanon Tongamrak",
      "Nuttamon Thungka",
      "Naebboon Hoonchareon",
      "Jitkomut Songsiri"
    ],
    "published": "2024-09-21",
    "abstract": "This paper presents an online platform showing Thailand solar irradiance map\nevery 30 minutes, available at https://www.cusolarforecast.com. The methodology\nfor estimating global horizontal irradiance (GHI) across Thailand relies on\ncloud index extracted from Himawari-8 satellite imagery, Ineichen clear-sky\nmodel with locally-tuned Linke turbidity, and machine learning models. The\nmethods take clear-sky irradiance, cloud index, re-analyzed GHI and temperature\ndata from the MERRA-2 database, and date-time as inputs for GHI estimation\nmodels, including LightGBM, LSTM, Informer, and Transformer. These are\nbenchmarked with the estimate from a commercial service X by evaluation of\n15-minute ground GHI data from 53 ground stations over 1.5 years during\n2022-2023. The results show that the four models exhibit comparable overall MAE\nperformance to the service X. The best model is LightGBM with an overall MAE of\n78.58 W/sqm and RMSE of 118.97 W/sqm, while the service X achieves the lowest\nMAE, RMSE, and MBE in cloudy condition. Obtaining re-analyzed MERRA-2 data for\nthe whole Thailand region is not economically feasible for deployment. When\nremoving these features, the Informer model has a winning performance in MAE of\n78.67 W/sqm. The obtained performance aligns with existing literature by taking\nthe climate zone and time granularity of data into consideration. As the map\nshows an estimate of GHI over 93,000 grids with a frequent update, the paper\nalso describes a computational framework for displaying the entire map. It\ntests the runtime performance of deep learning models in the GHI estimation\nprocess.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "On the Extrapolation of Generative Adversarial Networks for downscaling precipitation extremes in warmer climates",
    "url": "http://arxiv.org/abs/2409.13934v1",
    "authors": [
      "Neelesh Rampal",
      "Peter B. Gibson",
      "Steven Sherwood",
      "Gab Abramowitz"
    ],
    "published": "2024-09-20",
    "abstract": "While deep-learning downscaling algorithms can generate fine-scale climate\nprojections cost-effectively, it is still unclear how well they will\nextrapolate to unobserved climates. We assess the extrapolation capabilities of\na deterministic Convolutional Neural Network baseline and a Generative\nAdversarial Network (GAN) built with this baseline, trained to predict daily\nprecipitation simulated by a Regional Climate Model (RCM). Both approaches\nemulate future changes in annual mean precipitation well, even when trained on\nhistorical data, though training on a future climate improves performance. For\nextreme precipitation (99.5th percentile), RCM simulations predict a robust\nend-of-century increase with future warming (~5.8%/{\\deg}C on average from five\nsimulations). When trained on a future climate, GANs capture 97% of the\nwarming-driven increase in extreme precipitation compared to 65% in a\ndeterministic baseline. Even GANs trained historically capture 77% of this\nincrease. Overall, GANs offer better generalization for downscaling extremes,\nwhich is important in applications relying on historical data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "GAN"
    ],
    "applications": []
  },
  {
    "title": "Investigation of Time-Frequency Feature Combinations with Histogram Layer Time Delay Neural Networks",
    "url": "http://arxiv.org/abs/2409.13881v2",
    "authors": [
      "Amirmohammad Mohammadi",
      "Iren'e Masabarakiza",
      "Ethan Barnes",
      "Davelle Carreiro",
      "Alexandra Van Dine",
      "Joshua Peeples"
    ],
    "published": "2024-09-20",
    "abstract": "While deep learning has reduced the prevalence of manual feature extraction,\ntransformation of data via feature engineering remains essential for improving\nmodel performance, particularly for underwater acoustic signals. The methods by\nwhich audio signals are converted into time-frequency representations and the\nsubsequent handling of these spectrograms can significantly impact performance.\nThis work demonstrates the performance impact of using different combinations\nof time-frequency features in a histogram layer time delay neural network. An\noptimal set of features is identified with results indicating that specific\nfeature combinations outperform single data features.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-Sensor Deep Learning for Glacier Mapping",
    "url": "http://arxiv.org/abs/2409.12034v2",
    "authors": [
      "Codru\u0163-Andrei Diaconu",
      "Konrad Heidler",
      "Jonathan L. Bamber",
      "Harry Zekollari"
    ],
    "published": "2024-09-18",
    "abstract": "The more than 200,000 glaciers outside the ice sheets play a crucial role in\nour society by influencing sea-level rise, water resource management, natural\nhazards, biodiversity, and tourism. However, only a fraction of these glaciers\nbenefit from consistent and detailed in-situ observations that allow for\nassessing their status and changes over time. This limitation can, in part, be\novercome by relying on satellite-based Earth Observation techniques.\nSatellite-based glacier mapping applications have historically mainly relied on\nmanual and semi-automatic detection methods, while recently, a fast and notable\ntransition to deep learning techniques has started.\n  This chapter reviews how combining multi-sensor remote sensing data and deep\nlearning allows us to better delineate (i.e. map) glaciers and detect their\ntemporal changes. We explain how relying on deep learning multi-sensor\nframeworks to map glaciers benefits from the extensive availability of regional\nand global glacier inventories. We also analyse the rationale behind glacier\nmapping, the benefits of deep learning methodologies, and the inherent\nchallenges in integrating multi-sensor earth observation data with deep\nlearning algorithms.\n  While our review aims to provide a broad overview of glacier mapping efforts,\nwe highlight a few setups where deep learning multi-sensor remote sensing\napplications have a considerable potential added value. This includes\napplications for debris-covered and rock glaciers that are visually difficult\nto distinguish from surroundings and for calving glaciers that are in contact\nwith the ocean. These specific cases are illustrated through a series of visual\nimageries, highlighting some significant advantages and challenges when\ndetecting glacier changes, including dealing with seasonal snow cover, changing\ndebris coverage, and distinguishing glacier fronts from the surrounding sea\nice.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "DiffESM: Conditional Emulation of Temperature and Precipitation in Earth System Models with 3D Diffusion Models",
    "url": "http://arxiv.org/abs/2409.11601v1",
    "authors": [
      "Seth Bassetti",
      "Brian Hutchinson",
      "Claudia Tebaldi",
      "Ben Kravitz"
    ],
    "published": "2024-09-17",
    "abstract": "Earth System Models (ESMs) are essential for understanding the interaction\nbetween human activities and the Earth's climate. However, the computational\ndemands of ESMs often limit the number of simulations that can be run,\nhindering the robust analysis of risks associated with extreme weather events.\nWhile low-cost climate emulators have emerged as an alternative to emulate ESMs\nand enable rapid analysis of future climate, many of these emulators only\nprovide output on at most a monthly frequency. This temporal resolution is\ninsufficient for analyzing events that require daily characterization, such as\nheat waves or heavy precipitation. We propose using diffusion models, a class\nof generative deep learning models, to effectively downscale ESM output from a\nmonthly to a daily frequency. Trained on a handful of ESM realizations,\nreflecting a wide range of radiative forcings, our DiffESM model takes monthly\nmean precipitation or temperature as input, and is capable of producing daily\nvalues with statistical characteristics close to ESM output. Combined with a\nlow-cost emulator providing monthly means, this approach requires only a small\nfraction of the computational resources needed to run a large ensemble. We\nevaluate model behavior using a number of extreme metrics, showing that DiffESM\nclosely matches the spatio-temporal behavior of the ESM output it emulates in\nterms of the frequency and spatial characteristics of phenomena such as heat\nwaves, dry spells, or rainfall intensity.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": []
  },
  {
    "title": "Super Resolution On Global Weather Forecasts",
    "url": "http://arxiv.org/abs/2409.11502v2",
    "authors": [
      "Lawrence Zhang",
      "Adam Yang",
      "Rodz Andrie Amor",
      "Bryan Zhang",
      "Dhruv Rao"
    ],
    "published": "2024-09-17",
    "abstract": "Weather forecasting is a vitally important tool for tasks ranging from\nplanning day to day activities to disaster response planning. However, modeling\nweather has proven to be challenging task due to its chaotic and unpredictable\nnature. Each variable, from temperature to precipitation to wind, all influence\nthe path the environment will take. As a result, all models tend to rapidly\nlose accuracy as the temporal range of their forecasts increase. Classical\nforecasting methods use a myriad of physics-based, numerical, and stochastic\ntechniques to predict the change in weather variables over time. However, such\nforecasts often require a very large amount of data and are extremely\ncomputationally expensive. Furthermore, as climate and global weather patterns\nchange, classical models are substantially more difficult and time-consuming to\nupdate for changing environments. Fortunately, with recent advances in deep\nlearning and publicly available high quality weather datasets, deploying\nlearning methods for estimating these complex systems has become feasible. The\ncurrent state-of-the-art deep learning models have comparable accuracy to the\nindustry standard numerical models and are becoming more ubiquitous in practice\ndue to their adaptability. Our group seeks to improve upon existing deep\nlearning based forecasting methods by increasing spatial resolutions of global\nweather predictions. Specifically, we are interested in performing super\nresolution (SR) on GraphCast temperature predictions by increasing the global\nprecision from 1 degree of accuracy to 0.5 degrees, which is approximately\n111km and 55km respectively.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Surface solar radiation: AI satellite retrieval can outperform Heliosat and generalizes well to other climate zones",
    "url": "http://arxiv.org/abs/2409.16316v1",
    "authors": [
      "K. R. Schuurman",
      "A. Meyer"
    ],
    "published": "2024-09-16",
    "abstract": "Accurate estimates of surface solar irradiance (SSI) are essential for solar\nresource assessments and solar energy forecasts in grid integration and\nbuilding control applications. SSI estimates for spatially extended regions can\nbe retrieved from geostationary satellites such as Meteosat. Traditional SSI\nsatellite retrievals like Heliosat rely on physical radiative transfer\nmodelling. We introduce the first machine-learning-based satellite retrieval\nfor instantaneous SSI and demonstrate its capability to provide accurate and\ngeneralizable SSI estimates across Europe. Our deep learning retrieval provides\nnear real-time SSI estimates based on data-driven emulation of Heliosat and\nfine-tuning on pyranometer networks. By including SSI from ground stations, our\nSSI retrieval model can outperform Heliosat accuracy and generalize well to\nregions with other climates and surface albedos in cloudy conditions (clear-sky\nindex < 0.8). We also show that the SSI retrieved from Heliosat exhibits large\nbiases in mountain regions, and that training and fine-tuning our retrieval\nmodels on SSI data from ground stations strongly reduces these biases,\noutperforming Heliosat. Furthermore, we quantify the relative importance of the\nMeteosat channels and other predictor variables like solar zenith angle for the\naccuracy of our deep learning SSI retrieval model in different cloud\nconditions. We find that in cloudy conditions multiple near-infrared and\ninfrared channels enhance the performance. Our results can facilitate the\ndevelopment of more accurate satellite retrieval models of surface solar\nirradiance.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling",
    "url": "http://arxiv.org/abs/2409.16313v2",
    "authors": [
      "Teerapong Panboonyuen"
    ],
    "published": "2024-09-14",
    "abstract": "Forecasting sea surface currents is essential for applications such as\nmaritime navigation, environmental monitoring, and climate analysis,\nparticularly in regions like the Gulf of Thailand and the Andaman Sea. This\npaper introduces SEA-ViT, an advanced deep learning model that integrates\nVision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to\ncapture spatio-temporal covariance for predicting sea surface currents (U, V)\nusing high-frequency radar (HF) data. The name SEA-ViT is derived from ``Sea\nSurface Currents Forecasting using Vision Transformer,'' highlighting the\nmodel's emphasis on ocean dynamics and its use of the ViT architecture to\nenhance forecasting capabilities. SEA-ViT is designed to unravel complex\ndependencies by leveraging a rich dataset spanning over 30 years and\nincorporating ENSO indices (El Ni\\~no, La Ni\\~na, and neutral phases) to\naddress the intricate relationship between geographic coordinates and climatic\nvariations. This development enhances the predictive capabilities for sea\nsurface currents, supporting the efforts of the Geo-Informatics and Space\nTechnology Development Agency (GISTDA) in Thailand's maritime regions. The code\nand pretrained models are available at\n\\url{https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents}.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multi-scale decomposition of sea surface height snapshots using machine learning",
    "url": "http://arxiv.org/abs/2409.17354v1",
    "authors": [
      "Jingwen Lyu",
      "Yue Wang",
      "Christian Pedersen",
      "Spencer Jones",
      "Dhruv Balwada"
    ],
    "published": "2024-09-11",
    "abstract": "Knowledge of ocean circulation is important for understanding and predicting\nweather and climate, and managing the blue economy. This circulation can be\nestimated through Sea Surface Height (SSH) observations, but requires\ndecomposing the SSH into contributions from balanced and unbalanced motions\n(BMs and UBMs). This decomposition is particularly pertinent for the novel SWOT\nsatellite, which measures SSH at an unprecedented spatial resolution.\nSpecifically, the requirement, and the goal of this work, is to decompose\ninstantaneous SSH into BMs and UBMs. While a few studies using deep learning\n(DL) approaches have shown promise in framing this decomposition as an\nimage-to-image translation task, these models struggle to work well across a\nwide range of spatial scales and require extensive training data, which is\nscarce in this domain. These challenges are not unique to our task, and pervade\nmany problems requiring multi-scale fidelity. We show that these challenges can\nbe addressed by using zero-phase component analysis (ZCA) whitening and data\naugmentation; making this a viable option for SSH decomposition across scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Deep Learning for predicting rate-induced tipping",
    "url": "http://arxiv.org/abs/2409.07590v1",
    "authors": [
      "Yu Huang",
      "Sebastian Bathiany",
      "Peter Ashwin",
      "Niklas Boers"
    ],
    "published": "2024-09-11",
    "abstract": "Nonlinear dynamical systems exposed to changing forcing can exhibit\ncatastrophic transitions between alternative and often markedly different\nstates. The phenomenon of critical slowing down (CSD) can be used to anticipate\nsuch transitions if caused by a bifurcation and if the change in forcing is\nslow compared to the internal time scale of the system. However, in many\nreal-world situations, these assumptions are not met and transitions can be\ntriggered because the forcing exceeds a critical rate. For example, given the\npace of anthropogenic climate change in comparison to the internal time scales\nof key Earth system components, such as the polar ice sheets or the Atlantic\nMeridional Overturning Circulation, such rate-induced tipping poses a severe\nrisk. Moreover, depending on the realisation of random perturbations, some\ntrajectories may transition across an unstable boundary, while others do not,\neven under the same forcing. CSD-based indicators generally cannot distinguish\nthese cases of noise-induced tipping versus no tipping. This severely limits\nour ability to assess the risks of tipping, and to predict individual\ntrajectories. To address this, we make a first attempt to develop a deep\nlearning framework to predict transition probabilities of dynamical systems\nahead of rate-induced transitions. Our method issues early warnings, as\ndemonstrated on three prototypical systems for rate-induced tipping, subjected\nto time-varying equilibrium drift and noise perturbations. Exploiting\nexplainable artificial intelligence methods, our framework captures the\nfingerprints necessary for early detection of rate-induced tipping, even in\ncases of long lead times. Our findings demonstrate the predictability of\nrate-induced and noise-induced tipping, advancing our ability to determine safe\noperating spaces for a broader class of dynamical systems than possible so far.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Segmenting sea ice floes in close-range optical imagery with active contour and foundation models",
    "url": "http://arxiv.org/abs/2409.06641v2",
    "authors": [
      "Giulio Passerotti",
      "Alberto Alberello",
      "Marcello Vichi",
      "Luke G. Bennetts",
      "James Bailey",
      "Alessandro Toffoli"
    ],
    "published": "2024-09-10",
    "abstract": "The size and shape of sea ice floes play a crucial role in influencing\nocean-atmosphere energy exchanges, sea ice concentrations, albedo, and wave\npropagation through ice-covered waters. Despite the availability of diverse\nimage segmentation techniques for analyzing sea ice imagery, accurately\ndetecting and measuring floes remains a considerable challenge. This study\npresents a precise methodology for in-situ sea ice imagery acquisition,\nincluding automated orthorectification to correct perspective distortions. The\nimage dataset, collected during an Antarctic winter expedition, was used to\nevaluate various automated image segmentation approaches: the traditional GVF\nSnake algorithm and the advanced deep learning model, Segment Anything Model\n(SAM). To address the limitations of each method, a hybrid algorithm combining\ntraditional and AI-based techniques is proposed. The effectiveness of these\napproaches was validated through a detailed analysis of ice floe detection\naccuracy, floe size, and ice concentration statistics, with the outcomes\nnormalized against a manually segmented benchmark.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Deep Learning for Koopman Operator Estimation in Idealized Atmospheric Dynamics",
    "url": "http://arxiv.org/abs/2409.06522v1",
    "authors": [
      "David Millard",
      "Arielle Carr",
      "St\u00e9phane Gaudreault"
    ],
    "published": "2024-09-10",
    "abstract": "Deep learning is revolutionizing weather forecasting, with new data-driven\nmodels achieving accuracy on par with operational physical models for\nmedium-term predictions. However, these models often lack interpretability,\nmaking their underlying dynamics difficult to understand and explain. This\npaper proposes methodologies to estimate the Koopman operator, providing a\nlinear representation of complex nonlinear dynamics to enhance the transparency\nof data-driven models. Despite its potential, applying the Koopman operator to\nlarge-scale problems, such as atmospheric modeling, remains challenging. This\nstudy aims to identify the limitations of existing methods, refine these models\nto overcome various bottlenecks, and introduce novel convolutional neural\nnetwork architectures that capture simplified dynamics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "CAS-Canglong: A skillful 3D Transformer model for sub-seasonal to seasonal global sea surface temperature prediction",
    "url": "http://arxiv.org/abs/2409.05369v1",
    "authors": [
      "Longhao Wang",
      "Xuanze Zhang",
      "L. Ruby Leung",
      "Francis H. S. Chiew",
      "Amir AghaKouchak",
      "Kairan Ying",
      "Yongqiang Zhang"
    ],
    "published": "2024-09-09",
    "abstract": "Accurate prediction of global sea surface temperature at sub-seasonal to\nseasonal (S2S) timescale is critical for drought and flood forecasting, as well\nas for improving disaster preparedness in human society. Government departments\nor academic studies normally use physics-based numerical models to predict S2S\nsea surface temperature and corresponding climate indices, such as El\nNi\\~no-Southern Oscillation. However, these models are hampered by\ncomputational inefficiencies, limited retention of ocean-atmosphere initial\nconditions, and significant uncertainty and biases. Here, we introduce a novel\nthree-dimensional deep learning neural network to model the nonlinear and\ncomplex coupled atmosphere-ocean weather systems. This model incorporates\nclimatic and temporal features and employs a self-attention mechanism to\nenhance the prediction of global S2S sea surface temperature pattern. Compared\nto the physics-based models, it shows significant computational efficiency and\npredictive capability, improving one to three months sea surface temperature\npredictive skill by 13.7% to 77.1% in seven ocean regions with dominant\ninfluence on S2S variability over land. This achievement underscores the\nsignificant potential of deep learning for largely improving forecasting skills\nat the S2S scale over land.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning Framework for High-Resolution Air Temperature Downscaling Using LiDAR-Derived Urban Morphological Features",
    "url": "http://arxiv.org/abs/2409.02120v1",
    "authors": [
      "Fatemeh Chajaei",
      "Hossein Bagheri"
    ],
    "published": "2024-08-31",
    "abstract": "Climate models lack the necessary resolution for urban climate studies,\nrequiring computationally intensive processes to estimate high resolution air\ntemperatures. In contrast, Data-driven approaches offer faster and more\naccurate air temperature downscaling. This study presents a data-driven\nframework for downscaling air temperature using publicly available outputs from\nurban climate models, specifically datasets generated by UrbClim. The proposed\nframework utilized morphological features extracted from LiDAR data. To extract\nurban morphological features, first a three-dimensional building model was\ncreated using LiDAR data and deep learning models. Then, these features were\nintegrated with meteorological parameters such as wind, humidity, etc., to\ndownscale air temperature using machine learning algorithms. The results\ndemonstrated that the developed framework effectively extracted urban\nmorphological features from LiDAR data. Deep learning algorithms played a\ncrucial role in generating three-dimensional models for extracting the\naforementioned features. Also, the evaluation of air temperature downscaling\nresults using various machine learning models indicated that the LightGBM model\nhad the best performance with an RMSE of 0.352{\\deg}K and MAE of 0.215{\\deg}K.\nFurthermore, the examination of final air temperature maps derived from\ndownscaling showed that the developed framework successfully estimated air\ntemperatures at higher resolutions, enabling the identification of local air\ntemperature patterns at street level. The corresponding source codes are\navailable on GitHub:\nhttps://github.com/FatemehCh97/Air-Temperature-Downscaling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "AI-driven weather forecasts enable anticipated attribution of extreme events to human-made climate change",
    "url": "http://arxiv.org/abs/2408.16433v1",
    "authors": [
      "Bernat Jim\u00e9nez-Esteve",
      "David Barriopedro",
      "Juan Emmanuel Johnson",
      "Ricardo Garcia-Herrera"
    ],
    "published": "2024-08-29",
    "abstract": "Anthropogenic climate change (ACC) is altering the frequency and intensity of\nextreme weather events. Attributing individual extreme events (EEs) to ACC is\nbecoming crucial to assess the risks of climate change. Traditional attribution\nmethods often suffer from a selection bias, are computationally demanding, and\nprovide answers after the EE occurs. This study presents a ground-breaking\nhybrid attribution method by combining physics-based ACC estimates from global\nclimate models with deep-learning weather forecasts. This hybrid approach\ncircumvents the framing choices and accelerates the attribution process, paving\nthe way for operational anticipated global forecast-based attribution. We apply\nthis methodology to three distinct high-impact weather EEs. Despite some\nlimitations in predictability, the method uncovers ACC fingerprints in the\nforecasted fields of EEs. Specifically, forecasts successfully anticipate that\nACC exacerbated the 2018 Iberian heatwave, deepened hurricane Florence, and\nintensified the wind and precipitable water of the explosive cyclone Ciar\\'an.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution",
    "url": "http://arxiv.org/abs/2408.15993v2",
    "authors": [
      "Sungduk Yu",
      "Brian L. White",
      "Anahita Bhiwandiwalla",
      "Musashi Hinck",
      "Matthew Lyle Olson",
      "Yaniv Gurwicz",
      "Raanan Y. Rohekar",
      "Tung Nguyen",
      "Vasudev Lal"
    ],
    "published": "2024-08-28",
    "abstract": "Detecting and attributing temperature increases driven by climate change is\ncrucial for understanding global warming and informing adaptation strategies.\nHowever, distinguishing human-induced climate signals from natural variability\nremains challenging for traditional detection and attribution (D&A) methods,\nwhich rely on identifying specific \"fingerprints\" -- spatial patterns expected\nto emerge from external forcings such as greenhouse gas emissions. Deep\nlearning offers promise in discerning these complex patterns within expansive\nspatial datasets, yet the lack of standardized protocols has hindered\nconsistent comparisons across studies.\n  To address this gap, we introduce ClimDetect, a standardized dataset\ncomprising 1.17M daily climate snapshots paired with target climate change\nindicator variables. The dataset is curated from both CMIP6 climate model\nsimulations and real-world observation-assimilated reanalysis datasets (ERA5,\nJRA-3Q, and MERRA-2), and is designed to enhance model accuracy in detecting\nclimate change signals. ClimDetect integrates various input and target\nvariables used in previous research, ensuring comparability and consistency\nacross studies. We also explore the application of vision transformers (ViT) to\nclimate data -- a novel approach that, to our knowledge, has not been attempted\nbefore for climate change detection tasks. Our open-access data serve as a\nbenchmark for advancing climate science by enabling end-to-end model\ndevelopment and evaluation. ClimDetect is publicly accessible via Hugging Face\ndataset repository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Uncertainty-aware segmentation for rainfall prediction post processing",
    "url": "http://arxiv.org/abs/2408.16792v1",
    "authors": [
      "Simone Monaco",
      "Luca Monaco",
      "Daniele Apiletti"
    ],
    "published": "2024-08-28",
    "abstract": "Accurate precipitation forecasts are crucial for applications such as flood\nmanagement, agricultural planning, water resource allocation, and weather\nwarnings. Despite advances in numerical weather prediction (NWP) models, they\nstill exhibit significant biases and uncertainties, especially at high spatial\nand temporal resolutions. To address these limitations, we explore\nuncertainty-aware deep learning models for post-processing daily cumulative\nquantitative precipitation forecasts to obtain forecast uncertainties that lead\nto a better trade-off between accuracy and reliability. Our study compares\ndifferent state-of-the-art models, and we propose a variant of the well-known\nSDE-Net, called SDE U-Net, tailored to segmentation problems like ours. We\nevaluate its performance for both typical and intense precipitation events.\n  Our results show that all deep learning models significantly outperform the\naverage baseline NWP solution, with our implementation of the SDE U-Net showing\nthe best trade-off between accuracy and reliability. Integrating these models,\nwhich account for uncertainty, into operational forecasting systems can improve\ndecision-making and preparedness for weather-related events.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Generative Diffusion Model-based Downscaling of Observed Sea Surface Height over Kuroshio Extension since 2000",
    "url": "http://arxiv.org/abs/2408.12632v1",
    "authors": [
      "Qiuchang Han",
      "Xingliang Jiang",
      "Yang Zhao",
      "Xudong Wang",
      "Zhijin Li",
      "Renhe Zhang"
    ],
    "published": "2024-08-22",
    "abstract": "Satellite altimetry has been widely utilized to monitor global sea surface\ndynamics, enabling investigation of upper ocean variability from basin-scale to\nlocalized eddy ranges. However, the sparse spatial resolution of observational\naltimetry limits our understanding of oceanic submesoscale variability,\nprevalent at horizontal scales below 0.25o resolution. Here, we introduce a\nstate-of-the-art generative diffusion model to train high-resolution sea\nsurface height (SSH) reanalysis data and demonstrate its advantage in\nobservational SSH downscaling over the eddy-rich Kuroshio Extension region. The\ndiffusion-based model effectively downscales raw satellite-interpolated data\nfrom 0.25o resolution to 1/16o, corresponding to approximately 12-km\nwavelength. This model outperforms other high-resolution reanalysis datasets\nand neural network-based methods. Also, it successfully reproduces the spatial\npatterns and power spectra of satellite along-track observations. Our\ndiffusion-based results indicate that eddy kinetic energy at horizontal scales\nless than 250 km has intensified significantly since 2004 in the Kuroshio\nExtension region. These findings underscore the great potential of deep\nlearning in reconstructing satellite altimetry and enhancing our understanding\nof ocean dynamics at eddy scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Long-Range Vision-Based UAV-assisted Localization for Unmanned Surface Vehicles",
    "url": "http://arxiv.org/abs/2408.11429v1",
    "authors": [
      "Waseem Akram",
      "Siyuan Yang",
      "Hailiang Kuang",
      "Xiaoyu He",
      "Muhayy Ud Din",
      "Yihao Dong",
      "Defu Lin",
      "Lakmal Seneviratne",
      "Shaoming He",
      "Irfan Hussain"
    ],
    "published": "2024-08-21",
    "abstract": "The global positioning system (GPS) has become an indispensable navigation\nmethod for field operations with unmanned surface vehicles (USVs) in marine\nenvironments. However, GPS may not always be available outdoors because it is\nvulnerable to natural interference and malicious jamming attacks. Thus, an\nalternative navigation system is required when the use of GPS is restricted or\nprohibited. To this end, we present a novel method that utilizes an Unmanned\nAerial Vehicle (UAV) to assist in localizing USVs in GNSS-restricted marine\nenvironments. In our approach, the UAV flies along the shoreline at a\nconsistent altitude, continuously tracking and detecting the USV using a deep\nlearning-based approach on camera images. Subsequently, triangulation\ntechniques are applied to estimate the USV's position relative to the UAV,\nutilizing geometric information and datalink range from the UAV. We propose\nadjusting the UAV's camera angle based on the pixel error between the USV and\nthe image center throughout the localization process to enhance accuracy.\nAdditionally, visual measurements are integrated into an Extended Kalman Filter\n(EKF) for robust state estimation. To validate our proposed method, we utilize\na USV equipped with onboard sensors and a UAV equipped with a camera. A\nheterogeneous robotic interface is established to facilitate communication\nbetween the USV and UAV. We demonstrate the efficacy of our approach through a\nseries of experiments conducted during the ``Muhammad Bin Zayed International\nRobotic Challenge (MBZIRC-2024)'' in real marine environments, incorporating\nnoisy measurements and ocean disturbances. The successful outcomes indicate the\npotential of our method to complement GPS for USV navigation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Kilometer-Scale Convection Allowing Model Emulation using Generative Diffusion Modeling",
    "url": "http://arxiv.org/abs/2408.10958v1",
    "authors": [
      "Jaideep Pathak",
      "Yair Cohen",
      "Piyush Garg",
      "Peter Harrington",
      "Noah Brenowitz",
      "Dale Durran",
      "Morteza Mardani",
      "Arash Vahdat",
      "Shaoming Xu",
      "Karthik Kashinath",
      "Michael Pritchard"
    ],
    "published": "2024-08-20",
    "abstract": "Storm-scale convection-allowing models (CAMs) are an important tool for\npredicting the evolution of thunderstorms and mesoscale convective systems that\nresult in damaging extreme weather. By explicitly resolving convective dynamics\nwithin the atmosphere they afford meteorologists the nuance needed to provide\noutlook on hazard. Deep learning models have thus far not proven skilful at\nkm-scale atmospheric simulation, despite being competitive at coarser\nresolution with state-of-the-art global, medium-range weather forecasting. We\npresent a generative diffusion model called StormCast, which emulates the\nhigh-resolution rapid refresh (HRRR) model-NOAA's state-of-the-art 3km\noperational CAM. StormCast autoregressively predicts 99 state variables at km\nscale using a 1-hour time step, with dense vertical resolution in the\natmospheric boundary layer, conditioned on 26 synoptic variables. We present\nevidence of successfully learnt km-scale dynamics including competitive 1-6\nhour forecast skill for composite radar reflectivity alongside physically\nrealistic convective cluster evolution, moist updrafts, and cold pool\nmorphology. StormCast predictions maintain realistic power spectra for multiple\npredicted variables across multi-hour forecasts. Together, these results\nestablish the potential for autoregressive ML to emulate CAMs -- opening up new\nkm-scale frontiers for regional ML weather prediction and future climate hazard\ndynamical downscaling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Forecasting seasonal rainfall in SE Australia using Empirical Orthogonal Functions and Neural Networks",
    "url": "http://arxiv.org/abs/2408.10550v1",
    "authors": [
      "Stjepan Marcelja"
    ],
    "published": "2024-08-20",
    "abstract": "Quantitative forecasting of average rainfall into the next season remains\nhighly challenging, but in some favourable isolated cases may be possible with\na series of relatively simple steps. We chose to explore predictions of austral\nspringtime rainfall in SE Australia regions based on the surrounding ocean\nsurface temperatures during the winter. In the first stage, we search for\ncorrelations between the target rainfall and both the standard ocean climate\nindicators as well as the time series of surface temperature data expanded in\nterms of Empirical Orthogonal Functions (EOFs). In the case of the Indian\nOcean, during the winter the dominant EOF shows stronger correlation with the\nfuture rainfall than the commonly used Indian Ocean Dipole. Information sources\nwith the strongest correlation to the historical rainfall data are then used as\ninputs into deep learning artificial neural networks. The resulting hindcasts\nappear accurate for September and October and less reliable for November. We\nalso attempt to forecast the rainfall in several regions for the coming austral\nspring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "DUNE: A Machine Learning Deep UNet++ based Ensemble Approach to Monthly, Seasonal and Annual Climate Forecasting",
    "url": "http://arxiv.org/abs/2408.06262v1",
    "authors": [
      "Pratik Shukla",
      "Milton Halem"
    ],
    "published": "2024-08-12",
    "abstract": "Capitalizing on the recent availability of ERA5 monthly averaged long-term\ndata records of mean atmospheric and climate fields based on high-resolution\nreanalysis, deep-learning architectures offer an alternative to physics-based\ndaily numerical weather predictions for subseasonal to seasonal (S2S) and\nannual means. A novel Deep UNet++-based Ensemble (DUNE) neural architecture is\nintroduced, employing multi-encoder-decoder structures with residual blocks.\nWhen initialized from a prior month or year, this architecture produced the\nfirst AI-based global monthly, seasonal, or annual mean forecast of 2-meter\ntemperatures (T2m) and sea surface temperatures (SST). ERA5 monthly mean data\nis used as input for T2m over land, SST over oceans, and solar radiation at the\ntop of the atmosphere for each month of 40 years to train the model. Validation\nforecasts are performed for an additional two years, followed by five years of\nforecast evaluations to account for natural annual variability. AI-trained\ninference forecast weights generate forecasts in seconds, enabling ensemble\nseasonal forecasts. Root Mean Squared Error (RMSE), Anomaly Correlation\nCoefficient (ACC), and Heidke Skill Score (HSS) statistics are presented\nglobally and over specific regions. These forecasts outperform persistence,\nclimatology, and multiple linear regression for all domains. DUNE forecasts\ndemonstrate comparable statistical accuracy to NOAA's operational monthly and\nseasonal probabilistic outlook forecasts over the US but at significantly\nhigher resolutions. RMSE and ACC error statistics for other recent AI-based\ndaily forecasts also show superior performance for DUNE-based forecasts. The\nDUNE model's application to an ensemble data assimilation cycle shows\ncomparable forecast accuracy with a single high-resolution model, potentially\neliminating the need for retraining on extrapolated datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "MetMamba: Regional Weather Forecasting with Spatial-Temporal Mamba Model",
    "url": "http://arxiv.org/abs/2408.06400v2",
    "authors": [
      "Haoyu Qin",
      "Yungang Chen",
      "Qianchuan Jiang",
      "Pengchao Sun",
      "Xiancai Ye",
      "Chao Lin"
    ],
    "published": "2024-08-12",
    "abstract": "Deep Learning based Weather Prediction (DLWP) models have been improving\nrapidly over the last few years, surpassing state of the art numerical weather\nforecasts by significant margins. While much of the optimization effort is\nfocused on training curriculum to extend forecast range in the global context,\ntwo aspects remains less explored: limited area modeling and better backbones\nfor weather forecasting. We show in this paper that MetMamba, a DLWP model\nbuilt on a state-of-the-art state-space model, Mamba, offers notable\nperformance gains and unique advantages over other popular backbones using\ntraditional attention mechanisms and neural operators. We also demonstrate the\nfeasibility of deep learning based limited area modeling via coupled training\nwith a global host model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Comparative Study of Convolutional and Recurrent Neural Networks for Storm Surge Prediction in Tampa Bay",
    "url": "http://arxiv.org/abs/2408.05797v1",
    "authors": [
      "Mandana Farhang Ghahfarokhi",
      "Seyed Hossein Sonbolestan",
      "Mahta Zamanizadeh"
    ],
    "published": "2024-08-11",
    "abstract": "In this paper, we compare the performance of three common deep learning\narchitectures, CNN-LSTM, LSTM, and 3D-CNN, in the context of surrogate storm\nsurge modeling. The study site for this paper is the Tampa Bay area in Florida.\nUsing high-resolution atmospheric data from the reanalysis models and\nhistorical water level data from NOAA tide stations, we trained and tested\nthese models to evaluate their performance. Our findings indicate that the\nCNN-LSTM model outperforms the other architectures, achieving a test loss of\n0.010 and an R-squared (R2) score of 0.84. The LSTM model, although it achieved\nthe lowest training loss of 0.007 and the highest training R2 of 0.88,\nexhibited poorer generalization with a test loss of 0.014 and an R2 of 0.77.\nThe 3D-CNN model showed reasonable performance with a test loss of 0.011 and an\nR2 of 0.82 but displayed instability under extreme conditions. A case study on\nHurricane Ian, which caused a significant negative surge of -1.5 meters in\nTampa Bay indicates the CNN-LSTM model's robustness and accuracy in extreme\nscenarios.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "Personalized Federated Learning for improving radar based precipitation nowcasting on heterogeneous areas",
    "url": "http://arxiv.org/abs/2408.05761v1",
    "authors": [
      "Judith S\u00e1inz-Pardo D\u00edaz",
      "Mar\u00eda Castrillo",
      "Juraj Bartok",
      "Ignacio Heredia Cach\u00e1",
      "Irina Malkin Ond\u00edk",
      "Ivan Martynovskyi",
      "Khadijeh Alibabaei",
      "Lisana Berberi",
      "Valentin Kozlov",
      "\u00c1lvaro L\u00f3pez Garc\u00eda"
    ],
    "published": "2024-08-11",
    "abstract": "The increasing generation of data in different areas of life, such as the\nenvironment, highlights the need to explore new techniques for processing and\nexploiting data for useful purposes. In this context, artificial intelligence\ntechniques, especially through deep learning models, are key tools to be used\non the large amount of data that can be obtained, for example, from weather\nradars. In many cases, the information collected by these radars is not open,\nor belongs to different institutions, thus needing to deal with the distributed\nnature of this data. In this work, the applicability of a personalized\nfederated learning architecture, which has been called adapFL, on distributed\nweather radar images is addressed. To this end, given a single available radar\ncovering 400 km in diameter, the captured images are divided in such a way that\nthey are disjointly distributed into four different federated clients. The\nresults obtained with adapFL are analyzed in each zone, as well as in a central\narea covering part of the surface of each of the previously distributed areas.\nThe ultimate goal of this work is to study the generalization capability of\nthis type of learning technique for its extrapolation to use cases in which a\nrepresentative number of radars is available, whose data can not be centralized\ndue to technical, legal or administrative concerns. The results of this\npreliminary study indicate that the performance obtained in each zone with the\nadapFL approach allows improving the results of the federated learning\napproach, the individual deep learning models and the classical Continuity\nTracking Radar Echoes by Correlation approach.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Accurate deep learning-based filtering for chaotic dynamics by identifying instabilities without an ensemble",
    "url": "http://arxiv.org/abs/2408.04739v2",
    "authors": [
      "Marc Bocquet",
      "Alban Farchi",
      "Tobias S. Finn",
      "Charlotte Durand",
      "Sibo Cheng",
      "Yumeng Chen",
      "Ivo Pasmans",
      "Alberto Carrassi"
    ],
    "published": "2024-08-08",
    "abstract": "We investigate the ability to discover data assimilation (DA) schemes meant\nfor chaotic dynamics with deep learning. The focus is on learning the analysis\nstep of sequential DA, from state trajectories and their observations, using a\nsimple residual convolutional neural network, while assuming the dynamics to be\nknown. Experiments are performed with the Lorenz 96 dynamics, which display\nspatiotemporal chaos and for which solid benchmarks for DA performance exist.\nThe accuracy of the states obtained from the learned analysis approaches that\nof the best possibly tuned ensemble Kalman filter, and is far better than that\nof variational DA alternatives. Critically, this can be achieved while\npropagating even just a single state in the forecast step. We investigate the\nreason for achieving ensemble filtering accuracy without an ensemble. We\ndiagnose that the analysis scheme actually identifies key dynamical\nperturbations, mildly aligned with the unstable subspace, from the forecast\nstate alone, without any ensemble-based covariances representation. This\nreveals that the analysis scheme has learned some multiplicative ergodic\ntheorem associated to the DA process seen as a non-autonomous random dynamical\nsystem.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Underwater litter monitoring using consumer-grade aerial-aquatic speedy scanner (AASS) and deep learning based super-resolution reconstruction and detection network",
    "url": "http://arxiv.org/abs/2408.03564v2",
    "authors": [
      "Fan Zhao",
      "Yongying Liu",
      "Jiaqi Wang",
      "Yijia Chen",
      "Dianhan Xi",
      "Xinlei Shao",
      "Shigeru Tabeta",
      "Katsunori Mizuno"
    ],
    "published": "2024-08-07",
    "abstract": "Underwater litter is widely spread across aquatic environments such as lakes,\nrivers, and oceans, significantly impacting natural ecosystems. Current\nmonitoring technologies for detecting underwater litter face limitations in\nsurvey efficiency, cost, and environmental conditions, highlighting the need\nfor efficient, consumer-grade technologies for automatic detection. This\nresearch introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with\nSuper-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network.\nAASS enhances data acquisition efficiency over traditional methods, capturing\nhigh-quality images that accurately identify underwater waste. SRR improves\nimage-resolution by mitigating motion blur and insufficient resolution, thereby\nenhancing detection tasks. Specifically, the RCAN model achieved the highest\nmean average precision (mAP) of 78.6% for detection accuracy on reconstructed\nimages among the tested SRR models. With a magnification factor of 4, the SRR\ntest set shows an improved mAP compared to the conventional bicubic set. These\nresults demonstrate the effectiveness of the proposed method in detecting\nunderwater litter.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Super-Resolution"
    ]
  },
  {
    "title": "ORCAst: Operational High-Resolution Current Forecasts",
    "url": "http://arxiv.org/abs/2501.12054v1",
    "authors": [
      "Pierre Garcia",
      "In\u00e8s Larroche",
      "Am\u00e9lie Pesnec",
      "Hannah Bull",
      "Th\u00e9o Archambault",
      "Evangelos Moschos",
      "Alexandre Stegner",
      "Anastase Charantonis",
      "Dominique B\u00e9r\u00e9ziat"
    ],
    "published": "2025-01-21",
    "abstract": "We present ORCAst, a multi-stage, multi-arm network for Operational\nhigh-Resolution Current forecAsts over one week. Producing real-time nowcasts\nand forecasts of ocean surface currents is a challenging problem due to\nindirect or incomplete information from satellite remote sensing data. Entirely\ntrained on real satellite data and in situ measurements from drifters, our\nmodel learns to forecast global ocean surface currents using various sources of\nground truth observations in a multi-stage learning procedure. Our multi-arm\nencoder-decoder model architecture allows us to first predict sea surface\nheight and geostrophic currents from larger quantities of nadir and SWOT\naltimetry data, before learning to predict ocean surface currents from much\nmore sparse in situ measurements from drifters. Training our model on specific\nregions improves performance. Our model achieves stronger nowcast and forecast\nperformance in predicting ocean surface currents than various state-of-the-art\nmethods.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Nowcast"
    ]
  },
  {
    "title": "An analysis on OpenMetBuoy-v2021 drifter in-situ data and Lagrangian trajectory simulations in the Agulhas Current System",
    "url": "http://arxiv.org/abs/2409.20096v1",
    "authors": [
      "Bente Moerman",
      "\u00d8yvind Breivik",
      "Lars R. Hole",
      "Gaute Hope",
      "Johnny A. Johannessen",
      "Jean Rabault"
    ],
    "published": "2024-09-30",
    "abstract": "In order to perform a sensitivity analysis of Lagrangian trajectory models,\nLagrangian trajectory simulations have been compared to six OpenMetBuoy-v2021\ndrifter trajectories in the Agulhas Current System (Jan-Mar 2023). Three\ndifferent Lagrangian trajectory simulations have been assessed: (1) two offline\nLagrangian tracking tools, OpenDrift and Parcels, (2) three Eulerian ocean\nsurface current products, HYCOM, Mercator and Globcurrent, and (3) the addition\nof wind and/or wave forcing parameterizations. The latter has also been\nevaluated by strong ocean current, high wind speed and Stokes drift regimes.\n  Firstly, using the same time stepping scheme and linear interpolation\nmethods, the different Lagrangian simulators OpenDrift and Parcels, performed\nidentically. Secondly, the Globcurrent product showed the highest mean skill of\nthe three ocean current products, although it underestimated the speed for\nstrong ocean currents due to its spatial resolution. The HYCOM and Mercator\nmodel simulations showed, respectively, 40\\% and 15\\% lower skill than the\nGlobcurrent simulations. Finally, the addition of the Stokes drift and a wind\ndrift factor (WDF), improved the Lagrangian simulation performance in skill and\nspeed, especially in high wind (>10 m/s) and/or Stokes drift regimes (>0.15\nm/s). The optimal WDF for the OpenMetBuoy-v2021 is found to be ~1.8\\% and\n~2.3\\% for simulations including and excluding Stokes drift forcing\nrespectively. To further improve the incorporation of Stokes drift and direct\nwind drag on the trajectory simulations, a more physically based solution is\nadvised as there are still numerous wind and wave related processes that remain\nunresolved, like wave-current interactions and vertical shear.\n  To statistically strengthen the conclusions from this research, incorporating\nadditional observed drifter trajectories would be highly favourable.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "High-Frequency Radar observation of strong and contrasted currents: the Alderney race paradigm",
    "url": "http://arxiv.org/abs/2407.03827v1",
    "authors": [
      "Dylan Dumas",
      "Anne-Claire Bennis",
      "Charles-Antoine Gu\u00e9rin",
      "Guiomar Lopez",
      "Laurent Benoit"
    ],
    "published": "2024-07-04",
    "abstract": "The Alderney Race has been identified as a future site for the development of\ntidal energy, due to its bidirectional strong current reaching 5 m/s during\nspring tides. This hydrodynamics is very difficult to measure by in situ or\nremote sensing means. High-frequency coastal radars can provide a synoptic and\nnear-real-time view of such a complex circulation, but the classical processing\nalgorithms are not adapted to the extreme situation of strongly sheared\ncurrents. We propose an improved high-resolution direction-finding technique\nfor the azimuthal processing of such radar data. It uses phased-array systems\nand combines the advantages of the usual beam-forming technique to eliminate\nmany problems related to the distortion of Doppler spectra by extreme currents.\nThe method is evaluated with a unique data set of radar measurements at two\nradar frequencies (13 and 24.5 MHz) and three spatial resolutions (200, 750,\nand 1500 m). The radar-based surface currents are analyzed in the light of a\nhigh-resolution numerical model and also compared with in situ measurements.\nWhile high azimuthal resolution can be achieved in this way, it is shown that\nthe typical range resolutions of 750 and 1500 m are insufficient to account for\nthe strong spatial variations of the surface current at some specific times and\nlocations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "High Frequency Radar Observing System Simulation Experiment in the Western Mediterranean Sea: a Lagrangian assessment approach",
    "url": "http://arxiv.org/abs/2406.03579v2",
    "authors": [
      "Jaime Hernandez Lasheras",
      "Alejandro Orfila",
      "Alex Santana",
      "Ismael Hernandez Carrasco",
      "Baptiste Mourre"
    ],
    "published": "2024-06-05",
    "abstract": "The impact of the expansion of a high-frequency radar (HFR) system in a\ndynamic coastal area (the Ibiza Channel in the Western Mediterranean Sea) is\nevaluated through an Observing System Simulation Experiment (OSSE). The\ninstallation of two new antennas in the Iberian Peninsula would complement the\nexisting ones in the islands of Ibiza and Formentera, providing surface\ncurrents observations of the full channel. Two different configurations of the\nsame model, validated to give realistic simulations, are used: i) a Nature Run\n(NR) which is considered as the real ocean state and that is used to generate\npseudo-observations, and ii) a Control Run (CR) in which the\npseudo-observations are assimilated. The OSSE is first validated by comparison\nagainst a previous Observing System Experiment (OSE). The impact of the new\nantennas for forecasting surface currents is evaluated in two different periods\nwith different levels of agreement between NR and CR. The HFR expansion is\nfound to contribute to significantly correct the circulation patterns in the\nChannel, leading to surface merdional velocity error reductions up to 19%. The\neffects on the transport in the area are also analyzed from a Lagrangian\nperspective, showing that DA can help to better represent the Lagrangian\nCoherent Structures present in the NR and constrain the ocean dynamics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Synthetic RAW data generator for ESA HARMONY mission",
    "url": "http://arxiv.org/abs/2405.09938v1",
    "authors": [
      "Goulven Monnier",
      "Benjamin Camus",
      "Yann-Herv\u00e9 Hellouvry",
      "Pierre Dubois",
      "Erik de Witte"
    ],
    "published": "2024-05-16",
    "abstract": "In this paper, we introduce HEEPS/MARE, the end-to-end simulator developed\nfor the SAR oceanographic products of ESA Earth Explorer 10 mission, Harmony,\nexpected to launch in Decembre 2029. Harmony is primarily dedicated to the\nobservation of small-scale motion and deformation fields of the Earth surface\n(oceans, glaciers and ice sheets, solid Earth), thanks to passive SAR/ATI\nreceivers carried by two companion satellites for Sentinel-1. The paper focuses\non the raw data generator designed to efficiently simulate large,\nheterogeneous, moving oceanic areas and produce the acquired SAR/ATI bistatic\nIQ signals. The heterogeneous sea-surface model, bistatic scattering model,\nmulti-GPU implementation and achieved performance are emphasized. Finally,\nsample results are presented, to illustrate the ability of Harmony to map wind\nand surface current vectors at kilometric scale.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Scattering of surface waves by ocean currents: the U2H map",
    "url": "http://arxiv.org/abs/2402.05652v3",
    "authors": [
      "Han Wang",
      "Ana B. Villas B\u00f4as",
      "Jacques Vanneste",
      "William R. Young"
    ],
    "published": "2024-02-08",
    "abstract": "Ocean turbulence at meso- and submesocales affects the propagation of surface\nwaves through refraction and scattering, inducing spatial modulations in\nsignificant wave height (SWH). We develop a theoretical framework that relates\nthese modulations to the current that induces them. We exploit the asymptotic\nsmallness of the ratio of typical current speed to wave group speed to derive a\nlinear map -- the U2H map -- between surface current velocity and SWH anomaly.\nThe U2H map is a convolution, non-local in space, expressible as a product in\nFourier space by a factor independent of the magnitude of the wavenumber\nvector. Analytic expressions of the U2H map show how the SWH responds\ndifferently to the vortical and divergent parts of the current, and how the\nanisotropy of the wave spectrum is key to large current-induced SWH anomalies.\nWe implement the U2H map numerically and test its predictions against WAVEWATCH\nIII numerical simulations for both idealised and realistic current\nconfigurations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
    "url": "http://arxiv.org/abs/2406.02770v1",
    "authors": [
      "Kathrin Donandt",
      "Karim B\u00f6ttger",
      "Dirk S\u00f6ffker"
    ],
    "published": "2024-06-04",
    "abstract": "Accurate vessel trajectory prediction is necessary for save and efficient\nnavigation. Deep learning-based prediction models, esp. encoder-decoders, are\nrarely applied to inland navigation specifically. Approaches from the maritime\ndomain cannot directly be transferred to river navigation due to specific\ndriving behavior influencing factors. Different encoder-decoder architectures,\nincluding a transformer encoder-decoder, are compared herein for predicting the\nnext positions of inland vessels, given not only spatio-temporal information\nfrom AIS, but also river specific features. The results show that the\nreformulation of the regression task as classification problem and the\ninclusion of river specific features yield the lowest displacement errors. The\nstandard LSTM encoder-decoder outperforms the transformer encoder-decoder for\nthe data considered, but is computationally more expensive. In this study for\nthe first time a transformer-based encoder-decoder model is applied to the\nproblem of predicting the ship trajectory. Here, a feature vector using the\nriver-specific context of navigation input parameters is established. Future\nstudies can built on the proposed models, investigate the improvement of the\ncomputationally more efficient transformer, e.g. through further\nhyper-parameter optimization, and use additional river-specific information in\nthe context representation to further increase prediction accuracy.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction",
    "url": "http://arxiv.org/abs/2503.21834v1",
    "authors": [
      "Haomin Yu",
      "Tianyi Li",
      "Kristian Torp",
      "Christian S. Jensen"
    ],
    "published": "2025-03-27",
    "abstract": "Accurate vessel trajectory prediction facilitates improved navigational\nsafety, routing, and environmental protection. However, existing prediction\nmethods are challenged by the irregular sampling time intervals of the vessel\ntracking data from the global AIS system and the complexity of vessel movement.\nThese aspects render model learning and generalization difficult. To address\nthese challenges and improve vessel trajectory prediction, we propose the\nmulti-modal knowledge-enhanced framework (MAKER) for vessel trajectory\nprediction. To contend better with the irregular sampling time intervals, MAKER\nfeatures a Large language model-guided Knowledge Transfer (LKT) module that\nleverages pre-trained language models to transfer trajectory-specific\ncontextual knowledge effectively. To enhance the ability to learn complex\ntrajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning\n(KSL) module. This module employs kinematic knowledge to progressively\nintegrate complex patterns during training, allowing for adaptive learning and\nenhanced generalization. Experimental results on two vessel trajectory datasets\nshow that MAKER can improve the prediction accuracy of state-of-the-art methods\nby 12.08%-17.86%.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "Predicting Barge Presence and Quantity on Inland Waterways using Vessel Tracking Data: A Machine Learning Approach",
    "url": "http://arxiv.org/abs/2501.00615v1",
    "authors": [
      "Geoffery Agorkua",
      "Sarah Hernandez",
      "Maria Falquez",
      "Subhadipto Poddar",
      "Shihao Pang"
    ],
    "published": "2024-12-31",
    "abstract": "This study presents a machine learning approach to predict the number of\nbarges transported by vessels on inland waterways using tracking data from the\nAutomatic Identification System (AIS). While AIS tracks the location of tug and\ntow vessels, it does not monitor the presence or number of barges transported\nby those vessels. Understanding the number and types of barges conveyed along\nriver segments, between ports, and at ports is crucial for estimating the\nquantities of freight transported on the nation's waterways. This insight is\nalso valuable for waterway management and infrastructure operations impacting\nareas such as targeted dredging operations, and data-driven resource\nallocation. Labeled sample data was generated using observations from traffic\ncameras located along key river segments and matched to AIS data records. A\nsample of 164 vessels representing up to 42 barge convoys per vessel was used\nfor model development. The methodology involved first predicting barge presence\nand then predicting barge quantity. Features derived from the AIS data included\nspeed measures, vessel characteristics, turning measures, and interaction\nterms. For predicting barge presence, the AdaBoost model achieved an F1 score\nof 0.932. For predicting barge quantity, the Random Forest combined with an\nAdaBoost ensemble model achieved an F1 score of 0.886. Bayesian optimization\nwas used for hyperparameter tuning. By advancing predictive modeling for inland\nwaterways, this study offers valuable insights for transportation planners and\norganizations, which require detailed knowledge of traffic volumes, including\nthe flow of commodities, their destinations, and the tonnage moving in and out\nof ports.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "DisBeaNet: A Deep Neural Network to augment Unmanned Surface Vessels for maritime situational awareness",
    "url": "http://arxiv.org/abs/2405.06149v2",
    "authors": [
      "Srikanth Vemula",
      "Eulises Franco",
      "Michael Frye"
    ],
    "published": "2024-05-10",
    "abstract": "Intelligent detection and tracking of the vessels on the sea play a\nsignificant role in conducting traffic avoidance in unmanned surface\nvessels(USV). Current traffic avoidance software relies mainly on Automated\nIdentification System (AIS) and radar to track other vessels to avoid\ncollisions and acts as a typical perception system to detect targets. However,\nin a contested environment, emitting radar energy also presents the\nvulnerability to detection by adversaries. Deactivating these Radiofrequency\ntransmitting sources will increase the threat of detection and degrade the\nUSV's ability to monitor shipping traffic in the vicinity. Therefore, an\nintelligent visual perception system based on an onboard camera with passive\nsensing capabilities that aims to assist USV in addressing this problem is\npresented in this paper. This paper will present a novel low-cost vision\nperception system for detecting and tracking vessels in the maritime\nenvironment. This novel low-cost vision perception system is introduced using\nthe deep learning framework. A neural network, DisBeaNet, can detect vessels,\ntrack, and estimate the vessel's distance and bearing from the monocular\ncamera. The outputs obtained from this neural network are used to determine the\nlatitude and longitude of the identified vessel.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Spatio-temporal characterisation of underwater noise through semantic trajectories",
    "url": "http://arxiv.org/abs/2501.11131v1",
    "authors": [
      "Giulia Rovinelli",
      "Davide Rocchesso",
      "Marta Simeoni",
      "Esteban Zim\u00e1nyi",
      "Alessandra Raffaet\u00e0"
    ],
    "published": "2025-01-19",
    "abstract": "Underwater noise pollution from human activities, particularly shipping, has\nbeen recognised as a serious threat to marine life. The sound generated by\nvessels can have various adverse effects on fish and aquatic ecosystems in\ngeneral. In this setting, the estimation and analysis of the underwater noise\nproduced by vessels is an important challenge for the preservation of the\nmarine environment. In this paper we propose a model for the spatio-temporal\ncharacterisation of the underwater noise generated by vessels. The approach is\nbased on the reconstruction of the vessels' trajectories from Automatic\nIdentification System (AIS) data and on their deployment in a spatio-temporal\ndatabase. Trajectories are enriched with semantic information like the acoustic\ncharacteristics of the vessels' engines or the activity performed by the\nvessels. We define a model for underwater noise propagation and use the\ntrajectories' information to infer how noise propagates in the area of\ninterest. We develop our approach for the case study of the fishery activities\nin the Northern Adriatic sea, an area of the Mediterranean sea which is well\nknown to be highly exploited. We implement our approach using MobilityDB, an\nopen source geospatial trajectory data management and analysis platform, which\noffers spatio-temporal operators and indexes improving the efficiency of our\nsystem. We use this platform to conduct various analyses of the underwater\nnoise generated in the Northern Adriatic Sea, aiming at estimating the impact\nof fishing activities on underwater noise pollution and at demonstrating the\nflexibility and expressiveness of our approach.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Architecture for Trajectory-Based Fishing Ship Classification with AIS Data",
    "url": "http://arxiv.org/abs/2501.02038v1",
    "authors": [
      "David S\u00e1nchez Pedroche",
      "Daniel Amigo",
      "Jes\u00fas Garc\u00eda",
      "Jose M. Molina"
    ],
    "published": "2025-01-03",
    "abstract": "This paper proposes a data preparation process for managing real-world\nkinematic data and detecting fishing vessels. The solution is a binary\nclassification that classifies ship trajectories into either fishing or\nnon-fishing ships. The data used are characterized by the typical problems\nfound in classic data mining applications using real-world data, such as noise\nand inconsistencies. The two classes are also clearly unbalanced in the data, a\nproblem which is addressed using algorithms that resample the instances. For\nclassification, a series of features are extracted from spatiotemporal data\nthat represent the trajectories of the ships, available from sequences of\nAutomatic Identification System (AIS) reports. These features are proposed for\nthe modelling of ship behavior but, because they do not contain context-related\ninformation, the classification can be applied in other scenarios.\nExperimentation shows that the proposed data preparation process is useful for\nthe presented classification problem. In addition, positive results are\nobtained using minimal information.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Composing Open-domain Vision with RAG for Ocean Monitoring and Conservation",
    "url": "http://arxiv.org/abs/2412.02262v1",
    "authors": [
      "Sepand Dyanatkar",
      "Angran Li",
      "Alexander Dungate"
    ],
    "published": "2024-12-03",
    "abstract": "Climate change's destruction of marine biodiversity is threatening\ncommunities and economies around the world which rely on healthy oceans for\ntheir livelihoods. The challenge of applying computer vision to niche,\nreal-world domains such as ocean conservation lies in the dynamic and diverse\nenvironments where traditional top-down learning struggle with long-tailed\ndistributions, generalization, and domain transfer. Scalable species\nidentification for ocean monitoring is particularly difficult due to the need\nto adapt models to new environments and identify rare or unseen species. To\novercome these limitations, we propose leveraging bottom-up, open-domain\nlearning frameworks as a resilient, scalable solution for image and video\nanalysis in marine applications. Our preliminary demonstration uses pretrained\nvision-language models (VLMs) combined with retrieval-augmented generation\n(RAG) as grounding, leaving the door open for numerous architectural, training\nand engineering optimizations. We validate this approach through a preliminary\napplication in classifying fish from video onboard fishing vessels,\ndemonstrating impressive emergent retrieval and prediction capabilities without\ndomain-specific training or knowledge of the task itself.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Identifying companies and financial actors exposed to marine tipping points",
    "url": "http://arxiv.org/abs/2411.10307v1",
    "authors": [
      "Juan C. Rocha",
      "Jean-Baptiste Jouffray",
      "Frida Bengtsson",
      "Bianca-Ioana Voicu",
      "Paula A. S\u00e1nchez",
      "Victor Galaz"
    ],
    "published": "2024-11-15",
    "abstract": "Climate change and other anthropogenic pressures are likely to induce tipping\npoints in marine ecosystems, potentially leading to declines in primary\nproductivity and fisheries. Despite increasing attention to nature-related\nfinancial risks and opportunities within the ocean economy, the extent to which\nthese tipping points could affect investors has remained largely unexplored.\nHere we used satellite data to track fishing vessels operating in areas prone\nto marine regime shifts, as identified by their loss of resilience and\nvulnerability to marine heatwaves, and uncovered their corporate beneficial\nowners and shareholders. Despite some data gaps, we identified key countries,\ncompanies, and shareholders exposed to tipping risk. We also outline the\npotential challenges and opportunities that these actors may face if marine\necosystems shift to less productive states.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Vessel Re-identification and Activity Detection in Thermal Domain for Maritime Surveillance",
    "url": "http://arxiv.org/abs/2406.08294v1",
    "authors": [
      "Yasod Ginige",
      "Ransika Gunasekara",
      "Darsha Hewavitharana",
      "Manjula Ariyarathne",
      "Ranga Rodrigo",
      "Peshala Jayasekara"
    ],
    "published": "2024-06-12",
    "abstract": "Maritime surveillance is vital to mitigate illegal activities such as drug\nsmuggling, illegal fishing, and human trafficking. Vision-based maritime\nsurveillance is challenging mainly due to visibility issues at night, which\nresults in failures in re-identifying vessels and detecting suspicious\nactivities. In this paper, we introduce a thermal, vision-based approach for\nmaritime surveillance with object tracking, vessel re-identification, and\nsuspicious activity detection capabilities. For vessel re-identification, we\npropose a novel viewpoint-independent algorithm which compares features of the\nsides of the vessel separately (separate side-spaces) leveraging shape\ninformation in the absence of color features. We propose techniques to adapt\ntracking and activity detection algorithms for the thermal domain and train\nthem using a thermal dataset we created. This dataset will be the first\npublicly available benchmark dataset for thermal maritime surveillance. Our\nsystem is capable of re-identifying vessels with an 81.8% Top1 score and\nidentifying suspicious activities with a 72.4\\% frame mAP score; a new\nbenchmark for each task in the thermal domain.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General Optimization Problems",
    "url": "http://arxiv.org/abs/2405.02850v7",
    "authors": [
      "Jiawen Li",
      "Anwar PP Abdul Majeed",
      "Pascal Lefevre"
    ],
    "published": "2024-05-05",
    "abstract": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems. The HEO mimics the effects between quantum such as tunneling,\nentanglement. After the introduction to the HEO mechansims, the study presents\na comprehensive evaluation of HEO's performance against extensively-used\noptimization algorithms, including Particle Swarm Optimization (PSO), Genetic\nAlgorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer\n(GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary\nanalysis encompasses 14 benchmark functions with dimension 30, demonstrating\nHEO's effectiveness and adaptability in navigating general optimization\nproblems. The test of HEO in Pressure Vessel Design and Tubular Column Design\nalso infers its feasibility and potential in real-time applications. Further\nvalidation of HEO in Osmancik-97 and Cammeo Rice Classification achieves a\nhigher accuracy record.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "FAD-SAR: A Novel Fishing Activity Detection System via Synthetic Aperture Radar Images Based on Deep Learning Method",
    "url": "http://arxiv.org/abs/2404.18245v2",
    "authors": [
      "Yanbing Bai",
      "Siao Li",
      "Rui-Yang Ju",
      "Zihao Yang",
      "Jinze Yu",
      "Jen-Shiun Chiang"
    ],
    "published": "2024-04-28",
    "abstract": "Illegal, unreported, and unregulated (IUU) fishing activities seriously\naffect various aspects of human life. However, traditional methods for\ndetecting and monitoring IUU fishing activities at sea have limitations.\nAlthough synthetic aperture radar (SAR) can complement existing vessel\ndetection systems, extracting useful information from SAR images using\ntraditional methods remains a challenge, especially in IUU fishing. This paper\nproposes a deep learning based fishing activity detection system, which is\nimplemented on the xView3 dataset using six classical object detection models:\nSSD, RetinaNet, FSAF, FCOS, Faster R-CNN, and Cascade R-CNN. In addition, this\nwork employs different enhancement techniques to improve the performance of the\nFaster R-CNN model. The experimental results demonstrate that training the\nFaster R-CNN model using the Online Hard Example Mining (OHEM) strategy\nincreases the Avg-F1 value from 0.212 to 0.216.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Shipping traffic through the Arctic Ocean: spatial distribution, temporal evolution and its dependence on the sea ice extent",
    "url": "http://arxiv.org/abs/2403.01856v1",
    "authors": [
      "Jorge P. Rodr\u00edguez",
      "Konstantin Klemm",
      "Carlos M. Duarte",
      "V\u00edctor M. Egu\u00edluz"
    ],
    "published": "2024-03-04",
    "abstract": "The reduction in sea ice cover with Arctic warming facilitates the transit of\nships through routes that are remarkably shorter than the traditional shipping\nroutes. Automatic Identification System (AIS), ideally designed to avoid vessel\ncollisions, transmits on vessel navigation information (currently 27 types of\nmessages) such as name, position or speed, is a powerful data source to monitor\nthe progress of Arctic shipping as the ice cover decreases. Based on the\nanalysis of an online platform collecting shipping AIS data, we quantified the\nspatial distribution of shipping through the Arctic Ocean, its intensity and\nthe temporal evolution, in relation to the area released by the sea ice area.\nShipping through the Arctic Ocean is distributed spatially following a\nheavy-tailed distribution, implying heavy traffic through a limited Arctic\narea, with an exponent that depends on the vessel category. Fishing is the\ncategory with the largest spatial spread, with the width of shipping routes\ncorrelated with the proximal sea ice area. The time evolution of these routes\nis characterized by increasing extended periods of shipping activity through\nthe year. AIS data offers valuable information on the activity of the\ninternational fleet worldwide. In the context of the new international\nagreements, it is a valuable source to monitor shipping, fishing and the\npotential impact in marine life among other aspects. Here we have focused on\nthe Arctic shipping in recent years, which is rapidly growing, particularly\naround the Northeastern and Northwest Passage coastal routes, providing an\nopportunity for the design of shorter shipping routes and reduced greenhouse\ngas emissions from transport of goods, but at a risk of impacts on the Arctic\necosystem.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Prediction of the Economic Behavior of Fishery Biotechnology Companies Based on Machine Learning-Based Deep Metacellular Automata",
    "url": "http://arxiv.org/abs/2402.13509v2",
    "authors": [
      "Liguo Chen",
      "Hongyang Hua",
      "Xinyue Luo",
      "Guoli Xu",
      "Xu Yan"
    ],
    "published": "2024-02-21",
    "abstract": "Ocean warming significantly affects the fishing industry, with species like\nScottish herring and mackerel migrating northwards. Our research, a fusion of\nartificial intelligence, data science, and operations research, addresses this\ncrisis. Using Long Short Term Memory networks, we forecast sea surface\ntemperatures (SST) and model fish migratory patterns with Enhanced Cellular\nAutomata. A corrective factor within our model adjusts for human impact on SST,\nguiding diverse mitigation scenarios. We apply operational research to\nstrategize responses, including the modernization of fishing vessels as a less\ncostly alternative to relocation. Our data-driven approach, suggesting fleet\nmodernization, strategic relocation, and product diversification, offers an\neffective approach to mitigating the threats to the ocean warming phenomenon.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Experimental investigations of underwater and airborne noises produced by a large hovercraft in Ural River estuary",
    "url": "http://arxiv.org/abs/2401.14204v1",
    "authors": [
      "A. I. Vedenev",
      "O. Yu. Kochetov",
      "A. A. Lunkov",
      "A. S. Shurup",
      "S. S. Kassymbekova"
    ],
    "published": "2024-01-25",
    "abstract": "Simultaneous measurements of underwater and airborne noises produced by\nGriffon Hoverwork BHT130 hovercraft were carried out in environmentally\nsensitive area - wildlife preserve in the area of the Ural River estuary near\nthe Caspian Sea shelf. Measurements were organized to assess the possible\nnegative impact of noise from hovercraft on fish and birds in wildlife\npreserve. The particle velocity of underwater noise was estimated by using a\ngradient-type vector receiver. That was a distinctive aspect of the underwater\nnoise studies since the majority of fish perceives the sound in terms of\nvibration of particles, and only a few as the pressure. Using synchronous\nrecording of underwater and airborne noises, the mutual correlation of these\ndata was investigated. The obtained correlation levels between underwater and\nairborne noises produced by hovercraft can be used for simplified estimation of\nthe upper boundary of underwater noise level by measuring levels of airborne\nnoise. The measured and estimated maximal levels of underwater noises of\nhovercraft are considerably lower than noises from conventional vessels with\nunderwater engines, that makes hovercraft attractive alternative for use in\nlocations with high underwater noise requirements, such as Ural River estuary\nand Caspian Sea shelf.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case",
    "url": "http://arxiv.org/abs/2401.02278v1",
    "authors": [
      "Febrian Kurniawan",
      "Gandeva Bayu Satrya",
      "Firuz Kamalov"
    ],
    "published": "2024-01-04",
    "abstract": "The enormous demand for seafood products has led to exploitation of marine\nresources and near-extinction of some species. In particular, overfishing is\none the main issues in sustainable marine development. In alignment with the\nprotection of marine resources and sustainable fishing, this study proposes to\nadvance fish classification techniques that support identifying protected fish\nspecies using state-of-the-art machine learning. We use a custom modification\nof the MobileNet model to design a lightweight classifier called M-MobileNet\nthat is capable of running on limited hardware. As part of the study, we\ncompiled a labeled dataset of 37,462 images of fish found in the waters of the\nIndonesian archipelago. The proposed model is trained on the dataset to\nclassify images of the captured fish into their species and give\nrecommendations on whether they are consumable or not. Our modified MobileNet\nmodel uses only 50\\% of the top layer parameters with about 42% GTX 860M\nutility and achieves up to 97% accuracy in fish classification and determining\nits consumability. Given the limited computing capacity available on many\nfishing vessels, the proposed model provides a practical solution to on-site\nfish classification. In addition, synchronized implementation of the proposed\nmodel on multiple vessels can supply valuable information about the movement\nand location of different species of fish.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions",
    "url": "http://arxiv.org/abs/2504.13476v1",
    "authors": [
      "Jiadong Lou",
      "Bingqing Liu",
      "Yuanheng Xiong",
      "Xiaodong Zhang",
      "Xu Yuan"
    ],
    "published": "2025-04-18",
    "abstract": "Phytoplankton absorb and scatter light in unique ways, subtly altering the\ncolor of water, changes that are often minor for human eyes to detect but can\nbe captured by sensitive ocean color instruments onboard satellites from space.\nHyperspectral sensors, paired with advanced algorithms, are expected to\nsignificantly enhance the characterization of phytoplankton community\ncomposition, especially in coastal waters where ocean color remote sensing\napplications have historically encountered significant challenges. This study\npresents novel machine learning-based solutions for NASA's hyperspectral\nmissions, including EMIT and PACE, tackling high-fidelity retrievals of\nphytoplankton absorption coefficient and chlorophyll a from their hyperspectral\nremote sensing reflectance. Given that a single Rrs spectrum may correspond to\nvaried combinations of inherent optical properties and associated\nconcentrations, the Variational Autoencoder (VAE) is used as a backbone in this\nstudy to handle such multi-distribution prediction problems. We first time\ntailor the VAE model with innovative designs to achieve hyperspectral\nretrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex\nestuarine-coastal waters. Validation with extensive experimental observation\ndemonstrates superior performance of the VAE models with high precision and low\nbias. The in-depth analysis of VAE's advanced model structures and learning\ndesigns highlights the improvement and advantages of VAE-based solutions over\nthe mixture density network (MDN) approach, particularly on high-dimensional\ndata, such as PACE. Our study provides strong evidence that current EMIT and\nPACE hyperspectral data as well as the upcoming Surface Biology Geology mission\nwill open new pathways toward a better understanding of phytoplankton community\ndynamics in aquatic ecosystems when integrated with AI technologies.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Bayesian hierarchical framework for fusion of remote sensing data: An example with solar-induced fluorescence",
    "url": "http://arxiv.org/abs/2503.03901v1",
    "authors": [
      "Manju Johny",
      "Jonathan Hobbs",
      "Vineet Yadav",
      "Margaret Johnson",
      "Nicholas Parazoo",
      "Hai Nguyen",
      "Amy Braverman"
    ],
    "published": "2025-03-05",
    "abstract": "Solar-induced chlorophyll fluorescence (SIF) has emerged as an effective\nindicator of vegetation productivity and plant health. The global\nquantification of SIF and its associated uncertainties yields many important\ncapabilities, including improving carbon flux estimation, improving the\nidentification of carbon sources and sinks, monitoring a variety of ecosystems,\nand evaluating carbon sequestration efforts. Long-term, regional-to-global\nscale monitoring is now feasible with the availability of SIF estimates from\nmultiple Earth-observing satellites. These efforts can be aided by a rigorous\naccounting of the sources of uncertainty present in satellite SIF data\nproducts. In this paper, we introduce a Bayesian Hierarchical Model (BHM) for\nthe estimation of SIF and associated uncertainties from Orbiting Carbon\nObservatory-2 (OCO-2) satellite observations at one-degree resolution with\nglobal coverage. The hierarchical structure of our modeling framework allows\nfor convenient model specification, quantification of various sources of\nvariation, and the incorporation of seasonal SIF information through Fourier\nterms in the regression model. The modeling framework leverages the predictable\nseasonality of SIF in most temperate land areas. The resulting data product\ncomplements existing atmospheric carbon dioxide estimates at the same\nspatio-temporal resolution.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Fine scale depth regulation of invertebrate larvae around coastal fronts",
    "url": "http://arxiv.org/abs/2401.10303v1",
    "authors": [
      "Nicolas Weidberg",
      "Wayne Goschen",
      "Jennifer M. Jackson",
      "Paula Pattrick",
      "Christopher D. McQuaid",
      "Francesca Porri"
    ],
    "published": "2024-01-18",
    "abstract": "Vertical migrations of zooplankters have been widely described, but their\nactive movements through shallow, highly dynamic water columns within the inner\nshelf may be more complex and difficult to characterize. In this study,\ninvertebrate larvae, currents, and hydrographic variables were sampled at\ndifferent depths during and after the presence of fronts on three different\ncruises off the southern coast of South Africa. Internal wave dynamics were\nobserved in the hydrographic data set but also through satellite imagery,\nalthough strong surface convergent currents were absent and thermal\nstratification was weak. During the first two cruises, fronts were more\nconspicuous and they preceded strong onshore currents at depth which developed\nwith the rising tide. Vertical distributions of larvae changed accordingly,\nwith higher abundances at these deep layers once the front disappeared. The\nthird cruise was carried out during slack tides, the front was not conspicuous,\ndeep strong onshore currents did not occur afterward and larval distributions\ndid not change consistently through time. Overall, the vertical distributions\nof many larval taxa matched the vertical profiles of shoreward currents and\nmultivariate analyses revealed that these flows structured the larval\ncommunity, which was neither influenced by temperature nor chlorophyll. Thus,\nthe ability to regulate active vertical positioning may enhance shoreward\nadvection and determine nearshore larval distributions.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Mapping biodiversity at very-high resolution in Europe",
    "url": "http://arxiv.org/abs/2504.05231v1",
    "authors": [
      "C\u00e9sar Leblanc",
      "Lukas Picek",
      "Benjamin Deneu",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "R\u00e9mi Palard",
      "Alexis Joly"
    ],
    "published": "2025-04-07",
    "abstract": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "LLM",
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Climplicit: Climatic Implicit Embeddings for Global Ecological Tasks",
    "url": "http://arxiv.org/abs/2504.05089v2",
    "authors": [
      "Johannes Dollinger",
      "Damien Robert",
      "Elena Plekhanova",
      "Lukas Drees",
      "Jan Dirk Wegner"
    ],
    "published": "2025-04-07",
    "abstract": "Deep learning on climatic data holds potential for macroecological\napplications. However, its adoption remains limited among scientists outside\nthe deep learning community due to storage, compute, and technical expertise\nbarriers. To address this, we introduce Climplicit, a spatio-temporal\ngeolocation encoder pretrained to generate implicit climatic representations\nanywhere on Earth. By bypassing the need to download raw climatic rasters and\ntrain feature extractors, our model uses x3500 less disk space and\nsignificantly reduces computational needs for downstream tasks. We evaluate our\nClimplicit embeddings on biomes classification, species distribution modeling,\nand plant trait regression. We find that single-layer probing our Climplicit\nembeddings consistently performs better or on par with training a model from\nscratch on downstream tasks and overall better than alternative geolocation\nencoding models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Bayesian Deep Latent Class Regression",
    "url": "http://arxiv.org/abs/2503.17531v2",
    "authors": [
      "Yuren Zhou",
      "Yuqi Gu",
      "David B. Dunson"
    ],
    "published": "2025-03-21",
    "abstract": "High-dimensional categorical data arise in diverse scientific domains and are\noften accompanied by covariates. Latent class regression models are routinely\nused in such settings, reducing dimensionality by assuming conditional\nindependence of the categorical variables given a single latent class that\ndepends on covariates through a logistic regression model. However, such\nmethods become unreliable as the dimensionality increases. To address this, we\npropose a flexible family of deep latent class models. Our model satisfies key\ntheoretical properties, including identifiability and posterior consistency,\nand we establish a Bayes oracle clustering property that ensures robustness\nagainst the curse of dimensionality. We develop efficient posterior computation\nmethods, validate them through simulation studies, and apply our model to joint\nspecies distribution modeling in ecology.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling",
    "url": "http://arxiv.org/abs/2503.13057v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Gencer Sumbul",
      "Chiara Vanalli",
      "Benjamin Kellenberger",
      "Devis Tuia"
    ],
    "published": "2025-03-17",
    "abstract": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Heterogenous graph neural networks for species distribution modeling",
    "url": "http://arxiv.org/abs/2503.11900v2",
    "authors": [
      "Lauren Harrell",
      "Christine Kaeser-Chen",
      "Burcu Karagol Ayan",
      "Keith Anderson",
      "Michelangelo Conserva",
      "Elise Kleeman",
      "Maxim Neumann",
      "Matt Overlan",
      "Melissa Chapman",
      "Drew Purves"
    ],
    "published": "2025-03-14",
    "abstract": "Species distribution models (SDMs) are necessary for measuring and predicting\noccurrences and habitat suitability of species and their relationship with\nenvironmental factors. We introduce a novel presence-only SDM with graph neural\nnetworks (GNN). In our model, species and locations are treated as two distinct\nnode sets, and the learning task is predicting detection records as the edges\nthat connect locations to species. Using GNN for SDM allows us to model\nfine-grained interactions between species and the environment. We evaluate the\npotential of this methodology on the six-region dataset compiled by National\nCenter for Ecological Analysis and Synthesis (NCEAS) for benchmarking SDMs. For\neach of the regions, the heterogeneous GNN model is comparable to or\noutperforms previously-benchmarked single-species SDMs as well as a\nfeed-forward neural network baseline model.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "GNN",
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling",
    "url": "http://arxiv.org/abs/2502.03480v1",
    "authors": [
      "Diana Koldasbayeva",
      "Alexey Zaytsev"
    ],
    "published": "2025-01-27",
    "abstract": "Species Distribution Models (SDMs) often suffer from spatial autocorrelation\n(SAC), leading to biased performance estimates. We tested cross-validation (CV)\nstrategies - random splits, spatial blocking with varied distances,\nenvironmental (ENV) clustering, and a novel spatio-temporal method - under two\nproposed training schemes: LAST FOLD, widely used in spatial CV at the cost of\ndata loss, and RETRAIN, which maximizes data usage but risks reintroducing SAC.\nLAST FOLD consistently yielded lower errors and stronger correlations. Spatial\nblocking at an optimal distance (SP 422) and ENV performed best, achieving\nSpearman and Pearson correlations of 0.485 and 0.548, respectively, although\nENV may be unsuitable for long-term forecasts involving major environmental\nshifts. A spatio-temporal approach yielded modest benefits in our moderately\nvariable dataset, but may excel with stronger temporal changes. These findings\nhighlight the need to align CV approaches with the spatial and temporal\nstructure of SDM data, ensuring rigorous validation and reliable predictive\noutcomes.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Applying the maximum entropy principle to neural networks enhances multi-species distribution models",
    "url": "http://arxiv.org/abs/2412.19217v3",
    "authors": [
      "Maxime Ryckewaert",
      "Diego Marcos",
      "Christophe Botella",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2024-12-26",
    "abstract": "The rapid expansion of citizen science initiatives has led to a significant\ngrowth of biodiversity databases, and particularly presence-only (PO)\nobservations. PO data are invaluable for understanding species distributions\nand their dynamics, but their use in a Species Distribution Model (SDM) is\ncurtailed by sampling biases and the lack of information on absences. Poisson\npoint processes are widely used for SDMs, with Maxent being one of the most\npopular methods. Maxent maximises the entropy of a probability distribution\nacross sites as a function of predefined transformations of variables, called\nfeatures. In contrast, neural networks and deep learning have emerged as a\npromising technique for automatic feature extraction from complex input\nvariables. Arbitrarily complex transformations of input variables can be\nlearned from the data efficiently through backpropagation and stochastic\ngradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses\nneural networks to automatically learn shared features among species, using the\nmaximum entropy principle. To do so, it employs a normalised Poisson loss where\nfor each species, presence probabilities across sites are modelled by a neural\nnetwork. We evaluate DeepMaxent on a benchmark dataset known for its spatial\nsampling biases, using PO data for calibration and presence-absence (PA) data\nfor validation across six regions with different biological groups and\ncovariates. Our results indicate that DeepMaxent performs better than Maxent\nand other leading SDMs across all regions and taxonomic groups. The method\nperforms particularly well in regions of uneven sampling, demonstrating\nsubstantial potential to increase SDM performances. In particular, our approach\nyields more accurate predictions than traditional single-species models, which\nopens up new possibilities for methodological enhancement.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MiTREE: Multi-input Transformer Ecoregion Encoder for Species Distribution Modelling",
    "url": "http://arxiv.org/abs/2412.18995v1",
    "authors": [
      "Theresa Chen",
      "Yao-Yi Chiang"
    ],
    "published": "2024-12-25",
    "abstract": "Climate change poses an extreme threat to biodiversity, making it imperative\nto efficiently model the geographical range of different species. The\navailability of large-scale remote sensing images and environmental data has\nfacilitated the use of machine learning in Species Distribution Models (SDMs),\nwhich aim to predict the presence of a species at any given location.\nTraditional SDMs, reliant on expert observation, are labor-intensive, but\nadvancements in remote sensing and citizen science data have facilitated\nmachine learning approaches to SDM development. However, these models often\nstruggle with leveraging spatial relationships between different inputs -- for\ninstance, learning how climate data should inform the data present in satellite\nimagery -- without upsampling or distorting the original inputs. Additionally,\nlocation information and ecological characteristics at a location play a\ncrucial role in predicting species distribution models, but these aspects have\nnot yet been incorporated into state-of-the-art approaches. In this work, we\nintroduce MiTREE: a multi-input Vision-Transformer-based model with an\necoregion encoder. MiTREE computes spatial cross-modal relationships without\nupsampling as well as integrates location and ecological context. We evaluate\nour model on the SatBird Summer and Winter datasets, the goal of which is to\npredict bird species encounter rates, and we find that our approach improves\nupon state-of-the-art baselines.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Spatial Clustering of Citizen Science Data Improves Downstream Species Distribution Models",
    "url": "http://arxiv.org/abs/2412.15559v3",
    "authors": [
      "Nahian Ahmed",
      "Mark Roth",
      "Tyler A. Hallman",
      "W. Douglas Robinson",
      "Rebecca A. Hutchinson"
    ],
    "published": "2024-12-20",
    "abstract": "Citizen science biodiversity data present great opportunities for ecology and\nconservation across vast spatial and temporal scales. However, the\nopportunistic nature of these data lacks the sampling structure required by\nmodeling methodologies that address a pervasive challenge in ecological data\ncollection: imperfect detection, i.e., the likelihood of under-observing\nspecies on field surveys. Occupancy modeling is an example of an approach that\naccounts for imperfect detection by explicitly modeling the observation process\nseparately from the biological process of habitat selection. This produces\nspecies distribution models that speak to the pattern of the species on a\nlandscape after accounting for imperfect detection in the data, rather than the\npattern of species observations corrupted by errors. To achieve this benefit,\noccupancy models require multiple surveys of a site across which the site's\nstatus (i.e., occupied or not) is assumed constant. Since citizen science data\nare not collected under the required repeated-visit protocol, observations may\nbe grouped into sites post hoc. Existing approaches for constructing sites\ndiscard some observations and/or consider only geographic distance and not\nenvironmental similarity. In this study, we compare ten approaches for site\nconstruction in terms of their impact on downstream species distribution models\nfor 31 bird species in Oregon, using observations recorded in the eBird\ndatabase. We find that occupancy models built on sites constructed by spatial\nclustering algorithms perform better than existing alternatives.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "FathomVerse: A community science dataset for ocean animal discovery",
    "url": "http://arxiv.org/abs/2412.01701v1",
    "authors": [
      "Genevieve Patterson",
      "Joost Daniels",
      "Benjamin Woodward",
      "Kevin Barnard",
      "Giovanna Sainz",
      "Lonny Lundsten",
      "Kakani Katija"
    ],
    "published": "2024-12-02",
    "abstract": "Can computer vision help us explore the ocean? The ultimate challenge for\ncomputer vision is to recognize any visual phenomena, more than only the\nobjects and animals humans encounter in their terrestrial lives. Previous\ndatasets have explored everyday objects and fine-grained categories humans see\nfrequently. We present the FathomVerse v0 detection dataset to push the limits\nof our field by exploring animals that rarely come in contact with people in\nthe deep sea. These animals present a novel vision challenge.\n  The FathomVerse v0 dataset consists of 3843 images with 8092 bounding boxes\nfrom 12 distinct morphological groups recorded at two locations on the deep\nseafloor that are new to computer vision. It features visually perplexing\nscenarios such as an octopus intertwined with a sea star, and confounding\ncategories like vampire squids and sea spiders. This dataset can push forward\nresearch on topics like fine-grained transfer learning, novel category\ndiscovery, species distribution modeling, and carbon cycle analysis, all of\nwhich are important to the care and husbandry of our planet.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Joint Spatiotemporal Modeling of Zooplankton and Whale Abundance in a Dynamic Marine Environment",
    "url": "http://arxiv.org/abs/2411.06001v1",
    "authors": [
      "Bokgyeong Kang",
      "Erin M. Schliep",
      "Alan E. Gelfand",
      "Christopher W. Clark",
      "Christine A. Hudak",
      "Charles A. Mayo",
      "Ryan Schosberg",
      "Tina M. Yack",
      "Robert S. Schick"
    ],
    "published": "2024-11-08",
    "abstract": "North Atlantic right whales are an endangered species; their entire\npopulation numbers approximately 372 individuals, and they are subject to major\nanthropogenic threats. They feed on zooplankton species whose distribution\nshifts in a dynamic and warming oceanic environment. Because right whales in\nturn follow their shifting food resource, it is necessary to jointly study the\ndistribution of whales and their prey. The innovative joint species\ndistribution modeling (JSDM) contribution here is different from anything in\nthe large JDSM literature, reflecting the processes and data we have to work\nwith. Specifically, our JSDM supplies a geostatistical model for expected\namount of zooplankton collected at a site. We require a point pattern model for\nthe intensity of right whale abundance. The two process models are joined\nthrough a latent conditional-marginal specification. Further, each species has\ntwo data sources to inform their respective distributions and these sources\nrequire novel data fusion. What emerges is a complex multi-level model. Through\nsimulation we demonstrate the ability of our joint specification to identify\nmodel unknowns and learn better about the species distributions than modeling\nthem individually. We then apply our modeling to real data from Cape Cod Bay,\nMassachusetts in the U.S.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-Scale and Multimodal Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2411.04016v1",
    "authors": [
      "Nina van Tiel",
      "Robin Zbinden",
      "Emanuele Dalsasso",
      "Benjamin Kellenberger",
      "Lo\u00efc Pellissier",
      "Devis Tuia"
    ],
    "published": "2024-11-06",
    "abstract": "Species distribution models (SDMs) aim to predict the distribution of species\nby relating occurrence data with environmental variables. Recent applications\nof deep learning to SDMs have enabled new avenues, specifically the inclusion\nof spatial data (environmental rasters, satellite images) as model predictors,\nallowing the model to consider the spatial context around each species'\nobservations. However, the appropriate spatial extent of the images is not\nstraightforward to determine and may affect the performance of the model, as\nscale is recognized as an important factor in SDMs. We develop a modular\nstructure for SDMs that allows us to test the effect of scale in both single-\nand multi-scale settings. Furthermore, our model enables different scales to be\nconsidered for different modalities, using a late fusion approach. Results on\nthe GeoLifeCLEF 2023 benchmark indicate that considering multimodal data and\nlearning multi-scale representations leads to more accurate models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Hybrid Spatial Representations for Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2410.10937v2",
    "authors": [
      "Shiran Yuan",
      "Hao Zhao"
    ],
    "published": "2024-10-14",
    "abstract": "We address an important problem in ecology called Species Distribution\nModeling (SDM), whose goal is to predict whether a species exists at a certain\nposition on Earth. In particular, we tackle a challenging version of this task,\nwhere we learn from presence-only data in a community-sourced dataset, model a\nlarge number of species simultaneously, and do not use any additional\nenvironmental information. Previous work has used neural implicit\nrepresentations to construct models that achieve promising results. However,\nimplicit representations often generate predictions of limited spatial\nprecision. We attribute this limitation to their inherently global formulation\nand inability to effectively capture local feature variations. This issue is\nespecially pronounced with presence-only data and a large number of species. To\naddress this, we propose a hybrid embedding scheme that combines both implicit\nand explicit embeddings. Specifically, the explicit embedding is implemented\nwith a multiresolution hashgrid, enabling our models to better capture local\ninformation. Experiments demonstrate that our results exceed other works by a\nlarge margin on various standard benchmarks, and that the hybrid representation\nis better than both purely implicit and explicit ones. Qualitative\nvisualizations and comprehensive ablation studies reveal that our hybrid\nrepresentation successfully addresses the two main challenges. Our code is\nopen-sourced at https://github.com/Shiran-Yuan/HSR-SDM.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MALPOLON: A Framework for Deep Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2409.18102v1",
    "authors": [
      "Theo Larcher",
      "Lukas Picek",
      "Benjamin Deneu",
      "Titouan Lorieul",
      "Maximilien Servajean",
      "Alexis Joly"
    ],
    "published": "2024-09-26",
    "abstract": "This paper describes a deep-SDM framework, MALPOLON. Written in Python and\nbuilt upon the PyTorch library, this framework aims to facilitate training and\ninferences of deep species distribution models (deep-SDM) and sharing for users\nwith only general Python language skills (e.g., modeling ecologists) who are\ninterested in testing deep learning approaches to build new SDMs. More advanced\nusers can also benefit from the framework's modularity to run more specific\nexperiments by overriding existing classes while taking advantage of\npress-button examples to train neural networks on multiple classification tasks\nusing custom or provided raw and pre-processed datasets. The framework is\nopen-sourced on GitHub and PyPi along with extensive documentation and examples\nof use in various scenarios. MALPOLON offers straightforward installation,\nYAML-based configuration, parallel computing, multi-GPU utilization, baseline\nand foundational models for benchmarking, and extensive\ntutorials/documentation, aiming to enhance accessibility and performance\nscalability for ecologists and researchers.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Integrating systematic surveys with historical data to model the distribution of Ornithodoros turicata americanus, a vector of epidemiological concern in North America",
    "url": "http://arxiv.org/abs/2409.12761v1",
    "authors": [
      "Sebastian Botero-Canola",
      "Carson Torhorst",
      "Nicholas Canino",
      "Lorenza Beati",
      "Kathleen C. O Hara",
      "Angela M. James",
      "Samantha M. Wisely"
    ],
    "published": "2024-09-19",
    "abstract": "Globally, vector-borne diseases are increasing in distribution and frequency,\naffecting humans, domestic animals and livestock, and wildlife. Science-based\nmanagement and prevention of these diseases requires a sound understanding of\nthe distribution and environmental requirements of the vectors and hosts\ninvolved in disease transmission. Integrated Species Distribution Models (ISDM)\naccount for diverse data types through hierarchical modeling and represent a\nsignificant advancement in species distribution modeling that have not yet been\nleveraged in disease ecology. We used this approach, as implemented in the\nrecently developed R package RISDM, to assess the distribution of the soft tick\nsubspecies Ornithodoros turicata americanus. We created an ISDM for O. t.\namericanus, using systematically collected field data and historical records of\nthis tick species in the southeastern US, to predict its distribution and\nassess potential correlations with environmental variables. Given the novelty\nof this method, we compared the results to a conventional Maxent SDM and\nvalidated the results through data partitioning using true skills statistics\n(TSS), sensitivity, and area under the ROC curve (AUC) metrics. We found that a\ncombination of climatic variables describing seasonality and temperature\nextremes, along with the amount of sand in the soil, determined the predicted\nintensity of occurrence of this tick species. When projected in geographic\nspace, this distribution model predicted 62% of Florida as suitable habitat for\nthis tick species. The ISDM presented a higher TSS and AUC than the Maxent\nconventional model, while sensitivity was similar between both models. Our case\nexample shows the utility of ISDMs in disease ecology studies and highlights\nthe broad range of geographic suitability for this important disease vector.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Trophic Cascades and Habitat Suitability in Udanti Sitnadi Tiger Reserve: Impacts of Prey Depletion and Climate Change on Predator Prey Dynamics",
    "url": "http://arxiv.org/abs/2409.00193v1",
    "authors": [
      "Krishnendu Basak",
      "Chiranjib Chaudhuri",
      "M Suraj",
      "Moiz Ahmed"
    ],
    "published": "2024-08-30",
    "abstract": "This study investigates the trophic cascades and habitat suitability in\nUdanti Sitnadi Tiger Reserve (USTR), highlighting the roles of apex predators,\nsubordinate predators, and prey species in maintaining ecosystem balance. Using\nthe Trophic Species Distribution Model (SDM), we explored prey-predator\ninteractions and habitat suitability, revealing that tigers, due to prey\ndepletion, increasingly rely on cattle, while leopards adapt by preying on\nsmaller species. The study emphasizes the need for prey augmentation and\nhabitat restoration to support apex predators. Additionally, climate change\nprojections for 2021-2040 and 2081-2100 under CMIP6 scenarios SSP245 and SSP585\nindicate significant regional habitat shifts, necessitating adaptive management\nstrategies. Kuladighat is projected to face habitat contraction, while Sitanadi\nmay experience habitat expansion. Effective conservation efforts such as\nhabitat restoration, prey augmentation and predator recovery are the most\nimportant steps needed to maintain the purpose of a Tiger reserve and\nconservation potential of Udanti-Sonabeda Tiger Conservation Unit (TCU). To\nachieve these dynamics, focusing on community participation, anti-poaching\nmeasures, and scientific recommendations are the most crucial components to\nfocus on. This comprehensive analysis underscores the critical role of targeted\nconservation activities in prey-depleted landscapes to ensure the long-term\nsurvival of tigers and the overall health of forest ecosystems, enhancing\nbiodiversity and mitigating human-wildlife conflicts in USTR.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Dogs on forest trails; Understanding ecology of Striped Hyena and wild Canids in the presence of free-ranging dogs in Udanti-Sitanadi Tiger Reserve, Central India using Joint Distribution and Deep Neural Networks",
    "url": "http://arxiv.org/abs/2409.00185v1",
    "authors": [
      "Chiranjib Chaudhuri",
      "Krishnendu Basak",
      "M Suraj",
      "Moiz Ahmed",
      "Amit Kumar"
    ],
    "published": "2024-08-30",
    "abstract": "This study uses Joint Species Distribution Models (JSDMs) and Deep Neural\nNetworks (DNNs) to explore how wild carnivores and free-ranging dogs interact\nin the Udanti-Sitanadi Tiger Reserve (USTR) in Central India. The research\nfocuses on key species like the Striped Hyena, Grey Wolf, Golden Jackal, and\nIndian Fox, revealing significant overlaps in habitat with free-ranging dogs,\nespecially in densely populated areas like the Sitanadi region of the tiger\nreserve. These overlaps pose serious risks to wildlife through competition for\nresources, predation, and the spread of diseases. The study shows that the\nStriped Hyena prefers gentle slopes and forested areas, while the Grey Wolf\ntends to avoid cropland and thrives in regions with higher rainfall that\nsupports a stable prey base. The Golden Jackal, more adaptable than the others,\nfavors west-facing slopes and stable temperatures, whereas the Indian Fox is\nmainly found in the less disturbed, mountainous Kuladighat region.\nAdditionally, the study highlights the potential impacts of climate change,\npredicting that the Grey Wolf could face habitat extinction under more severe\nscenarios. These findings underscore the urgent need for conservation\nstrategies tailored to address both dog wild carnivore interactions and the\ngrowing challenges posed by climate change, focusing on protecting the critical\nhabitats of vulnerable species like the Striped Hyena and Grey Wolf.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Generating Binary Species Range Maps",
    "url": "http://arxiv.org/abs/2408.15956v1",
    "authors": [
      "Filip Dorm",
      "Christian Lange",
      "Scott Loarie",
      "Oisin Mac Aodha"
    ],
    "published": "2024-08-28",
    "abstract": "Accurately predicting the geographic ranges of species is crucial for\nassisting conservation efforts. Traditionally, range maps were manually created\nby experts. However, species distribution models (SDMs) and, more recently,\ndeep learning-based variants offer a potential automated alternative. Deep\nlearning-based SDMs generate a continuous probability representing the\npredicted presence of a species at a given location, which must be binarized by\nsetting per-species thresholds to obtain binary range maps. However, selecting\nappropriate per-species thresholds to binarize these predictions is non-trivial\nas different species can require distinct thresholds. In this work, we evaluate\ndifferent approaches for automatically identifying the best thresholds for\nbinarizing range maps using presence-only data. This includes approaches that\nrequire the generation of additional pseudo-absence data, along with ones that\nonly require presence data. We also propose an extension of an existing\npresence-only technique that is more robust to outliers. We perform a detailed\nevaluation of different thresholding techniques on the tasks of binary range\nestimation and large-scale fine-grained visual classification, and we\ndemonstrate improved performance over existing pseudo-absence free approaches\nusing our method.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "GeoPlant: Spatial Plant Species Prediction Dataset",
    "url": "http://arxiv.org/abs/2408.13928v2",
    "authors": [
      "Lukas Picek",
      "Christophe Botella",
      "Maximilien Servajean",
      "C\u00e9sar Leblanc",
      "R\u00e9mi Palard",
      "Th\u00e9o Larcher",
      "Benjamin Deneu",
      "Diego Marcos",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2024-08-25",
    "abstract": "The difficulty of monitoring biodiversity at fine scales and over large areas\nlimits ecological knowledge and conservation efforts. To fill this gap, Species\nDistribution Models (SDMs) predict species across space from spatially explicit\nfeatures. Yet, they face the challenge of integrating the rich but\nheterogeneous data made available over the past decade, notably millions of\nopportunistic species observations and standardized surveys, as well as\nmultimodal remote sensing data. In light of that, we have designed and\ndeveloped a new European-scale dataset for SDMs at high spatial resolution\n(10--50m), including more than 10k species (i.e., most of the European flora).\nThe dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive\nPresence-Absence survey records, all accompanied by diverse environmental\nrasters (e.g., elevation, human footprint, and soil) traditionally used in\nSDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10\nm resolution, a 20-year time series of climatic variables, and satellite time\nseries from the Landsat program. In addition to the data, we provide an openly\naccessible SDM benchmark (hosted on Kaggle), which has already attracted an\nactive community and a set of strong baselines for single predictor/modality\nand multimodal approaches. All resources, e.g., the dataset, pre-trained\nmodels, and baseline methods (in the form of notebooks), are available on\nKaggle, allowing one to start with our dataset literally with two mouse clicks.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Fast fitting of phylogenetic mixed-effects models",
    "url": "http://arxiv.org/abs/2408.05333v2",
    "authors": [
      "Bert van der Veen",
      "Robert Brian O'Hara"
    ],
    "published": "2024-08-09",
    "abstract": "Mixed-effects models are among the most commonly used statistical methods for\nthe exploration of multispecies data. In recent years, also Joint Species\nDistribution Models and Generalized Linear Latent Variale Models have gained in\npopularity when the goal is to incorporate residual covariation between species\nthat cannot be explained due to measured environmental covariates. Few software\nimplementations of such models exist that can additionally incorporate\nphylogenetic information, and those that exist tend to utilize Markov chain\nMonte Carlo methods for estimation, so that model fitting takes a long time. In\nthis article we develop new methods for quickly and flexibly fitting\nphylogenetic mixed-effects models, potentially incorporating residual\ncovariation between species using latent variables, with the possibility to\nestimate the strength of phylogenetic structuring in species responses per\nenvironmental covariate, and while incorporating correlation between different\ncovariate effects. By combining Variational approximations, a sparse\napproximation to the phylogenetic precision matrix, and parallel computation,\nphylogenetic mixed-effects models can be fitted much more quickly than the\ncurrent state-of-the-art. Two simulation studies demonstrate that the proposed\ncombination of approximations is fast and enjoys high accuracy. We explore\nsensitivity of the approximation to the ordering of species with a real world\ndataset of wood-decaying fungi.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Implication of modelling choices on connectivity estimation: A comparative analysis",
    "url": "http://arxiv.org/abs/2407.09564v1",
    "authors": [
      "Marie Soret",
      "Sylvain Moulherat",
      "Maxime Lenormand",
      "Sandra Luque"
    ],
    "published": "2024-07-05",
    "abstract": "We focus on connectivity methods used to understand and predict how\nlandscapes and habitats facilitate or impede the movement and dispersal of\nspecies. Our objective is to compare the implication of methodological choices\nat three stages of the modelling framework: landscape characterisation,\nconnectivity estimation, and connectivity assessment. What are the convergences\nand divergences of different modelling approaches? What are the implications of\ntheir combined results for landscape planning? We implemented two landscape\ncharacterisation approaches: expert opinion and species distribution model\n(SDM); four connectivity estimation models: Euclidean distance, least-cost\npaths (LCP), circuit theory, and stochastic movement simulation (SMS); and two\nconnectivity indices: flux and area-weighted flux (dPCflux). We compared\noutcomes such as movement maps and habitat prioritisation for a rural landscape\nin southwestern France. Landscape characterisation is the main factor\ninfluencing connectivity assessment. The movement maps reflect the models'\nassumptions: LCP produced narrow beams reflecting the optimal pathways; whereas\ncircuit theory and SMS produced wider estimation reflecting movement\nstochasticity, with SMS integrating behavioural drivers. The indices\nhighlighted different aspects: dPCflux the surface of suitable habitats and\nflux their proximity. We recommend focusing on landscape characterisation\nbefore engaging further in the modelling framework. We emphasise the importance\nof stochasticity and behavioural drivers in connectivity, which can be\nreflected using circuit theory, SMS or other stochastic individual-based\nmodels. We stress the importance of using multiple indices to capture the\nmulti-factorial aspect of connectivity.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning",
    "url": "http://arxiv.org/abs/2406.15658v3",
    "authors": [
      "Nemin Wu",
      "Qian Cao",
      "Zhangyu Wang",
      "Zeping Liu",
      "Yanlin Qi",
      "Jielu Zhang",
      "Joshua Ni",
      "Xiaobai Yao",
      "Hongxu Ma",
      "Lan Mu",
      "Stefano Ermon",
      "Tanuja Ganu",
      "Akshay Nambi",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published": "2024-06-21",
    "abstract": "Spatial representation learning (SRL) aims at learning general-purpose neural\nnetwork representations from various types of spatial data (e.g., points,\npolylines, polygons, networks, images, etc.) in their native formats. Learning\ngood spatial representations is a fundamental problem for various downstream\napplications such as species distribution modeling, weather forecasting,\ntrajectory generation, geographic question answering, etc. Even though SRL has\nbecome the foundation of almost all geospatial artificial intelligence (GeoAI)\nresearch, we have not yet seen significant efforts to develop an extensive deep\nlearning framework and benchmark to support SRL model development and\nevaluation. To fill this gap, we propose TorchSpatial, a learning framework and\nbenchmark for location (point) encoding, which is one of the most fundamental\ndata types of spatial representation learning. TorchSpatial contains three key\ncomponents: 1) a unified location encoding framework that consolidates 15\ncommonly recognized location encoders, ensuring scalability and reproducibility\nof the implementations; 2) the LocBench benchmark tasks encompassing 7\ngeo-aware image classification and 10 geo-aware image regression datasets; 3) a\ncomprehensive suite of evaluation metrics to quantify geo-aware model's overall\nperformance as well as their geographic bias, with a novel Geo-Bias Score\nmetric. Finally, we provide a detailed analysis and insights into the model\nperformance and geographic bias of different location encoders. We believe\nTorchSpatial will foster future advancement of spatial representation learning\nand spatial fairness in GeoAI research. The TorchSpatial model framework and\nLocBench benchmark are available at https://github.com/seai-lab/TorchSpatial,\nand the Geo-Bias Score evaluation framework is available at\nhttps://github.com/seai-lab/PyGBS.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Cumulant-based approximation for fast and efficient prediction for species distribution",
    "url": "http://arxiv.org/abs/2405.14456v1",
    "authors": [
      "Osamu Komori",
      "Yusuke Saigusa",
      "Shinto Eguchi",
      "Yasuhiro Kubota"
    ],
    "published": "2024-05-23",
    "abstract": "Species distribution modeling plays an important role in estimating the\nhabitat suitability of species using environmental variables. For this purpose,\nMaxent and the Poisson point process are popular and powerful methods\nextensively employed across various ecological and biological sciences.\nHowever, the computational speed becomes prohibitively slow when using huge\nbackground datasets, which is often the case with fine-resolution data or\nglobal-scale estimations. To address this problem, we propose a computationally\nefficient species distribution model using a cumulant-based approximation (CBA)\napplied to the loss function of $\\gamma$-divergence. Additionally, we introduce\na sequential estimating algorithm with an $L_1$ penalty to select important\nenvironmental variables closely associated with species distribution. The\nregularized geometric-mean method, derived from the CBA, demonstrates high\ncomputational efficiency and estimation accuracy. Moreover, by applying CBA to\nMaxent, we establish that Maxent and Fisher linear discriminant analysis are\nequivalent under a normality assumption. This equivalence leads to an highly\nefficient computational method for estimating species distribution. The\neffectiveness of our proposed methods is illustrated through simulation studies\nand by analyzing data on 226 species from the National Centre for Ecological\nAnalysis and Synthesis and 709 Japanese vascular plant species. The\ncomputational efficiency of the proposed methods is significantly improved\ncompared to Maxent, while maintaining comparable estimation accuracy. A R\npackage {\\tt CBA} is also prepared to provide all programming codes used in\nsimulation studies and real data analysis.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "A Comparison of Joint Species Distribution Models for Percent Cover Data",
    "url": "http://arxiv.org/abs/2403.11562v1",
    "authors": [
      "Pekka Korhonen",
      "Francis K. C. Hui",
      "Jenni Niku",
      "Sara Taskinen",
      "Bert van der Veen"
    ],
    "published": "2024-03-18",
    "abstract": "1. Joint species distribution models (JSDMs) have gained considerable\ntraction among ecologists over the past decade, due to their capacity to answer\na wide range of questions at both the species- and the community-level. The\nfamily of generalized linear latent variable models in particular has proven\npopular for building JSDMs, being able to handle many response types including\npresence-absence data, biomass, overdispersed and/or zero-inflated counts.\n  2. We extend latent variable models to handle percent cover data, with\nvegetation, sessile invertebrate, and macroalgal cover data representing the\nprime examples of such data arising in community ecology.\n  3. Sparsity is a commonly encountered challenge with percent cover data.\nResponses are typically recorded as percentages covered per plot, though some\nspecies may be completely absent or present, i.e., have 0% or 100% cover\nrespectively, rendering the use of beta distribution inadequate.\n  4. We propose two JSDMs suitable for percent cover data, namely a hurdle beta\nmodel and an ordered beta model. We compare the two proposed approaches to a\nbeta distribution for shifted responses, transformed presence-absence data, and\nan ordinal model for percent cover classes. Results demonstrate the hurdle beta\nJSDM was generally the most accurate at retrieving the latent variables and\npredicting ecological percent cover data.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Imbalance-aware Presence-only Loss Function for Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2403.07472v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Marc Ru\u00dfwurm",
      "Devis Tuia"
    ],
    "published": "2024-03-12",
    "abstract": "In the face of significant biodiversity decline, species distribution models\n(SDMs) are essential for understanding the impact of climate change on species\nhabitats by connecting environmental conditions to species occurrences.\nTraditionally limited by a scarcity of species observations, these models have\nsignificantly improved in performance through the integration of larger\ndatasets provided by citizen science initiatives. However, they still suffer\nfrom the strong class imbalance between species within these datasets, often\nresulting in the penalization of rare species--those most critical for\nconservation efforts. To tackle this issue, this study assesses the\neffectiveness of training deep learning models using a balanced presence-only\nloss function on large citizen science-based datasets. We demonstrate that this\nimbalance-aware loss function outperforms traditional loss functions across\nvarious datasets and tasks, particularly in accurately modeling rare species\nwith limited observations.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Infinite joint species distribution models",
    "url": "http://arxiv.org/abs/2402.13384v2",
    "authors": [
      "Federica Stolf",
      "David B. Dunson"
    ],
    "published": "2024-02-20",
    "abstract": "Joint species distribution models are popular in ecology for modeling\ncovariate effects on species occurrence, while characterizing cross-species\ndependence. Data consist of multivariate binary indicators of the occurrences\nof different species in each sample, along with sample-specific covariates. A\nkey problem is that current models implicitly assume that the list of species\nunder consideration is predefined and finite, while for highly diverse groups\nof organisms, it is impossible to anticipate which species will be observed in\na study and discovery of unknown species is common. This article proposes a new\nmodeling paradigm for statistical ecology, which generalizes traditional\nmultivariate probit models to accommodate large numbers of rare species and new\nspecies discovery. We discuss theoretical properties of the proposed modeling\nparadigm and implement efficient algorithms for posterior computation.\nSimulation studies and applications to fungal biodiversity data provide\ncompelling support for the new modeling class.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Novel community data in ecology -- properties and prospects",
    "url": "http://arxiv.org/abs/2401.10860v1",
    "authors": [
      "Florian Hartig",
      "Nerea Abrego",
      "Alex Bush",
      "Jonathan M. Chase",
      "Gurutzeta Guillera-Arroita",
      "Mathew A. Leibold",
      "Otso Ovaskainen",
      "Lo\u00efc Pellissier",
      "Maximilian Pichler",
      "Giovanni Poggiato",
      "Laura Pollock",
      "Sara Si-Moussi",
      "Wilfried Thuiller",
      "Duarte S. Viana",
      "David I. Warton",
      "Damaris Zurell",
      "Douglas W. Yu"
    ],
    "published": "2024-01-19",
    "abstract": "New technologies for acquiring biological information such as eDNA, acoustic\nor optical sensors, make it possible to generate spatial community observations\nat unprecedented scales. The potential of these novel community data to\nstandardize community observations at high spatial, temporal, and taxonomic\nresolution and at large spatial scale ('many rows and many columns') has been\nwidely discussed, but so far, there has been little integration of these data\nwith ecological models and theory. Here, we review these developments and\nhighlight emerging solutions, focusing on statistical methods for analyzing\nnovel community data, in particular joint species distribution models; the new\necological questions that can be answered with these data; and the potential\nimplications of these developments for policy and conservation.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "A sequential Monte Carlo algorithm for data assimilation problems in ecology",
    "url": "http://arxiv.org/abs/2401.06515v1",
    "authors": [
      "Kwaku Peprah Adjei",
      "Rob Cooke",
      "Nick Isaac",
      "Robert B. O'Hara"
    ],
    "published": "2024-01-12",
    "abstract": "1. Temporal trends in species distributions are necessary for monitoring\nchanges in biodiversity, which aids policymakers and conservationists in making\ninformed decisions. Dynamic species distribution models are often fitted to\necological time series data using Markov Chain Monte Carlo algorithms to\nproduce these temporal trends. However, the fitted models can be time-consuming\nto produce and run, making it inefficient to refit them as new observations\nbecome available.\n  2. We propose an algorithm that updates model parameters and the latent state\ndistribution (e.g. true occupancy) using the saved information from a\npreviously fitted model. This algorithm capitalises on the strength of\nimportance sampling to generate new posterior samples of interest by updating\nthe model output. The algorithm was validated with simulation studies on linear\nGaussian state space models and occupancy models, and we applied the framework\nto Crested Tits in Switzerland and Yellow Meadow Ants in the UK.\n  3. We found that models updated with the proposed algorithm captured the true\nmodel parameters and latent state values as good as the models refitted to the\nexpanded dataset. Moreover, the updated models were much faster to run and\npreserved the trajectory of the derived quantities.\n  4. The proposed approach serves as an alternative to conventional methods for\nupdating state-space models (SSMs), and it is most beneficial when the fitted\nSSMs have a long run time. Overall, we provide a Monte Carlo algorithm to\nefficiently update complex models, a key issue in developing biodiversity\nmodels and indicators.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts",
    "url": "http://arxiv.org/abs/2401.05470v1",
    "authors": [
      "Joaquim Estopinan",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "Fran\u00e7ois Munoz",
      "Alexis Joly"
    ],
    "published": "2024-01-10",
    "abstract": "The post-2020 global biodiversity framework needs ambitious, research-based\ntargets. Estimating the accelerated extinction risk due to climate change is\ncritical. The International Union for Conservation of Nature (IUCN) measures\nthe extinction risk of species. Automatic methods have been developed to\nprovide information on the IUCN status of under-assessed taxa. However, these\ncompensatory methods are based on current species characteristics, mainly\ngeographical, which precludes their use in future projections. Here, we\nevaluate a novel method for classifying the IUCN status of species benefiting\nfrom the generalisation power of species distribution models based on deep\nlearning. Our method matches state-of-the-art classification performance while\nrelying on flexible SDM-based features that capture species' environmental\npreferences. Cross-validation yields average accuracies of 0.61 for status\nclassification and 0.78 for binary classification. Climate change will reshape\nfuture species distributions. Under the species-environment equilibrium\nhypothesis, SDM projections approximate plausible future outcomes. Two extremes\nof species dispersal capacity are considered: unlimited or null. The projected\nspecies distributions are translated into features feeding our IUCN\nclassification method. Finally, trends in threatened species are analysed over\ntime and i) by continent and as a function of average ii) latitude or iii)\naltitude. The proportion of threatened species is increasing globally, with\ncritical rates in Africa, Asia and South America. Furthermore, the proportion\nof threatened species is predicted to peak around the two Tropics, at the\nEquator, in the lowlands and at altitudes of 800-1,500 m.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale",
    "url": "http://arxiv.org/abs/2401.04691v1",
    "authors": [
      "Joaquim Estopinan",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly",
      "Fran\u00e7ois Munoz"
    ],
    "published": "2024-01-09",
    "abstract": "Although increasing threats on biodiversity are now widely recognised, there\nare no accurate global maps showing whether and where species assemblages are\nat risk. We hereby assess and map at kilometre resolution the conservation\nstatus of the iconic orchid family, and discuss the insights conveyed at\nmultiple scales. We introduce a new Deep Species Distribution Model trained on\n1M occurrences of 14K orchid species to predict their assemblages at global\nscale and at kilometre resolution. We propose two main indicators of the\nconservation status of the assemblages: (i) the proportion of threatened\nspecies, and (ii) the status of the most threatened species in the assemblage.\nWe show and analyze the variation of these indicators at World scale and in\nrelation to currently protected areas in Sumatra island. Global and interactive\nmaps available online show the indicators of conservation status of orchid\nassemblages, with sharp spatial variations at all scales. The highest level of\nthreat is found at Madagascar and the neighbouring islands. In Sumatra, we\nfound good correspondence of protected areas with our indicators, but\nsupplementing current IUCN assessments with status predictions results in\nalarming levels of species threat across the island. Recent advances in deep\nlearning enable reliable mapping of the conservation status of species\nassemblages on a global scale. As an umbrella taxon, orchid family provides a\nreference for identifying vulnerable ecosystems worldwide, and prioritising\nconservation actions both at international and local levels.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning",
    "url": "http://arxiv.org/abs/2401.02989v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Benjamin Kellenberger",
      "Lloyd Hughes",
      "Devis Tuia"
    ],
    "published": "2024-01-03",
    "abstract": "Species distribution modeling is a highly versatile tool for understanding\nthe intricate relationship between environmental conditions and species\noccurrences. However, the available data often lacks information on confirmed\nspecies absence and is limited to opportunistically sampled, presence-only\nobservations. To overcome this limitation, a common approach is to employ\npseudo-absences, which are specific geographic locations designated as negative\nsamples. While pseudo-absences are well-established for single-species\ndistribution models, their application in the context of multi-species neural\nnetworks remains underexplored. Notably, the significant class imbalance\nbetween species presences and pseudo-absences is often left unaddressed.\nMoreover, the existence of different types of pseudo-absences (e.g., random and\ntarget-group background points) adds complexity to the selection process.\nDetermining the optimal combination of pseudo-absences types is difficult and\ndepends on the characteristics of the data, particularly considering that\ncertain types of pseudo-absences can be used to mitigate geographic biases. In\nthis paper, we demonstrate that these challenges can be effectively tackled by\nintegrating pseudo-absences in the training of multi-species neural networks\nthrough modifications to the loss function. This adjustment involves assigning\ndifferent weights to the distinct terms of the loss function, thereby\naddressing both the class imbalance and the choice of pseudo-absence types.\nAdditionally, we propose a strategy to set these loss weights using spatial\nblock cross-validation with presence-only data. We evaluate our approach using\na benchmark dataset containing independent presence-absence data from six\ndifferent regions and report improved results when compared to competing\napproaches.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook",
    "url": "http://arxiv.org/abs/2505.00630v1",
    "authors": [
      "Muyi Bao",
      "Shuchang Lyu",
      "Zhaoyang Xu",
      "Huiyu Zhou",
      "Jinchang Ren",
      "Shiming Xiang",
      "Xiangtai Li",
      "Guangliang Cheng"
    ],
    "published": "2025-05-01",
    "abstract": "Deep learning has profoundly transformed remote sensing, yet prevailing\narchitectures like Convolutional Neural Networks (CNNs) and Vision Transformers\n(ViTs) remain constrained by critical trade-offs: CNNs suffer from limited\nreceptive fields, while ViTs grapple with quadratic computational complexity,\nhindering their scalability for high-resolution remote sensing data. State\nSpace Models (SSMs), particularly the recently proposed Mamba architecture,\nhave emerged as a paradigm-shifting solution, combining linear computational\nscaling with global context modeling. This survey presents a comprehensive\nreview of Mamba-based methodologies in remote sensing, systematically analyzing\nabout 120 studies to construct a holistic taxonomy of innovations and\napplications. Our contributions are structured across five dimensions: (i)\nfoundational principles of vision Mamba architectures, (ii) micro-architectural\nadvancements such as adaptive scan strategies and hybrid SSM formulations,\n(iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids\nand frequency-domain adaptations, (iv) rigorous benchmarking against\nstate-of-the-art methods in multiple application tasks, such as object\ndetection, semantic segmentation, change detection, etc. and (v) critical\nanalysis of unresolved challenges with actionable future directions. By\nbridging the gap between SSM theory and remote sensing practice, this survey\nestablishes Mamba as a transformative framework for remote sensing analysis. To\nour knowledge, this paper is the first systematic review of Mamba architectures\nin remote sensing. Our work provides a structured foundation for advancing\nresearch in remote sensing systems through SSM-based methods. We curate an\nopen-source repository\n(https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster\ncommunity-driven advancements.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Field-scale soil moisture estimated from Sentinel-1 SAR data using a knowledge-guided deep learning approach",
    "url": "http://arxiv.org/abs/2505.00265v1",
    "authors": [
      "Yi Yu",
      "Patrick Filippi",
      "Thomas F. A. Bishop"
    ],
    "published": "2025-05-01",
    "abstract": "Soil moisture (SM) estimation from active microwave data remains challenging\ndue to the complex interactions between radar backscatter and surface\ncharacteristics. While the water cloud model (WCM) provides a semi-physical\napproach for understanding these interactions, its empirical component often\nlimits performance across diverse agricultural landscapes. This research\npresents preliminary efforts for developing a knowledge-guided deep learning\napproach, which integrates WCM principles into a long short-term memory (LSTM)\nmodel, to estimate field SM using Sentinel-1 Synthetic Aperture Radar (SAR)\ndata. Our proposed approach leverages LSTM's capacity to capture spatiotemporal\ndependencies while maintaining physical consistency through a modified\ndual-component loss function, including a WCM-based semi-physical component and\na boundary condition regularisation. The proposed approach is built upon the\nsoil backscatter coefficients isolated from the total backscatter, together\nwith Landsat-resolution vegetation information and surface characteristics. A\nfour-fold spatial cross-validation was performed against in-situ SM data to\nassess the model performance. Results showed the proposed approach reduced SM\nretrieval uncertainties by 0.02 m$^3$/m$^3$ and achieved correlation\ncoefficients (R) of up to 0.64 in areas with varying vegetation cover and\nsurface conditions, demonstrating the potential to address the\nover-simplification in WCM.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2504.21491v1",
    "authors": [
      "Qinfeng Zhu",
      "Yunxi Jiang",
      "Lei Fan"
    ],
    "published": "2025-04-30",
    "abstract": "We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping",
    "url": "http://arxiv.org/abs/2504.21194v1",
    "authors": [
      "Vedika Srivastava",
      "Hemant Kumar Singh",
      "Jaisal Singh"
    ],
    "published": "2025-04-29",
    "abstract": "This paper presents a novel approach to geolocating images captured from the\nInternational Space Station (ISS) using advanced machine learning algorithms.\nDespite having precise ISS coordinates, the specific Earth locations depicted\nin astronaut-taken photographs often remain unidentified. Our research\naddresses this gap by employing three distinct image processing pipelines: a\nNeural Network based approach, a SIFT based method, and GPT-4 model. Each\npipeline is tailored to process high-resolution ISS imagery, identifying both\nnatural and man-made geographical features. Through extensive evaluation on a\ndiverse dataset of over 140 ISS images, our methods demonstrate significant\npromise in automated geolocation with varied levels of success. The NN approach\nshowed a high success rate in accurately matching geographical features, while\nthe SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided\nenriched geographical descriptions alongside location predictions. This\nresearch contributes to the fields of remote sensing and Earth observation by\nenhancing the accuracy and efficiency of geolocating space-based imagery,\nthereby aiding environmental monitoring and global mapping efforts.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery",
    "url": "http://arxiv.org/abs/2504.19996v1",
    "authors": [
      "Andreas Kalogeras",
      "Dimitrios Bormpoudakis",
      "Iason Tsardanidis",
      "Dimitra A. Loka",
      "Charalampos Kontoes"
    ],
    "published": "2025-04-28",
    "abstract": "The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data",
    "url": "http://arxiv.org/abs/2504.19991v1",
    "authors": [
      "Ioannis Kontogiorgakis",
      "Iason Tsardanidis",
      "Dimitrios Bormpoudakis",
      "Ilias Tsoumas",
      "Dimitra A. Loka",
      "Christos Noulas",
      "Alexandros Tsitouras",
      "Charalampos Kontoes"
    ],
    "published": "2025-04-28",
    "abstract": "Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as commonly rely on on-ground\nfield surveys, which are often costly, time-consuming and subject to delays. In\norder to tackle this problem, we leverage Earth Observation (EO) data and\nMachine Learning (ML). Specifically, we developed an ML approach for mapping\nfour distinct weed management methods (Mowing, Tillage, Chemical-spraying, and\nNo practice) in orchards using satellite image time series (SITS) data from two\ndifferent sources: Sentinel-2 (S2) and PlanetScope (PS). The findings\ndemonstrate the potential of ML-driven remote sensing to enhance the efficiency\nand accuracy of weed management mapping in orchards.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype",
    "url": "http://arxiv.org/abs/2504.19074v1",
    "authors": [
      "Anyong Qin",
      "Chaoqi Yuan",
      "Qiang Li",
      "Feng Yang",
      "Tiecheng Song",
      "Chenqiang Gao"
    ],
    "published": "2025-04-27",
    "abstract": "Convolutional neural networks (CNNs) are effective for hyperspectral image\n(HSI) classification, but their 3D convolutional structures introduce high\ncomputational costs and limited generalization in few-shot scenarios. Domain\nshifts caused by sensor differences and environmental variations further hinder\ncross-dataset adaptability. Metric-based few-shot learning (FSL) prototype\nnetworks mitigate this problem, yet their performance is sensitive to prototype\nquality, especially with limited samples. To overcome these challenges, a\ndual-branch residual network that integrates spatial and spectral features via\nparallel branches is proposed in this letter. Additionally, more robust refined\nprototypes are obtained through a regulation term. Furthermore, a kernel\nprobability matching strategy aligns source and target domain features,\nalleviating domain shift. Experiments on four publicly available HSI datasets\nillustrate that the proposal achieves superior performance compared to other\nmethods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Evaluating AI-Driven Automated Map Digitization in QGIS",
    "url": "http://arxiv.org/abs/2504.18777v1",
    "authors": [
      "Diana Febrita"
    ],
    "published": "2025-04-26",
    "abstract": "Map digitization is an important process that converts maps into digital\nformats that can be used for further analysis. This process typically requires\na deep human involvement because of the need for interpretation and\ndecision-making when translating complex features. With the advancement of\nartificial intelligence, there is an alternative to conducting map digitization\nwith the help of machine learning techniques. Deepness, or Deep Neural Remote\nSensing, is an advanced AI-driven tool designed and integrated as a plugin in\nQGIS application. This research focuses on assessing the effectiveness of\nDeepness in automated digitization. This study analyses AI-generated\ndigitization results from Google Earth imagery and compares them with digitized\noutputs from OpenStreetMap (OSM) to evaluate performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network",
    "url": "http://arxiv.org/abs/2505.00495v1",
    "authors": [
      "Nguyen Van Thanh",
      "Nguyen Dang Huynh",
      "Nguyen Ngoc Tan",
      "Nguyen Thai Minh",
      "Nguyen Nam Hoang"
    ],
    "published": "2025-05-01",
    "abstract": "A storm is a type of extreme weather. Therefore, forecasting the path of a\nstorm is extremely important for protecting human life and property. However,\nstorm forecasting is very challenging because storm trajectories frequently\nchange. In this study, we propose an improved deep learning method using a\nTransformer network to predict the movement trajectory of a storm over the next\n6 hours. The storm data used to train the model was obtained from the National\nOceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that\nthe proposed method is more accurate than traditional methods. Moreover, the\nproposed method is faster and more cost-effective",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AI-Enhanced Automatic Design of Efficient Underwater Gliders",
    "url": "http://arxiv.org/abs/2505.00222v1",
    "authors": [
      "Peter Yichen Chen",
      "Pingchuan Ma",
      "Niklas Hagemann",
      "John Romanishin",
      "Wei Wang",
      "Daniela Rus",
      "Wojciech Matusik"
    ],
    "published": "2025-04-30",
    "abstract": "The development of novel autonomous underwater gliders has been hindered by\nlimited shape diversity, primarily due to the reliance on traditional design\ntools that depend heavily on manual trial and error. Building an automated\ndesign framework is challenging due to the complexities of representing glider\nshapes and the high computational costs associated with modeling complex\nsolid-fluid interactions. In this work, we introduce an AI-enhanced automated\ncomputational framework designed to overcome these limitations by enabling the\ncreation of underwater robots with non-trivial hull shapes. Our approach\ninvolves an algorithm that co-optimizes both shape and control signals,\nutilizing a reduced-order geometry representation and a differentiable\nneural-network-based fluid surrogate model. This end-to-end design workflow\nfacilitates rapid iteration and evaluation of hydrodynamic performance, leading\nto the discovery of optimal and complex hull shapes across various control\nsettings. We validate our method through wind tunnel experiments and swimming\npool gliding tests, demonstrating that our computationally designed gliders\nsurpass manually designed counterparts in terms of energy efficiency. By\naddressing challenges in efficient shape representation and neural fluid\nsurrogate models, our work paves the way for the development of highly\nefficient underwater gliders, with implications for long-range ocean\nexploration and environmental monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves",
    "url": "http://arxiv.org/abs/2504.21195v1",
    "authors": [
      "Kelsey E. Ennis",
      "Elizabeth A. Barnes",
      "Marybeth C. Arcodia",
      "Martin A. Fernandez",
      "Eric D. Maloney"
    ],
    "published": "2025-04-29",
    "abstract": "Extreme heat is the deadliest weather-related hazard in the United States.\nFurthermore, it is increasing in intensity, frequency, and duration, making\nskillful forecasts vital to protecting life and property. Traditional numerical\nweather prediction (NWP) models struggle with extreme heat for medium-range and\nsubseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial\nintelligence-based weather prediction (AIWP) models are progressing rapidly.\nHowever, it is largely unknown how well AIWP models forecast extremes,\nespecially for medium-range and S2S timescales. This study investigates 2-m\ntemperature forecasts for 60 heat waves across the four boreal seasons and over\nfour CONUS regions at lead times up to 20 days, using two AIWP models (Google\nGraphCast and Pangu-Weather) and one traditional NWP model (NOAA United\nForecast System Global Ensemble Forecast System (UFS GEFS)). First, case study\nanalyses show that both AIWP models and the UFS GEFS exhibit consistent cold\nbiases on regional scales in the 5-10 days of lead time before heat wave onset.\nGraphCast is the more skillful AIWP model, outperforming UFS GEFS and\nPangu-Weather in most locations. Next, the two AIWP models are isolated and\nanalyzed across all heat waves and seasons, with events split among the model's\ntesting (2018-2023) and training (1979-2017) periods. There are cold biases\nbefore and during the heat waves in both models and all seasons, except\nPangu-Weather in winter, which exhibits a mean warm bias before heat wave\nonset. Overall, results offer encouragement that AIWP models may be useful for\nmedium-range and S2S predictability of extreme heat.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning (ML)-Physics Fusion Model Outperforms Both Physics-Only and ML-Only Models in Typhoon Predictions",
    "url": "http://arxiv.org/abs/2504.20852v1",
    "authors": [
      "Zeyi Niu",
      "Wei Huang",
      "Hao Li",
      "Xuliang Fan",
      "Yuhua Yang",
      "Mengqi Yang",
      "Bo Qin"
    ],
    "published": "2025-04-29",
    "abstract": "Data-driven machine learning (ML) models, such as FuXi, exhibit notable\nlimitations in forecasting typhoon intensity and structure. This study presents\na comprehensive evaluation of FuXi-SHTM, a hybrid ML-physics model, using all\n2024 western North Pacific typhoon cases. The FuXi-SHTM hybrid demonstrates\nclear improvements in both track and intensity forecasts compared to the\nstandalone SHTM, FuXi, and ECMWF HRES models. Compared to FuXi alone, FuXi-SHTM\nreduces typhoon track forecast errors by 16.5% and 5.2% at lead times of 72 h\nand 120 h, respectively, and reduces intensity forecast errors by 59.7% and\n47.6%. Furthermore, FuXi-SHTM simulates cloud structures more realistically\ncompared to SHTM, and achieves superior representation of the 10-m wind fields\nin both intensity and spatial structure compared to FuXi and SHTM. Increasing\nthe resolution of FuXi-SHTM from 9 km to 3 km further enhances intensity\nforecasts, highlighting the critical role of the resolution of the physical\nmodel in advancing hybrid forecasting capabilities.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Quantifying the Influence of Climate on Storm Activity Using Machine Learning",
    "url": "http://arxiv.org/abs/2504.20521v1",
    "authors": [
      "Or Hadas",
      "Yohai Kaspi"
    ],
    "published": "2025-04-29",
    "abstract": "Extratropical storms shape midlatitude weather and vary due to both the\nslowly evolving climate and the rapid changes in synoptic conditions. While the\ninfluence of each factor has been studied extensively, their relative\nimportance remains unclear. Here, we quantify the climate's relative importance\nin both mean storm activity and individual storm development using 84 years of\nERA-5 data and Convolutional Neural Networks (CNN). We find that the\nconstructed CNN model predicts more than 90% of the variability in the mean\nstorm activity. However, a similar model predicts up to a third of the\nvariability in individual storm features, such as intensity, growth time, and\nstorm trajectory, showing their variability is dominated by synoptic\nconditions. Isolating present-day climate change yields a signal-to-noise ratio\n(SNR) of about 0.16% for storm-intensity attribution, whereas the SNR for heat\nanomalies is ~50 times higher, highlighting that focusing on variables more\ndirectly tied to warming provides a clearer attribution pathway.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model",
    "url": "http://arxiv.org/abs/2504.20238v1",
    "authors": [
      "P. Trent Vonich",
      "Gregory J. Hakim"
    ],
    "published": "2025-04-28",
    "abstract": "Atmospheric predictability research has long held that the limit of skillful\ndeterministic weather forecasts is about 14 days. We challenge this limit using\nGraphCast, a machine-learning weather model, by optimizing forecast initial\nconditions using gradient-based techniques for twice-daily forecasts spanning\n2020. This approach yields an average error reduction of 86% at 10 days, with\nskill lasting beyond 30 days. Mean optimal initial-condition perturbations\nreveal large-scale, spatially coherent corrections to ERA5, primarily\nreflecting an intensification of the Hadley circulation. Forecasts using\nGraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21%\nerror reduction, peaking at 4 days, indicating that analysis corrections\nreflect a combination of both model bias and a reduction in analysis error.\nThese results demonstrate that, given accurate initial conditions, skillful\ndeterministic forecasts are consistently achievable far beyond two weeks,\nchallenging long-standing assumptions about the limits of atmospheric\npredictability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States",
    "url": "http://arxiv.org/abs/2504.20129v1",
    "authors": [
      "Arun M. Saranathan",
      "Mahmoud Saeedimoghaddam",
      "Brandon Smith",
      "Deepthi Raghunandan",
      "Grey Nearing",
      "Craig Pelissier"
    ],
    "published": "2025-04-28",
    "abstract": "Snow is an essential input for various land surface models. Seasonal snow\nestimates are available as snow water equivalent (SWE) from process-based\nreanalysis products or locally from in situ measurements. While the reanalysis\nproducts are computationally expensive and available at only fixed spatial and\ntemporal resolutions, the in situ measurements are highly localized and sparse.\nTo address these issues and enable the analysis of the effect of a large suite\nof physical, morphological, and geological conditions on the presence and\namount of snow, we build a Long Short-Term Memory (LSTM) network, which is able\nto estimate the SWE based on time series input of the various\nphysical/meteorological factors as well static spatial/morphological factors.\nSpecifically, this model breaks down the SWE estimation into two separate\ntasks: (i) a classification task that indicates the presence/absence of snow on\na specific day and (ii) a regression task that indicates the height of the SWE\non a specific day in the case of snow presence. The model is trained using\nphysical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows\nin the western United States. We will show that trained LSTM models have a\nclassification accuracy of $\\geq 93\\%$ for the presence of snow and a\ncoefficient of correlation of $\\sim 0.9$ concerning their SWE estimates. We\nwill also demonstrate that the models can generalize both spatially and\ntemporally to previously unseen data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Digital Twin-based Out-of-Distribution Detection in Autonomous Vessels",
    "url": "http://arxiv.org/abs/2504.19816v1",
    "authors": [
      "Erblin Isaku",
      "Hassan Sartaj",
      "Shaukat Ali"
    ],
    "published": "2025-04-28",
    "abstract": "An autonomous vessel (AV) is a complex cyber-physical system (CPS) with\nsoftware enabling many key functionalities, e.g., navigation software enables\nan AV to autonomously or semi-autonomously follow a path to its destination.\nDigital twins of such AVs enable advanced functionalities such as running\nwhat-if scenarios, performing predictive maintenance, and enabling fault\ndiagnosis. Due to technological improvements, real-time analyses using\ncontinuous data from vessels' real-time operations have become increasingly\npossible. However, the literature has little explored developing advanced\nanalyses in real-time data in AVs with digital twins built with machine\nlearning techniques. To this end, we present a novel digital twin-based\napproach (ODDIT) to detect future out-of-distribution (OOD) states of an AV\nbefore reaching them, enabling proactive intervention. Such states may indicate\nanomalies requiring attention (e.g., manual correction by the ship master) and\nassist testers in scenario-centered testing. The digital twin consists of two\nmachine-learning models predicting future vessel states and whether the\npredicted state will be OOD. We evaluated ODDIT with five vessels across\nwaypoint and zigzag maneuvering under simulated conditions, including sensor\nand actuator noise and environmental disturbances i.e., ocean current. ODDIT\nachieved high accuracy in detecting OOD states, with AUROC and TNR@TPR95 scores\nreaching 99\\% across multiple vessels.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics",
    "url": "http://arxiv.org/abs/2504.19066v1",
    "authors": [
      "Deeksha Varshney",
      "Keane Ong",
      "Rui Mao",
      "Erik Cambria",
      "Gianmarco Mengaldo"
    ],
    "published": "2025-04-27",
    "abstract": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Exploring the Potential of Latent Embeddings for Sea Ice Characterization using ICESat-2 Data",
    "url": "http://arxiv.org/abs/2504.18668v1",
    "authors": [
      "Daehyeon Han",
      "Morteza Karimzadeh"
    ],
    "published": "2025-04-25",
    "abstract": "The Ice, Cloud, and Elevation Satellite-2 (ICESat-2) provides high-resolution\nmeasurements of sea ice height. Recent studies have developed machine learning\nmethods on ICESat-2 data, primarily focusing on surface type classification.\nHowever, the heavy reliance on manually collected labels requires significant\ntime and effort for supervised learning, as it involves cross-referencing track\nmeasurements with overlapping background optical imagery. Additionally, the\ncoincidence of ICESat-2 tracks with background images is relatively rare due to\nthe different overpass patterns and atmospheric conditions. To address these\nlimitations, this study explores the potential of unsupervised autoencoder on\nunlabeled data to derive latent embeddings. We develop autoencoder models based\non Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) to\nreconstruct topographic sequences from ICESat-2 and derive embeddings. We then\napply Uniform Manifold Approximation and Projection (UMAP) to reduce dimensions\nand visualize the embeddings. Our results show that embeddings from\nautoencoders preserve the overall structure but generate relatively more\ncompact clusters compared to the original ICESat-2 data, indicating the\npotential of embeddings to lessen the number of required labels samples.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM",
      "Autoencoder"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation",
    "url": "http://arxiv.org/abs/2504.18181v1",
    "authors": [
      "Yvonne Jenniges",
      "Maike Sonnewald",
      "Sebastian Maneth",
      "Are Olsen",
      "Boris P. Koch"
    ],
    "published": "2025-04-25",
    "abstract": "Defining ocean regions and water masses helps to understand marine processes\nand can serve downstream-tasks such as defining marine protected areas.\nHowever, such definitions are often a result of subjective decisions\npotentially producing misleading, unreproducible results. Here, the aim was to\nobjectively define regions of the North Atlantic. For this, a data-driven,\nsystematic machine learning approach was applied to generate and validate ocean\nclusters employing external, internal and relative validation techniques. About\n300 million measured salinity, temperature, and oxygen, nitrate, phosphate and\nsilicate concentration values served as input for various clustering methods\n(KMeans, agglomerative Ward, and Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN)). Uniform Manifold Approximation and\nProjection (UMAP) emphasised (dis-)similarities in the data while reducing\ndimensionality. Based on a systematic validation of the considered clustering\nmethods and their hyperparameters, the results showed that UMAP-DBSCAN best\nrepresented the data. To address stochastic variability, 100 UMAP-DBSCAN\nclustering runs were conducted and aggregated using Native Emergent Manifold\nInterrogation (NEMI), producing a final set of 321 clusters. Reproducibility\nwas evaluated by calculating the ensemble overlap (88.81 +- 1.8%) and the mean\ngrid cell-wise uncertainty estimated by NEMI (15.49 +- 20%). The presented\nclustering results agreed very well with common water mass definitions. This\nstudy revealed a more detailed regionalization compared to previous concepts\nsuch as the Longhurst provinces. The applied method is objective, efficient and\nreproducible and will support future research focusing on biogeochemical\ndifferences and changes in oceanic regions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes",
    "url": "http://arxiv.org/abs/2504.20303v1",
    "authors": [
      "Junlin Guo",
      "James R. Zimmer-Dauphinee",
      "Jordan M. Nieusma",
      "Siqi Lu",
      "Quan Liu",
      "Ruining Deng",
      "Can Cui",
      "Jialin Yue",
      "Yizhe Lin",
      "Tianyuan Yao",
      "Juming Xiong",
      "Junchao Zhu",
      "Chongyu Qu",
      "Yuechen Yang",
      "Mitchell Wilkes",
      "Xiao Wang",
      "Parker VanValkenburgh",
      "Steven A. Wernke",
      "Yuankai Huo"
    ],
    "published": "2025-04-28",
    "abstract": "By mapping sites at large scales using remotely sensed data, archaeologists\ncan generate unique insights into long-term demographic trends, inter-regional\nsocial networks, and past adaptations to climate change. Remote sensing surveys\ncomplement field-based approaches, and their reach can be especially great when\ncombined with deep learning and computer vision techniques. However,\nconventional supervised deep learning methods face challenges in annotating\nfine-grained archaeological features at scale. While recent vision foundation\nmodels have shown remarkable success in learning large-scale remote sensing\ndata with minimal annotations, most off-the-shelf solutions are designed for\nRGB images rather than multi-spectral satellite imagery, such as the 8-band\ndata used in our study. In this paper, we introduce DeepAndes, a\ntransformer-based vision foundation model trained on three million\nmulti-spectral satellite images, specifically tailored for Andean archaeology.\nDeepAndes incorporates a customized DINOv2 self-supervised learning algorithm\noptimized for 8-band multi-spectral imagery, marking the first foundation model\ndesigned explicitly for the Andes region. We evaluate its image understanding\nperformance through imbalanced image classification, image instance retrieval,\nand pixel-level semantic segmentation tasks. Our experiments show that\nDeepAndes achieves superior F1 scores, mean average precision, and Dice scores\nin few-shot learning scenarios, significantly outperforming models trained from\nscratch or pre-trained on smaller datasets. This underscores the effectiveness\nof large-scale self-supervised pre-training in archaeological remote sensing.\nCodes will be available on https://github.com/geopacha/DeepAndes.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach",
    "url": "http://arxiv.org/abs/2505.03299v1",
    "authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "St\u00e9phane May",
      "S\u00e9bastien Lef\u00e8vre"
    ],
    "published": "2025-05-06",
    "abstract": "Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery",
    "url": "http://arxiv.org/abs/2505.02829v1",
    "authors": [
      "Jerome Quenum",
      "Wen-Han Hsieh",
      "Tsung-Han Wu",
      "Ritwik Gupta",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published": "2025-05-05",
    "abstract": "Segmentation models can recognize a pre-defined set of objects in images.\nHowever, models that can reason over complex user queries that implicitly refer\nto multiple objects of interest are still in their infancy. Recent advances in\nreasoning segmentation--generating segmentation masks from complex, implicit\nquery text--demonstrate that vision-language models can operate across an open\ndomain and produce reasonable outputs. However, our experiments show that such\nmodels struggle with complex remote-sensing imagery. In this work, we introduce\nLISAt, a vision-language model designed to describe complex remote-sensing\nscenes, answer questions about them, and segment objects of interest. We\ntrained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,\nwith 27,615 annotations over 9,205 images, and a multimodal pretraining\ndataset, PreGRES, containing over 1 million question-answer pairs. LISAt\noutperforms existing geospatial foundation models such as RS-GPT4V by over\n10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses\nstate-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %\n(gIoU). Our model, datasets, and code are available at\nhttps://lisat-bair.github.io/LISAt/",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation",
    "url": "http://arxiv.org/abs/2505.03844v1",
    "authors": [
      "Sol\u00e8ne Debuys\u00e8re",
      "Nicolas Trouv\u00e9",
      "Nathan Letheule",
      "Olivier L\u00e9v\u00eaque",
      "Elise Colin"
    ],
    "published": "2025-05-05",
    "abstract": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has\nincreased considerably in recent years, with datasets commercially available.\nHowever, the acquisition of high-resolution SAR images in airborne\nconfigurations, remains costly and limited. Thus, the lack of open source,\nwell-labeled, or easily exploitable SAR text-image datasets is a barrier to the\nuse of existing foundation models in remote sensing applications. In this\ncontext, synthetic image generation is a promising solution to augment this\nscarce data, enabling a broader range of applications. Leveraging over 15 years\nof ONERA's extensive archival airborn data from acquisition campaigns, we\ncreated a comprehensive training dataset of 110 thousands SAR images to exploit\na 3.5 billion parameters pre-trained latent diffusion model. In this work, we\npresent a novel approach utilizing spatial conditioning techniques within a\nfoundation model to transform satellite SAR imagery into airborne SAR\nrepresentations. Additionally, we demonstrate that our pipeline is effective\nfor bridging the realism of simulated images generated by ONERA's physics-based\nsimulator EMPRISE. Our method explores a key application of AI in advancing SAR\nimaging technology. To the best of our knowledge, we are the first to introduce\nthis approach in the literature.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "url": "http://arxiv.org/abs/2505.03368v1",
    "authors": [
      "Stef De Sabbata",
      "Stefano Mizzaro",
      "Kevin Roitero"
    ],
    "published": "2025-05-06",
    "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder",
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling",
    "url": "http://arxiv.org/abs/2505.04802v1",
    "authors": [
      "Xiao Wang",
      "Jong-Youl Choi",
      "Takuya Kurihaya",
      "Isaac Lyngaas",
      "Hong-Jun Yoon",
      "Ming Fan",
      "Nasik Muhammad Nafi",
      "Aristeidis Tsaris",
      "Ashwin M. Aji",
      "Maliha Hossain",
      "Mohamed Wahib",
      "Dali Wang",
      "Peter Thornton",
      "Prasanna Balaprakash",
      "Moetasim Ashfaq",
      "Dan Lu"
    ],
    "published": "2025-05-07",
    "abstract": "Sparse observations and coarse-resolution climate models limit effective\nregional decision-making, underscoring the need for robust downscaling.\nHowever, existing AI methods struggle with generalization across variables and\ngeographies and are constrained by the quadratic complexity of Vision\nTransformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation\nmodel for global, hyper-resolution climate downscaling. ORBIT-2 incorporates\ntwo key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture\nwith residual learning and Bayesian regularization for efficient, robust\nprediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces\nself-attention complexity from quadratic to linear, enabling long-sequence\nprocessing and massive parallelism. ORBIT-2 scales to 10 billion parameters\nacross 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and\n92-98% strong scaling efficiency. It supports downscaling to 0.9 km global\nresolution and processes sequences up to 4.2 billion tokens. On 7 km resolution\nbenchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98\nto 0.99 against observation data.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery",
    "url": "http://arxiv.org/abs/2505.05321v1",
    "authors": [
      "Chintan B. Maniyar",
      "Minakshi Kumar",
      "Gengchen Mai"
    ],
    "published": "2025-05-08",
    "abstract": "Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks",
    "url": "http://arxiv.org/abs/2505.03522v1",
    "authors": [
      "Haotong Cheng",
      "Zhiqi Zhang",
      "Hao Li",
      "Xinshang Zhang"
    ],
    "published": "2025-05-06",
    "abstract": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning",
    "url": "http://arxiv.org/abs/2505.03327v1",
    "authors": [
      "Jos\u00e9-Luis Bueso-Bello",
      "Benjamin Chauvel",
      "Daniel Carcereri",
      "Philipp Posovszky",
      "Pietro Milillo",
      "Jennifer Ruiz",
      "Juan-Carlos Fern\u00e1ndez-Diaz",
      "Carolina Gonz\u00e1lez",
      "Michele Martone",
      "Ronny H\u00e4nsch",
      "Paola Rizzoli"
    ],
    "published": "2025-05-06",
    "abstract": "Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Sampling Kantorovich operators for speckle noise reduction using a Down-Up scaling approach and gap filling in remote sensing images",
    "url": "http://arxiv.org/abs/2505.02422v1",
    "authors": [
      "Danilo Costarelli",
      "Mariarosaria Natale"
    ],
    "published": "2025-05-05",
    "abstract": "In the literature, several approaches have been proposed for restoring and\nenhancing remote sensing images, including methods based on interpolation,\nfiltering, and deep learning. In this paper, we investigate the application of\nmultivariate sampling Kantorovich (SK) operators for image reconstruction, with\na particular focus on gap filling and speckle noise reduction. To understand\nthe accuracy performances of the proposed algorithms, we first derive a\nquantitative estimate in $C(\\R^n)$ for the error of approximation using the\nEuler-Maclaurin summation formula, which provides sharper error bounds under\nminimal regularity conditions. We also establish a convergence result and a\nquantitative estimate with respect to the dissimilarity index measured by the\ncontinuous SSIM for functions in Lebesgue spaces. Additionally, we prove a\nmultidimensional linear prediction result, which is used to design a new\nSK-based reconstruction algorithm to handle missing data, that we call LP-SK\nalgorithm. To address speckle noise, we integrate SK operators into a newly\nproposed Down-Up scaling approach. Numerical tests are presented on synthetic\nand real SAR images to validate the proposed methods. Performance is assessed\nusing similarity metrics such as SSIM and PSNR, along with speckle-specific\nindexes. Comparative analysis with state-of-the-art techniques highlights the\neffectiveness of the proposed approaches.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks",
    "url": "http://arxiv.org/abs/2505.03034v1",
    "authors": [
      "Sweta Rai",
      "Douglas W. Nychka",
      "Soutir Bandyopadhyay"
    ],
    "published": "2025-05-05",
    "abstract": "Data derived from remote sensing or numerical simulations often have a\nregular gridded structure and are large in volume, making it challenging to\nfind accurate spatial models that can fill in missing grid cells or simulate\nthe process effectively, especially in the presence of spatial heterogeneity\nand heavy-tailed marginal distributions. To overcome this issue, we present a\nspatial autoregressive modeling framework, which maps observations at a\nlocation and its neighbors to independent random variables. This is a highly\nflexible modeling approach and well-suited for non-Gaussian fields, providing\nsimpler interpretability. In particular, we consider the SAR model with\nGeneralized Extreme Value distribution innovations to combine the observation\nat a central grid location with its neighbors, capturing extreme spatial\nbehavior based on the heavy-tailed innovations. While these models are fast to\nsimulate by exploiting the sparsity of the key matrices in the computations,\nthe maximum likelihood estimation of the parameters is prohibitive due to the\nintractability of the likelihood, making optimization challenging. To overcome\nthis, we train a convolutional neural network on a large training set that\ncovers a useful parameter space, and then use the trained network for fast\nparameter estimation. Finally, we apply this model to analyze annual maximum\nprecipitation data from ERA-Interim-driven Weather Research and Forecasting\n(WRF) simulations, allowing us to explore its spatial extreme behavior across\nNorth America.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Reduced Cloud Cover Errors in a Hybrid AI-Climate Model Through Equation Discovery And Automatic Tuning",
    "url": "http://arxiv.org/abs/2505.04358v1",
    "authors": [
      "Arthur Grundner",
      "Tom Beucler",
      "Julien Savre",
      "Axel Lauer",
      "Manuel Schlund",
      "Veronika Eyring"
    ],
    "published": "2025-05-07",
    "abstract": "Climate models rely on parameterizations that account for the effects of\nsmall-scale processes on large-scale dynamics. Particularly cloud-related\nparameterizations remain a major source of uncertainty in climate projections.\nWhile hybrid Earth system models (ESMs) with machine learning-based\nparameterizations could improve current ESMs, deep learning approaches often\nlack interpretability, physical consistency, and computational efficiency.\nFurthermore, most data-driven parameterizations are trained in a stand-alone\nfashion and fail within ESMs, partly due to the difficulty of tuning the ESM to\naccommodate new, non-traditional schemes. In this study, we introduce a novel\ntwo-step pipeline for improving a climate model with data-driven\nparameterizations. First, we incorporate a physically consistent, data-driven\ncloud cover parameterization into the ICON global atmospheric model. The\nparameterization, a diagnostic equation derived from storm-resolving\nsimulations via symbolic regression, retains the interpretability and\nefficiency of traditional parameterizations while improving fidelity. Second,\nwe introduce an automated, gradient-free tuning procedure to recalibrate the\nnew climate model with Earth observations. We employ the Nelder-Mead algorithm\nand progressively increase simulation length, making our approach simple,\ncomputationally efficient, and easily extendable to other ESMs. The tuned\nhybrid model significantly reduces some long-standing biases in cloud cover and\nradiative budgets, particularly over regions such as the Southern Ocean and the\nsubtropical stratocumulus regions. Moreover, it remains robust under +4K\nsurface warming. Our results highlight the potential of data-driven\nparameterizations when combined with model tuning. This framework offers an\nautomatic, efficient and practical approach to enhancing climate projections\nwithout losing performance or interpretability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Big Data Architecture for Large Organizations",
    "url": "http://arxiv.org/abs/2505.04717v1",
    "authors": [
      "Fathima Nuzla Ismail",
      "Abira Sengupta",
      "Shanika Amarasoma"
    ],
    "published": "2025-05-07",
    "abstract": "The exponential growth of big data has transformed how large organisations\nleverage information to drive innovation, optimise processes, and maintain\ncompetitive advantages. However, managing and extracting insights from vast,\nheterogeneous data sources requires a scalable, secure, and well-integrated big\ndata architecture. This paper proposes a comprehensive big data framework that\naligns with organisational objectives while ensuring flexibility, scalability,\nand governance. The architecture encompasses multiple layers, including data\ningestion, transformation, storage, analytics, machine learning, and security,\nincorporating emerging technologies such as Generative AI (GenAI) and low-code\nmachine learning. Cloud-based implementations across Google Cloud, AWS, and\nMicrosoft Azure are analysed, highlighting their tools and capabilities.\nAdditionally, this study explores advancements in big data architecture,\nincluding AI-driven automation, data mesh, and Data Ocean paradigms. By\nestablishing a structured, adaptable framework, this research provides a\nfoundational blueprint for large organisations to harness big data as a\nstrategic asset effectively.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast",
    "url": "http://arxiv.org/abs/2505.04396v1",
    "authors": [
      "Jingnan Wang",
      "Jie Chao",
      "Shangshang Yang",
      "Congyi Nai",
      "Kaijun Ren",
      "Kefeng Deng",
      "Xi Chen",
      "Yaxin Liu",
      "Hanqiuzi Wen",
      "Ziniu Xiao",
      "Lifeng Zhang",
      "Xiaodong Wang",
      "Jiping Guan",
      "Baoxiang Pan"
    ],
    "published": "2025-05-07",
    "abstract": "The planning and operation of renewable energy, especially wind power, depend\ncrucially on accurate, timely, and high-resolution weather information.\nCoarse-grid global numerical weather forecasts are typically downscaled to meet\nthese requirements, introducing challenges of scale inconsistency, process\nrepresentation error, computation cost, and entanglement of distinct\nuncertainty sources from chaoticity, model bias, and large-scale forcing. We\naddress these challenges by learning the climatological distribution of a\ntarget wind farm using its high-resolution numerical weather simulations. An\noptimal combination of this learned high-resolution climatological prior with\ncoarse-grid large scale forecasts yields highly accurate, fine-grained,\nfull-variable, large ensemble of weather pattern forecasts. Using observed\nmeteorological records and wind turbine power outputs as references, the\nproposed methodology verifies advantageously compared to existing\nnumerical/statistical forecasting-downscaling pipelines, regarding either\ndeterministic/probabilistic skills or economic gains. Moreover, a 100-member,\n10-day forecast with spatial resolution of 1 km and output frequency of 15 min\ntakes < 1 hour on a moderate-end GPU, as contrast to $\\mathcal{O}(10^3)$ CPU\nhours for conventional numerical simulation. By drastically reducing\ncomputational costs while maintaining accuracy, our method paves the way for\nmore efficient and reliable renewable energy planning and operation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Parameter estimation for land-surface models using machine learning libraries",
    "url": "http://arxiv.org/abs/2505.02979v1",
    "authors": [
      "Ruiyue Huang",
      "Claire E. Heaney",
      "Maarten van Reeuwijk"
    ],
    "published": "2025-05-05",
    "abstract": "The Neural Networks for Partial Differential Equations (NN4PDEs) approach is\nused to determine the parameters of a simple land-surface model using PyTorch's\nbackpropagation engine. In order to test the inverse model, a synthetic dataset\nis created by running the model in forward mode with known parameter values to\ncreate soil temperature time series that can be used as observations for the\ninverse model. We show that it is not possible to obtain a reliable parameter\nestimation using a single observed soil temperature time series. Using\nmeasurements at two depths, reliable parameter estimates can be obtained\nalthough it is not possible to differentiate between latent and sensible heat\nfluxes. We apply the inverse model to urban flux tower data in Phoenix, United\nStates, and show that the thermal conductivity, volumetric heat capacity, and\nthe combined sensible-latent heat transfer coefficient can be reliably\nestimated using an observed value for the effective surface albedo. The\nresulting model accurately predicts the outgoing longwave radiation, conductive\nsoil fluxes and the combined sensible-latent heat fluxes.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2505.08723v1",
    "authors": [
      "Xiaolei Qin",
      "Di Wang",
      "Jing Zhang",
      "Fengxiang Wang",
      "Xin Su",
      "Bo Du",
      "Liangpei Zhang"
    ],
    "published": "2025-05-13",
    "abstract": "Satellite image time series (SITS) provide continuous observations of the\nEarth's surface, making them essential for applications such as environmental\nmanagement and disaster assessment. However, existing spatiotemporal foundation\nmodels rely on plain vision transformers, which encode entire temporal\nsequences without explicitly capturing multiscale spatiotemporal relationships\nbetween land objects. This limitation hinders their effectiveness in downstream\ntasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision\ntransformer foundation model tailored for SITS analysis. At its core, we\nintroduce a spatiotemporal gyroscope attention mechanism that dynamically\ncaptures evolving multiscale patterns across both time and space. For\npre-training, we curate MillionST, a large-scale dataset of one million images\nfrom 100,000 geographic locations, each captured across 10 temporal phases over\nfive years, encompassing diverse geospatial changes and seasonal variations.\nLeveraging this dataset, we adapt masked image modeling to pre-train TiMo,\nenabling it to effectively learn and encode generalizable spatiotemporal\nrepresentations.Extensive experiments across multiple spatiotemporal\ntasks-including deforestation monitoring, land cover segmentation, crop type\nclassification, and flood detection-demonstrate TiMo's superiority over\nstate-of-the-art methods. Code, model, and dataset will be released at\nhttps://github.com/MiliLab/TiMo.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Climate in a Bottle: Towards a Generative Foundation Model for the Kilometer-Scale Global Atmosphere",
    "url": "http://arxiv.org/abs/2505.06474v1",
    "authors": [
      "Noah D. Brenowitz",
      "Tao Ge",
      "Akshay Subramaniam",
      "Aayush Gupta",
      "David M. Hall",
      "Morteza Mardani",
      "Arash Vahdat",
      "Karthik Kashinath",
      "Michael S. Pritchard"
    ],
    "published": "2025-05-10",
    "abstract": "AI emulators offer a path to compressing, boosting limited ensembles, and\nimproving the latency of interacting with petabyte-scale climate prediction\ndata. However, prevailing auto-regressive paradigms offer limited flexibility,\nand are challenging to train on climate time horizons due to drifts,\ninstabilities and component-coupling challenges. Conditionally generative\nmodels offer an appealing alternative. In this context we demonstrate a\ngenerative diffusion-based framework -- Climate in a Bottle (cBottle) -- for\nemulating global km-scale climate simulations and reanalysis on the equal-area\nHEALPix grid. cBottle consists of two model stages: a globally-trained\ncoarse-resolution image generator that generates 100km (50k-pixel) fields given\nmonthly average sea surface temperatures and solar conditioning, followed by a\nlocally-trained 16x super-resolution stage that generates 5km (12.5M-pixel)\nfields; global super-resolution is made affordable using an overlapping\npatch-based multi-diffusion. Overall, cBottle shows promise as an emulator\nacross a battery of climate model diagnostics, including diurnal-to-seasonal\nscale variability, large-scale modes of variability, tropical cyclone\nstatistics, and trends of climate change and weather extremes. Moreover,\ncBottle is a step towards a foundation model, by bridging multiple data\nmodalities (reanalysis and simulation) with corresponding utility beyond\nemulation to tasks such as zero-shot bias correction, climate downscaling, and\nchannel in-filling.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction",
    "url": "http://arxiv.org/abs/2505.10027v1",
    "authors": [
      "Shijie Lyu"
    ],
    "published": "2025-05-15",
    "abstract": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing",
    "url": "http://arxiv.org/abs/2505.08302v1",
    "authors": [
      "Oishee Bintey Hoque",
      "Nibir Chandra Mandal",
      "Abhijin Adiga",
      "Samarth Swarup",
      "Sayjro Kossi Nouwakpo",
      "Amanda Wilson",
      "Madhav Marathe"
    ],
    "published": "2025-05-13",
    "abstract": "Accurate mapping of irrigation methods is crucial for sustainable\nagricultural practices and food systems. However, existing models that rely\nsolely on spectral features from satellite imagery are ineffective due to the\ncomplexity of agricultural landscapes and limited training data, making this a\nchallenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a\nnovel Swin-Transformer based approach that uses (i) a specialized projection\nmatrix to encode crop to irrigation probability, (ii) a spatial attention map\nto identify agricultural lands from non-agricultural lands, (iii)\nbi-directional cross-attention to focus complementary information from\ndifferent modalities, and (iv) a weighted ensemble for combining predictions\nfrom images and crop information. Our experimentation on five states in the US\nshows up to 22.9\\% (IoU) improvement over baseline with a 71.4% (IoU)\nimprovement for hard-to-classify drip irrigation. In addition, we propose a\ntwo-phase transfer learning approach to enhance cross-state irrigation mapping,\nachieving a 51% IoU boost in a state with limited labeled data. The ability to\nachieve baseline performance with only 40% of the training data highlights its\nefficiency, reducing the dependency on extensive manual labeling efforts and\nmaking large-scale, automated irrigation mapping more feasible and\ncost-effective.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction",
    "url": "http://arxiv.org/abs/2505.06905v1",
    "authors": [
      "Jian Song",
      "Hongruixuan Chen",
      "Naoto Yokoya"
    ],
    "published": "2025-05-11",
    "abstract": "Monocular height estimation (MHE) from very-high-resolution (VHR) remote\nsensing imagery via deep learning is notoriously challenging due to the lack of\nsufficient structural information. Conventional digital elevation models\n(DEMs), typically derived from airborne LiDAR or multi-view stereo, remain\ncostly and geographically limited. Recently, models trained on synthetic data\nand refined through domain adaptation have shown remarkable performance in MHE,\nyet it remains unclear how these models make predictions or how reliable they\ntruly are. In this paper, we investigate a state-of-the-art MHE model trained\npurely on synthetic data to explore where the model looks when making height\npredictions. Through systematic analyses, we find that the model relies heavily\non shadow cues, a factor that can lead to overestimation or underestimation of\nheights when shadows deviate from expected norms. Furthermore, the inherent\ndifficulty of evaluating regression tasks with the human eye underscores\nadditional limitations of purely synthetic training. To address these issues,\nwe propose a novel correction pipeline that integrates sparse, imperfect global\nLiDAR measurements (ICESat-2) with deep-learning outputs to improve local\naccuracy and achieve spatially consistent corrections. Our method comprises two\nstages: pre-processing raw ICESat-2 data, followed by a random forest-based\napproach to densely refine height estimates. Experiments in three\nrepresentative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal\nsubstantial error reductions, with mean absolute error (MAE) decreased by\n22.8\\%, 6.9\\%, and 4.9\\%, respectively. These findings highlight the critical\nrole of shadow awareness in synthetic data-driven models and demonstrate how\nfusing imperfect real-world LiDAR data can bolster the robustness of MHE,\npaving the way for more reliable and scalable 3D mapping solutions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Predicting butterfly species presence from satellite imagery using soft contrastive regularisation",
    "url": "http://arxiv.org/abs/2505.09306v1",
    "authors": [
      "Thijs L van der Plas",
      "Stephen Law",
      "Michael JO Pocock"
    ],
    "published": "2025-05-14",
    "abstract": "The growing demand for scalable biodiversity monitoring methods has fuelled\ninterest in remote sensing data, due to its widespread availability and\nextensive coverage. Traditionally, the application of remote sensing to\nbiodiversity research has focused on mapping and monitoring habitats, but with\nincreasing availability of large-scale citizen-science wildlife observation\ndata, recent methods have started to explore predicting multi-species presence\ndirectly from satellite images. This paper presents a new data set for\npredicting butterfly species presence from satellite data in the United\nKingdom. We experimentally optimise a Resnet-based model to predict\nmulti-species presence from 4-band satellite images, and find that this model\nespecially outperforms the mean rate baseline for locations with high species\nbiodiversity. To improve performance, we develop a soft, supervised contrastive\nregularisation loss that is tailored to probabilistic labels (such as\nspecies-presence data), and demonstrate that this improves prediction accuracy.\nIn summary, our new data set and contrastive regularisation method contribute\nto the open challenge of accurately predicting species biodiversity from remote\nsensing data, which is key for efficient biodiversity monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FedRS-Bench: Realistic Federated Learning Datasets and Benchmarks in Remote Sensing",
    "url": "http://arxiv.org/abs/2505.08325v1",
    "authors": [
      "Haodong Zhao",
      "Peng Peng",
      "Chiyu Chen",
      "Linqing Huang",
      "Gongshen Liu"
    ],
    "published": "2025-05-13",
    "abstract": "Remote sensing (RS) images are usually produced at an unprecedented scale,\nyet they are geographically and institutionally distributed, making centralized\nmodel training challenging due to data-sharing restrictions and privacy\nconcerns. Federated learning (FL) offers a solution by enabling collaborative\nmodel training across decentralized RS data sources without exposing raw data.\nHowever, there lacks a realistic federated dataset and benchmark in RS. Prior\nworks typically rely on manually partitioned single dataset, which fail to\ncapture the heterogeneity and scale of real-world RS data, and often use\ninconsistent experimental setups, hindering fair comparison. To address this\ngap, we propose a realistic federated RS dataset, termed FedRS. FedRS consists\nof eight datasets that cover various sensors and resolutions and builds 135\nclients, which is representative of realistic operational scenarios. Data for\neach client come from the same source, exhibiting authentic federated\nproperties such as skewed label distributions, imbalanced client data volumes,\nand domain heterogeneity across clients. These characteristics reflect\npractical challenges in federated RS and support evaluation of FL methods at\nscale. Based on FedRS, we implement 10 baseline FL algorithms and evaluation\nmetrics to construct the comprehensive FedRS-Bench. The experimental results\ndemonstrate that FL can consistently improve model performance over training on\nisolated data silos, while revealing performance trade-offs of different\nmethods under varying client heterogeneity and availability conditions. We hope\nFedRS-Bench will accelerate research on large-scale, realistic FL in RS by\nproviding a standardized, rich testbed and facilitating fair comparisons across\nfuture works. The source codes and dataset are available at\nhttps://fedrs-bench.github.io/.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset",
    "url": "http://arxiv.org/abs/2505.07396v2",
    "authors": [
      "Olaf Wysocki",
      "Benedikt Schwab",
      "Manoj Kumar Biswanath",
      "Michael Greza",
      "Qilin Zhang",
      "Jingwei Zhu",
      "Thomas Froech",
      "Medhini Heeramaglore",
      "Ihab Hijazi",
      "Khaoula Kanna",
      "Mathias Pechinger",
      "Zhaiyu Chen",
      "Yao Sun",
      "Alejandro Rueda Segura",
      "Ziyang Xu",
      "Omar AbdelGafar",
      "Mansour Mehranfar",
      "Chandan Yeshwanth",
      "Yueh-Cheng Liu",
      "Hadi Yazdi",
      "Jiapan Wang",
      "Stefan Auer",
      "Katharina Anders",
      "Klaus Bogenberger",
      "Andre Borrmann",
      "Angela Dai",
      "Ludwig Hoegner",
      "Christoph Holst",
      "Thomas H. Kolbe",
      "Ferdinand Ludwig",
      "Matthias Nie\u00dfner",
      "Frank Petzold",
      "Xiao Xiang Zhu",
      "Boris Jutzi"
    ],
    "published": "2025-05-12",
    "abstract": "Urban Digital Twins (UDTs) have become essential for managing cities and\nintegrating complex, heterogeneous data from diverse sources. Creating UDTs\ninvolves challenges at multiple process stages, including acquiring accurate 3D\nsource data, reconstructing high-fidelity 3D models, maintaining models'\nupdates, and ensuring seamless interoperability to downstream tasks. Current\ndatasets are usually limited to one part of the processing chain, hampering\ncomprehensive UDTs validation. To address these challenges, we introduce the\nfirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.\nThis dataset includes georeferenced, semantically aligned 3D models and\nnetworks along with various terrestrial, mobile, aerial, and satellite\nobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently\n767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high\naccuracy, and multimodal data integration, the benchmark supports robust\nanalysis of sensors and the development of advanced reconstruction methods.\nAdditionally, we explore downstream tasks demonstrating the potential of\nTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar\npotential analysis, point cloud semantic segmentation, and LoD3 building\nreconstruction. We are convinced this contribution lays a foundation for\novercoming current limitations in UDT creation, fostering new research\ndirections and practical solutions for smarter, data-driven urban environments.\nThe project is available under: https://tum2t.win",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Online Feedback Efficient Active Target Discovery in Partially Observable Environments",
    "url": "http://arxiv.org/abs/2505.06535v1",
    "authors": [
      "Anindya Sarkar",
      "Binglin Ji",
      "Yevgeniy Vorobeychik"
    ],
    "published": "2025-05-10",
    "abstract": "In various scientific and engineering domains, where data acquisition is\ncostly, such as in medical imaging, environmental monitoring, or remote\nsensing, strategic sampling from unobserved regions, guided by prior\nobservations, is essential to maximize target discovery within a limited\nsampling budget. In this work, we introduce Diffusion-guided Active Target\nDiscovery (DiffATD), a novel method that leverages diffusion dynamics for\nactive target discovery. DiffATD maintains a belief distribution over each\nunobserved state in the environment, using this distribution to dynamically\nbalance exploration-exploitation. Exploration reduces uncertainty by sampling\nregions with the highest expected entropy, while exploitation targets areas\nwith the highest likelihood of discovering the target, indicated by the belief\ndistribution and an incrementally trained reward model designed to learn the\ncharacteristics of the target. DiffATD enables efficient target discovery in a\npartially observable environment within a fixed sampling budget, all without\nrelying on any prior supervised training. Furthermore, DiffATD offers\ninterpretability, unlike existing black-box policies that require extensive\nsupervised training. Through extensive experiments and ablation studies across\ndiverse domains, including medical imaging and remote sensing, we show that\nDiffATD performs significantly better than baselines and competitively with\nsupervised methods that operate under full environmental observability.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting",
    "url": "http://arxiv.org/abs/2505.10191v1",
    "authors": [
      "Qingyu Zheng",
      "Qi Shao",
      "Guijun Han",
      "Wei Li",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2025-05-15",
    "abstract": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the\nocean, and their impact on the energy cascade of the global ocean cannot be\nignored. Eddy-resolving ocean forecasting is providing more reliable protection\nfor fisheries and navigational safety, but also presents significant scientific\nchallenges and high computational costs for traditional numerical models.\nArtificial intelligence (AI)-based weather and ocean forecasting systems are\nbecoming powerful tools that balance forecast performance with computational\nefficiency. However, the complex multiscale features in the ocean dynamical\nsystem make AI models still face many challenges in mesoscale eddy forecasting\n(especially regional modelling). Here, we develop LanTu, a regional\neddy-resolving ocean forecasting system based on dynamics-enhanced deep\nlearning. We incorporate cross-scale interactions into LanTu and construct\nmultiscale physical constraint for optimising LanTu guided by knowledge of eddy\ndynamics in order to improve the forecasting skill of LanTu for mesoscale\nevolution. The results show that LanTu outperforms the existing advanced\noperational numerical ocean forecasting system (NOFS) and AI-based ocean\nforecasting system (AI-OFS) in temperature, salinity, sea level anomaly and\ncurrent prediction, with a lead time of more than 10 days. Our study highlights\nthat dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for\neddy-resolving ocean forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "An AI-driven framework for the prediction of personalised health response to air pollution",
    "url": "http://arxiv.org/abs/2505.10556v1",
    "authors": [
      "Nazanin Zounemat Kermani",
      "Sadjad Naderi",
      "Claire H. Dilliway",
      "Claire E. Heaney",
      "Shrreya Behll",
      "Boyang Chen",
      "Hisham Abubakar-Waziri",
      "Alexandra E. Porter",
      "Marc Chadeau-Hyam",
      "Fangxin Fang",
      "Ian M. Adcock",
      "Kian Fan Chung",
      "Christopher C. Pain"
    ],
    "published": "2025-05-15",
    "abstract": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Score-based diffusion nowcasting of GOES imagery",
    "url": "http://arxiv.org/abs/2505.10432v1",
    "authors": [
      "Randy J. Chase",
      "Katherine Haynes",
      "Lander Ver Hoef",
      "Imme Ebert-Uphoff"
    ],
    "published": "2025-05-15",
    "abstract": "Clouds and precipitation are important for understanding weather and climate.\nSimulating clouds and precipitation with traditional numerical weather\nprediction is challenging because of the sub-grid parameterizations required.\nMachine learning has been explored for forecasting clouds and precipitation,\nbut early machine learning methods often created blurry forecasts. In this\npaper we explore a newer method, named score-based diffusion, to nowcast (zero\nto three hour forecast) clouds and precipitation. We discuss the background and\nintuition of score-based diffusion models - thus providing a starting point for\nthe community - while exploring the methodology's use for nowcasting\ngeostationary infrared imagery. We experiment with three main types of\ndiffusion models: a standard score-based diffusion model (Diff); a residual\ncorrection diffusion model (CorrDiff); and a latent diffusion model (LDM). Our\nresults show that the diffusion models are able to not only advect existing\nclouds, but also generate and decay clouds, including convective initiation.\nThese results are surprising because the forecasts are initiated with only the\npast 20 mins of infrared satellite imagery. A case study qualitatively shows\nthe preservation of high resolution features longer into the forecast than a\nconventional mean-squared error trained U-Net. The best of the three diffusion\nmodels tested was the CorrDiff approach, outperforming all other diffusion\nmodels, the traditional U-Net, and a persistence forecast by one to two kelvin\non root mean squared error. The diffusion models also enable out-of-the-box\nensemble generation, which shows skillful calibration, with the spread of the\nensemble correlating well to the error.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "Diffusion Models"
    ],
    "applications": [
      "Forecast",
      "Nowcast"
    ]
  },
  {
    "title": "Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations",
    "url": "http://arxiv.org/abs/2505.09284v1",
    "authors": [
      "Panqi Chen",
      "Yifan Sun",
      "Lei Cheng",
      "Yang Yang",
      "Weichang Li",
      "Yang Liu",
      "Weiqing Liu",
      "Jiang Bian",
      "Shikai Fang"
    ],
    "published": "2025-05-14",
    "abstract": "Modeling and reconstructing multidimensional physical dynamics from sparse\nand off-grid observations presents a fundamental challenge in scientific\nresearch. Recently, diffusion-based generative modeling shows promising\npotential for physical simulation. However, current approaches typically\noperate on on-grid data with preset spatiotemporal resolution, but struggle\nwith the sparsely observed and continuous nature of real-world physical\ndynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in\nFunctional Tucker space, a novel framework that generates full-field evolution\nof physical dynamics from irregular sparse observations. SDIFT leverages the\nfunctional Tucker model as the latent space representer with proven universal\napproximation property, and represents observations as latent functions and\nTucker core sequences. We then construct a sequential diffusion model with\ntemporally augmented UNet in the functional Tucker space, denoising noise drawn\nfrom a Gaussian process to generate the sequence of core tensors.\n  At the posterior sampling stage, we propose a Message-Passing Posterior\nSampling mechanism, enabling conditional generation of the entire sequence\nguided by observations at limited time steps. We validate SDIFT on three\nphysical systems spanning astronomical (supernova explosions, light-year\nscale), environmental (ocean sound speed fields, kilometer scale), and\nmolecular (organic liquid, millimeter scale) domains, demonstrating significant\nimprovements in both reconstruction accuracy and computational efficiency\ncompared to state-of-the-art approaches.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control",
    "url": "http://arxiv.org/abs/2505.07045v1",
    "authors": [
      "Junjie Yu",
      "John S. Schreck",
      "David John Gagne",
      "Keith W. Oleson",
      "Jie Li",
      "Yongtu Liang",
      "Qi Liao",
      "Mingfei Sun",
      "David O. Topping",
      "Zhonghua Zheng"
    ],
    "published": "2025-05-11",
    "abstract": "Reinforcement learning (RL)-based heating, ventilation, and air conditioning\n(HVAC) control has emerged as a promising technology for reducing building\nenergy consumption while maintaining indoor thermal comfort. However, the\nefficacy of such strategies is influenced by the background climate and their\nimplementation may potentially alter both the indoor climate and local urban\nclimate. This study proposes an integrated framework combining RL with an urban\nclimate model that incorporates a building energy model, aiming to evaluate the\nefficacy of RL-based HVAC control across different background climates, impacts\nof RL strategies on indoor climate and local urban climate, and the\ntransferability of RL strategies across cities. Our findings reveal that the\nreward (defined as a weighted combination of energy consumption and thermal\ncomfort) and the impacts of RL strategies on indoor climate and local urban\nclimate exhibit marked variability across cities with different background\nclimates. The sensitivity of reward weights and the transferability of RL\nstrategies are also strongly influenced by the background climate. Cities in\nhot climates tend to achieve higher rewards across most reward weight\nconfigurations that balance energy consumption and thermal comfort, and those\ncities with more varying atmospheric temperatures demonstrate greater RL\nstrategy transferability. These findings underscore the importance of\nthoroughly evaluating RL-based HVAC control strategies in diverse climatic\ncontexts. This study also provides a new insight that city-to-city learning\nwill potentially aid the deployment of RL-based HVAC control.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models",
    "url": "http://arxiv.org/abs/2505.16793v1",
    "authors": [
      "Xiang Li",
      "Yong Tao",
      "Siyuan Zhang",
      "Siwei Liu",
      "Zhitong Xiong",
      "Chunbo Luo",
      "Lu Liu",
      "Mykola Pechenizkiy",
      "Xiao Xiang Zhu",
      "Tianjin Huang"
    ],
    "published": "2025-05-22",
    "abstract": "Earth observation foundation models have shown strong generalization across\nmultiple Earth observation tasks, but their robustness under real-world\nperturbations remains underexplored. To bridge this gap, we introduce REOBench,\nthe first comprehensive benchmark for evaluating the robustness of Earth\nobservation foundation models across six tasks and twelve types of image\ncorruptions, including both appearance-based and geometric perturbations. To\nensure realistic and fine-grained evaluation, our benchmark focuses on\nhigh-resolution optical remote sensing images, which are widely used in\ncritical applications such as urban planning and disaster response. We conduct\na systematic evaluation of a broad range of models trained using masked image\nmodeling, contrastive learning, and vision-language pre-training paradigms. Our\nresults reveal that (1) existing Earth observation foundation models experience\nsignificant performance degradation when exposed to input corruptions. (2) The\nseverity of degradation varies across tasks, model architectures, backbone\nsizes, and types of corruption, with performance drop varying from less than 1%\nto over 20%. (3) Vision-language models show enhanced robustness, particularly\nin multimodal tasks. REOBench underscores the vulnerability of current Earth\nobservation foundation models to real-world corruptions and provides actionable\ninsights for developing more robust and reliable models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation",
    "url": "http://arxiv.org/abs/2505.16540v1",
    "authors": [
      "Inbal Cohen",
      "Boaz Meivar",
      "Peihan Tu",
      "Shai Avidan",
      "Gal Oren"
    ],
    "published": "2025-05-22",
    "abstract": "Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2505.15334v1",
    "authors": [
      "Bernardin Ligan",
      "Khalide Jbilou",
      "Fahd Kalloubi",
      "Ahmed Ratnani"
    ],
    "published": "2025-05-21",
    "abstract": "Foundation models have achieved great success across diverse domains,\nincluding remote sensing (RS), thanks to their versatility and strong\ngeneralization abilities. However, most RS foundation models are designed for\nmultispectral data, while hyperspectral imagery (HSI) - with its hundreds of\nspectral bands - remains less explored. Fine-tuning such models for downstream\ntasks is also challenging, often demanding considerable memory and storage. In\nthis paper, we propose an efficient framework to fine-tune SpectralGPT, a\nmultispectral foundation model, for hyperspectral image classification (HSIC).\nWe explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including\nLow-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank\nKronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for\nlow-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce\nKronA+, which applies a similar mechanism to the Kronecker matrices. We\nevaluate our approach on five datasets from different sensors, showing\ncompetitive performance with state-of-the-art HSI models. Our full fine-tuning\n(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral\nfoundation model on some datasets while requiring only a quarter of the\ntraining epochs. Under the same number of epochs, KronA+ reaches similar\nperformance with far fewer trainable parameters - just 0.056 percent - and adds\nonly approximately 0.2 megabytes of storage, making it the most effective PEFT\nmethod tested.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation",
    "url": "http://arxiv.org/abs/2505.15147v1",
    "authors": [
      "Quanwei Liu",
      "Tao Huang",
      "Yanni Dong",
      "Jiaqi Yang",
      "Wei Xiang"
    ],
    "published": "2025-05-21",
    "abstract": "Remote sensing images (RSIs) capture both natural and human-induced changes\non the Earth's surface, serving as essential data for environmental monitoring,\nurban planning, and resource management. Semantic segmentation (SS) of RSIs\nenables the fine-grained interpretation of surface features, making it a\ncritical task in remote sensing analysis. With the increasing diversity and\nvolume of RSIs collected by sensors on various platforms, traditional\nprocessing methods struggle to maintain efficiency and accuracy. In response,\ndeep learning (DL) has emerged as a transformative approach, enabling\nsubstantial advances in remote sensing image semantic segmentation (RSISS) by\nautomating feature extraction and improving segmentation accuracy across\ndiverse modalities. This paper revisits the evolution of DL-based RSISS by\ncategorizing existing approaches into four stages: the early pixel-based\nmethods, the prevailing patch-based and tile-based techniques, and the emerging\nimage-based strategies enabled by foundation models. We analyze these\ndevelopments from the perspective of feature extraction and learning\nstrategies, revealing the field's progression from pixel-level to tile-level\nand from unimodal to multimodal segmentation. Furthermore, we conduct a\ncomprehensive evaluation of nearly 40 advanced techniques on a unified dataset\nto quantitatively characterize their performance and applicability. This review\noffers a holistic view of DL-based SS for RS, highlighting key advancements,\ncomparative insights, and open challenges to guide future research.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives",
    "url": "http://arxiv.org/abs/2505.14361v1",
    "authors": [
      "Xingxing Weng",
      "Chao Pang",
      "Gui-Song Xia"
    ],
    "published": "2025-05-20",
    "abstract": "Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts",
    "url": "http://arxiv.org/abs/2505.14088v1",
    "authors": [
      "Xi Chen",
      "Shen Yan",
      "Juelin Zhu",
      "Chen Chen",
      "Yu Liu",
      "Maojun Zhang"
    ],
    "published": "2025-05-20",
    "abstract": "We introduce Land-MoE, a novel approach for multispectral land cover\nclassification (MLCC). Spectral shift, which emerges from disparities in\nsensors and geospatial conditions, poses a significant challenge in this\ndomain. Existing methods predominantly rely on domain adaptation and\ngeneralization strategies, often utilizing small-scale models that exhibit\nlimited performance. In contrast, Land-MoE addresses these issues by\nhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,\nto fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.\nSpecifically, Land-MoE comprises two key modules: the mixture of low-rank token\nexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages\nrank-differentiated tokens to generate diverse feature adjustments for\nindividual instances within multispectral images. By dynamically combining\nlearnable low-rank token experts of varying ranks, it enhances the robustness\nagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on\nthe refined features. This process enables the model to effectively capture\nfrequency band information that is strongly correlated with semantic essence,\nwhile simultaneously suppressing frequency noise irrelevant to the task.\nComprehensive experiments on MLCC tasks involving cross-sensor and\ncross-geospatial setups demonstrate that Land-MoE outperforms existing methods\nby a large margin. Additionally, the proposed approach has also achieved\nstate-of-the-art performance in domain generalization semantic segmentation\ntasks of RGB remote sensing images.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models",
    "url": "http://arxiv.org/abs/2505.12108v1",
    "authors": [
      "Jiancheng Pan",
      "Shiye Lei",
      "Yuqian Fu",
      "Jiahao Li",
      "Yanxing Liu",
      "Yuze Sun",
      "Xiao He",
      "Long Peng",
      "Xiaomeng Huang",
      "Bo Zhao"
    ],
    "published": "2025-05-17",
    "abstract": "Remote sensing image (RSI) interpretation typically faces challenges due to\nthe scarcity of labeled data, which limits the performance of RSI\ninterpretation tasks. To tackle this challenge, we propose EarthSynth, a\ndiffusion-based generative foundation model that enables synthesizing\nmulti-category, cross-satellite labeled Earth observation for downstream RSI\ninterpretation tasks. To the best of our knowledge, EarthSynth is the first to\nexplore multi-task generation for remote sensing. EarthSynth, trained on the\nEarthSynth-180K dataset, employs the Counterfactual Composition training\nstrategy to improve training data diversity and enhance category control.\nFurthermore, a rule-based method of R-Filter is proposed to filter more\ninformative synthetic data for downstream tasks. We evaluate our EarthSynth on\nscene classification, object detection, and semantic segmentation in open-world\nscenarios, offering a practical solution for advancing RSI interpretation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation",
    "url": "http://arxiv.org/abs/2505.15077v1",
    "authors": [
      "Alessandro dos Santos Ferreira",
      "Ana Paula Marques Ramos",
      "Jos\u00e9 Marcato Junior",
      "Wesley Nunes Gon\u00e7alves"
    ],
    "published": "2025-05-21",
    "abstract": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN",
      "Diffusion Models"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry",
    "url": "http://arxiv.org/abs/2505.13088v1",
    "authors": [
      "Zhaoyi Wang",
      "Shengyu Huang",
      "Jemil Avers Butt",
      "Yuanzhou Cai",
      "Matej Varga",
      "Andreas Wieser"
    ],
    "published": "2025-05-19",
    "abstract": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction",
    "url": "http://arxiv.org/abs/2505.12280v1",
    "authors": [
      "Sijie Zhao",
      "Feng Liu",
      "Xueliang Zhang",
      "Hao Chen",
      "Pengfeng Xiao",
      "Lei Bai"
    ],
    "published": "2025-05-18",
    "abstract": "The proliferation of diverse remote sensing data has spurred advancements in\ndense prediction tasks, yet significant challenges remain in handling data\nheterogeneity. Remote sensing imagery exhibits substantial variability across\ntemporal, spectral, and spatial (TSS) dimensions, complicating unified data\nprocessing. Current deep learning models for dense prediction tasks, such as\nsemantic segmentation and change detection, are typically tailored to specific\ninput-output configurations. Consequently, variations in data dimensionality or\ntask requirements often lead to significant performance degradation or model\nincompatibility, necessitating costly retraining or fine-tuning efforts for\ndifferent application scenarios. This paper introduces the\nTemporal-Spectral-Spatial Unified Network (TSSUN), a novel architecture\ndesigned for unified representation and modeling of remote sensing data across\ndiverse TSS characteristics and task types. TSSUN employs a\nTemporal-Spectral-Spatial Unified Strategy that leverages meta-information to\ndecouple and standardize input representations from varied temporal, spectral,\nand spatial configurations, and similarly unifies output structures for\ndifferent dense prediction tasks and class numbers. Furthermore, a Local-Global\nWindow Attention mechanism is proposed to efficiently capture both local\ncontextual details and global dependencies, enhancing the model's adaptability\nand feature extraction capabilities. Extensive experiments on multiple datasets\ndemonstrate that a single TSSUN model effectively adapts to heterogeneous\ninputs and unifies various dense prediction tasks. The proposed approach\nconsistently achieves or surpasses state-of-the-art performance, highlighting\nits robustness and generalizability for complex remote sensing applications\nwithout requiring task-specific modifications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples",
    "url": "http://arxiv.org/abs/2505.12069v1",
    "authors": [
      "Shenzhou Liu",
      "Di Wang",
      "Haonan Guo",
      "Chengxi Han",
      "Wenzhi Zeng"
    ],
    "published": "2025-05-17",
    "abstract": "Accurate and fine-grained crop yield prediction plays a crucial role in\nadvancing global agriculture. However, the accuracy of pixel-level yield\nestimation based on satellite remote sensing data has been constrained by the\nscarcity of ground truth data. To address this challenge, we propose a novel\napproach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This\nframework introduces an effective multi-task feature-sharing strategy, where\nfeatures extracted from a shared backbone network are simultaneously utilized\nby both crop yield prediction decoders and crop classification decoders with\nthe ability to fuse information between them. This design allows MT-CYP-Net to\nbe trained with extremely sparse crop yield point labels and crop type labels,\nwhile still generating detailed pixel-level crop yield maps. Concretely, we\ncollected 1,859 yield point labels along with corresponding crop type labels\nand satellite images from eight farms in Heilongjiang Province, China, in 2023,\ncovering soybean, maize, and rice crops, and constructed a sparse crop yield\nlabel dataset. MT-CYP-Net is compared with three classical machine learning and\ndeep learning benchmark methods in this dataset. Experimental results not only\nindicate the superiority of MT-CYP-Net compared to previous methods on multiple\ntypes of crops but also demonstrate the potential of deep networks on precise\npixel-level crop yield prediction, especially with limited data labels.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Beluga Whale Detection from Satellite Imagery with Point Labels",
    "url": "http://arxiv.org/abs/2505.12066v1",
    "authors": [
      "Yijie Zheng",
      "Jinxuan Yang",
      "Yu Chen",
      "Yaxuan Wang",
      "Yihang Lu",
      "Guoqing Li"
    ],
    "published": "2025-05-17",
    "abstract": "Very high-resolution (VHR) satellite imagery has emerged as a powerful tool\nfor monitoring marine animals on a large scale. However, existing deep\nlearning-based whale detection methods usually require manually created,\nhigh-quality bounding box annotations, which are labor-intensive to produce.\nMoreover, existing studies often exclude ``uncertain whales'', individuals that\nhave ambiguous appearances in satellite imagery, limiting the applicability of\nthese models in real-world scenarios. To address these limitations, this study\nintroduces an automated pipeline for detecting beluga whales and harp seals in\nVHR satellite imagery. The pipeline leverages point annotations and the Segment\nAnything Model (SAM) to generate precise bounding box annotations, which are\nused to train YOLOv8 for multiclass detection of certain whales, uncertain\nwhales, and harp seals. Experimental results demonstrated that SAM-generated\nannotations significantly improved detection performance, achieving higher\n$\\text{F}_\\text{1}$-scores compared to traditional buffer-based annotations.\nYOLOv8 trained on SAM-labeled boxes achieved an overall\n$\\text{F}_\\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,\nwith superior performance in dense scenes. The proposed approach not only\nreduces the manual effort required for annotation but also enhances the\ndetection of uncertain whales, offering a more comprehensive solution for\nmarine animal monitoring. This method holds great potential for extending to\nother species, habitats, and remote sensing platforms, as well as for\nestimating whale biometrics, thereby advancing ecological monitoring and\nconservation efforts. The codes for our label and detection pipeline are\npublicly available at http://github.com/voyagerxvoyagerx/beluga-seeker .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection",
    "url": "http://arxiv.org/abs/2505.11793v1",
    "authors": [
      "Jianing Wang",
      "Siying Guo",
      "Zheng Hua",
      "Runhu Huang",
      "Jinyu Hu",
      "Maoguo Gong"
    ],
    "published": "2025-05-17",
    "abstract": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ]
  },
  {
    "title": "Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors",
    "url": "http://arxiv.org/abs/2505.14122v1",
    "authors": [
      "Ehsan Masoudian",
      "Ali Mirzaei",
      "Hossein Bagheri"
    ],
    "published": "2025-05-20",
    "abstract": "This study investigates the multifaceted factors influencing wildfire risk in\nIran, focusing on the interplay between climatic conditions and human\nactivities. Utilizing advanced remote sensing, geospatial information system\n(GIS) processing techniques such as cloud computing, and machine learning\nalgorithms, this research analyzed the impact of climatic parameters,\ntopographic features, and human-related factors on wildfire susceptibility\nassessment and prediction in Iran. Multiple scenarios were developed for this\npurpose based on the data sampling strategy. The findings revealed that\nclimatic elements such as soil moisture, temperature, and humidity\nsignificantly contribute to wildfire susceptibility, while human\nactivities-particularly population density and proximity to powerlines-also\nplayed a crucial role. Furthermore, the seasonal impact of each parameter was\nseparately assessed during warm and cold seasons. The results indicated that\nhuman-related factors, rather than climatic variables, had a more prominent\ninfluence during the seasonal analyses. This research provided new insights\ninto wildfire dynamics in Iran by generating high-resolution wildfire\nsusceptibility maps using advanced machine learning classifiers. The generated\nmaps identified high risk areas, particularly in the central Zagros region, the\nnortheastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting\nthe urgent need for effective fire management strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction",
    "url": "http://arxiv.org/abs/2505.14747v1",
    "authors": [
      "Fatemeh Chajaei",
      "Hossein Bagheri"
    ],
    "published": "2025-05-20",
    "abstract": "Three-dimensional reconstruction of buildings, particularly at Level of\nDetail 1 (LOD1), plays a crucial role in various applications such as urban\nplanning, urban environmental studies, and designing optimized transportation\nnetworks. This study focuses on assessing the potential of LiDAR data for\naccurate 3D building reconstruction at LOD1 and extracting morphological\nfeatures from these models. Four deep semantic segmentation models, U-Net,\nAttention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning\nto extract building footprints from LiDAR data. The results showed that U-Net3+\nand Attention U-Net outperformed the others, achieving IoU scores of 0.833 and\n0.814, respectively. Various statistical measures, including maximum, range,\nmode, median, and the 90th percentile, were used to estimate building heights,\nresulting in the generation of 3D models at LOD1. As the main contribution of\nthe research, the impact of segmentation accuracy on the quality of 3D building\nmodeling and the accuracy of morphological features like building area and\nexternal wall surface area was investigated. The results showed that the\naccuracy of building identification (segmentation performance) significantly\naffects the 3D model quality and the estimation of morphological features,\ndepending on the height calculation method. Overall, the UNet3+ method,\nutilizing the 90th percentile and median measures, leads to accurate height\nestimation of buildings and the extraction of morphological features.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Continuous Domain Generalization",
    "url": "http://arxiv.org/abs/2505.13519v1",
    "authors": [
      "Zekun Cai",
      "Yiheng Yao",
      "Guangji Bai",
      "Renhe Jiang",
      "Xuan Song",
      "Ryosuke Shibasaki",
      "Liang Zhao"
    ],
    "published": "2025-05-17",
    "abstract": "Real-world data distributions often shift continuously across multiple latent\nfactors such as time, geography, and socioeconomic context. However, existing\ndomain generalization approaches typically treat domains as discrete or\nevolving along a single axis (e.g., time), which fails to capture the complex,\nmulti-dimensional nature of real-world variation. This paper introduces the\ntask of Continuous Domain Generalization (CDG), which aims to generalize\npredictive models to unseen domains defined by arbitrary combinations of\ncontinuous variation descriptors. We present a principled framework grounded in\ngeometric and algebraic theory, showing that optimal model parameters across\ndomains lie on a low-dimensional manifold. To model this structure, we propose\na Neural Lie Transport Operator (NeuralLTO), which enables structured parameter\ntransitions by enforcing geometric continuity and algebraic consistency. To\nhandle noisy or incomplete domain descriptors, we introduce a gating mechanism\nto suppress irrelevant dimensions and a local chart-based strategy for robust\ngeneralization. Extensive experiments on synthetic and real-world\ndatasets-including remote sensing, scientific documents, and traffic\nforecasting-demonstrate that our method significantly outperforms existing\nbaselines in generalization accuracy and robustness under descriptor\nimperfections.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation",
    "url": "http://arxiv.org/abs/2505.15802v1",
    "authors": [
      "Sarah E. Wessinger",
      "Leslie N. Smith",
      "Jacob Gull",
      "Jonathan Gehman",
      "Zachary Beever",
      "Andrew J. Kammerer"
    ],
    "published": "2025-05-21",
    "abstract": "Accurately estimating the refractive environment over multiple frequencies\nwithin the marine atmospheric boundary layer is crucial for the effective\ndeployment of radar technologies. Traditional parabolic equation simulations,\nwhile effective, can be computationally expensive and time-intensive, limiting\ntheir practical application. This communication explores a novel approach using\ndeep neural networks to estimate the pattern propagation factor, a critical\nparameter for characterizing environmental impacts on signal propagation.\nImage-to-image translation generators designed to ingest modified refractivity\ndata and generate predictions of pattern propagation factors over the same\ndomain were developed. Findings demonstrate that deep neural networks can be\ntrained to analyze multiple frequencies and reasonably predict the pattern\npropagation factor, offering an alternative to traditional methods.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis",
    "url": "http://arxiv.org/abs/2505.14285v1",
    "authors": [
      "Eirini Panteli",
      "Paulo E. Santos",
      "Nabil Humphrey"
    ],
    "published": "2025-05-20",
    "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Terrain-aware Deep Learning for Wind Energy Applications: From Kilometer-scale Forecasts to Fine Wind Fields",
    "url": "http://arxiv.org/abs/2505.12732v1",
    "authors": [
      "Chensen Lin",
      "Ruian Tie",
      "Shihong Yi",
      "Xiaohui Zhong",
      "Hao Li"
    ],
    "published": "2025-05-19",
    "abstract": "High-resolution wind information is essential for wind energy planning and\npower forecasting, particularly in regions with complex terrain. However, most\nAI-based weather forecasting models operate at kilometer-scale resolution,\nconstrained by the reanalysis datasets they are trained on. Here we introduce\nFuXi-CFD, an AI-based downscaling framework designed to generate detailed\nthree-dimensional wind fields at 30-meter horizontal resolution, using only\ncoarse-resolution atmospheric inputs. The model is trained on a large-scale\ndataset generated via computational fluid dynamics (CFD), encompassing a wide\nrange of terrain types, surface roughness, and inflow conditions. Remarkably,\nFuXi-CFD predicts full 3D wind structures -- including vertical wind and\nturbulent kinetic energy -- based solely on horizontal wind input at 10 meters\nabove ground, the typical output of AI-based forecast systems. It achieves\nCFD-comparable accuracy while reducing inference time from hours to seconds. By\nbridging the resolution gap between regional forecasts and site-specific wind\ndynamics, FuXi-CFD offers a scalable and operationally efficient solution to\nsupport the growing demands of renewable energy deployment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Improving the Predictability of the Madden-Julian Oscillation at Subseasonal Scales with Gaussian Process Models",
    "url": "http://arxiv.org/abs/2505.15934v1",
    "authors": [
      "Haoyuan Chen",
      "Emil Constantinescu",
      "Vishwas Rao",
      "Cristiana Stan"
    ],
    "published": "2025-05-21",
    "abstract": "The Madden--Julian Oscillation (MJO) is an influential climate phenomenon\nthat plays a vital role in modulating global weather patterns. In spite of the\nimprovement in MJO predictions made by machine learning algorithms, such as\nneural networks, most of them cannot provide the uncertainty levels in the MJO\nforecasts directly. To address this problem, we develop a nonparametric\nstrategy based on Gaussian process (GP) models. We calibrate GPs using\nempirical correlations and we propose a posteriori covariance correction.\nNumerical experiments demonstrate that our model has better prediction skills\nthan the ANN models for the first five lead days. Additionally, our posteriori\ncovariance correction extends the probabilistic coverage by more than three\nweeks.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Multi-Tiered Bayesian Network Coastal Compound Flood Analysis Framework",
    "url": "http://arxiv.org/abs/2505.15520v1",
    "authors": [
      "Ziyue Liu",
      "Meredith L. Carr",
      "Norberto C. Nadal-Caraballo",
      "Luke A. Aucoin",
      "Madison C. Yawn",
      "Michelle T. Bensi"
    ],
    "published": "2025-05-21",
    "abstract": "Coastal compound floods (CCFs) are triggered by the interaction of multiple\nmechanisms, such as storm surges, storm rainfall, tides, and river flow. These\nevents can bring significant damage to communities, and there is an increasing\ndemand for accurate and efficient probabilistic analyses of CCFs to support\nrisk assessments and decision-making. In this study, a multi-tiered Bayesian\nnetwork (BN) CCF analysis framework is established. In this framework, multiple\ntiers of BN models with different complexities are designed for application\nwith varying levels of data availability and computational resources. A case\nstudy is conducted in New Orleans, LA, to demonstrate this framework. In the\nTier-1 BN model, storm surges and river flow are incorporated based on\nhydrodynamic simulations. A seasonality node is used to capture the dependence\nbetween concurrent river flow and tropical cyclone (TC) parameters. In the\nTier-2 BN model, joint distribution models of TC parameters are built for\nseparate TC intensity categories. TC-induced rainfall is modeled as input to\nhydraulic simulations. In the Tier-3 BN model, potential variations of\nmeteorological conditions are incorporated by quantifying their effects on TC\nactivity and coastal water level. Flood antecedent conditions are also\nincorporated to more completely represent the conditions contributing to flood\nseverity. In this case study, a series of joint distribution, numerical,\nmachine learning, and experimental models are used to compute conditional\nprobability tables needed for BNs. A series of probabilistic analyses is\nperformed based on these BN models, including CCF hazard curve construction and\nCCF deaggregation. The results of the analysis demonstrate the promise of this\nframework in performing CCF hazard analysis under varying levels of resource\navailability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning",
    "url": "http://arxiv.org/abs/2505.12904v1",
    "authors": [
      "Hilde I. Hummel",
      "Arwin Gansekoele",
      "Sandjai Bhulai",
      "Rob van der Mei"
    ],
    "published": "2025-05-19",
    "abstract": "The increasing level of sound pollution in marine environments poses an\nincreased threat to ocean health, making it crucial to monitor underwater\nnoise. By monitoring this noise, the sources responsible for this pollution can\nbe mapped. Monitoring is performed by passively listening to these sounds. This\ngenerates a large amount of data records, capturing a mix of sound sources such\nas ship activities and marine mammal vocalizations. Although machine learning\noffers a promising solution for automatic sound classification, current\nstate-of-the-art methods implement supervised learning. This requires a large\namount of high-quality labeled data that is not publicly available. In\ncontrast, a massive amount of lower-quality unlabeled data is publicly\navailable, offering the opportunity to explore unsupervised learning\ntechniques. This research explores this possibility by implementing an\nunsupervised Contrastive Learning approach. Here, a Conformer-based encoder is\noptimized by the so-called Variance-Invariance-Covariance Regularization loss\nfunction on these lower-quality unlabeled data and the translation to the\nlabeled data is made. Through classification tasks involving recognizing ship\ntypes and marine mammal vocalizations, our method demonstrates to produce\nrobust and generalized embeddings. This shows to potential of unsupervised\nmethods for various automatic underwater acoustic analysis tasks.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
    "url": "http://arxiv.org/abs/2505.21375v1",
    "authors": [
      "Fengxiang Wang",
      "Mingshuo Chen",
      "Yueying Li",
      "Di Wang",
      "Haotian Wang",
      "Zonghao Guo",
      "Zefan Wang",
      "Boqi Shan",
      "Long Lan",
      "Yulin Wang",
      "Hongzhen Wang",
      "Wenjing Yang",
      "Bo Du",
      "Jing Zhang"
    ],
    "published": "2025-05-27",
    "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data\nfor Earth observation but pose challenges for existing multimodal foundation\nmodels due to two key bottlenecks: (1) limited availability of UHR training\ndata, and (2) token explosion caused by the large image size. To address data\nscarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA\n(avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in\nRS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,\nour pilot studies reveal significant redundancy in RS images: crucial\ninformation is concentrated in a small subset of object-centric tokens, while\npruning background tokens (e.g., ocean or forest) can even improve performance.\nMotivated by these findings, we propose two strategies: Background Token\nPruning and Anchored Token Selection, to reduce the memory footprint while\npreserving key semantics.Integrating these techniques, we introduce\nGeoLLaVA-8K, the first RS-focused multimodal large language model capable of\nhandling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework.\nTrained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art\non the XLRS-Bench.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping",
    "url": "http://arxiv.org/abs/2505.21357v2",
    "authors": [
      "Wenyuan Li",
      "Shunlin Liang",
      "Keyan Chen",
      "Yongzhe Chen",
      "Han Ma",
      "Jianglei Xu",
      "Yichuan Ma",
      "Shikang Guan",
      "Husheng Fang",
      "Zhenwei Shi"
    ],
    "published": "2025-05-27",
    "abstract": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nhttps://github.com/flyakon/AgriFM.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2505.19447v1",
    "authors": [
      "Hengtong Shen",
      "Haiyan Gu",
      "Haitao Li",
      "Yi Yang",
      "Agen qiu"
    ],
    "published": "2025-05-26",
    "abstract": "Self-Supervised Learning (SSL) enables us to pre-train foundation models\nwithout costly labeled data. Among SSL methods, Contrastive Learning (CL)\nmethods are better at obtaining accurate semantic representations in noise\ninterference. However, due to the significant domain gap, while CL methods have\nachieved great success in many computer vision tasks, they still require\nspecific adaptation for Remote Sensing (RS) images. To this end, we present a\nnovel self-supervised method called PerA, which produces all-purpose RS\nfeatures through semantically Perfectly Aligned sample pairs. Specifically,\nPerA obtains features from sampled views by applying spatially disjoint masks\nto augmented images rather than random cropping. With disjoint masks, we divide\npatches from different views into different parts that are semantically aligned\nbut inconsistent in appearance. Our framework provides high-quality features by\nensuring consistency between teacher and student and predicting learnable mask\ntokens. Compared to previous contrastive methods, our method demonstrates\nhigher memory efficiency and can be trained with larger batches due to its\nsparse inputs. We also collect an unlabeled pre-training dataset, which\ncontains about 5 million RS images. We conducted experiments on multiple\ndownstream task datasets and achieved performance comparable to previous\nstate-of-the-art methods with a limited model scale, which verified the\nsuperiority of our method. We hope this work will contribute to practical\nremote sensing interpretation works.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation",
    "url": "http://arxiv.org/abs/2505.23597v1",
    "authors": [
      "Georgios Voulgaris"
    ],
    "published": "2025-05-29",
    "abstract": "The accurate semantic segmentation of tree crowns within remotely sensed data\nis crucial for scientific endeavours such as forest management, biodiversity\nstudies, and carbon sequestration quantification. However, precise segmentation\nremains challenging due to complexities in the forest canopy, including\nshadows, intricate backgrounds, scale variations, and subtle spectral\ndifferences among tree species. Compared to the traditional methods, Deep\nLearning models improve accuracy by extracting informative and discriminative\nfeatures, but often fall short in capturing the aforementioned complexities.\n  To address these challenges, we propose PerceptiveNet, a novel model\nincorporating a Logarithmic Gabor-parameterised convolutional layer with\ntrainable filter parameters, alongside a backbone that extracts salient\nfeatures while capturing extensive context and spatial information through a\nwider receptive field. We investigate the impact of Log-Gabor, Gabor, and\nstandard convolutional layers on semantic segmentation performance through\nextensive experimentation. Additionally, we conduct an ablation study to assess\nthe contributions of individual layers and their combinations to overall model\nperformance, and we evaluate PerceptiveNet as a backbone within a novel hybrid\nCNN-Transformer model. Our results outperform state-of-the-art models,\ndemonstrating significant performance improvements on a tree crown dataset\nwhile generalising across domains, including two benchmark aerial scene\nsemantic segmentation datasets with varying complexities.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Collaborative Learning for Unsupervised Multimodal Remote Sensing Image Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based Image Translation",
    "url": "http://arxiv.org/abs/2505.22000v1",
    "authors": [
      "Xiaochen Wei",
      "Weiwei Guo",
      "Wenxian Yu"
    ],
    "published": "2025-05-28",
    "abstract": "The substantial modality-induced variations in radiometric, texture, and\nstructural characteristics pose significant challenges for the accurate\nregistration of multimodal images. While supervised deep learning methods have\ndemonstrated strong performance, they often rely on large-scale annotated\ndatasets, limiting their practical application. Traditional unsupervised\nmethods usually optimize registration by minimizing differences in feature\nrepresentations, yet often fail to robustly capture geometric discrepancies,\nparticularly under substantial spatial and radiometric variations, thus\nhindering convergence stability. To address these challenges, we propose a\nCollaborative Learning framework for Unsupervised Multimodal Image\nRegistration, named CoLReg, which reformulates unsupervised registration\nlearning into a collaborative training paradigm comprising three components:\n(1) a cross-modal image translation network, MIMGCD, which employs a learnable\nMaximum Index Map (MIM) guided conditional diffusion model to synthesize\nmodality-consistent image pairs; (2) a self-supervised intermediate\nregistration network which learns to estimate geometric transformations using\naccurate displacement labels derived from MIMGCD outputs; (3) a distilled\ncross-modal registration network trained with pseudo-label predicted by the\nintermediate network. The three networks are jointly optimized through an\nalternating training strategy wherein each network enhances the performance of\nthe others. This mutual collaboration progressively reduces modality\ndiscrepancies, enhances the quality of pseudo-labels, and improves registration\naccuracy. Extensive experimental results on multiple datasets demonstrate that\nour ColReg achieves competitive or superior performance compared to\nstate-of-the-art unsupervised approaches and even surpasses several supervised\nbaselines.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Kernel Space Diffusion Model for Efficient Remote Sensing Pansharpening",
    "url": "http://arxiv.org/abs/2505.18991v1",
    "authors": [
      "Hancong Jin",
      "Zihan Cao",
      "Liangjian Deng"
    ],
    "published": "2025-05-25",
    "abstract": "Pansharpening is a fundamental task in remote sensing that integrates\nhigh-resolution panchromatic imagery (PAN) with low-resolution multispectral\nimagery (LRMS) to produce an enhanced image with both high spatial and spectral\nresolution. Despite significant progress in deep learning-based approaches,\nexisting methods often fail to capture the global priors inherent in remote\nsensing data distributions. Diffusion-based models have recently emerged as\npromising solutions due to their powerful distribution mapping capabilities;\nhowever, they suffer from significant inference latency, which limits their\npractical applicability. In this work, we propose the Kernel Space Diffusion\nModel (KSDiff), a novel approach that leverages diffusion processes in a latent\nspace to generate convolutional kernels enriched with global contextual\ninformation, thereby improving pansharpening quality while enabling faster\ninference. Specifically, KSDiff constructs these kernels through the\nintegration of a low-rank core tensor generator and a unified factor generator,\norchestrated by a structure-aware multi-head attention mechanism. We further\nintroduce a two-stage training strategy tailored for pansharpening, enabling\nKSDiff to serve as a framework for enhancing existing pansharpening\narchitectures. Experiments on three widely used datasets, including\nWorldView-3, GaoFen-2, and QuickBird, demonstrate the superior performance of\nKSDiff both qualitatively and quantitatively. Code will be released upon\npossible acceptance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation",
    "url": "http://arxiv.org/abs/2505.18461v1",
    "authors": [
      "Morteza Karimzadeh",
      "Zhongying Wang",
      "James L. Crooks"
    ],
    "published": "2025-05-24",
    "abstract": "Deep learning models have demonstrated success in geospatial applications,\nyet quantifying the role of geolocation information in enhancing model\nperformance and geographic generalizability remains underexplored. A new\ngeneration of location encoders have emerged with the goal of capturing\nattributes present at any given location for downstream use in predictive\nmodeling. Being a nascent area of research, their evaluation has remained\nlargely limited to static tasks such as species distributions or average\ntemperature mapping. In this paper, we discuss and quantify the impact of\nincorporating geolocation into deep learning for a real-world application\ndomain that is characteristically dynamic (with fast temporal change) and\nspatially heterogeneous at high resolutions: estimating surface-level daily\nPM2.5 levels using remotely sensed and ground-level data. We build on a\nrecently published deep learning-based PM2.5 estimation model that achieves\nstate-of-the-art performance on data observed in the contiguous United States.\nWe examine three approaches for incorporating geolocation: excluding\ngeolocation as a baseline, using raw geographic coordinates, and leveraging\npretrained location encoders. We evaluate each approach under within-region\n(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance\nmetrics indicate that while na\\\"ive incorporation of raw geographic coordinates\nimproves within-region performance by retaining the interpolative value of\ngeographic location, it can hinder generalizability across regions. In\ncontrast, pretrained location encoders like GeoCLIP enhance predictive\nperformance and geographic generalizability for both WR and OoR scenarios.\nHowever, qualitative analysis reveals artifact patterns caused by high-degree\nbasis functions and sparse upstream samples in certain areas, and ablation\nresults indicate varying performance among location encoders...",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences",
    "url": "http://arxiv.org/abs/2505.22008v1",
    "authors": [
      "Jing-An Sun",
      "Hang Fan",
      "Junchao Gong",
      "Ben Fei",
      "Kun Chen",
      "Fenghua Ling",
      "Wenlong Zhang",
      "Wanghan Xu",
      "Li Yan",
      "Pierre Gentine",
      "Lei Bai"
    ],
    "published": "2025-05-28",
    "abstract": "Data assimilation (DA) aims to estimate the full state of a dynamical system\nby combining partial and noisy observations with a prior model forecast,\ncommonly referred to as the background. In atmospheric applications, this\nproblem is fundamentally ill-posed due to the sparsity of observations relative\nto the high-dimensional state space. Traditional methods address this challenge\nby simplifying background priors to regularize the solution, which are\nempirical and require continual tuning for application. Inspired by alignment\ntechniques in text-to-image diffusion models, we propose Align-DA, which\nformulates DA as a generative process and uses reward signals to guide\nbackground priors, replacing manual tuning with data-driven alignment.\nSpecifically, we train a score-based model in the latent space to approximate\nthe background-conditioned prior, and align it using three complementary reward\nsignals for DA: (1) assimilation accuracy, (2) forecast skill initialized from\nthe assimilated state, and (3) physical adherence of the analysis fields.\nExperiments with multiple reward signals demonstrate consistent improvements in\nanalysis quality across different evaluation metrics and observation-guidance\nstrategies. These results show that preference alignment, implemented as a soft\nconstraint, can automatically adapt complex background priors tailored to DA,\noffering a promising new direction for advancing the field.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs",
    "url": "http://arxiv.org/abs/2505.22686v1",
    "authors": [
      "Ange-Clement Akazan",
      "Verlon Roel Mbingui",
      "Gnankan Landry Regis N'guessan",
      "Issa Karambal"
    ],
    "published": "2025-05-27",
    "abstract": "Weather forecasting is crucial for managing risks and economic planning,\nparticularly in tropical Africa, where extreme events severely impact\nlivelihoods. Yet, existing forecasting methods often struggle with the region's\ncomplex, non-linear weather patterns. This study benchmarks deep recurrent\nneural networks such as $\\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and\nKolmogorov-Arnold-based models $(\\texttt{KAN} and \\texttt{TKAN})$ for daily\nforecasting of temperature, precipitation, and pressure in two tropical cities:\nAbidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce\ntwo customized variants of $ \\texttt{TKAN}$ that replace its original\n$\\texttt{SiLU}$ activation function with $ \\texttt{GeLU}$ and \\texttt{MiSH},\nrespectively. Using station-level meteorological data spanning from 2010 to\n2024, we evaluate all the models on standard regression metrics. $\\texttt{KAN}$\nachieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,\n$\\texttt{MSE} < 0.0014~^\\circ C ^2$), while $\\texttt{TKAN}$ variants minimize\nabsolute errors for precipitation forecasting in low-rainfall regimes. The\ncustomized $\\texttt{TKAN}$ models demonstrate improvements over the standard\n$\\texttt{TKAN}$ across both datasets. Classical \\texttt{RNNs} remain highly\ncompetitive for atmospheric pressure ($R^2 \\approx 0.83{-}0.86$), outperforming\n$\\texttt{KAN}$-based models in this task. These results highlight the potential\nof spline-based neural architectures for efficient and data-efficient\nforecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM",
      "RNN"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation",
    "url": "http://arxiv.org/abs/2505.21020v1",
    "authors": [
      "Yuan Gao",
      "Ruiqi Shu",
      "Hao Wu",
      "Fan Xu",
      "Yanfei Xiang",
      "Ruijian Gou",
      "Qingsong Wen",
      "Xian Wu",
      "Xiaomeng Huang"
    ],
    "published": "2025-05-27",
    "abstract": "Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically\nimportant for marine research, yet remains challenging due to its substantial\nthermal inertia and extended time delay. Machine learning (ML)-based models\nhave demonstrated significant advancements in simulation accuracy and\ncomputational efficiency compared to traditional numerical methods.\nNevertheless, a significant limitation of current ML models for S2S ocean\nsimulation is their inadequate incorporation of physical consistency and the\nslow-changing properties of the ocean system. In this work, we propose a neural\nocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive\ngraph neural network to emulate diverse physical phenomena associated with\nocean systems effectively. Specifically, we propose a multi-stage framework\ntailored to model the ocean's slowly changing nature. Additionally, we\nintroduce a multi-scale interactive messaging module to capture complex\ndynamical behaviors, such as gradient changes and multiplicative coupling\nrelationships inherent in ocean dynamics. Extensive experimental evaluations\nconfirm that our proposed NeuralOM outperforms state-of-the-art models in S2S\nand extreme event simulation. The codes are available at\nhttps://github.com/YuanGao-YG/NeuralOM.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Automated data curation for self-supervised learning in underwater acoustic analysis",
    "url": "http://arxiv.org/abs/2505.20066v1",
    "authors": [
      "Hilde I Hummel",
      "Sandjai Bhulai",
      "Burooj Ghani",
      "Rob van der Mei"
    ],
    "published": "2025-05-26",
    "abstract": "The sustainability of the ocean ecosystem is threatened by increased levels\nof sound pollution, making monitoring crucial to understand its variability and\nimpact. Passive acoustic monitoring (PAM) systems collect a large amount of\nunderwater sound recordings, but the large volume of data makes manual analysis\nimpossible, creating the need for automation. Although machine learning offers\na potential solution, most underwater acoustic recordings are unlabeled.\nSelf-supervised learning models have demonstrated success in learning from\nlarge-scale unlabeled data in various domains like computer vision, Natural\nLanguage Processing, and audio. However, these models require large, diverse,\nand balanced datasets for training in order to generalize well. To address\nthis, a fully automated self-supervised data curation pipeline is proposed to\ncreate a diverse and balanced dataset from raw PAM data. It integrates\nAutomatic Identification System (AIS) data with recordings from various\nhydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw\naudio data is sampled and then combined with AIS samples to create a balanced\nand diverse dataset. The resulting curated dataset enables the development of\nself-supervised learning models, facilitating various tasks such as monitoring\nmarine mammals and assessing sound pollution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Accelerated Bayesian calibration and uncertainty quantification of RANS turbulence model parameters for stratified atmospheric boundary layer flows",
    "url": "http://arxiv.org/abs/2505.18756v1",
    "authors": [
      "E. Y. Shin",
      "M. F. Howland"
    ],
    "published": "2025-05-24",
    "abstract": "In operational weather models, the effects of turbulence in the atmospheric\nboundary layer (ABL) on the resolved flow are modeled using turbulence\nparameterizations. These parameterizations typically use a predetermined set of\nmodel parameters that are tuned to limited data from canonical flows. Using\nthese fixed parameters results in deterministic predictions that neglect\nuncertainty in the unresolved turbulence processes. In this study, we perform a\nmachine learning-accelerated Bayesian inversion of a single-column model of the\nABL. This approach is used to calibrate and quantify uncertainty in model\nparameters of Reynolds-averaged Navier-Stokes turbulence models. Following\nverification of the uncertainty quantification methodology, we learn the\nparameters and their uncertainties in two different turbulence models\nconditioned on scale-resolving large-eddy simulation data over a range of ABL\nstabilities. We show how Bayesian inversion of a numerical model improves flow\npredictions by investigating the underlying mean momentum budgets. Further, we\nshow that uncertainty quantification based on neutral surface layer data\nrecovers the relationships between parameters derived from theoretical\nmodeling, but that learning the parameters based on stable ABL data or data\nfrom outside the surface layer can lead to different relationships than neutral\nsurface layer theory. Systematic uncertainty reduction methods reveal that (1)\nsampling wind speed up to the ABL height can reduce uncertainty in key model\nparameters by up to 84%, and (2) assimilating fluid flow quantities beyond\nfirst-order moment statistics can further reduce uncertainty in ways that wind\nspeed assimilation alone cannot achieve. The parameters learned using Bayesian\nuncertainty quantification generally yield lower error than standard\ndeterministic parameters in out-of-sample tests and provide uncertainty\nintervals on predictions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A validated coupled three-dimensional hydrodynamic and spectral wind-wave model for the western north Atlantic Ocean",
    "url": "http://arxiv.org/abs/2505.18803v1",
    "authors": [
      "Maria Venolia",
      "Reza Marsooli",
      "Jaime R. Calzada"
    ],
    "published": "2025-05-24",
    "abstract": "Wind-wave and ocean current interactions affect critical coastal and oceanic\nprocesses, yet modeling these interactions presents significant challenges. The\nwestern North Atlantic Ocean provides an ideal test environment for coupled\nhydrodynamics and wind wave models, thanks to its energetic surface currents\nsuch as the Gulf Stream. This study evaluates a high-resolution coupled SCHISM\nWWM III model, utilizing NOAA's 'STOFS-3D-Atlantic' computational mesh, while\nincorporating three-dimensional baroclinic dynamics to account for density\nstratification effects. We evaluate the model's calculated water level and\ntidal predictions against NOAA tide gauge measurements during December 2016.\nThe coupled model demonstrates robust skills in reproducing tidal constituents,\nnon-tidal components, and total water level predictions along the U.S. East and\nGulf of Mexico Coasts. In addition, we systematically evaluate three wave\nphysics parameterizations (Ardhuin, Makin and Stam, and Cycle Three) in the\nspectral wave model to quantify their effects on the modeled wave\ncharacteristics. This validated modeling framework enhances our ability to\nunderstand and predict complex coastal and oceanic processes, offering\nsignificant applications for coastal management, maritime operations, and\nclimate adaptation planning throughout the western North Atlantic region.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery",
    "url": "http://arxiv.org/abs/2506.03388v1",
    "authors": [
      "Pengyu Chen",
      "Xiao Huang",
      "Teng Fei",
      "Sicheng Wang"
    ],
    "published": "2025-06-03",
    "abstract": "Environmental soundscapes convey substantial ecological and social\ninformation regarding urban environments; however, their potential remains\nlargely untapped in large-scale geographic analysis. In this study, we\ninvestigate the extent to which urban sounds correspond with visual scenes by\ncomparing various visual representation strategies in capturing acoustic\nsemantics. We employ a multimodal approach that integrates geo-referenced sound\nrecordings with both street-level and remote sensing imagery across three major\nglobal cities: London, New York, and Tokyo. Utilizing the AST model for audio,\nalong with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV\nfor semantic segmentation, we extract embeddings and class-level features to\nevaluate cross-modal similarity. The results indicate that street view\nembeddings demonstrate stronger alignment with environmental sounds compared to\nsegmentation outputs, whereas remote sensing segmentation is more effective in\ninterpreting ecological categories through a Biophony--Geophony--Anthrophony\n(BGA) framework. These findings imply that embedding-based models offer\nsuperior semantic alignment, while segmentation-based methods provide\ninterpretable links between visual structure and acoustic ecology. This work\nadvances the burgeoning field of multimodal urban sensing by offering novel\nperspectives for incorporating sound into geospatial analysis.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings",
    "url": "http://arxiv.org/abs/2506.02868v1",
    "authors": [
      "Amal S. Perera",
      "David Fernandez",
      "Chandi Witharana",
      "Elias Manos",
      "Michael Pimenta",
      "Anna K. Liljedahl",
      "Ingmar Nitze",
      "Yili Yang",
      "Todd Nicholson",
      "Chia-Yu Hsu",
      "Wenwen Li",
      "Guido Grosse"
    ],
    "published": "2025-06-03",
    "abstract": "Accurate mapping of permafrost landforms, thaw disturbances, and human-built\ninfrastructure at pan-Arctic scale using sub-meter satellite imagery is\nincreasingly critical. Handling petabyte-scale image data requires\nhigh-performance computing and robust feature detection models. While\nconvolutional neural network (CNN)-based deep learning approaches are widely\nused for remote sensing (RS),similar to the success in transformer based large\nlanguage models, Vision Transformers (ViTs) offer advantages in capturing\nlong-range dependencies and global context via attention mechanisms. ViTs\nsupport pretraining via self-supervised learning-addressing the common\nlimitation of labeled data in Arctic feature detection and outperform CNNs on\nbenchmark datasets. Arctic also poses challenges for model generalization,\nespecially when features with the same semantic class exhibit diverse spectral\ncharacteristics. To address these issues for Arctic feature detection, we\nintegrate geospatial location embeddings into ViTs to improve adaptation across\nregions. This work investigates: (1) the suitability of pre-trained ViTs as\nfeature extractors for high-resolution Arctic remote sensing tasks, and (2) the\nbenefit of combining image and location embeddings. Using previously published\ndatasets for Arctic feature detection, we evaluate our models on three\ntasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and\nhuman-built infrastructure. We empirically explore multiple configurations to\nfuse image embeddings and location embeddings. Results show that ViTs with\nlocation embeddings outperform prior CNN-based models on two of the three tasks\nincluding F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating\nthe potential of transformer-based models with spatial awareness for Arctic RS\napplications.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale",
    "url": "http://arxiv.org/abs/2506.01297v3",
    "authors": [
      "Ya Wen",
      "Jixuan Cai",
      "Qiyao Ma",
      "Linyan Li",
      "Xinhua Chen",
      "Chris Webster",
      "Yulun Zhou"
    ],
    "published": "2025-06-02",
    "abstract": "Representation learning of geospatial locations remains a core challenge in\nachieving general geospatial intelligence. Current embedding methods often lack\nversatility, limiting their utility across diverse tasks in both human and\nnatural domains. We present MobCLIP, the first nationwide general-purpose\nlocation encoder, integrating an unprecedented diversity of data modalities\nthrough effective and scalable multimodal fusion. Adopting a novel CLIP-based\narchitecture, our framework aligns 100M+ POIs, nationwide remote sensing\nimagery, and structured demographic statistics with a billion-edge mobility\ngraph. By tokenizing spatial locations into grid cells inspired by Vision\nTransformers, we establish a unified representation space bridging mobility\npatterns and multimodal features. To rigorously evaluate the general-purpose\neffectiveness of MobCLIP, we construct a benchmark dataset composed of 11\ndownstream prediction tasks across social, economic, and natural domains.\nExperiments show that MobCLIP, with four input modalities and a compact\n128-dimensional representation space, achieves significantly superior\ngeneral-purpose predictive performances than state-of-the-art models by an\naverage of 35%. Thanks to the effective integration of human-centric\nmodalities, the performance gain is particularly profound in human-centric\ntasks, such as energy consumption (+260%), offline retail consumption amount\n(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we\nfurther demonstrate the scaling behavior in geospatial representation learning.\nWe open-source code and pretrained models at:\nhttps://github.com/ylzhouchris/MobCLIP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "LLM",
      "CLIP"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels",
    "url": "http://arxiv.org/abs/2506.02534v1",
    "authors": [
      "Sining Chen",
      "Yilei Shi",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-06-03",
    "abstract": "Monocular height estimation is considered the most efficient and\ncost-effective means of 3D perception in remote sensing, and it has attracted\nmuch attention since the emergence of deep learning. While training neural\nnetworks requires a large amount of data, data with perfect labels are scarce\nand only available within developed regions. The trained models therefore lack\ngeneralizability, which limits the potential for large-scale application of\nexisting methods. We tackle this problem for the first time, by introducing\ndata with imperfect labels into training pixel-wise height estimation networks,\nincluding labels that are incomplete, inexact, and inaccurate compared to\nhigh-quality labels. We propose an ensemble-based pipeline compatible with any\nmonocular height estimation network. Taking the challenges of noisy labels,\ndomain shift, and long-tailed distribution of height values into consideration,\nwe carefully design the architecture and loss functions to leverage the\ninformation concealed in imperfect labels using weak supervision through\nbalanced soft losses and ordinal constraints. We conduct extensive experiments\non two datasets with different resolutions, DFC23 (0.5 to 1 m) and GBH (3 m).\nThe results indicate that the proposed pipeline outperforms baselines by\nachieving more balanced performance across various domains, leading to\nimprovements of average root mean square errors up to 22.94 %, and 18.62 % on\nDFC23 and GBH, respectively. The efficacy of each design component is validated\nthrough ablation studies. Code is available at\nhttps://github.com/zhu-xlab/weakim2h.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Image Restoration Learning via Noisy Supervision in the Fourier Domain",
    "url": "http://arxiv.org/abs/2506.00564v1",
    "authors": [
      "Haosen Liu",
      "Jiahao Liu",
      "Shan Tan",
      "Edmund Y. Lam"
    ],
    "published": "2025-05-31",
    "abstract": "Noisy supervision refers to supervising image restoration learning with noisy\ntargets. It can alleviate the data collection burden and enhance the practical\napplicability of deep learning techniques. However, existing methods suffer\nfrom two key drawbacks. Firstly, they are ineffective in handling spatially\ncorrelated noise commonly observed in practical applications such as low-light\nimaging and remote sensing. Secondly, they rely on pixel-wise loss functions\nthat only provide limited supervision information. This work addresses these\nchallenges by leveraging the Fourier domain. We highlight that the Fourier\ncoefficients of spatially correlated noise exhibit sparsity and independence,\nmaking them easier to handle. Additionally, Fourier coefficients contain global\ninformation, enabling more significant supervision. Motivated by these\ninsights, we propose to establish noisy supervision in the Fourier domain. We\nfirst prove that Fourier coefficients of a wide range of noise converge in\ndistribution to the Gaussian distribution. Exploiting this statistical\nproperty, we establish the equivalence between using noisy targets and clean\ntargets in the Fourier domain. This leads to a unified learning framework\napplicable to various image restoration tasks, diverse network architectures,\nand different noise models. Extensive experiments validate the outstanding\nperformance of this framework in terms of both quantitative indices and\nperceptual quality.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review",
    "url": "http://arxiv.org/abs/2506.03938v1",
    "authors": [
      "C\u00e9dric L\u00e9onard",
      "Dirk Stober",
      "Martin Schulz"
    ],
    "published": "2025-06-04",
    "abstract": "New UAV technologies and the NewSpace era are transforming Earth Observation\nmissions and data acquisition. Numerous small platforms generate large data\nvolume, straining bandwidth and requiring onboard decision-making to transmit\nhigh-quality information in time. While Machine Learning allows real-time\nautonomous processing, FPGAs balance performance with adaptability to\nmission-specific requirements, enabling onboard deployment. This review\nsystematically analyzes 66 experiments deploying ML models on FPGAs for Remote\nSensing applications. We introduce two distinct taxonomies to capture both\nefficient model architectures and FPGA implementation strategies. For\ntransparency and reproducibility, we follow PRISMA 2020 guidelines and share\nall data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Validating remotely sensed biomass estimates with forest inventory data in the western US",
    "url": "http://arxiv.org/abs/2506.03120v1",
    "authors": [
      "Xiuyu Cao",
      "Joseph O. Sexton",
      "Panshi Wang",
      "Dimitrios Gounaridis",
      "Neil H. Carter",
      "Kai Zhu"
    ],
    "published": "2025-06-03",
    "abstract": "Monitoring aboveground biomass (AGB) and its density (AGBD) at high\nresolution is essential for carbon accounting and ecosystem management. While\nNASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission\nprovides globally distributed reference measurements for AGBD estimation, the\nmajority of commercial remote sensing products based on GEDI remain without\nrigorous or independent validation. Here, we present an independent regional\nvalidation of an AGBD dataset offered by terraPulse, Inc., based on independent\nreference data from the US Forest Service Forest Inventory and Analysis (FIA)\nprogram. Aggregated to 64,000-hectare hexagons and US counties across the US\nstates of Utah, Nevada, and Washington, we found very strong agreement between\nterraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE =\n26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale,\nagreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95.\nSpatial and statistical analyses indicated that terraPulse AGBD values tended\nto exceed FIA estimates in non-forest areas, likely due to FIA's limited\nsampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited\nlower values in high-biomass forests, likely due to saturation effects in its\noptical remote-sensing covariates. This study advances operational carbon\nmonitoring by delivering a scalable framework for comprehensive AGBD validation\nusing independent FIA data, as well as a benchmark validation of a new\ncommercial dataset for global biomass monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery",
    "url": "http://arxiv.org/abs/2506.03114v1",
    "authors": [
      "Michelle Chen",
      "David Russell",
      "Amritha Pallavoor",
      "Derek Young",
      "Jane Wu"
    ],
    "published": "2025-06-03",
    "abstract": "Large-scale delineation of individual trees from remote sensing imagery is\ncrucial to the advancement of ecological research, particularly as climate\nchange and other environmental factors rapidly transform forest landscapes\nacross the world. Current RGB tree segmentation methods rely on training\nspecialized machine learning models with labeled tree datasets. While these\nlearning-based approaches can outperform manual data collection when accurate,\nthe existing models still depend on training data that's hard to scale. In this\npaper, we investigate the efficacy of using a state-of-the-art image\nsegmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for\nindividual tree detection and segmentation. We evaluate a pretrained SAM2 model\non two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot\ntransfer by using predictions from an existing tree detection model as prompts.\nOur results suggest that SAM2 not only has impressive generalization\ncapabilities, but also can form a natural synergy with specialized methods\ntrained on in-domain labeled data. We find that applying large pretrained\nmodels to problems in remote sensing is a promising avenue for future progress.\nWe make our code available at:\nhttps://github.com/open-forest-observatory/tree-detection-framework.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Improving Post-Processing for Quantitative Precipitation Forecasting Using Deep Learning: Learning Precipitation Physics from High-Resolution Observations",
    "url": "http://arxiv.org/abs/2506.03842v1",
    "authors": [
      "ChangJae Lee",
      "Heecheol Yang",
      "Byeonggwon Kim"
    ],
    "published": "2025-06-04",
    "abstract": "Accurate quantitative precipitation forecasting (QPF) remains one of the main\nchallenges in numerical weather prediction (NWP), primarily due to the\ndifficulty of representing the full complexity of atmospheric microphysics\nthrough parameterization schemes. This study introduces a deep learning-based\npost-processing model, DL-QPF, which diagnoses precipitation fields from\nmeteorological forecasts by learning directly from high-resolution radar\nestimates precipitation. The DL-QPF model is constructed using a\nPatch-conditional Generative Adversarial Network (Patch-cGAN) architecture\ncombined with a U-Net generator and a discriminator. The generator learns\nmeteorological features relevant to precipitation, while the adversarial loss\nfrom the discriminator encourages the generation of realistic rainfall patterns\nand distributions. Training is performed on three years of warm-season data\nover the Korean Peninsula, with input variables derived from ECMWF's Integrated\nForecasting System High-Resolution forecast (IFS-HRES). Model verification is\nconducted against multiple reference models, including global (IFS-HRES, KIM),\nregional (KIM-Regional, KIM-LENS), and AI-based (GraphCast) forecasts.\nVerification across multiple rainfall thresholds shows that DL-QPF achieves a\nfrequency bias near one and superior success ratios. Particularly for heavy and\nintense rainfall events, DL-QPF outperforms both conventional NWP and an AI\nmodel, demonstrating improved skill in capturing high-intensity precipitation.\nThis study highlights the potential of observational data-driven deep learning\napproaches in post-processing QPF. By directly learning from observations,\nDL-QPF reduces systematic biases and enhances the realism of forecasted\nrainfall distributions. These results demonstrate the model's potential to\nenhance QPF realism.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "GAN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Probabilistic measures afford fair comparisons of AIWP and NWP model output",
    "url": "http://arxiv.org/abs/2506.03744v1",
    "authors": [
      "Tilmann Gneiting",
      "Tobias Biegert",
      "Kristof Kraus",
      "Eva-Maria Walz",
      "Alexander I. Jordan",
      "Sebastian Lerch"
    ],
    "published": "2025-06-04",
    "abstract": "We introduce a new measure for fair and meaningful comparisons of\nsingle-valued output from artificial intelligence based weather prediction\n(AIWP) and numerical weather prediction (NWP) models, called potential\ncontinuous ranked probability score (PC). In a nutshell, we subject the\ndeterministic backbone of physics-based and data-driven models post hoc to the\nsame statistical postprocessing technique, namely, isotonic distributional\nregression (IDR). Then we find PC as the mean continuous ranked probability\nscore (CRPS) of the postprocessed probabilistic forecasts. The nonnegative PC\nmeasure quantifies potential predictive performance and is invariant under\nstrictly increasing transformations of the model output. PC attains its most\ndesirable value of zero if, and only if, the weather outcome Y is a fixed,\nnon-decreasing function of the model output X. The PC measure is recorded in\nthe unit of the outcome, has an upper bound of one half times the mean absolute\ndifference between outcomes, and serves as a proxy for the mean CRPS of\nreal-time, operational probabilistic products. When applied to WeatherBench 2\ndata, our approach demonstrates that the data-driven GraphCast model\noutperforms the leading, physics-based European Centre for Medium Range Weather\nForecasts (ECMWF) high-resolution (HRES) model. Furthermore, the PC measure for\nthe HRES model aligns exceptionally well with the mean CRPS of the operational\nECMWF ensemble. Across application domains, our approach affords comparisons of\nsingle-valued forecasts in settings where the pre-specification of a loss\nfunction -- which is the usual, and principally superior, procedure in forecast\ncontests, administrative, and benchmarks settings -- places competitors on\nunequal footings.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
    "url": "http://arxiv.org/abs/2506.03210v1",
    "authors": [
      "Qiusheng Huang",
      "Yuan Niu",
      "Xiaohui Zhong",
      "Anboyu Guo",
      "Lei Chen",
      "Dianjun Zhang",
      "Xuefeng Zhang",
      "Hao Li"
    ],
    "published": "2025-06-03",
    "abstract": "Accurate, high-resolution ocean forecasting is crucial for maritime\noperations and environmental monitoring. While traditional numerical models are\ncapable of producing sub-daily, eddy-resolving forecasts, they are\ncomputationally intensive and face challenges in maintaining accuracy at fine\nspatial and temporal scales. In contrast, recent data-driven approaches offer\nimproved computational efficiency and emerging potential, yet typically operate\nat daily resolution and struggle with sub-daily predictions due to error\naccumulation over time. We introduce FuXi-Ocean, the first data-driven global\nocean forecasting model achieving six-hourly predictions at eddy-resolving\n1/12{\\deg} spatial resolution, reaching depths of up to 1500 meters. The model\narchitecture integrates a context-aware feature extraction module with a\npredictive network employing stacked attention blocks. The core innovation is\nthe Mixture-of-Time (MoT) module, which adaptively integrates predictions from\nmultiple temporal contexts by learning variable-specific reliability ,\nmitigating cumulative errors in sequential forecasting. Through comprehensive\nexperimental evaluation, FuXi-Ocean demonstrates superior skill in predicting\nkey variables, including temperature, salinity, and currents, across multiple\ndepths.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Using Diffusion Models to do Data Assimilation",
    "url": "http://arxiv.org/abs/2506.02249v1",
    "authors": [
      "Daniel Hodyss",
      "Matthias Morzfeld"
    ],
    "published": "2025-06-02",
    "abstract": "The recent surge in machine learning (ML) methods for geophysical modeling\nhas raised the question of how these same ML methods might be applied to data\nassimilation (DA). We focus on diffusion modeling (a form of generative\nartificial intelligence) and on systems that can perform the entire DA, rather\nthan on ML-based tools that are used within an otherwise conventional DA\nsystem. We explain that there are (at least) three different types of\ndiffusion-based DA systems and we show in detail that the three systems differ\nin the type of posterior distribution they target for sampling. The different\nposterior distributions correspond to different priors and\\slash or\nlikelihoods, which in turn results in different types of training data sets,\ndifferent computational requirements and different qualities of their state\nestimates. We discuss the implications of these findings for the use of\ndiffusion modeling in DA.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ROC Curves for Spatial Point Patterns and Presence-Absence Data",
    "url": "http://arxiv.org/abs/2506.03414v1",
    "authors": [
      "Adrian Baddeley",
      "Ege Rubak",
      "Suman Rakshit",
      "Gopalan Nair"
    ],
    "published": "2025-06-03",
    "abstract": "Receiver Operating Characteristic (ROC) curves have recently been used to\nevaluate the performance of models for spatial presence-absence or\npresence-only data. Applications include species distribution modelling and\nmineral prospectivity analysis. We clarify the interpretation of the ROC curve\nin this context. Contrary to statements in the literature, ROC does not measure\ngoodness-of-fit of a spatial model, and its interpretation as a measure of\npredictive ability is weak; it is a measure of ranking ability, insensitive to\nthe precise form of the model. To gain insight we draw connections between ROC\nand existing statistical techniques for spatial point pattern data. The area\nunder the ROC curve (AUC) is related to hypothesis tests of the null hypothesis\nthat the explanatory variables have no effect. The shape of the ROC curve has a\ndiagnostic interpretation. This suggests several new techniques, which extend\nthe scope of application of ROC curves for spatial data, to support variable\nselection and model selection, analysis of segregation between different types\nof points, adjustment for a baseline, and analysis of spatial case-control\ndata. The new techniques are illustrated with several real example datasets.\nOpen source R code implementing the techniques is available in the development\nversion of our package spatstat [Baddeley and Turner, 2005, Baddeley et al.,\n2015] and will be included in the next public release.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Modelling benthic animals in space and time using Bayesian Point Process with cross validation: the case of Holoturians",
    "url": "http://arxiv.org/abs/2506.01763v2",
    "authors": [
      "Daniele Poggio",
      "Gian Mario Sangiovanni",
      "Gianluca Mastrantonio",
      "Giovanna Jona Lasinio",
      "Edoardo Casoli",
      "Stefano Moro",
      "Daniele Ventura"
    ],
    "published": "2025-06-02",
    "abstract": "Understanding the spatial distribution of Holothurians is an essential task\nfor ecosystem monitoring and sustainable management, particularly in the\nMediterranean habitats. However, species distribution modeling is often\ncomplicated by the presence-only nature of the data and heterogeneous sampling\ndesigns. This study develops a spatio-temporal framework based on Log-Gaussian\nCox Processes to analyze Holothurians' positions collected across nine survey\ncampaigns conducted from 2022 to 2024 near Giglio Island, Italy. The surveys\ncombined high-resolution photogrammetry with diver-based visual censuses,\nleading to varying detection probabilities across habitats, especially within\nPosidonia oceanica meadows. We adopt a model with a shared spatial Gaussian\nprocess component to accommodate this complexity, accounting for habitat\nstructure, environmental covariates, and temporal variability. Model estimation\nis performed using Integrated Nested Laplace Approximation. We evaluate the\npredictive performances of alternative model specifications through a novel\nk-fold cross-validation strategy for point processes, using the Continuous\nRanked Probability Score. Our approach provides a flexible and computationally\nefficient framework for integrating heterogeneous presence-only data in marine\necology and comparing the predictive ability of alternative models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models",
    "url": "http://arxiv.org/abs/2506.08780v1",
    "authors": [
      "Isaac Corley",
      "Lakshay Sharma",
      "Ruth Crasto"
    ],
    "published": "2025-06-10",
    "abstract": "The Landsat program offers over 50 years of globally consistent Earth\nimagery. However, the lack of benchmarks for this data constrains progress\ntowards Landsat-based Geospatial Foundation Models (GFM). In this paper, we\nintroduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that\nadapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and\nLC100-L. We establish baseline and standardized evaluation methods across both\ncommon architectures and Landsat foundation models pretrained on the SSL4EO-L\ndataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract\nbetter representations for downstream tasks in comparison to ImageNet,\nincluding performance gains of +4% OA and +5.1% mAP on EuroSAT-L and\nBigEarthNet-L.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation",
    "url": "http://arxiv.org/abs/2506.08772v2",
    "authors": [
      "Jiayi Song",
      "Kaiyu Li",
      "Xiangyong Cao",
      "Deyu Meng"
    ],
    "published": "2025-06-10",
    "abstract": "Semantic segmentation in remote sensing images is crucial for various\napplications, yet its performance is heavily reliant on large-scale,\nhigh-quality pixel-wise annotations, which are notoriously expensive and\ntime-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a\npromising alternative to mitigate this data dependency. However, existing SSS\nmethods often struggle with the inherent distribution mismatch between limited\nlabeled data and abundant unlabeled data, leading to suboptimal generalization.\nTo alleviate this issue, we attempt to introduce the Vision Foundation Models\n(VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs\npossess robust generalization capabilities that can effectively bridge this\ndistribution gap and provide strong semantic priors for SSS. Inspired by this,\nwe introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework\nthat leverages the powerful semantic knowledge embedded in VFMs to guide\nsemi-supervised learning in remote sensing. Specifically, RS-MTDF employs\nmultiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing\nfeature-level distillation to align student features with their robust\nrepresentations. To further enhance discriminative power, the distilled\nknowledge is seamlessly fused into the student decoder. Extensive experiments\non three challenging remote sensing datasets demonstrate that RS-MTDF\nconsistently achieves state-of-the-art performance. Notably, our method\noutperforms existing approaches across various label ratios on LoveDA and\nsecures the highest IoU in the majority of semantic categories. These results\nunderscore the efficacy of multi-teacher VFM guidance in significantly\nenhancing both generalization and semantic understanding for remote sensing\nsegmentation. Ablation studies further validate the contribution of each\nproposed module.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial",
    "url": "http://arxiv.org/abs/2506.10386v1",
    "authors": [
      "Jerry Yan",
      "Chinmay Talegaonkar",
      "Nicholas Antipa",
      "Eric Terrill",
      "Sophia Merrifield"
    ],
    "published": "2025-06-12",
    "abstract": "The burial state of anthropogenic objects on the seafloor provides insight\ninto localized sedimentation dynamics and is also critical for assessing\necological risks, potential pollutant transport, and the viability of recovery\nor mitigation strategies for hazardous materials such as munitions. Accurate\nburial depth estimation from remote imagery remains difficult due to partial\nocclusion, poor visibility, and object degradation. This work introduces a\ncomputer vision pipeline, called PoseIDON, which combines deep foundation model\nfeatures with multiview photogrammetry to estimate six degrees of freedom\nobject pose and the orientation of the surrounding seafloor from ROV video.\nBurial depth is inferred by aligning CAD models of the objects with observed\nimagery and fitting a local planar approximation of the seafloor. The method is\nvalidated using footage of 54 objects, including barrels and munitions,\nrecorded at a historic ocean dumpsite in the San Pedro Basin. The model\nachieves a mean burial depth error of approximately 10 centimeters and resolves\nspatial burial patterns that reflect underlying sediment transport processes.\nThis approach enables scalable, non-invasive mapping of seafloor burial and\nsupports environmental assessment at contaminated sites.",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity",
    "url": "http://arxiv.org/abs/2506.08051v1",
    "authors": [
      "Mahmuda Sultana Mimi",
      "Md Monzurul Islam",
      "Anannya Ghosh Tusti",
      "Shriyank Somvanshi",
      "Subasish Das"
    ],
    "published": "2025-06-09",
    "abstract": "Understanding the spatial and temporal dynamics of automated vehicle (AV)\ncrash severity is critical for advancing urban mobility safety and\ninfrastructure planning. In this work, we introduce ST-GraphNet, a\nspatio-temporal graph neural network framework designed to model and predict AV\ncrash severity by using both fine-grained and region-aggregated spatial graphs.\nUsing a balanced dataset of 2,352 real-world AV-related crash reports from\nTexas (2024), including geospatial coordinates, crash timestamps, SAE\nautomation levels, and narrative descriptions, we construct two complementary\ngraph representations: (1) a fine-grained graph with individual crash events as\nnodes, where edges are defined via spatio-temporal proximity; and (2) a\ncoarse-grained graph where crashes are aggregated into Hexagonal Hierarchical\nSpatial Indexing (H3)-based spatial cells, connected through hexagonal\nadjacency. Each node in the graph is enriched with multimodal data, including\nsemantic, spatial, and temporal attributes, including textual embeddings from\ncrash narratives using a pretrained Sentence-BERT model. We evaluate various\ngraph neural network (GNN) architectures, such as Graph Convolutional Networks\n(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN\n(DSTGCN), to classify crash severity and predict high-risk regions. Our\nproposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3\ngraph, achieves a test accuracy of 97.74\\%, substantially outperforming the\nbest fine-grained model (64.7\\% test accuracy). These findings highlight the\neffectiveness of spatial aggregation, dynamic message passing, and multi-modal\nfeature integration in capturing the complex spatio-temporal patterns\nunderlying AV crash severity.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": []
  },
  {
    "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images",
    "url": "http://arxiv.org/abs/2506.09553v1",
    "authors": [
      "Ligao Deng",
      "Yupeng Deng",
      "Yu Meng",
      "Jingbo Chen",
      "Zhihao Xi",
      "Diyou Liu",
      "Qifeng Chu"
    ],
    "published": "2025-06-11",
    "abstract": "Road networks are crucial for mapping, autonomous driving, and disaster\nresponse. While manual annotation is costly, deep learning offers efficient\nextraction. Current methods include postprocessing (prone to errors), global\nparallel (fast but misses nodes), and local iterative (accurate but slow). We\npropose GLD-Road, a two-stage model combining global efficiency and local\nprecision. First, it detects road nodes and connects them via a Connect Module.\nThen, it iteratively refines broken roads using local searches, drastically\nreducing computation. Experiments show GLD-Road outperforms state-of-the-art\nmethods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also\nreduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++\n(local). The experimental results are available at\nhttps://github.com/ucas-dlg/GLD-Road.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2506.06667v1",
    "authors": [
      "Yu-Hsuan Ho",
      "Ali Mostafavi"
    ],
    "published": "2025-06-07",
    "abstract": "Most post-disaster damage classifiers succeed only when destructive forces\nleave clear spectral or structural signatures -- conditions rarely present\nafter inundation. Consequently, existing models perform poorly at identifying\nflood-related building damages. The model presented in this study,\nFlood-DamageSense, addresses this gap as the first deep-learning framework\npurpose-built for building-level flood-damage assessment. The architecture\nfuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical\nbasemaps and an inherent flood-risk layer that encodes long-term exposure\nprobabilities, guiding the network toward plausibly affected structures even\nwhen compositional change is minimal. A multimodal Mamba backbone with a\nsemi-Siamese encoder and task-specific decoders jointly predicts (1) graded\nbuilding-damage states, (2) floodwater extent, and (3) building footprints.\nTraining and evaluation on Hurricane Harvey (2017) imagery from Harris County,\nTexas -- supported by insurance-derived property-damage extents -- show a mean\nF1 improvement of up to 19 percentage points over state-of-the-art baselines,\nwith the largest gains in the frequently misclassified \"minor\" and \"moderate\"\ndamage categories. Ablation studies identify the inherent-risk feature as the\nsingle most significant contributor to this performance boost. An end-to-end\npost-processing pipeline converts pixel-level outputs to actionable,\nbuilding-scale damage maps within minutes of image acquisition. By combining\nrisk-aware modeling with SAR's all-weather capability, Flood-DamageSense\ndelivers faster, finer-grained, and more reliable flood-damage intelligence to\nsupport post-disaster decision-making and resource allocation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Comparative Study of U-Net Architectures for Change Detection in Satellite Images",
    "url": "http://arxiv.org/abs/2506.07925v1",
    "authors": [
      "Yaxita Amin",
      "Naimisha S Trivedi",
      "Rashmi Bhattad"
    ],
    "published": "2025-06-09",
    "abstract": "Remote sensing change detection is essential for monitoring the everchanging\nlandscapes of the Earth. The U-Net architecture has gained popularity for its\ncapability to capture spatial information and perform pixel-wise\nclassification. However, their application in the Remote sensing field remains\nlargely unexplored. Therefore, this paper fill the gap by conducting a\ncomprehensive analysis of 34 papers. This study conducts a comparison and\nanalysis of 18 different U-Net variations, assessing their potential for\ndetecting changes in remote sensing. We evaluate both benefits along with\ndrawbacks of each variation within the framework of this particular\napplication. We emphasize variations that are explicitly built for change\ndetection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.\nThe analysis highlights the significance of aspects such as managing data from\ndifferent time periods and collecting relationships over a long distance to\nenhance the precision of change detection. This study provides valuable\ninsights for researchers and practitioners that choose U-Net versions for\nremote sensing change detection tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "A multi-scale loss formulation for learning a probabilistic model with proper score optimisation",
    "url": "http://arxiv.org/abs/2506.10868v1",
    "authors": [
      "Simon Lang",
      "Martin Leutbecher",
      "Pedro Maciel"
    ],
    "published": "2025-06-12",
    "abstract": "We assess the impact of a multi-scale loss formulation for training\nprobabilistic machine-learned weather forecasting models. The multi-scale loss\nis tested in AIFS-CRPS, a machine-learned weather forecasting model developed\nat the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS-CRPS is\ntrained by directly optimising the almost fair continuous ranked probability\nscore (afCRPS). The multi-scale loss better constrains small scale variability\nwithout negatively impacting forecast skill. This opens up promising directions\nfor future work in scale-aware model training.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Skillful joint probabilistic weather forecasting from marginals",
    "url": "http://arxiv.org/abs/2506.10772v1",
    "authors": [
      "Ferran Alet",
      "Ilan Price",
      "Andrew El-Kadi",
      "Dominic Masters",
      "Stratis Markou",
      "Tom R. Andersson",
      "Jacklynn Stott",
      "Remi Lam",
      "Matthew Willson",
      "Alvaro Sanchez-Gonzalez",
      "Peter Battaglia"
    ],
    "published": "2025-06-12",
    "abstract": "Machine learning (ML)-based weather models have rapidly risen to prominence\ndue to their greater accuracy and speed than traditional forecasts based on\nnumerical weather prediction (NWP), recently outperforming traditional\nensembles in global probabilistic weather forecasting. This paper presents FGN,\na simple, scalable and flexible modeling approach which significantly\noutperforms the current state-of-the-art models. FGN generates ensembles via\nlearned model-perturbations with an ensemble of appropriately constrained\nmodels. It is trained directly to minimize the continuous rank probability\nscore (CRPS) of per-location forecasts. It produces state-of-the-art ensemble\nforecasts as measured by a range of deterministic and probabilistic metrics,\nmakes skillful ensemble tropical cyclone track predictions, and captures joint\nspatial structure despite being trained only on marginals.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Pushing the Limits of Extreme Weather: Constructing Extreme Heatwave Storylines with Differentiable Climate Models",
    "url": "http://arxiv.org/abs/2506.10660v1",
    "authors": [
      "Tim Whittaker",
      "Alejandro Di Luca"
    ],
    "published": "2025-06-12",
    "abstract": "Understanding the plausible upper bounds of extreme weather events is\nessential for risk assessment in a warming climate. Existing methods, based on\nlarge ensembles of physics-based models, are often computationally expensive or\nlack the fidelity needed to simulate rare, high-impact extremes. Here, we\npresent a novel framework that leverages a differentiable hybrid climate model,\nNeuralGCM, to optimize initial conditions and generate physically consistent\nworst-case heatwave trajectories. Applied to the 2021 Pacific Northwest\nheatwave, our method produces temperature anomalies up to 3.7 $^\\circ$C above\nthe most extreme member of a 75-member ensemble. These trajectories feature\nintensified atmospheric blocking and amplified Rossby wave patterns--hallmarks\nof severe heat events. Our results demonstrate that differentiable climate\nmodels can efficiently explore the upper tails of event likelihoods, providing\na powerful new approach for constructing targeted storylines of extreme weather\nunder climate change.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Prediction of steady states in a marine ecosystem model by a machine learning technique",
    "url": "http://arxiv.org/abs/2506.10475v1",
    "authors": [
      "Sarker Miraz Mahfuz",
      "Thomas Slawig"
    ],
    "published": "2025-06-12",
    "abstract": "We used precomputed steady states obtained by a spin-up for a global marine\necosystem model as training data to build a mapping from the small number of\nbiogeochemical model parameters onto the three-dimensional converged steady\nannual cycle. The mapping was performed by a conditional variational\nautoencoder (CVAE) with mass correction. Applied for test data, we show that\nthe prediction obtained by the CVAE already gives a reasonable good\napproximation of the steady states obtained by a regular spin-up. However, the\npredictions do not reach the same level of annual periodicity as those obtained\nin the original spin-up data. Thus, we took the predictions as initial values\nfor a spin-up. We could show that the number of necessary iterations,\ncorresponding to model years, to reach a prescribed stopping criterion in the\nspin-up could be significantly reduced compared to the use of the originally\nuniform, constant initial value. The amount of reduction depends on the applied\nstopping criterion, measuring the periodicity of the solution. The savings in\nneeded iterations and, thus, computing time for the spin-up ranges from 50 to\n95\\%, depending on the stopping criterion for the spin-up. We compared these\nresults with the use of the mean of the training data as an initial value. We\nfound that this also accelerates the spin-up, but only by a much lower factor.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion",
    "url": "http://arxiv.org/abs/2506.10391v1",
    "authors": [
      "Yuanyi Song",
      "Pumeng Lyu",
      "Ben Fei",
      "Fenghua Ling",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "published": "2025-06-12",
    "abstract": "Accurate reconstruction of ocean is essential for reflecting global climate\ndynamics and supporting marine meteorological research. Conventional methods\nface challenges due to sparse data, algorithmic complexity, and high\ncomputational costs, while increasing usage of machine learning (ML) method\nremains limited to reconstruction problems at the sea surface and local\nregions, struggling with issues like cloud occlusion. To address these\nlimitations, this paper proposes ReconMOST, a data-driven guided diffusion\nmodel framework for multi-layer sea temperature reconstruction. Specifically,\nwe first pre-train an unconditional diffusion model using a large collection of\nhistorical numerical simulation data, enabling the model to attain physically\nconsistent distribution patterns of ocean temperature fields. During the\ngeneration phase, sparse yet high-accuracy in-situ observational data are\nutilized as guidance points for the reverse diffusion process, generating\naccurate reconstruction results. Importantly, in regions lacking direct\nobservational data, the physically consistent spatial distribution patterns\nlearned during pre-training enable implicitly guided and physically plausible\nreconstructions. Our method extends ML-based SST reconstruction to a global,\nmulti-layer setting, handling over 92.5% missing data while maintaining\nreconstruction accuracy, spatial resolution, and superior generalization\ncapability. We pre-train our model on CMIP6 numerical simulation data and\nconduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The\nresults of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on\nreconstruction, and 0.633 on total, respectively, demonstrating the\neffectiveness and robustness of the proposed framework. Our source code is\navailable at https://github.com/norsheep/ReconMOST.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Causal Climate Emulation with Bayesian Filtering",
    "url": "http://arxiv.org/abs/2506.09891v1",
    "authors": [
      "Sebastian Hickman",
      "Ilija Trajkovic",
      "Julia Kaltenborn",
      "Francis Pelletier",
      "Alex Archibald",
      "Yaniv Gurwicz",
      "Peer Nowack",
      "David Rolnick",
      "Julien Boussard"
    ],
    "published": "2025-06-11",
    "abstract": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale",
    "url": "http://arxiv.org/abs/2506.09733v1",
    "authors": [
      "Minjong Cheon"
    ],
    "published": "2025-06-11",
    "abstract": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "DEF: Diffusion-augmented Ensemble Forecasting",
    "url": "http://arxiv.org/abs/2506.07324v1",
    "authors": [
      "David Millard",
      "Arielle Carr",
      "St\u00e9phane Gaudreault",
      "Ali Baheri"
    ],
    "published": "2025-06-08",
    "abstract": "We present DEF (\\textbf{\\ul{D}}iffusion-augmented \\textbf{\\ul{E}}nsemble\n\\textbf{\\ul{F}}orecasting), a novel approach for generating initial condition\nperturbations. Modern approaches to initial condition perturbations are\nprimarily designed for numerical weather prediction (NWP) solvers, limiting\ntheir applicability in the rapidly growing field of machine learning for\nweather prediction. Consequently, stochastic models in this domain are often\ndeveloped on a case-by-case basis. We demonstrate that a simple conditional\ndiffusion model can (1) generate meaningful structured perturbations, (2) be\napplied iteratively, and (3) utilize a guidance term to intuitivey control the\nlevel of perturbation. This method enables the transformation of any\ndeterministic neural forecasting system into a stochastic one. With our\nstochastic extended systems, we show that the model accumulates less error over\nlong-term forecasts while producing meaningful forecast distributions. We\nvalidate our approach on the 5.625$^\\circ$ ERA5 reanalysis dataset, which\ncomprises atmospheric and surface variables over a discretized global grid,\nspanning from the 1960s to the present. On this dataset, our method\ndemonstrates improved predictive performance along with reasonable spread\nestimates.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places",
    "url": "http://arxiv.org/abs/2506.14570v1",
    "authors": [
      "Mohammad Hashemi",
      "Andreas Zufle"
    ],
    "published": "2025-06-17",
    "abstract": "Capturing human mobility is essential for modeling how people interact with\nand move through physical spaces, reflecting social behavior, access to\nresources, and dynamic spatial patterns. To support scalable and transferable\nanalysis across diverse geographies and contexts, there is a need for a\ngeneralizable foundation model for spatiotemporal data. While foundation models\nhave transformed language and vision, they remain limited in handling the\nunique challenges posed by the spatial, temporal, and semantic complexity of\nmobility data. This vision paper advocates for a new class of spatial\nfoundation models that integrate geolocation semantics with human mobility\nacross multiple scales. Central to our vision is a shift from modeling discrete\npoints of interest to understanding places: dynamic, context-rich regions\nshaped by human behavior and mobility that may comprise many places of\ninterest. We identify key gaps in adaptability, scalability, and multi-granular\nreasoning, and propose research directions focused on modeling places and\nenabling efficient learning. Our goal is to guide the development of scalable,\ncontext-aware models for next-generation geospatial intelligence. These models\nunlock powerful applications ranging from personalized place discovery and\nlogistics optimization to urban planning, ultimately enabling smarter and more\nresponsive spatial decision-making.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation",
    "url": "http://arxiv.org/abs/2506.13599v1",
    "authors": [
      "Yuwei Du",
      "Jie Feng",
      "Jian Yuan",
      "Yong Li"
    ],
    "published": "2025-06-16",
    "abstract": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset",
    "url": "http://arxiv.org/abs/2506.14765v1",
    "authors": [
      "Nikolaos Dionelis",
      "Jente Bosmans",
      "Riccardo Musto",
      "Giancarlo Paoletti",
      "Simone Sarti",
      "Giacomo Cascarano",
      "Casper Fibaek",
      "Luke Camilleri",
      "Bertrand Le Saux",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-06-17",
    "abstract": "Today, Earth Observation (EO) satellites generate massive volumes of data,\nwith the Copernicus Sentinel-2 constellation alone producing approximately\n1.6TB per day. To fully exploit this information, it is essential to pretrain\nEO Foundation Models (FMs) on large unlabeled datasets, enabling efficient\nfine-tuning for several different downstream tasks with minimal labeled data.\nIn this work, we present the scaling-up of our recently proposed EO Foundation\nModel, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which\ncovers the vast majority of the Earth's surface, as well as on the specialized\nsubset FastTOM 2TB that does not include oceans and ice. We develop and study\nvarious PhilEO model variants with different numbers of parameters and\narchitectures. Finally, we fine-tune the models on the PhilEO Bench for road\ndensity estimation, building density pixel-wise regression, and land cover\nsemantic segmentation, and we evaluate the performance. Our results demonstrate\nthat for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB\nmodel outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots\nfor road density estimation and building density regression, PhilEO 200M\nFastTOM outperforms all the other models. The effectiveness of both dataset and\nmodel scaling is validated using the PhilEO Bench. We also study the impact of\narchitecture scaling, transitioning from U-Net Convolutional Neural Networks\n(CNN) to Vision Transformers (ViT).",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Regression"
    ]
  },
  {
    "title": "Mapping Farmed Landscapes from Remote Sensing",
    "url": "http://arxiv.org/abs/2506.13993v1",
    "authors": [
      "Michelangelo Conserva",
      "Alex Wilson",
      "Charlotte Stanton",
      "Vishal Batchu",
      "Varun Gulshan"
    ],
    "published": "2025-06-16",
    "abstract": "Effective management of agricultural landscapes is critical for meeting\nglobal biodiversity targets, but efforts are hampered by the absence of\ndetailed, large-scale ecological maps. To address this, we introduce\nFarmscapes, the first large-scale (covering most of England), high-resolution\n(25cm) map of rural landscape features, including ecologically vital elements\nlike hedgerows, woodlands, and stone walls. This map was generated using a deep\nlearning segmentation model trained on a novel, dataset of 942 manually\nannotated tiles derived from aerial imagery. Our model accurately identifies\nkey habitats, achieving high f1-scores for woodland (96\\%) and farmed land\n(95\\%), and demonstrates strong capability in segmenting linear features, with\nan F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google\nEarth Engine, we provide a powerful, open-access tool for ecologists and\npolicymakers. This work enables data-driven planning for habitat restoration,\nsupports the monitoring of initiatives like the EU Biodiversity Strategy, and\nlays the foundation for advanced analysis of landscape connectivity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data",
    "url": "http://arxiv.org/abs/2506.13505v1",
    "authors": [
      "Vasiliki Balaska",
      "Ioannis Tsampikos Papapetros",
      "Katerina Maria Oikonomou",
      "Loukas Bampis",
      "Antonios Gasteratos"
    ],
    "published": "2025-06-16",
    "abstract": "The mining sector increasingly adopts digital tools to improve operational\nefficiency, safety, and data-driven decision-making. One of the key challenges\nremains the reliable acquisition of high-resolution, geo-referenced spatial\ninformation to support core activities such as extraction planning and on-site\nmonitoring. This work presents an integrated system architecture that combines\nUAV-based sensing, LiDAR terrain modeling, and deep learning-based object\ndetection to generate spatially accurate information for open-pit mining\nenvironments. The proposed pipeline includes geo-referencing, 3D\nreconstruction, and object localization, enabling structured spatial outputs to\nbe integrated into an industrial digital twin platform. Unlike traditional\nstatic surveying methods, the system offers higher coverage and automation\npotential, with modular components suitable for deployment in real-world\nindustrial contexts. While the current implementation operates in post-flight\nbatch mode, it lays the foundation for real-time extensions. The system\ncontributes to the development of AI-enhanced remote sensing in mining by\ndemonstrating a scalable and field-validated geospatial data workflow that\nsupports situational awareness and infrastructure safety.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder",
    "url": "http://arxiv.org/abs/2506.14906v1",
    "authors": [
      "Robert Czupryniak",
      "Abhishek Chakraborty",
      "Andrew N. Jordan",
      "John C. Howell"
    ],
    "published": "2025-06-17",
    "abstract": "We apply machine learning methods to demonstrate range superresolution in\nremote sensing radar detection. Specifically, we implement a denoising\nautoencoder to estimate the distance between two equal intensity scatterers in\nthe subwavelength regime. The machine learning models are trained on waveforms\nsubject to a bandlimit constraint such that ranges much smaller than the\ninverse bandlimit are optimized in their precision. The autoencoder achieves\neffective dimensionality reduction, with the bottleneck layer exhibiting a\nstrong and consistent correlation with the true scatterer separation. We\nconfirm reproducibility across different training sessions and network\ninitializations by analyzing the scaled encoder outputs and their robustness to\nnoise. We investigate the behavior of the bottleneck layer for the following\ntypes of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse,\nand a theoretically near-optimal pulse created from a spherical Bessel function\nbasis. The Bessel signal performs best, followed by the triangle wave, with the\nsinc signal performing worst, highlighting the crucial role of signal design in\nthe success of machine-learning-based range resolution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Projecting U.S. coastal storm surge risks and impacts with deep learning",
    "url": "http://arxiv.org/abs/2506.13963v1",
    "authors": [
      "Julian R. Rice",
      "Karthik Balaguru",
      "Fadia Ticona Rollano",
      "John Wilson",
      "Brent Daniel",
      "David Judi",
      "Ning Sun",
      "L. Ruby Leung"
    ],
    "published": "2025-06-16",
    "abstract": "Storm surge is one of the deadliest hazards posed by tropical cyclones (TCs),\nyet assessing its current and future risk is difficult due to the phenomenon's\nrarity and physical complexity. Recent advances in artificial intelligence\napplications to natural hazard modeling suggest a new avenue for addressing\nthis problem. We utilize a deep learning storm surge model to efficiently\nestimate coastal surge risk in the United States from 900,000 synthetic TC\nevents, accounting for projected changes in TC behavior and sea levels. The\nderived historical 100-year surge (the event with a 1% yearly exceedance\nprobability) agrees well with historical observations and other modeling\ntechniques. When coupled with an inundation model, we find that heightened TC\nintensities and sea levels by the end of the century result in a 50% increase\nin population at risk. Key findings include markedly heightened risk in\nFlorida, and critical thresholds identified in Georgia and South Carolina.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Towards NoahMP-AI: Enhancing Land Surface Model Prediction with Deep Learning",
    "url": "http://arxiv.org/abs/2506.12919v1",
    "authors": [
      "Mahmoud Mbarak",
      "Manmeet Singh",
      "Naveen Sudharsan",
      "Zong-Liang Yang"
    ],
    "published": "2025-06-15",
    "abstract": "Accurate soil moisture prediction during extreme events remains a critical\nchallenge for Earth system modeling, with profound implications for drought\nmonitoring, flood forecasting, and climate adaptation strategies. While land\nsurface models (LSMs) provide physically-based predictions, they exhibit\nsystematic biases during extreme conditions when their parameterizations\noperate outside calibrated ranges. Here we present NoahMP-AI, a physics-guided\ndeep learning framework that addresses this challenge by leveraging the\ncomplete Noah-MP land surface model as a comprehensive physics-based feature\ngenerator while using machine learning to correct systematic biases against\nsatellite observations. We employ a 3D U-Net architecture that processes\nNoah-MP outputs (soil moisture, latent heat flux, and sensible heat flux) to\npredict SMAP-validated soil moisture across two contrasting extreme events: a\nprolonged drought (March-September 2022) and Hurricane Beryl (July 2024) over\nTexas. Our results demonstrate substantial improvements over standalone Noah-MP\npredictions, with R-squared values improving from -0.57 to 0.46 during drought\nconditions, while maintaining physical consistency and spatial coherence. The\nframework's ability to preserve Noah-MP's physical relationships while learning\nobservation-based corrections represents a significant advance in hybrid Earth\nsystem modeling. This work establishes both a practical tool for operational\nforecasting and a benchmark for investigating the optimal integration of\nphysics-based understanding with data-driven learning in environmental\nprediction systems.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AI-Informed Model Analogs for Subseasonal-to-Seasonal Prediction",
    "url": "http://arxiv.org/abs/2506.14022v1",
    "authors": [
      "Jacob B. Landsberg",
      "Elizabeth A. Barnes",
      "Matthew Newman"
    ],
    "published": "2025-06-16",
    "abstract": "Subseasonal-to-seasonal forecasting is crucial for public health, disaster\npreparedness, and agriculture, and yet it remains a particularly challenging\ntimescale to predict. We explore the use of an interpretable AI-informed model\nanalog forecasting approach, previously employed on longer timescales, to\nimprove S2S predictions. Using an artificial neural network, we learn a mask of\nweights to optimize analog selection and showcase its versatility across three\nvaried prediction tasks: 1) classification of Week 3-4 Southern California\nsummer temperatures; 2) regional regression of Month 1 midwestern U.S. summer\ntemperatures; and 3) classification of Month 1-2 North Atlantic wintertime\nupper atmospheric winds. The AI-informed analogs outperform traditional analog\nforecasting approaches, as well as climatology and persistence baselines, for\ndeterministic and probabilistic skill metrics on both climate model and\nreanalysis data. We find the analog ensembles built using the AI-informed\napproach also produce better predictions of temperature extremes and improve\nrepresentation of forecast uncertainty. Finally, by using an interpretable-AI\nframework, we analyze the learned masks of weights to better understand S2S\nsources of predictability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Digging deeper: deep joint species distribution modeling reveals environmental drivers of Earthworm Communities",
    "url": "http://arxiv.org/abs/2506.13568v1",
    "authors": [
      "Sara Si-moussi",
      "Wilfried Thuiller",
      "Esther Galbrun",
      "Thibaud Deca\u00ebns",
      "Sylvain G\u00e9rard",
      "Daniel F. March\u00e1n",
      "Claire Marsden",
      "Yvan Capowiez",
      "Micka\u00ebl Hedde"
    ],
    "published": "2025-06-16",
    "abstract": "Earthworms are key drivers of soil function, influencing organic matter\nturnover, nutrient cycling, and soil structure. Understanding the environmental\ncontrols on their distribution is essential for predicting the impacts of land\nuse and climate change on soil ecosystems. While local studies have identified\nabiotic drivers of earthworm communities, broad-scale spatial patterns remain\nunderexplored.\n  We developed a multi-species, multi-task deep learning model to jointly\npredict the distribution of 77 earthworm species across metropolitan France,\nusing historical (1960-1970) and contemporary (1990-2020) records. The model\nintegrates climate, soil, and land cover variables to estimate habitat\nsuitability. We applied SHapley Additive exPlanations (SHAP) to identify key\nenvironmental drivers and used species clustering to reveal ecological response\ngroups.\n  The joint model achieved high predictive performance (TSS >= 0.7) and\nimproved predictions for rare species compared to traditional species\ndistribution models. Shared feature extraction across species allowed for more\nrobust identification of common and contrasting environmental responses.\nPrecipitation variability, temperature seasonality, and land cover emerged as\ndominant predictors of earthworm distribution. Species clustering revealed\ndistinct ecological strategies tied to climatic and land use gradients.\n  Our study advances both the methodological and ecological understanding of\nsoil biodiversity. We demonstrate the utility of interpretable deep learning\napproaches for large-scale soil fauna modeling and provide new insights into\nearthworm habitat specialization. These findings support improved soil\nbiodiversity monitoring and conservation planning in the face of global\nenvironmental change.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis",
    "url": "http://arxiv.org/abs/2506.20380v1",
    "authors": [
      "Zhengpeng Feng",
      "Sadiq Jaffer",
      "Jovana Knezevic",
      "Silja Sormunen",
      "Robin Young",
      "Madeline Lisaius",
      "Markus Immitzer",
      "James Ball",
      "Clement Atzberger",
      "David A. Coomes",
      "Anil Madhavapeddy",
      "Andrew Blake",
      "Srinivasan Keshav"
    ],
    "published": "2025-06-25",
    "abstract": "Satellite remote sensing (RS) enables a wide array of downstream Earth\nobservation (EO) applications, including climate modeling, carbon accounting,\nand strategies for conservation and sustainable land use. We present TESSERA, a\nnovel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning\n(SSL) to generate global, robust representations at 10m scale from pixel-level\nsatellite time series data. TESSERA combines information from only optical and\nSAR data streams using two parallel Transformer-based encoders: one dedicated\nto Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected\nspectral bands) to create representations that are then fused using a\nmultilayer perceptron (MLP), resulting in a global representation map covering\nthe years 2017 to 2024. Our precomputed representations set a new\nstate-of-the-art performance benchmark and our open-source approach\ndemocratizes access to high-performance, high-resolution representations. We\nbenchmark the performance of TESSERA in five diverse tasks, comparing our work\nwith state-of-the-art task-specific models and other foundation models. Our\nresults show that TESSERA outperforms both traditional RS baselines and the\nleading geospatial foundation models in these diverse downstream tasks.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition",
    "url": "http://arxiv.org/abs/2506.20174v2",
    "authors": [
      "Man Duc Chuc"
    ],
    "published": "2025-06-25",
    "abstract": "Foundation models are rapidly transforming Earth Observation data mining by\nenabling generalizable and scalable solutions for key tasks such as scene\nclassification and semantic segmentation. While most efforts in the geospatial\ndomain have focused on developing large models trained from scratch using\nmassive Earth Observation datasets, an alternative strategy that remains\nunderexplored is the reuse and combination of existing pretrained models. In\nthis study, we investigate whether foundation models pretrained on remote\nsensing and general vision datasets can be effectively combined to improve\nperformance across a diverse set of key Earth Observation tasks. Using the\nGEO-Bench benchmark, we evaluate several prominent models, including Prithvi,\nHiera, and DOFA, on eleven datasets covering a range of spatial resolutions,\nsensor modalities, and task types. The results show that feature-level\nensembling of smaller pretrained models can match or exceed the performance of\nmuch larger models, while requiring less training time and computational\nresources. Moreover, the study highlights the potential of applying knowledge\ndistillation to transfer the strengths of ensembles into more compact models,\noffering a practical path for deploying foundation models in real-world Earth\nObservation applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2506.19585v1",
    "authors": [
      "Gencer Sumbul",
      "Chang Xu",
      "Emanuele Dalsasso",
      "Devis Tuia"
    ],
    "published": "2025-06-24",
    "abstract": "From optical sensors to microwave radars, leveraging the complementary\nstrengths of remote sensing (RS) sensors is crucial for achieving dense\nspatio-temporal monitoring of our planet. In contrast, recent deep learning\nmodels, whether task-specific or foundational, are often specific to single\nsensors or to fixed combinations: adapting such models to different sensory\ninputs requires both architectural changes and re-training, limiting\nscalability and generalization across multiple RS sensors. On the contrary, a\nsingle model able to modulate its feature representations to accept diverse\nsensors as input would pave the way to agile and flexible multi-sensor RS data\nprocessing. To address this, we introduce SMARTIES, a generic and versatile\nfoundation model lifting sensor-specific/dependent efforts and enabling\nscalability and generalization to diverse RS sensors: SMARTIES projects data\nfrom heterogeneous sensors into a shared spectrum-aware space, enabling the use\nof arbitrary combinations of bands both for training and inference. To obtain\nsensor-agnostic representations, we train a single, unified transformer model\nreconstructing masked multi-sensor data with cross-sensor token mixup. On both\nsingle- and multi-modal tasks across diverse sensors, SMARTIES outperforms\nprevious models that rely on sensor-specific pretraining. Our code and\npretrained models are available at https://gsumbul.github.io/SMARTIES.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2506.21109v1",
    "authors": [
      "Luosheng Xu",
      "Dalin Zhang",
      "Zhaohui Song"
    ],
    "published": "2025-06-26",
    "abstract": "Remote sensing change detection is essential for monitoring urban expansion,\ndisaster assessment, and resource management, offering timely, accurate, and\nlarge-scale insights into dynamic landscape transformations. While deep\nlearning has revolutionized change detection, the increasing complexity and\ncomputational demands of modern models have not necessarily translated into\nsignificant accuracy gains. Instead of following this trend, this study\nexplores a more efficient approach, focusing on lightweight models that\nmaintain high accuracy while minimizing resource consumption, which is an\nessential requirement for on-satellite processing. To this end, we propose\nFlickCD, which means quick flick then get great results, pushing the boundaries\nof the performance-resource trade-off. FlickCD introduces an Enhanced\nDifference Module (EDM) to amplify critical feature differences between\ntemporal phases while suppressing irrelevant variations such as lighting and\nweather changes, thereby reducing computational costs in the subsequent change\ndecoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion\nBlocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global\nSelf-Attention (EGSA) to efficiently capture semantic information at multiple\nscales, preserving both coarse- and fine-grained changes. Extensive experiments\non four benchmark datasets demonstrate that FlickCD reduces computational and\nstorage overheads by more than an order of magnitude while achieving\nstate-of-the-art (SOTA) performance or incurring only a minor (<1\\% F1)\naccuracy trade-off. The implementation code is publicly available at\nhttps://github.com/xulsh8/FlickCD.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Photon Absorption Remote Sensing (PARS): Comprehensive Absorption Imaging Enabling Label-Free Biomolecule Characterization and Mapping",
    "url": "http://arxiv.org/abs/2506.20069v1",
    "authors": [
      "Benjamin R. Ecclestone",
      "James A. Tummon Simmons",
      "James E. D. Tweel",
      "Deepak Dinakaran",
      "Parsin Haji Reza"
    ],
    "published": "2025-06-25",
    "abstract": "Label-free optical absorption microscopy techniques continue to evolve as\npromising tools for label-free histopathological imaging of cells and tissues.\nHowever, critical challenges relating to specificity and contrast, as compared\nto current gold-standard methods continue to hamper adoption. This work\nintroduces Photon Absorption Remote Sensing (PARS), a new absorption microscope\nmodality, which simultaneously captures the dominant de-excitation processes\nfollowing an absorption event. In PARS, radiative (auto-fluorescence) and\nnon-radiative (photothermal and photoacoustic) relaxation processes are\ncollected simultaneously, providing enhanced specificity to a range of\nbiomolecules. As an example, a multiwavelength PARS system featuring UV (266\nnm) and visible (532 nm) excitation is applied to imaging human skin, and\nmurine brain tissue samples. It is shown that PARS can directly characterize,\ndifferentiate, and unmix, clinically relevant biomolecules inside complex\ntissues samples using established statistical processing methods. Gaussian\nmixture models (GMM) are used to characterize clinically relevant biomolecules\n(e.g., white, and gray matter) based on their PARS signals, while non-negative\nleast squares (NNLS) is applied to map the biomolecule abundance in murine\nbrain tissues, without stained ground truth images or deep-learning methods.\nPARS unmixing and abundance estimates are directly validated and compared\nagainst chemically stained ground truth images, and deep learning based-image\ntransforms. Overall, it is found that the PARS unique and rich contrast may\nprovide comprehensive, and otherwise inaccessible, label-free characterization\nof molecular pathology, representing a new source of data to develop AI and\nmachine learning methods for diagnostics and visualization.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Video Compression for Spatiotemporal Earth System Data",
    "url": "http://arxiv.org/abs/2506.19656v1",
    "authors": [
      "Oscar J. Pellicer-Valero",
      "Cesar Aybar",
      "Gustau Camps Valls"
    ],
    "published": "2025-06-24",
    "abstract": "Large-scale Earth system datasets, from high-resolution remote sensing\nimagery to spatiotemporal climate model outputs, exhibit characteristics\nanalogous to those of standard videos. Their inherent spatial, temporal, and\nspectral redundancies can thus be readily exploited by established video\ncompression techniques. Here, we present xarrayvideo, a Python library for\ncompressing multichannel spatiotemporal datasets by encoding them as videos.\nOur approach achieves compression ratios of up to 250x while maintaining high\nfidelity by leveraging standard, well-optimized video codecs through ffmpeg. We\ndemonstrate the library's effectiveness on four real-world multichannel\nspatiotemporal datasets: DynamicEarthNet (very high resolution Planet images),\nDeepExtremeCubes (high resolution Sentinel-2 images), ERA5 (weather reanalysis\ndata), and the SimpleS2 dataset (high resolution multichannel Sentinel-2\nimages), achieving Peak Signal-to-Noise Ratios (PSNRs) of 55.86, 40.60, 46.58,\nand 43.23 dB at 0.1 bits per pixel per band (bpppb) and 65.91, 54.28, 62.90,\nand 55.04 dB at 1 bpppb. We are redistributing two of these datasets,\nDeepExtremeCubes (2.3 Tb) and DynamicEarthNet (525 Gb), in the\nmachine-learning-ready and cloud-ready TACO format through HuggingFace at\nsignificantly reduced sizes (270 Gb and 8.5 Gb, respectively) without\ncompromising quality (PSNR 55.77-56.65 and 60.15). No performance loss is\nobserved when the compressed versions of these datasets are used in their\nrespective deep learning-based downstream tasks (next step reflectance\nprediction and landcover segmentation). In conclusion, xarrayvideo presents an\nefficient solution for handling the rapidly growing size of Earth observation\ndatasets, making advanced compression techniques accessible and practical to\nthe Earth science community. The library is available for use at\nhttps://github.com/IPL-UV/xarrayvideo",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "MambaOutRS: A Hybrid CNN-Fourier Architecture for Remote Sensing Image Classification",
    "url": "http://arxiv.org/abs/2506.19561v1",
    "authors": [
      "Minjong Cheon",
      "Changbae Mun"
    ],
    "published": "2025-06-24",
    "abstract": "Recent advances in deep learning for vision tasks have seen the rise of State\nSpace Models (SSMs) like Mamba, celebrated for their linear scalability.\nHowever, their adaptation to 2D visual data often necessitates complex\nmodifications that may diminish efficiency. In this paper, we introduce\nMambaOutRS, a novel hybrid convolutional architecture for remote sensing image\nclassification that re-evaluates the necessity of recurrent SSMs. MambaOutRS\nbuilds upon stacked Gated CNN blocks for local feature extraction and\nintroduces a novel Fourier Filter Gate (FFG) module that operates in the\nfrequency domain to capture global contextual information efficiently. Our\narchitecture employs a four-stage hierarchical design and was extensively\nevaluated on challenging remote sensing datasets: UC Merced, AID,\nNWPU-RESISC45, and EuroSAT. MambaOutRS consistently achieved state-of-the-art\n(SOTA) performance across these benchmarks. Notably, our MambaOutRS-t variant\n(24.0M parameters) attained the highest F1-scores of 98.41\\% on UC Merced and\n95.99\\% on AID, significantly outperforming existing baselines, including\nlarger transformer models and Mamba-based architectures, despite using\nconsiderably fewer parameters. An ablation study conclusively demonstrates the\ncritical role of the Fourier Filter Gate in enhancing the model's ability to\ncapture global spatial patterns, leading to robust and accurate classification.\nThese results strongly suggest that the complexities of recurrent SSMs can be\neffectively superseded by a judicious combination of gated convolutions for\nspatial mixing and frequency-based gates for spectral global context. Thus,\nMambaOutRS provides a compelling and efficient paradigm for developing\nhigh-performance deep learning models in remote sensing and other vision\ndomains, particularly where computational efficiency is paramount.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Vision Transformer-Based Time-Series Image Reconstruction for Cloud-Filling Applications",
    "url": "http://arxiv.org/abs/2506.19591v1",
    "authors": [
      "Lujun Li",
      "Yiqun Wang",
      "Radu State"
    ],
    "published": "2025-06-24",
    "abstract": "Cloud cover in multispectral imagery (MSI) poses significant challenges for\nearly season crop mapping, as it leads to missing or corrupted spectral\ninformation. Synthetic aperture radar (SAR) data, which is not affected by\ncloud interference, offers a complementary solution, but lack sufficient\nspectral detail for precise crop mapping. To address this, we propose a novel\nframework, Time-series MSI Image Reconstruction using Vision Transformer (ViT),\nto reconstruct MSI data in cloud-covered regions by leveraging the temporal\ncoherence of MSI and the complementary information from SAR from the attention\nmechanism. Comprehensive experiments, using rigorous reconstruction evaluation\nmetrics, demonstrate that Time-series ViT framework significantly outperforms\nbaselines that use non-time-series MSI and SAR or time-series MSI without SAR,\neffectively enhancing MSI image reconstruction in cloud-covered regions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing",
    "url": "http://arxiv.org/abs/2506.18587v1",
    "authors": [
      "Antoine Saget",
      "Baptiste Lafabregue",
      "Antoine Cornu\u00e9jols",
      "Pierre Gan\u00e7arski"
    ],
    "published": "2025-06-23",
    "abstract": "Given the abundance of unlabeled Satellite Image Time Series (SITS) and the\nscarcity of labeled data, contrastive self-supervised pretraining emerges as a\nnatural tool to leverage this vast quantity of unlabeled data. However,\ndesigning effective data augmentations for contrastive learning remains\nchallenging for time series. We introduce a novel resampling-based augmentation\nstrategy that generates positive pairs by upsampling time series and extracting\ndisjoint subsequences while preserving temporal coverage. We validate our\napproach on multiple agricultural classification benchmarks using Sentinel-2\nimagery, showing that it outperforms common alternatives such as jittering,\nresizing, and masking. Further, we achieve state-of-the-art performance on the\nS2-Agri100 dataset without employing spatial information or temporal encodings,\nsurpassing more complex masked-based SSL frameworks. Our method offers a\nsimple, yet effective, contrastive learning augmentation for remote sensing\ntime series.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Extreme Learning Machines for Exoplanet Simulations: A Faster, Lightweight Alternative to Deep Learning",
    "url": "http://arxiv.org/abs/2506.19679v1",
    "authors": [
      "Tara P. A. Tahseen",
      "Lu\u00eds F. Sim\u00f5es",
      "Kai Hou Yip",
      "Nikolaos Nikolaou",
      "Jo\u00e3o M. Mendon\u00e7a",
      "Ingo P. Waldmann"
    ],
    "published": "2025-06-24",
    "abstract": "Increasing resolution and coverage of astrophysical and climate data\nnecessitates increasingly sophisticated models, often pushing the limits of\ncomputational feasibility. While emulation methods can reduce calculation\ncosts, the neural architectures typically used--optimised via gradient\ndescent--are themselves computationally expensive to train, particularly in\nterms of data generation requirements. This paper investigates the utility of\nthe Extreme Learning Machine (ELM) as a lightweight, non-gradient-based machine\nlearning algorithm for accelerating complex physical models.\n  We evaluate ELM surrogate models in two test cases with different data\nstructures: (i) sequentially-structured data, and (ii) image-structured data.\nFor test case (i), where the number of samples $N$ >> the dimensionality of\ninput data $d$, ELMs achieve remarkable efficiency, offering a 100,000$\\times$\nfaster training time and a 40$\\times$ faster prediction speed compared to a\nBi-Directional Recurrent Neural Network (BIRNN), whilst improving upon BIRNN\ntest performance. For test case (ii), characterised by $d >> N$ and image-based\ninputs, a single ELM was insufficient, but an ensemble of 50 individual ELM\npredictors achieves comparable accuracy to a benchmark Convolutional Neural\nNetwork (CNN), with a 16.4$\\times$ reduction in training time, though costing a\n6.9$\\times$ increase in prediction time. We find different sample efficiency\ncharacteristics between the test cases: in test case (i) individual ELMs\ndemonstrate superior sample efficiency, requiring only 0.28% of the training\ndataset compared to the benchmark BIRNN, while in test case (ii) the ensemble\napproach requires 78% of the data used by the CNN to achieve comparable\nresults--representing a trade-off between sample efficiency and model\ncomplexity.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A deep-learning model for predicting daily PM2.5 concentration in response to emission reduction",
    "url": "http://arxiv.org/abs/2506.18018v1",
    "authors": [
      "Shigan Liu",
      "Guannan Geng",
      "Yanfei Xiang",
      "Hejun Hu",
      "Xiaodong Liu",
      "Xiaomeng Huang",
      "Qiang Zhang"
    ],
    "published": "2025-06-22",
    "abstract": "Air pollution remains a leading global health threat, with fine particulate\nmatter (PM2.5) contributing to millions of premature deaths annually. Chemical\ntransport models (CTMs) are essential tools for evaluating how emission\ncontrols improve air quality and save lives, but they are computationally\nintensive. Reduced form models accelerate simulations but sacrifice\nspatial-temporal granularity, accuracy, and flexibility. Here we present\nCleanAir, a deep-learning-based model developed as an efficient alternative to\nCTMs in simulating daily PM2.5 and its chemical compositions in response to\nprecursor emission reductions at 36 km resolution, which could predict PM2.5\nconcentration for a full year within 10 seconds on a single GPU, a speed five\norders of magnitude faster. Built on a Residual Symmetric 3D U-Net architecture\nand trained on more than 2,400 emission reduction scenarios generated by a\nwell-validated Community Multiscale Air Quality (CMAQ) model, CleanAir\ngeneralizes well across unseen meteorological years and emission patterns. It\nproduces results comparable to CMAQ in both absolute concentrations and\nemission-induced changes, enabling efficient, full-coverage simulations across\nshort-term interventions and long-term planning horizons. This advance empowers\nresearchers and policymakers to rapidly evaluate a wide range of air quality\nstrategies and assess the associated health impacts, thereby supporting more\nresponsive and informed environmental decision-making.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks",
    "url": "http://arxiv.org/abs/2506.20181v1",
    "authors": [
      "Ronald Katende"
    ],
    "published": "2025-06-25",
    "abstract": "We develop a principled framework for discovering causal structure in partial\ndifferential equations (PDEs) using physics-informed neural networks and\ncounterfactual perturbations. Unlike classical residual minimization or sparse\nregression methods, our approach quantifies operator-level necessity through\nfunctional interventions on the governing dynamics. We introduce causal\nsensitivity indices and structural deviation metrics to assess the influence of\ncandidate differential operators within neural surrogates. Theoretically, we\nprove exact recovery of the causal operator support under restricted isometry\nor mutual coherence conditions, with residual bounds guaranteeing\nidentifiability. Empirically, we validate the framework on both synthetic and\nreal-world datasets across climate dynamics, tumor diffusion, and ocean flows.\nOur method consistently recovers governing operators even under noise,\nredundancy, and data scarcity, outperforming standard PINNs and DeepONets in\nstructural fidelity. This work positions causal PDE discovery as a tractable\nand interpretable inference task grounded in structural causal models and\nvariational residual analysis.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting",
    "url": "http://arxiv.org/abs/2506.20024v1",
    "authors": [
      "Salva R\u00fchling Cachay",
      "Miika Aittala",
      "Karsten Kreis",
      "Noah Brenowitz",
      "Arash Vahdat",
      "Morteza Mardani",
      "Rose Yu"
    ],
    "published": "2025-06-24",
    "abstract": "Diffusion models are a powerful tool for probabilistic forecasting, yet most\napplications in high-dimensional chaotic systems predict future snapshots\none-by-one. This common approach struggles to model complex temporal\ndependencies and fails to explicitly account for the progressive growth of\nuncertainty inherent to such systems. While rolling diffusion frameworks, which\napply increasing noise to forecasts at longer lead times, have been proposed to\naddress this, their integration with state-of-the-art, high-fidelity diffusion\ntechniques remains a significant challenge. We tackle this problem by\nintroducing Elucidated Rolling Diffusion Models (ERDM), the first framework to\nsuccessfully unify a rolling forecast structure with the principled, performant\ndesign of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM\ncomponents-its noise schedule, network preconditioning, and Heun sampler-to the\nrolling forecast setting. The success of this integration is driven by three\nkey contributions: (i) a novel loss weighting scheme that focuses model\ncapacity on the mid-range forecast horizons where determinism gives way to\nstochasticity; (ii) an efficient initialization strategy using a pre-trained\nEDM for the initial window; and (iii) a bespoke hybrid sequence architecture\nfor robust spatiotemporal feature extraction under progressive denoising. On 2D\nNavier-Stokes simulations and ERA5 global weather forecasting at 1.5^\\circ\nresolution, ERDM consistently outperforms key diffusion-based baselines,\nincluding conditional autoregressive EDM. ERDM offers a flexible and powerful\ngeneral framework for tackling diffusion-based sequence generation problems\nwhere modeling escalating uncertainty is paramount. Code is available at:\nhttps://github.com/salvaRC/erdm",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models",
    "url": "http://arxiv.org/abs/2506.18124v1",
    "authors": [
      "Shaoxiu Wei",
      "Mingchao Liang",
      "Florian Meyer"
    ],
    "published": "2025-06-22",
    "abstract": "Multiobject tracking (MOT) is an important task in applications including\nautonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT\nmethods are model-based and combine sequential Bayesian estimation with data\nassociation and an object birth model. More recent methods are fully\ndata-driven and rely on the training of neural networks. Both approaches offer\ndistinct advantages in specific settings. In particular, model-based methods\nare generally applicable across a wide range of scenarios, whereas data-driven\nMOT achieves superior performance in scenarios where abundant labeled data for\ntraining is available. A natural thought is whether a general framework can\nintegrate the two approaches. This paper introduces a hybrid method that\nutilizes neural networks to enhance specific aspects of the statistical model\nin Bayesian MOT that have been identified as overly simplistic. By doing so,\nthe performance of the prediction and update steps of Bayesian MOT is improved.\nTo ensure tractable computation, our framework uses belief propagation to avoid\nhigh-dimensional operations combined with sequential Monte Carlo methods to\nperform low-dimensional operations efficiently. The resulting method combines\nthe flexibility and robustness of model-based approaches with the capability to\nlearn complex information from data of neural networks. We evaluate the\nperformance of the proposed method based on the nuScenes autonomous driving\ndataset and demonstrate that it has state-of-the-art performance",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation",
    "url": "http://arxiv.org/abs/2507.00356v1",
    "authors": [
      "Zhiwei Yi",
      "Xin Cheng",
      "Jingyu Ma",
      "Ruifei Zhu",
      "Junwei Tian",
      "Yuanxiu Zhou",
      "Xinge Zhao",
      "Hongzhe Li"
    ],
    "published": "2025-07-01",
    "abstract": "Deep learning methods have significantly advanced the development of\nintelligent rinterpretation in remote sensing (RS), with foundational model\nresearch based on large-scale pre-training paradigms rapidly reshaping various\ndomains of Earth Observation (EO). However, compared to the open accessibility\nand high spatiotemporal coverage of medium-resolution data, the limited\nacquisition channels for ultra-high-resolution optical RS imagery have\nconstrained the progress of high-resolution remote sensing vision foundation\nmodels (RSVFM). As the world's largest sub-meter-level commercial RS satellite\nconstellation, the Jilin-1 constellation possesses abundant sub-meter-level\nimage resources. This study proposes CGEarthEye, a RSVFM framework specifically\ndesigned for Jilin-1 satellite characteristics, comprising five backbones with\ndifferent parameter scales with totaling 2.1 billion parameters. To enhance the\nrepresentational capacity of the foundation model, we developed JLSSD, the\nfirst 15-million-scale multi-temporal self-supervised learning (SSL) dataset\nfeaturing global coverage with quarterly temporal sampling within a single\nyear, constructed through multi-level representation clustering and sampling\nstrategies. The framework integrates seasonal contrast, augmentation-based\ncontrast, and masked patch token contrastive strategies for pre-training.\nComprehensive evaluations across 10 benchmark datasets covering four typical RS\ntasks demonstrate that the CGEarthEye consistently achieves state-of-the-art\n(SOTA) performance. Further analysis reveals CGEarthEye's superior\ncharacteristics in feature visualization, model convergence, parameter\nefficiency, and practical mapping applications. This study anticipates that the\nexceptional representation capabilities of CGEarthEye will facilitate broader\nand more efficient applications of Jilin-1 data in traditional EO application.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery",
    "url": "http://arxiv.org/abs/2507.01747v1",
    "authors": [
      "Nora Gourmelon",
      "Marcel Dreier",
      "Martin Mayr",
      "Thorsten Seehaus",
      "Dakota Pyles",
      "Matthias Braun",
      "Andreas Maier",
      "Vincent Christlein"
    ],
    "published": "2025-07-02",
    "abstract": "Glaciers are losing ice mass at unprecedented rates, increasing the need for\naccurate, year-round monitoring to understand frontal ablation, particularly\nthe factors driving the calving process. Deep learning models can extract\ncalving front positions from Synthetic Aperture Radar imagery to track seasonal\nice losses at the calving fronts of marine- and lake-terminating glaciers. The\ncurrent state-of-the-art model relies on ImageNet-pretrained weights. However,\nthey are suboptimal due to the domain shift between the natural images in\nImageNet and the specialized characteristics of remote sensing imagery, in\nparticular for Synthetic Aperture Radar imagery. To address this challenge, we\npropose two novel self-supervised multimodal pretraining techniques that\nleverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14\nSentinel-2 images of Arctic glaciers, with one optical image per glacier in the\ndataset. Additionally, we introduce a novel hybrid model architecture that\ncombines a Swin Transformer encoder with a residual Convolutional Neural\nNetwork (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean\ndistance error of 293 m on the \"CAlving Fronts and where to Find thEm\" (CaFFe)\nbenchmark dataset, outperforming the prior best model by 67 m. Evaluating an\nensemble of the proposed model on a multi-annotator study of the benchmark\ndataset reveals a mean distance error of 75 m, approaching the human\nperformance of 38 m. This advancement enables precise monitoring of seasonal\nchanges in glacier calving fronts.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images",
    "url": "http://arxiv.org/abs/2507.01502v1",
    "authors": [
      "Ozan Durgut",
      "Beril Kallfelz-Sirmacek",
      "Cem Unsalan"
    ],
    "published": "2025-07-02",
    "abstract": "Global warming, loss of biodiversity, and air pollution are among the most\nsignificant problems facing Earth. One of the primary challenges in addressing\nthese issues is the lack of monitoring forests to protect them. To tackle this\nproblem, it is important to leverage remote sensing and computer vision methods\nto automate monitoring applications. Hence, automatic tree crown detection\nalgorithms emerged based on traditional and deep learning methods. In this\nstudy, we first introduce two different tree crown detection methods based on\nthese approaches. Then, we form a novel rule-based approach that integrates\nthese two methods to enhance robustness and accuracy of tree crown detection\nresults. While traditional methods are employed for feature extraction and\nsegmentation of forested areas, deep learning methods are used to detect tree\ncrowns in our method. With the proposed rule-based approach, we post-process\nthese results, aiming to increase the number of detected tree crowns through\nneighboring trees and localized operations. We compare the obtained results\nwith the proposed method in terms of the number of detected tree crowns and\nreport the advantages, disadvantages, and areas for improvement of the obtained\noutcomes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions",
    "url": "http://arxiv.org/abs/2507.01123v1",
    "authors": [
      "Rahul A. Burange",
      "Harsh K. Shinde",
      "Omkar Mutyalwar"
    ],
    "published": "2025-07-01",
    "abstract": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data",
    "url": "http://arxiv.org/abs/2506.22939v1",
    "authors": [
      "Ghufran A. Omran",
      "Wassan Saad Abduljabbar Hayale",
      "Ahmad AbdulQadir AlRababah",
      "Israa Ibraheem Al-Barazanchi",
      "Ravi Sekhar",
      "Pritesh Shah",
      "Sushma Parihar",
      "Harshavardhan Reddy Penubadi"
    ],
    "published": "2025-06-28",
    "abstract": "Scene categorization (SC) in remotely acquired images is an important subject\nwith broad consequences in different fields, including catastrophe control,\necological observation, architecture for cities, and more. Nevertheless, its\nseveral apps, reaching a high degree of accuracy in SC from distant observation\ndata has demonstrated to be difficult. This is because traditional conventional\ndeep learning models require large databases with high variety and high levels\nof noise to capture important visual features. To address these problems, this\ninvestigation file introduces an innovative technique referred to as the\nCuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type\nof scenes in remote sensing data. The investigation compares the execution of\nCO-BRNN with current techniques, including Multilayer Perceptron- Convolutional\nNeural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory\n(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),\nGraph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional\nNeural Networks Data Augmentation (CNN-DA). The results demonstrate that\nCO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,\nMLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance\nof physical confirmation to ensure the efficiency of satellite data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions",
    "url": "http://arxiv.org/abs/2507.01547v1",
    "authors": [
      "Ubada El Joulani",
      "Tatiana Kalganova",
      "Stergios-Aristoteles Mitoulis",
      "Sotirios Argyroudis"
    ],
    "published": "2025-07-02",
    "abstract": "Critical infrastructure, such as transport networks, underpins economic\ngrowth by enabling mobility and trade. However, ageing assets, climate change\nimpacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging\nfrom natural disasters to cyber attacks and conflicts pose growing risks to\ntheir resilience and functionality. This review paper explores how emerging\ndigital technologies, specifically Artificial Intelligence (AI), can enhance\ndamage assessment and monitoring of transport infrastructure. A systematic\nliterature review examines existing AI models and datasets for assessing damage\nin roads, bridges, and other critical infrastructure impacted by natural\ndisasters. Special focus is given to the unique challenges and opportunities\nassociated with bridge damage detection due to their structural complexity and\ncritical role in connectivity. The integration of SAR (Synthetic Aperture\nRadar) data with AI models is also discussed, with the review revealing a\ncritical research gap: a scarcity of studies applying AI models to SAR data for\ncomprehensive bridge damage assessment. Therefore, this review aims to identify\nthe research gaps and provide foundations for AI-driven solutions for assessing\nand monitoring critical transport infrastructures.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Advancements in Weed Mapping: A Systematic Review",
    "url": "http://arxiv.org/abs/2507.01269v1",
    "authors": [
      "Mohammad Jahanbakht",
      "Alex Olsen",
      "Ross Marchant",
      "Emilie Fillols",
      "Mostafa Rahimi Azghadi"
    ],
    "published": "2025-07-02",
    "abstract": "Weed mapping plays a critical role in precision management by providing\naccurate and timely data on weed distribution, enabling targeted control and\nreduced herbicide use. This minimizes environmental impacts, supports\nsustainable land management, and improves outcomes across agricultural and\nnatural environments. Recent advances in weed mapping leverage ground-vehicle\nRed Green Blue (RGB) cameras, satellite and drone-based remote sensing combined\nwith sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The\nresulting data are processed using advanced techniques including big data\nanalytics and machine learning, significantly improving the spatial and\ntemporal resolution of weed maps and enabling site-specific management\ndecisions. Despite a growing body of research in this domain, there is a lack\nof comprehensive literature reviews specifically focused on weed mapping. In\nparticular, the absence of a structured analysis spanning the entire mapping\npipeline, from data acquisition to processing techniques and mapping tools,\nlimits progress in the field. This review addresses these gaps by\nsystematically examining state-of-the-art methods in data acquisition (sensor\nand platform technologies), data processing (including annotation and\nmodelling), and mapping techniques (such as spatiotemporal analysis and\ndecision support tools). Following PRISMA guidelines, we critically evaluate\nand synthesize key findings from the literature to provide a holistic\nunderstanding of the weed mapping landscape. This review serves as a\nfoundational reference to guide future research and support the development of\nefficient, scalable, and sustainable weed management systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution",
    "url": "http://arxiv.org/abs/2506.23566v1",
    "authors": [
      "Luigi Sigillo",
      "Renato Giamba",
      "Danilo Comminiello"
    ],
    "published": "2025-06-30",
    "abstract": "The acquisition of high-resolution satellite imagery is often constrained by\nthe spatial and temporal limitations of satellite sensors, as well as the high\ncosts associated with frequent observations. These challenges hinder\napplications such as environmental monitoring, disaster response, and\nagricultural management, which require fine-grained and high-resolution data.\nIn this paper, we propose MWT-Diff, an innovative framework for satellite image\nsuper-resolution (SR) that combines latent diffusion models with wavelet\ntransforms to address these challenges. At the core of the framework is a novel\nmetadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates\nembeddings that capture metadata attributes, multi-scale frequency information,\nand temporal relationships. The embedded feature representations steer the\nhierarchical diffusion dynamics, through which the model progressively\nreconstructs high-resolution satellite imagery from low-resolution inputs. This\nprocess preserves critical spatial characteristics including textural patterns,\nboundary discontinuities, and high-frequency spectral components essential for\ndetailed remote sensing analysis. The comparative analysis of MWT-Diff across\nmultiple datasets demonstrated favorable performance compared to recent\napproaches, as measured by standard perceptual quality metrics including FID\nand LPIPS.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification",
    "url": "http://arxiv.org/abs/2506.23462v1",
    "authors": [
      "Manaswi Kulahara",
      "Gautam Siddharth Kashyap",
      "Nipun Joshi",
      "Arpita Soni"
    ],
    "published": "2025-06-30",
    "abstract": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Accurate Mediterranean Sea forecasting via graph-based deep learning",
    "url": "http://arxiv.org/abs/2506.23900v1",
    "authors": [
      "Daniel Holmberg",
      "Emanuela Clementi",
      "Italo Epicoco",
      "Teemu Roos"
    ],
    "published": "2025-06-30",
    "abstract": "Accurate ocean forecasting systems are essential for understanding marine\ndynamics, which play a crucial role in sectors such as shipping, aquaculture,\nenvironmental monitoring, and coastal risk management. Traditional numerical\nsolvers, while effective, are computationally expensive and time-consuming.\nRecent advancements in machine learning have revolutionized weather\nforecasting, offering fast and energy-efficient alternatives. Building on these\nadvancements, we introduce SeaCast, a neural network designed for\nhigh-resolution regional ocean forecasting. SeaCast employs a graph-based\nframework to effectively handle the complex geometry of ocean grids and\nintegrates external forcing data tailored to the regional ocean context. Our\napproach is validated through experiments at a high horizontal resolution using\nthe operational numerical forecasting system of the Mediterranean Sea, along\nwith both numerical and data-driven atmospheric forcings. Results demonstrate\nthat SeaCast consistently outperforms the operational model in forecast skill,\nmarking a significant advancement in regional ocean prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion",
    "url": "http://arxiv.org/abs/2507.01354v1",
    "authors": [
      "Chugang Yi",
      "Minghan Yu",
      "Weikang Qian",
      "Yixin Wen",
      "Haizhao Yang"
    ],
    "published": "2025-07-02",
    "abstract": "Effective hydrological modeling and extreme weather analysis demand\nprecipitation data at a kilometer-scale resolution, which is significantly\nfiner than the 10 km scale offered by standard global products like IMERG. To\naddress this, we propose the Wavelet Diffusion Model (WDM), a generative\nframework that achieves 10x spatial super-resolution (downscaling to 1 km) and\ndelivers a 9x inference speedup over pixel-based diffusion models. WDM is a\nconditional diffusion model that learns the learns the complex structure of\nprecipitation from MRMS radar data directly in the wavelet domain. By focusing\non high-frequency wavelet coefficients, it generates exceptionally realistic\nand detailed 1-km precipitation fields. This wavelet-based approach produces\nvisually superior results with fewer artifacts than pixel-space models, and\ndelivers a significant gains in sampling efficiency. Our results demonstrate\nthat WDM provides a robust solution to the dual challenges of accuracy and\nspeed in geoscience super-resolution, paving the way for more reliable\nhydrological forecasts.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "Guided Unconditional and Conditional Generative Models for Super-Resolution and Inference of Quasi-Geostrophic Turbulence",
    "url": "http://arxiv.org/abs/2507.00719v1",
    "authors": [
      "Anantha Narayanan Suresh Babu",
      "Akhil Sadam",
      "Pierre F. J. Lermusiaux"
    ],
    "published": "2025-07-01",
    "abstract": "Typically, numerical simulations of the ocean, weather, and climate are\ncoarse, and observations are sparse and gappy. In this work, we apply four\ngenerative diffusion modeling approaches to super-resolution and inference of\nforced two-dimensional quasi-geostrophic turbulence on the beta-plane from\ncoarse, sparse, and gappy observations. Two guided approaches minimally adapt a\npre-trained unconditional model: SDEdit modifies the initial condition, and\nDiffusion Posterior Sampling (DPS) modifies the reverse diffusion process\nscore. The other two conditional approaches, a vanilla variant and\nclassifier-free guidance, require training with paired high-resolution and\nobservation data. We consider eight test cases spanning: two regimes, eddy and\nanisotropic-jet turbulence; two Reynolds numbers, 10^3 and 10^4; and two\nobservation types, 4x coarse-resolution fields and coarse, sparse and gappy\nobservations. Our comprehensive skill metrics include norms of the\nreconstructed vorticity fields, turbulence statistical quantities, and\nquantification of the super-resolved probabilistic ensembles and their errors.\nWe also study the sensitivity to tuning parameters such as guidance strength.\nResults show that SDEdit generates unphysical fields, while DPS generates\nreasonable reconstructions at low computational cost but with smoothed\nfine-scale features. Both conditional approaches require re-training, but they\nreconstruct missing fine-scale features, are cycle-consistent with\nobservations, and possess the correct statistics such as energy spectra.\nFurther, their mean model errors are highly correlated with and predictable\nfrom their ensemble standard deviations. Results highlight the trade-offs\nbetween ease of implementation, fidelity (sharpness), and cycle-consistency of\nthe diffusion models, and offer practical guidance for deployment in\ngeophysical inverse problems.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "MAPEX: Modality-Aware Pruning of Experts for Remote Sensing Foundation Models",
    "url": "http://arxiv.org/abs/2507.07527v1",
    "authors": [
      "Joelle Hanna",
      "Linus Scheibenreif",
      "Damian Borth"
    ],
    "published": "2025-07-10",
    "abstract": "Remote sensing data is commonly used for tasks such as flood mapping,\nwildfire detection, or land-use studies. For each task, scientists carefully\nchoose appropriate modalities or leverage data from purpose-built instruments.\nRecent work on remote sensing foundation models pre-trains computer vision\nmodels on large amounts of remote sensing data. These large-scale models tend\nto focus on specific modalities, often optical RGB or multispectral data. For\nmany important applications, this introduces a mismatch between the application\nmodalities and the pre-training data. Moreover, the large size of foundation\nmodels makes them expensive and difficult to fine-tune on typically small\ndatasets for each task. We address this mismatch with MAPEX, a remote sensing\nfoundation model based on mixture-of-modality experts. MAPEX is pre-trained on\nmulti-modal remote sensing data with a novel modality-conditioned token routing\nmechanism that elicits modality-specific experts. To apply the model on a\nspecific task, we propose a modality aware pruning technique, which only\nretains experts specialized for the task modalities. This yields efficient\nmodality-specific models while simplifying fine-tuning and deployment for the\nmodalities of interest. We experimentally validate MAPEX on diverse remote\nsensing datasets and show strong performance compared to fully supervised\ntraining and state-of-the-art remote sensing foundation models. Code is\navailable at https://github.com/HSG-AIML/MAPEX.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models",
    "url": "http://arxiv.org/abs/2507.06231v1",
    "authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Bowen Chen",
      "Jiafan Zhang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-07-08",
    "abstract": "Referring Remote Sensing Image Segmentation provides a flexible and\nfine-grained framework for remote sensing scene analysis via vision-language\ncollaborative interpretation. Current approaches predominantly utilize a\nthree-stage pipeline encompassing dual-modal encoding, cross-modal interaction,\nand pixel decoding. These methods demonstrate significant limitations in\nmanaging complex semantic relationships and achieving precise cross-modal\nalignment, largely due to their coupled processing mechanism that conflates\ntarget localization with boundary delineation. This architectural coupling\namplifies error propagation under semantic ambiguity while restricting model\ngeneralizability and interpretability. To address these issues, we propose\nRSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow\ninto a collaborative dual-stage framework: coarse localization followed by fine\nsegmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with\nSAM's segmentation generalizability through strategic foundation model\ncollaboration. Specifically, CLIP is employed as the dual-modal encoder to\nactivate target features within its pre-aligned semantic space and generate\nlocalization prompts. To mitigate CLIP's misactivation challenges in\nmulti-entity scenarios described by referring texts, a cascaded second-order\nprompter is devised, which enhances precision through implicit reasoning via\ndecomposition of text embeddings into complementary semantic subspaces. These\noptimized semantic prompts subsequently direct the SAM to generate pixel-level\nrefined masks, thereby completing the semantic transmission pipeline. Extensive\nexperiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2\nsurpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex\nsemantic interpretation. Code is available at:\nhttps://github.com/KyanChen/RSRefSeg2.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "From General to Specialized: The Need for Foundational Models in Agriculture",
    "url": "http://arxiv.org/abs/2507.05390v1",
    "authors": [
      "Vishal Nedungadi",
      "Xingguo Xiong",
      "Aike Potze",
      "Ron Van Bree",
      "Tao Lin",
      "Marc Ru\u00dfwurm",
      "Ioannis N. Athanasiadis"
    ],
    "published": "2025-07-07",
    "abstract": "Food security remains a global concern as population grows and climate change\nintensifies, demanding innovative solutions for sustainable agricultural\nproductivity. Recent advances in foundation models have demonstrated remarkable\nperformance in remote sensing and climate sciences, and therefore offer new\nopportunities for agricultural monitoring. However, their application in\nchallenges related to agriculture-such as crop type mapping, crop phenology\nestimation, and crop yield estimation-remains under-explored. In this work, we\nquantitatively evaluate existing foundational models to assess their\neffectivity for a representative set of agricultural tasks. From an\nagricultural domain perspective, we describe a requirements framework for an\nideal agricultural foundation model (CropFM). We then survey and compare\nexisting general-purpose foundational models in this framework and empirically\nevaluate two exemplary of them in three representative agriculture specific\ntasks. Finally, we highlight the need for a dedicated foundational model\ntailored specifically to agriculture.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Time2Agri: Temporal Pretext Tasks for Agricultural Monitoring",
    "url": "http://arxiv.org/abs/2507.04366v1",
    "authors": [
      "Moti Rattan Gupta",
      "Anupam Sobti"
    ],
    "published": "2025-07-06",
    "abstract": "Self Supervised Learning(SSL) has emerged as a prominent paradigm for\nlabel-efficient learning, and has been widely utilized by remote sensing\nfoundation models(RSFMs). Recent RSFMs including SatMAE, DoFA, primarily rely\non masked autoencoding(MAE), contrastive learning or some combination of them.\nHowever, these pretext tasks often overlook the unique temporal characteristics\nof agricultural landscape, namely nature's cycle. Motivated by this gap, we\npropose three novel agriculture-specific pretext tasks, namely Time-Difference\nPrediction(TD), Temporal Frequency Prediction(FP), and Future-Frame\nPrediction(FF). Comprehensive evaluation on SICKLE dataset shows FF achieves\n69.6% IoU on crop mapping and FP reduces yield prediction error to 30.7% MAPE,\noutperforming all baselines, and TD remains competitive on most tasks. Further,\nwe also scale FF to the national scale of India, achieving 54.2% IoU\noutperforming all baselines on field boundary delineation on FTW India dataset.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction",
    "url": "http://arxiv.org/abs/2507.07388v1",
    "authors": [
      "Zesheng Liu",
      "Maryam Rahnemoonfar"
    ],
    "published": "2025-07-10",
    "abstract": "Gaining a deeper understanding of the thickness and variability of internal\nice layers in Radar imagery is essential in monitoring the snow accumulation,\nbetter evaluating ice dynamics processes, and minimizing uncertainties in\nclimate models. Radar sensors, capable of penetrating ice, capture detailed\nradargram images of internal ice layers. In this work, we introduce GRIT, graph\ntransformer for ice layer thickness. GRIT integrates an inductive geometric\ngraph learning framework with an attention mechanism, designed to map the\nrelationships between shallow and deeper ice layers. Compared to baseline graph\nneural networks, GRIT demonstrates consistently lower prediction errors. These\nresults highlight the attention mechanism's effectiveness in capturing temporal\nchanges across ice layers, while the graph transformer combines the strengths\nof transformers for learning long-range dependencies with graph neural networks\nfor capturing spatial patterns, enabling robust modeling of complex\nspatiotemporal dynamics.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction",
    "url": "http://arxiv.org/abs/2507.06806v1",
    "authors": [
      "Eya Cherif",
      "Arthur Ouaknine",
      "Luke A. Brown",
      "Phuong D. Dao",
      "Kyle R. Kovach",
      "Bing Lu",
      "Daniel Mederer",
      "Hannes Feilhauer",
      "Teja Kattenborn",
      "David Rolnick"
    ],
    "published": "2025-07-09",
    "abstract": "Plant traits such as leaf carbon content and leaf mass are essential\nvariables in the study of biodiversity and climate change. However,\nconventional field sampling cannot feasibly cover trait variation at\necologically meaningful spatial scales. Machine learning represents a valuable\nsolution for plant trait prediction across ecosystems, leveraging hyperspectral\ndata from remote sensing. Nevertheless, trait prediction from hyperspectral\ndata is challenged by label scarcity and substantial domain shifts (\\eg across\nsensors, ecological distributions), requiring robust cross-domain methods.\nHere, we present GreenHyperSpectra, a pretraining dataset encompassing\nreal-world cross-sensor and cross-ecosystem samples designed to benchmark trait\nprediction with semi- and self-supervised methods. We adopt an evaluation\nframework encompassing in-distribution and out-of-distribution scenarios. We\nsuccessfully leverage GreenHyperSpectra to pretrain label-efficient\nmulti-output regression models that outperform the state-of-the-art supervised\nbaseline. Our empirical analyses demonstrate substantial improvements in\nlearning spectral representations for trait prediction, establishing a\ncomprehensive methodological framework to catalyze research at the intersection\nof representation learning and plant functional traits assessment. All code and\ndata are available at: https://github.com/echerif18/HyspectraSSL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing",
    "url": "http://arxiv.org/abs/2507.04842v1",
    "authors": [
      "Colin Laganier",
      "Liam Fletcher",
      "Elim Kwan",
      "Richard Walters",
      "Victoria Nockles"
    ],
    "published": "2025-07-07",
    "abstract": "Rapid analysis of satellite data is vital for many remote sensing\napplications, from disaster response to environmental monitoring, but is\nbecoming harder to achieve with the increasing volumes of data generated by\nmodern satellites. On-satellite machine learning (ML) offers a potential\nsolution, by reducing latency associated with transmission of these large data\nvolumes to ground stations, but state-of-the-art models are often too large or\npower-hungry for satellite deployment. Vessel detection using Synthetic\nAperture Radar (SAR) is a critical time-sensitive task for maritime security\nthat exemplifies this challenge. SAR vessel detection has previously been\ndemonstrated only by ML models that either are too large for satellite\ndeployment, have not been developed for sufficiently low-power hardware, or\nhave only been developed and tested on small SAR datasets that do not\nsufficiently represent the real-world task. Here we address this issue by\ndeveloping and deploying a new efficient and highly performant SAR vessel\ndetection model, using a customised YOLOv8 architecture specifically optimized\nfor FPGA-based processing within common satellite power constraints (<10W). We\ntrain and evaluate our model on the largest and most diverse open SAR vessel\ndataset, xView3-SAR, and deploy it on a Kria KV260 MPSoC. We show that our\nFPGA-based model has detection and classification performance only ~2% and 3%\nlower than values from state-of-the-art GPU-based models, despite being two to\nthree orders of magnitude smaller in size. This work demonstrates small yet\nhighly performant ML models for time-critical SAR analysis, paving the way for\nmore autonomous, responsive, and scalable Earth observation systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Generative Lagrangian data assimilation for ocean dynamics under extreme sparsity",
    "url": "http://arxiv.org/abs/2507.06479v1",
    "authors": [
      "Niloofar Asefi",
      "Leonard Lupin-Jimenez",
      "Tianning Wu",
      "Ruoying He",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-07-09",
    "abstract": "Reconstructing ocean dynamics from observational data is fundamentally\nlimited by the sparse, irregular, and Lagrangian nature of spatial sampling,\nparticularly in subsurface and remote regions. This sparsity poses significant\nchallenges for forecasting key phenomena such as eddy shedding and rogue waves.\nTraditional data assimilation methods and deep learning models often struggle\nto recover mesoscale turbulence under such constraints. We leverage a deep\nlearning framework that combines neural operators with denoising diffusion\nprobabilistic models (DDPMs) to reconstruct high-resolution ocean states from\nextremely sparse Lagrangian observations. By conditioning the generative model\non neural operator outputs, the framework accurately captures small-scale,\nhigh-wavenumber dynamics even at $99\\%$ sparsity (for synthetic data) and\n$99.9\\%$ sparsity (for real satellite observations). We validate our method on\nbenchmark systems, synthetic float observations, and real satellite data,\ndemonstrating robust performance under severe spatial sampling limitations as\ncompared to other deep learning baselines.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning in Acoustics: A Review and Open-Source Repository",
    "url": "http://arxiv.org/abs/2507.04419v1",
    "authors": [
      "Ryan A. McCarthy",
      "You Zhang",
      "Samuel A. Verburg",
      "William F. Jenkins",
      "Peter Gerstoft"
    ],
    "published": "2025-07-06",
    "abstract": "Acoustic data provide scientific and engineering insights in fields ranging\nfrom bioacoustics and communications to ocean and earth sciences. In this\nreview, we survey recent advances and the transformative potential of machine\nlearning (ML) in acoustics, including deep learning (DL). Using the Python\nhigh-level programming language, we demonstrate a broad collection of ML\ntechniques to detect and find patterns for classification, regression, and\ngeneration in acoustics data automatically. We have ML examples including\nacoustic data classification, generative modeling for spatial audio, and\nphysics-informed neural networks. This work includes AcousticsML, a set of\npractical Jupyter notebook examples on GitHub demonstrating ML benefits and\nencouraging researchers and practitioners to apply reproducible data-driven\napproaches to acoustic challenges.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "HRRRCast: a data-driven emulator for regional weather forecasting at convection allowing scales",
    "url": "http://arxiv.org/abs/2507.05658v1",
    "authors": [
      "Daniel Abdi",
      "Isidora Jankov",
      "Paul Madden",
      "Vanderlei Vargas",
      "Timothy A. Smith",
      "Sergey Frolov",
      "Montgomery Flora",
      "Corey Potvin"
    ],
    "published": "2025-07-08",
    "abstract": "The High-Resolution Rapid Refresh (HRRR) model is a convection-allowing model\nused in operational weather forecasting across the contiguous United States\n(CONUS). To provide a computationally efficient alternative, we introduce\nHRRRCast, a data-driven emulator built with advanced machine learning\ntechniques. HRRRCast includes two architectures: a ResNet-based model (ResHRRR)\nand a Graph Neural Network-based model (GraphHRRR). ResHRRR uses convolutional\nneural networks enhanced with squeeze-and-excitation blocks and Feature-wise\nLinear Modulation, and supports probabilistic forecasting via the Denoising\nDiffusion Implicit Model (DDIM). To better handle longer lead times, we train a\nsingle model to predict multiple lead times (1h, 3h, and 6h), then use a greedy\nrollout strategy during inference. When evaluated on composite reflectivity\nover the full CONUS domain using ensembles of 3 to 10 members, ResHRRR\noutperforms HRRR forecast at light rainfall threshold (20 dBZ) and achieves\ncompetitive performance at moderate thresholds (30 dBZ). Our work advances the\nStormCast model of Pathak et al. [21] by: a) training on the full CONUS domain,\nb) using multiple lead times to improve long-range skill, c) training on\nanalysis data instead of the +1h post-analysis data inadvertently used in\nStormCast, and d) incorporating future GFS states as inputs, enabling\ndownscaling that improves long-lead accuracy. Grid-, neighborhood-, and\nobject-based metrics confirm better storm placement, lower frequency bias, and\nhigher success ratios than HRRR. HRRRCast ensemble forecasts also maintain\nsharper spatial detail, with power spectra more closely matching HRRR analysis.\nWhile GraphHRRR underperforms in its current form, it lays groundwork for\nfuture graph-based forecasting. HRRRCast represents a step toward efficient,\ndata-driven regional weather prediction with competitive accuracy and ensemble\ncapability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Site-Specific Parameterization of Ocean Spectra for Power Estimates of Wave Energy Converters",
    "url": "http://arxiv.org/abs/2507.05440v1",
    "authors": [
      "Rafael Baez Ramirez",
      "Ethan J. Sloan",
      "Carlos Alejandro Michel\u00e9n Str\u00f6fer"
    ],
    "published": "2025-07-07",
    "abstract": "Estimating the mean annual power of a wave energy converter (WEC) through the\nmethod of bins relies on a parametric representation of all possible sea\nstates. In practice, two-parameter spectra based on significant wave height and\nenergy period are ubiquitous. Two-parameter spectra have been shown\ninsufficient in capturing the range of spectral shapes that can occur in an\nactual ocean environment. Furthermore, through sensitivity analysis, these\ntwo-parameters have been shown to be insufficient for predicting power\nperformance of WECs. Four parameter spectra, which expand the parameter space\nto include two additional shape parameters have been shown sufficient in\ncapturing sea state variance, but their effect on mean power estimates has not\nbeen presented. This work directly looks at the effects of incorporating\n4-parameter spectra into annual power estimates compared to using the\ntraditional 2-parameter spectra. We use two different 4-parameter spectra: one\nfrom the literature and a novel machine learning-based autoencoder, presented\nhere. Both are shown to improve the information retained when parameterizing\nspectra. The site-specific autoencoder performs consistently the best across\ntwo case studies of mean annual power prediction, achieving an error around 1%\nin each instance. The 2-parameter spectra resulted in less consistent\npredictive performance, with errors of -8% and 1% in the two case studies. For\nthe case study where all three models performed well, it is shown that the low\nerror in the 2-parameter model is attributable to a symmetrical distribution of\nlarge errors whereas both 4-parameter spectra result in relatively low errors\nthroughout the parametric space. These results highlight the need for more\nsophisticated resource characterization methods for estimating the power\nperformance of WECs and suggest site-specific machine learning-based spectra\nare an adequate option.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Interpretable Machine Learning for Urban Heat Mitigation: Attribution and Weighting of Multi-Scale Drivers",
    "url": "http://arxiv.org/abs/2507.04802v1",
    "authors": [
      "David Tschan",
      "Zhi Wang",
      "Jan Carmeliet",
      "Yongling Zhao"
    ],
    "published": "2025-07-07",
    "abstract": "Urban heat islands (UHIs) are often accentuated during heat waves (HWs) and\npose a public health risk. Mitigating UHIs requires urban planners to first\nestimate how urban heat is influenced by different land use types (LUTs) and\ndrivers across scales - from synoptic-scale climatic background processes to\nsmall-scale urban- and scale-bridging features. This study proposes to classify\nthese drivers into driving (D), urban (U), and local (L) features,\nrespectively. To increase interpretability and enhance computation efficiency,\na LUT-distinguishing machine learning approach is proposed as a fast emulator\nfor Weather Research and Forecasting model coupled to a Single-Layer Urban\nCanopy Model (WRF-SLUCM) to predict ground- (TSK) and 2-meter air temperature\n(T2). Using random forests (RFs) with extreme gradient boosting (XGB) trained\non WRF-SLUCM output over Zurich, Switzerland, during heatwave (HW) periods in\n2017 and 2019, this study proposes LUT-based (LB) models that categorize\nfeatures by scales and practical controllability, allowing optional categorical\nweighting. This approach enables category-specific feature ranking and\nsensitivity estimation of T2 and TSK to most important small-scale drivers -\nmost notably surface emissivity, albedo, and leaf area index (LAI). Models\nemploying the LB framework are statistically significantly more accurate than\nmodels that do not, with higher performance when more HW data is included in\ntraining. With RF-XGB robustly performing optimal with unit weights, the method\nsubstantially increase interpretability. Despite the needs to reduce\nstatistical uncertainties and testing the method on other cities, the proposed\napproach offers urban planners a direct framework for feasibility-centered UHI\nmitigation assessment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Continental scale habitat modelling with artificial intelligence and multimodal earth observation",
    "url": "http://arxiv.org/abs/2507.09732v1",
    "authors": [
      "Sara Si-Moussi",
      "Stephan Hennekens",
      "Sander Mucher",
      "Stan Los",
      "Wilfried Thuiller"
    ],
    "published": "2025-07-13",
    "abstract": "Habitats integrate the abiotic conditions and biophysical structures that\nsupport biodiversity and sustain nature's contributions to people. As these\necosystems face mounting pressure from human activities, accurate,\nhigh-resolution habitat maps are essential for effective conservation and\nrestoration. Yet current maps often fall short in thematic or spatial\nresolution because they must (1) model several mutually exclusive habitat types\nthat co-occur across landscapes and (2) cope with severe class imbalance that\ncomplicate multi-class training. Here, we evaluated how high-resolution remote\nsensing (RS) data and Artificial Intelligence (AI) tools can improve habitat\nclassification over large geographic extents at fine thematic resolution. Using\nvegetation plots from the European Vegetation Archive, we modelled Level 3\nEUNIS habitats across Europe and assessed multiple modelling strategies against\nindependent validation datasets. Strategies that exploited the hierarchical\nnature of habitat nomenclatures resolved classification ambiguities, especially\nin fragmented landscapes. Integrating multi-spectral (MSI) and synthetic\naperture radar (SAR) imagery, particularly through Earth Observation Foundation\nmodels, enhanced within-formation discrimination and overall performance.\nFinally, ensemble machine learning that corrects class imbalance boosted\naccuracy further. Our methodological framework is transferable beyond Europe\nand adaptable to other classification systems. Future research should advance\ntemporal modelling of dynamic habitats, extend to habitat segmentation and\nquality assessment, and exploit next-generation EO data paired with\nhigher-quality in-situ observations.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges",
    "url": "http://arxiv.org/abs/2507.09562v1",
    "authors": [
      "Yidong Jiang"
    ],
    "published": "2025-07-13",
    "abstract": "The Segment Anything Model (SAM) has revolutionized image segmentation\nthrough its innovative prompt-based approach, yet the critical role of prompt\nengineering in its success remains underexplored. This paper presents the first\ncomprehensive survey focusing specifically on prompt engineering techniques for\nSAM and its variants. We systematically organize and analyze the rapidly\ngrowing body of work in this emerging field, covering fundamental\nmethodologies, practical applications, and key challenges. Our review reveals\nhow prompt engineering has evolved from simple geometric inputs to\nsophisticated multimodal approaches, enabling SAM's adaptation across diverse\ndomains including medical imaging and remote sensing. We identify unique\nchallenges in prompt optimization and discuss promising research directions.\nThis survey fills an important gap in the literature by providing a structured\nframework for understanding and advancing prompt engineering in foundation\nmodels for segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge",
    "url": "http://arxiv.org/abs/2507.09202v1",
    "authors": [
      "Wuxin Wang",
      "Weicheng Ni",
      "Lilan Huang",
      "Tao Hao",
      "Ben Fei",
      "Shuo Ma",
      "Taikang Yuan",
      "Yanlai Zhao",
      "Kefeng Deng",
      "Xiaoyong Li",
      "Boheng Duan",
      "Lei Bai",
      "Kaijun Ren"
    ],
    "published": "2025-07-12",
    "abstract": "Recent advancements in Artificial Intelligence (AI) demonstrate significant\npotential to revolutionize weather forecasting. However, most AI-driven models\nrely on Numerical Weather Prediction (NWP) systems for initial condition\npreparation, which often consumes hours on supercomputers. Here we introduce\nXiChen, the first observation-scalable fully AI-driven global weather\nforecasting system, whose entire pipeline, from Data Assimilation (DA) to\nmedium-range forecasting, can be accomplished within only 17 seconds. XiChen is\nbuilt upon a foundation model that is pre-trained for weather forecasting.\nMeanwhile, this model is subsequently fine-tuned to serve as both observation\noperators and DA models, thereby scalably assimilating conventional and raw\nsatellite observations. Furthermore, the integration of four-dimensional\nvariational knowledge ensures that XiChen's DA and medium-range forecasting\naccuracy rivals that of operational NWP systems, amazingly achieving a skillful\nforecasting lead time exceeding 8.25 days. These findings demonstrate that\nXiChen holds strong potential toward fully AI-driven weather forecasting\nindependent of NWP systems.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation",
    "url": "http://arxiv.org/abs/2507.12857v1",
    "authors": [
      "Shiqi Huang",
      "Shuting He",
      "Huaiyuan Qin",
      "Bihan Wen"
    ],
    "published": "2025-07-17",
    "abstract": "Most existing remote sensing instance segmentation approaches are designed\nfor close-vocabulary prediction, limiting their ability to recognize novel\ncategories or generalize across datasets. This restricts their applicability in\ndiverse Earth observation scenarios. To address this, we introduce\nopen-vocabulary (OV) learning for remote sensing instance segmentation. While\ncurrent OV segmentation models perform well on natural image datasets, their\ndirect application to remote sensing faces challenges such as diverse\nlandscapes, seasonal variations, and the presence of small or ambiguous objects\nin aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$\n($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary\n$\\textbf{RE}$mote sensing instance segmentation), a framework that integrates\nmulti-granularity scene context, i.e., regional context and global context, to\nenhance both visual and textual representations. Specifically, we introduce\nRegion-Aware Integration, which refines class embeddings with regional context\nto improve object distinguishability. Additionally, we propose Global Context\nAdaptation, which enriches naive text embeddings with remote sensing global\ncontext, creating a more adaptable and expressive linguistic latent space for\nthe classifier. We establish new benchmarks for OV remote sensing instance\nsegmentation across diverse datasets. Experimental results demonstrate that,\nour proposed method achieves SOTA performance, which provides a robust solution\nfor large-scale, real-world geospatial analysis. Our code is available at\nhttps://github.com/HuangShiqi128/SCORE.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "A Deep-Learning Framework for Land-Sliding Classification from Remote Sensing Image",
    "url": "http://arxiv.org/abs/2507.12939v1",
    "authors": [
      "Hieu Tang",
      "Truong Vo",
      "Dong Pham",
      "Toan Nguyen",
      "Lam Pham",
      "Truong Nguyen"
    ],
    "published": "2025-07-17",
    "abstract": "The use of satellite imagery combined with deep learning to support automatic\nlandslide detection is becoming increasingly widespread. However, selecting an\nappropriate deep learning architecture to optimize performance while avoiding\noverfitting remains a critical challenge. To address these issues, we propose a\ndeep-learning based framework for landslide detection from remote sensing image\nin this paper. The proposed framework presents an effective combination of the\nonline an offline data augmentation to tackle the imbalanced data, a backbone\nEfficientNet\\_Large deep learning model for extracting robust embedding\nfeatures, and a post-processing SVM classifier to balance and enhance the\nclassification performance. The proposed model achieved an F1-score of 0.8938\non the public test set of the Zindi challenge.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Precision Spatio-Temporal Feature Fusion for Robust Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2507.11523v1",
    "authors": [
      "Buddhi Wijenayake",
      "Athulya Ratnayake",
      "Praveen Sumanasekara",
      "Nichula Wasalathilaka",
      "Mathivathanan Piratheepan",
      "Roshan Godaliyadda",
      "Mervyn Ekanayake",
      "Vijitha Herath"
    ],
    "published": "2025-07-15",
    "abstract": "Remote sensing change detection is vital for monitoring environmental and\nurban transformations but faces challenges like manual feature extraction and\nsensitivity to noise. Traditional methods and early deep learning models, such\nas convolutional neural networks (CNNs), struggle to capture long-range\ndependencies and global context essential for accurate change detection in\ncomplex scenes. While Transformer-based models mitigate these issues, their\ncomputational complexity limits their applicability in high-resolution remote\nsensing. Building upon ChangeMamba architecture, which leverages state space\nmodels for efficient global context modeling, this paper proposes precision\nfusion blocks to capture channel-wise temporal variations and per-pixel\ndifferences for fine-grained change detection. An enhanced decoder pipeline,\nincorporating lightweight channel reduction mechanisms, preserves local details\nwith minimal computational cost. Additionally, an optimized loss function\ncombining Cross Entropy, Dice and Lovasz objectives addresses class imbalance\nand boosts Intersection-over-Union (IoU). Evaluations on SYSU-CD, LEVIR-CD+,\nand WHU-CD datasets demonstrate superior precision, recall, F1 score, IoU, and\noverall accuracy compared to state-of-the-art methods, highlighting the\napproach's robustness for remote sensing change detection. For complete\ntransparency, the codes and pretrained models are accessible at\nhttps://github.com/Buddhi19/MambaCD.git",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images",
    "url": "http://arxiv.org/abs/2507.11143v1",
    "authors": [
      "Lam Pham",
      "Cam Le",
      "Hieu Tang",
      "Khang Truong",
      "Truong Nguyen",
      "Jasmin Lampert",
      "Alexander Schindler",
      "Martin Boyer",
      "Son Phan"
    ],
    "published": "2025-07-15",
    "abstract": "In recent years, landslide disasters have reported frequently due to the\nextreme weather events of droughts, floods , storms, or the consequence of\nhuman activities such as deforestation, excessive exploitation of natural\nresources. However, automatically observing landslide is challenging due to the\nextremely large observing area and the rugged topography such as mountain or\nhighland. This motivates us to propose an end-to-end deep-learning-based model\nwhich explores the remote sensing images for automatically observing landslide\nevents. By considering remote sensing images as the input data, we can obtain\nfree resource, observe large and rough terrains by time. To explore the remote\nsensing images, we proposed a novel neural network architecture which is for\ntwo tasks of landslide detection and landslide segmentation. We evaluated our\nproposed model on three different benchmark datasets of LandSlide4Sense, Bijie,\nand Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,\n93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU\nscores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,\nNepal datasets. These experimental results prove potential to integrate our\nproposed model into real-life landslide observation systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks",
    "url": "http://arxiv.org/abs/2507.10381v1",
    "authors": [
      "Aaryam Sharma"
    ],
    "published": "2025-07-14",
    "abstract": "Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection",
    "url": "http://arxiv.org/abs/2507.09541v1",
    "authors": [
      "Zihao Xiong",
      "Fei Zhou",
      "Fengyi Wu",
      "Shuai Yuan",
      "Maixia Fu",
      "Zhenming Peng",
      "Jian Yang",
      "Yimian Dai"
    ],
    "published": "2025-07-13",
    "abstract": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2507.13120v1",
    "authors": [
      "Xiaozheng Jiang",
      "Wei Zhang",
      "Xuerui Mao"
    ],
    "published": "2025-07-17",
    "abstract": "Detecting tiny objects in remote sensing (RS) imagery has been a\nlong-standing challenge due to their extremely limited spatial information,\nweak feature representations, and dense distributions across complex\nbackgrounds. Despite numerous efforts devoted, mainstream detectors still\nunderperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a\nmulti-stage feature fusion and enhancement model explicitly tailored for RS\ntiny object detection in various RS scenarios. RS-TinyNet comes with two novel\ndesigns: tiny object saliency modeling and feature integrity reconstruction.\nGuided by these principles, we design three step-wise feature enhancement\nmodules. Among them, the multi-dimensional collaborative attention (MDCA)\nmodule employs multi-dimensional attention to enhance the saliency of tiny\nobjects. Additionally, the auxiliary reversible branch (ARB) and a progressive\nfusion detection head (PFDH) module are introduced to preserve information flow\nand fuse multi-level features to bridge semantic gaps and retain structural\ndetail. Comprehensive experiments on public RS dataset AI-TOD show that our\nRS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and\n6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior\ndetection performance in diverse RS scenarios. These results demonstrate that\nthe proposed multi-stage feature fusion strategy offers an effective and\npractical solution for tiny object detection in complex RS environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows",
    "url": "http://arxiv.org/abs/2507.12590v1",
    "authors": [
      "Judy Long",
      "Tao Liu",
      "Sean Alexander Woznicki",
      "Miljana Markovi\u0107",
      "Oskar Marko",
      "Molly Sears"
    ],
    "published": "2025-07-16",
    "abstract": "Crop mapping involves identifying and classifying crop types using spatial\ndata, primarily derived from remote sensing imagery. This study presents the\nfirst comprehensive review of large-scale, pixel-wise crop mapping workflows,\nencompassing both conventional supervised methods and emerging transfer\nlearning approaches. To identify the optimal supervised crop mapping workflows,\nwe conducted systematic experiments, comparing six widely adopted satellite\nimage-based preprocessing methods, alongside eleven supervised pixel-wise\nclassification models. Additionally, we assessed the synergistic impact of\nvaried training sample sizes and variable combinations. Moreover, we identified\noptimal transfer learning techniques for different magnitudes of domain shift.\nThe evaluation of best methods was conducted across five diverse agricultural\nsites. Landsat 8 served as the primary satellite data source. Labels come from\nCDL trusted pixels and field surveys.\n  Our findings reveal three key insights. First, fine-scale interval\npreprocessing paired with Transformer models consistently delivered optimal\nperformance for both supervised and transferable workflows. RF offered rapid\ntraining and competitive performance in conventional supervised learning and\ndirect transfer to similar domains. Second, transfer learning techniques\nenhanced workflow adaptability, with UDA being effective for homogeneous crop\nclasses while fine-tuning remains robust across diverse scenarios. Finally,\nworkflow choice depends heavily on the availability of labeled samples. With a\nsufficient sample size, supervised training typically delivers more accurate\nand generalizable results. Below a certain threshold, transfer learning that\nmatches the level of domain shift is a viable alternative to achieve crop\nmapping. Repository:\nBest-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening",
    "url": "http://arxiv.org/abs/2507.10461v1",
    "authors": [
      "Tao Tang",
      "Chengxu Yang"
    ],
    "published": "2025-07-14",
    "abstract": "Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area",
    "url": "http://arxiv.org/abs/2507.10084v1",
    "authors": [
      "Haonan Chen",
      "Xin Tong"
    ],
    "published": "2025-07-14",
    "abstract": "To address the prevalent challenges of domain shift and small sample sizes in\nremote sensing image water body segmentation, this study proposes and validates\na two-stage transfer learning strategy based on the SegFormer model. The\napproach begins by training a foundational segmentation model on a diverse\nsource domain, where it achieves an Intersection over Union (IoU) of 68.80% on\nits validation set, followed by fine-tuning on data from the distinct target\ndomain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by\nhighly complex topography and spectral features -- the experimental results\ndemonstrate that this strategy significantly boosts the IoU for the water body\nsegmentation task from 25.50% (for direct transfer) to 64.84%. This not only\neffectively resolves the model performance degradation caused by domain\ndiscrepancy but also provides an effective technical paradigm for\nhigh-precision thematic information extraction in data-scarce and\nenvironmentally unique remote sensing scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A Multimodal Data Fusion Generative Adversarial Network for Real Time Underwater Sound Speed Field Construction",
    "url": "http://arxiv.org/abs/2507.11812v1",
    "authors": [
      "Wei Huang",
      "Yuqiang Huang",
      "Yanan Wu",
      "Tianhe Xu",
      "Junting Wang",
      "Hao Zhang"
    ],
    "published": "2025-07-16",
    "abstract": "Sound speed profiles (SSPs) are essential parameters underwater that affects\nthe propagation mode of underwater signals and has a critical impact on the\nenergy efficiency of underwater acoustic communication and accuracy of\nunderwater acoustic positioning. Traditionally, SSPs can be obtained by\nmatching field processing (MFP), compressive sensing (CS), and deep learning\n(DL) methods. However, existing methods mainly rely on on-site underwater sonar\nobservation data, which put forward strict requirements on the deployment of\nsonar observation systems. To achieve high-precision estimation of sound\nvelocity distribution in a given sea area without on-site underwater data\nmeasurement, we propose a multi-modal data-fusion generative adversarial\nnetwork model with residual attention block (MDF-RAGAN) for SSP construction.\nTo improve the model's ability for capturing global spatial feature\ncorrelations, we embedded the attention mechanisms, and use residual modules\nfor deeply capturing small disturbances in the deep ocean sound velocity\ndistribution caused by changes of SST. Experimental results on real open\ndataset show that the proposed model outperforms other state-of-the-art\nmethods, which achieves an accuracy with an error of less than 0.3m/s.\nSpecifically, MDF-RAGAN not only outperforms convolutional neural network (CNN)\nand spatial interpolation (SITP) by nearly a factor of two, but also achieves\nabout 65.8\\% root mean square error (RMSE) reduction compared to mean profile,\nwhich fully reflects the enhancement of overall profile matching by\nmulti-source fusion and cross-modal attention.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale",
    "url": "http://arxiv.org/abs/2507.12144v1",
    "authors": [
      "Boris Bonev",
      "Thorsten Kurth",
      "Ankur Mahesh",
      "Mauro Bisson",
      "Jean Kossaifi",
      "Karthik Kashinath",
      "Anima Anandkumar",
      "William D. Collins",
      "Michael S. Pritchard",
      "Alexander Keller"
    ],
    "published": "2025-07-16",
    "abstract": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency",
    "url": "http://arxiv.org/abs/2507.10893v1",
    "authors": [
      "Minjong Cheon",
      "Eunhan Goo",
      "Su-Hyeon Shin",
      "Muhammad Ahmed",
      "Hyungjun Kim"
    ],
    "published": "2025-07-15",
    "abstract": "Recently, AI-based weather forecast models have achieved impressive advances.\nThese models have reached accuracy levels comparable to traditional NWP\nsystems, marking a significant milestone in data-driven weather prediction.\nHowever, they mostly leverage Transformer-based architectures, which often\nleads to high training complexity and resource demands due to the massive\nparameter sizes. In this study, we introduce a modernized CNN-based model for\nglobal weather forecasting that delivers competitive accuracy while\nsignificantly reducing computational requirements. To present a systematic\nmodernization roadmap, we highlight key architectural enhancements across\nmultiple design scales from an earlier CNN-based approach. KAI-a incorporates a\nscale-invariant architecture and InceptionNeXt-based blocks within a\ngeophysically-aware design, tailored to the structure of Earth system data.\nTrained on the ERA5 daily dataset with 67 atmospheric variables, the model\ncontains about 7 million parameters and completes training in just 12 hours on\na single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the\nperformance of state-of-the-art models in medium-range weather forecasting,\nwhile offering a significantly lightweight design. Furthermore, case studies on\nthe 2018 European heatwave and the East Asian summer monsoon demonstrate\nKAI-a's robust skill in capturing extreme events, reinforcing its practical\nutility.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling",
    "url": "http://arxiv.org/abs/2507.09211v1",
    "authors": [
      "Xinyue Liu",
      "Xiao Peng",
      "Shuyue Yan",
      "Yuntian Chen",
      "Dongxiao Zhang",
      "Zhixiao Niu",
      "Hui-Min Wang",
      "Xiaogang He"
    ],
    "published": "2025-07-12",
    "abstract": "Observed records of climate extremes provide an incomplete picture of risk,\nmissing \"unseen\" extremes that exceed historical bounds. In parallel,\nneglecting spatial dependence undervalues the risk of synchronized hazards that\namplify impacts. To address these challenges, we develop DeepX-GAN\n(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial\nNetwork), a knowledge-informed deep generative model designed to better capture\nthe spatial structure of rare extremes. The zero-shot generalizability of\nDeepX-GAN enables simulation of unseen extremes that fall outside historical\nexperience yet remain statistically plausible. We define two types of unseen\nextremes: \"checkmate\" extremes that directly hit targets, and \"stalemate\"\nextremes that narrowly miss. These unrealized scenarios expose latent risks in\nfragile systems and may reinforce a false sense of resilience if overlooked.\nNear misses, in particular, can prompt either proactive adaptation or dangerous\ncomplacency, depending on how they are interpreted. Applying DeepX-GAN to the\nMiddle East and North Africa (MENA), we find that these unseen extremes\ndisproportionately affect regions with high vulnerability and low socioeconomic\nreadiness, but differ in urgency and interpretation. Future warming could\nexpand and redistribute these unseen extremes, with emerging exposure hotspots\nin Indo-Pakistan and Central Africa. This distributional shift highlights\ncritical blind spots in conventional hazard planning and underscores the need\nto develop spatially adaptive policies that anticipate emergent risk hotspots\nrather than simply extrapolating from historical patterns.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": []
  },
  {
    "title": "Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates",
    "url": "http://arxiv.org/abs/2507.09166v1",
    "authors": [
      "Louise Largeau",
      "Erwan Koch",
      "David Leutwyler",
      "Gregoire Mariethoz",
      "Valerie Chavez-Demoulin",
      "Tom Beucler"
    ],
    "published": "2025-07-12",
    "abstract": "The coarse spatial resolution of gridded climate models, such as general\ncirculation models, limits their direct use in projecting socially relevant\nvariables like extreme precipitation. Most downscaling methods estimate the\nconditional distributions of extremes by generating large ensembles,\ncomplicating the assessment of robustness under distributional shifts, such as\nthose induced by climate change. To better understand and potentially improve\nrobustness, we propose super-resolving the parameters of the target variable's\nprobability distribution directly using analytically tractable mappings. Within\na perfect-model framework over Switzerland, we demonstrate that vector\ngeneralized linear and additive models can super-resolve the generalized\nextreme value distribution of summer hourly precipitation extremes from coarse\nprecipitation fields and topography. We introduce the notion of a \"robustness\ngap\", defined as the difference in predictive error between present-trained and\nfuture-trained models, and use it to diagnose how model structure affects the\ngeneralization of each quantile to a pseudo-global warming scenario. By\nevaluating multiple model configurations, we also identify an upper limit on\nthe super-resolution factor based on the spatial auto- and cross-correlation of\nprecipitation and elevation, beyond which coarse precipitation loses predictive\nvalue. Our framework is broadly applicable to variables governed by parametric\ndistributions and offers a model-agnostic diagnostic for understanding when and\nwhy empirical downscaling generalizes to climate change and extremes.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Uncovering symmetric and asymmetric species associations from community and environmental data",
    "url": "http://arxiv.org/abs/2507.09317v1",
    "authors": [
      "Sara Si-Moussi",
      "Esther Galbrun",
      "Mickael Hedde",
      "Giovanni Poggiato",
      "Matthias Rohr",
      "Wilfried Thuiller"
    ],
    "published": "2025-07-12",
    "abstract": "There is no much doubt that biotic interactions shape community assembly and\nultimately the spatial co-variations between species. There is a hope that the\nsignal of these biotic interactions can be observed and retrieved by\ninvestigating the spatial associations between species while accounting for the\ndirect effects of the environment. By definition, biotic interactions can be\nboth symmetric and asymmetric. Yet, most models that attempt to retrieve\nspecies associations from co-occurrence or co-abundance data internally assume\nsymmetric relationships between species. Here, we propose and validate a\nmachine-learning framework able to retrieve bidirectional associations by\nanalyzing species community and environmental data.\n  Our framework (1) models pairwise species associations as directed influences\nfrom a source to a target species, parameterized with two species-specific\nlatent embeddings: the effect of the source species on the community, and the\nresponse of the target species to the community; and (2) jointly fits these\nassociations within a multi-species conditional generative model with different\nmodes of interactions between environmental drivers and biotic associations.\nUsing both simulated and empirical data, we demonstrate the ability of our\nframework to recover known asymmetric and symmetric associations and highlight\nthe properties of the learned association networks. By comparing our approach\nto other existing models such as joint species distribution models and\nprobabilistic graphical models, we show its superior capacity at retrieving\nsymmetric and asymmetric interactions. The framework is intuitive, modular and\nbroadly applicable across various taxonomic groups.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation",
    "url": "http://arxiv.org/abs/2507.16716v1",
    "authors": [
      "Yiguo He",
      "Junjie Zhu",
      "Yiying Li",
      "Xiaoyu Zhang",
      "Chunping Qiu",
      "Jun Wang",
      "Qiangjuan Huang",
      "Ke Yang"
    ],
    "published": "2025-07-22",
    "abstract": "The application of Vision-language foundation models (VLFMs) to remote\nsensing (RS) imagery has garnered significant attention due to their superior\ncapability in various downstream tasks. A key challenge lies in the scarcity of\nhigh-quality, large-scale, image-text paired training data. Recently, several\nworks introduced extensive image-text datasets for RS and trained their VLFMs.\nHowever, due to the rudimentary methods used for generating captions, the\nquality of datasets is suboptimal, requiring larger volumes of training data,\nwhile only yielding modest performance improvements. In this paper, we propose\na two-stage method named MpGI(Multi-Perspective Generation and Integration) for\ngenerating high-quality text captions for RS images. Firstly, we generate\ndistinct and detailed descriptions from different perspectives using\nRule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs\ngeneration methods. Next, we utilize Large Language Models (LLMs) to integrate\nthese diverse descriptions into comprehensive captions, capturing details from\nmultiple perspectives. Finally, we have created the HQRS-IT-210K dataset,\nincluding about 210,000 RS images and 1.3 million captions. We fine-tuned two\nVLFMs using our dataset: CLIP, a discriminative model, and CoCa, an\nimage-to-text generative model. This process resulted in our proposed HQRS-CLIP\nand RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed\nthe previous SOTA RS CLIP model in various downstream tasks while using only\n4.2\\% of the training data. RS-CoCa outperforms other advanced approaches\nacross benchmark datasets and can generate captions for RS images that rival or\neven exceed manual annotations. Dataset, pre-trained models, and codes will be\nreleased at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "CLIP"
    ],
    "applications": []
  },
  {
    "title": "A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges",
    "url": "http://arxiv.org/abs/2507.18376v1",
    "authors": [
      "Xing Hua",
      "Haodong Chen",
      "Qianqian Duan",
      "Danfeng Hong",
      "Ruijiao Li",
      "Huiliang Shang",
      "Linghua Jiang",
      "Haima Yang",
      "Dawei Zhang"
    ],
    "published": "2025-07-24",
    "abstract": "With the global population growing and arable land resources becoming\nincreasingly scarce,smart agriculture and precision agriculture have emerged as\nkey directions for the future ofagricultural development.Artificial\nintelligence (AI) technologies, particularly deep learning models, have found\nwidespread applications in areas such as crop monitoring and pest detection. As\nan emerging generative model, diffusion models have shown significant promise\nin tasks like agricultural image processing, data augmentation, and remote\nsensing. Compared to traditional generative adversarial networks (GANs),\ndiffusion models offer superior training stability and generation quality,\neffectively addressing challenges such as limited agricultural data and\nimbalanced image samples. This paper reviews the latest advancements in the\napplication of diffusion models in agriculture, focusing on their potential in\ncrop pest and disease detection, remote sensing image enhancement, crop growth\nprediction, and agricultural resource management. Experimental results\ndemonstrate that diffusion models significantly improve model accuracy and\nrobustness in data augmentation, image generation, and denoising, especially in\ncomplex environments. Despite challenges related to computational efficiency\nand generalization capabilities, diffusion models are expected to play an\nincreasingly important role in smart and precision agriculture as technology\nadvances, providing substantial support for the sustainable development of\nglobal agriculture.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN",
      "Diffusion Models"
    ],
    "applications": [
      "Detection",
      "Image Generation",
      "Forecast"
    ]
  },
  {
    "title": "Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels for Building Detection",
    "url": "http://arxiv.org/abs/2507.16657v1",
    "authors": [
      "Shuang Song",
      "Yang Tang",
      "Rongjun Qin"
    ],
    "published": "2025-07-22",
    "abstract": "Deep learning has significantly advanced building segmentation in remote\nsensing, yet models struggle to generalize on data of diverse geographic\nregions due to variations in city layouts and the distribution of building\ntypes, sizes and locations. However, the amount of time-consuming annotated\ndata for capturing worldwide diversity may never catch up with the demands of\nincreasingly data-hungry models. Thus, we propose a novel approach: re-training\nmodels at test time using synthetic data tailored to the target region's city\nlayout. This method generates geo-typical synthetic data that closely\nreplicates the urban structure of a target area by leveraging geospatial data\nsuch as street network from OpenStreetMap. Using procedural modeling and\nphysics-based rendering, very high-resolution synthetic images are created,\nincorporating domain randomization in building shapes, materials, and\nenvironmental illumination. This enables the generation of virtually unlimited\ntraining samples that maintain the essential characteristics of the target\nenvironment. To overcome synthetic-to-real domain gaps, our approach integrates\ngeo-typical data into an adversarial domain adaptation framework for building\nsegmentation. Experiments demonstrate significant performance enhancements,\nwith median improvements of up to 12%, depending on the domain gap. This\nscalable and cost-effective method blends partial geographic knowledge with\nsynthetic imagery, providing a promising solution to the \"model collapse\" issue\nin purely synthetic datasets. It offers a practical pathway to improving\ngeneralization in remote sensing building segmentation without extensive\nreal-world annotations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing",
    "url": "http://arxiv.org/abs/2507.16228v1",
    "authors": [
      "Shreelekha Revankar",
      "Utkarsh Mall",
      "Cheng Perng Phoo",
      "Kavita Bala",
      "Bharath Hariharan"
    ],
    "published": "2025-07-22",
    "abstract": "Natural disasters cause devastating damage to communities and infrastructure\nevery year. Effective disaster response is hampered by the difficulty of\naccessing affected areas during and after events. Remote sensing has allowed us\nto monitor natural disasters in a remote way. More recently there have been\nadvances in computer vision and deep learning that help automate satellite\nimagery analysis, However, they remain limited by their narrow focus on\nspecific disaster types, reliance on manual expert interpretation, and lack of\ndatasets with sufficient temporal granularity or natural language annotations\nfor tracking disaster progression. We present MONITRS, a novel multimodal\ndataset of more than 10,000 FEMA disaster events with temporal satellite\nimagery and natural language annotations from news articles, accompanied by\ngeotagged locations, and question-answer pairs. We demonstrate that fine-tuning\nexisting MLLMs on our dataset yields significant performance improvements for\ndisaster monitoring tasks, establishing a new benchmark for machine\nlearning-assisted disaster response systems. Code can be found at:\nhttps://github.com/ShreelekhaR/MONITRS",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery",
    "url": "http://arxiv.org/abs/2507.16849v1",
    "authors": [
      "Yi-Shan Chu",
      "Hsuan-Cheng Wei"
    ],
    "published": "2025-07-21",
    "abstract": "We propose a vision transformer (ViT)-based deep learning framework to refine\ndisaster-affected area segmentation from remote sensing imagery, aiming to\nsupport and enhance the Emergent Value Added Product (EVAP) developed by the\nTaiwan Space Agency (TASA). The process starts with a small set of manually\nannotated regions. We then apply principal component analysis (PCA)-based\nfeature space analysis and construct a confidence index (CI) to expand these\nlabels, producing a weakly supervised training set. These expanded labels are\nthen used to train ViT-based encoder-decoder models with multi-band inputs from\nSentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder\nvariants and multi-stage loss strategies to improve performance under limited\nsupervision. During the evaluation, model predictions are compared with\nhigher-resolution EVAP output to assess spatial coherence and segmentation\nconsistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes\nwildfire demonstrate that our framework improves the smoothness and reliability\nof segmentation results, offering a scalable approach for disaster mapping when\naccurate ground truth is unavailable.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover",
    "url": "http://arxiv.org/abs/2507.18099v1",
    "authors": [
      "Naman Srivastava",
      "Joel D Joy",
      "Yash Dixit",
      "Swarup E",
      "Rakshit Ramesh"
    ],
    "published": "2025-07-24",
    "abstract": "Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning, and is one of the key elements in developing smart and sustainable\ncities.This study evaluates advanced LULC mapping techniques, focusing on\nLook-Up Table (LUT)-based Atmospheric Correction applied to Cartosat\nMultispectral (MX) sensor images, followed by supervised and semi-supervised\nlearning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo\nSupervision (CPS). The CPS model is further refined with dynamic weighting,\nenhancing pseudo-label reliability during training. This comprehensive approach\nanalyses the accuracy and utility of LULC mapping techniques for various urban\nplanning applications. A case study of Hyderabad, India, illustrates\nsignificant land use changes due to rapid urbanization. By analyzing Cartosat\nMX images over time, we highlight shifts such as urban sprawl, shrinking green\nspaces, and expanding industrial areas. This demonstrates the practical utility\nof these techniques for urban planners and policymakers.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents",
    "url": "http://arxiv.org/abs/2507.18067v1",
    "authors": [
      "Abdessamad El-Kabid",
      "Loubna Benabbou",
      "Redouane Lguensat",
      "Alex Hern\u00e1ndez-Garc\u00eda"
    ],
    "published": "2025-07-24",
    "abstract": "Accurate modeling of physical systems governed by partial differential\nequations is a central challenge in scientific computing. In oceanography,\nhigh-resolution current data are critical for coastal management, environmental\nmonitoring, and maritime safety. However, available satellite products, such as\nCopernicus data for sea water velocity at ~0.08 degrees spatial resolution and\nglobal ocean models, often lack the spatial granularity required for detailed\nlocal analyses. In this work, we (a) introduce a supervised deep learning\nframework based on neural operators for solving PDEs and providing arbitrary\nresolution solutions, and (b) propose downscaling models with an application to\nCopernicus ocean current data. Additionally, our method can model surrogate\nPDEs and predict solutions at arbitrary resolution, regardless of the input\nresolution. We evaluated our model on real-world Copernicus ocean current data\nand synthetic Navier-Stokes simulation datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Bayesian Deep Learning for Convective Initiation Nowcasting Uncertainty Estimation",
    "url": "http://arxiv.org/abs/2507.16219v1",
    "authors": [
      "Da Fan",
      "David John Gagne II",
      "Steven J. Greybush",
      "Eugene E. Clothiaux",
      "John S. Schreck",
      "Chaopeng Shen"
    ],
    "published": "2025-07-22",
    "abstract": "This study evaluated the probability and uncertainty forecasts of five\nrecently proposed Bayesian deep learning methods relative to a deterministic\nresidual neural network (ResNet) baseline for 0-1 h convective initiation (CI)\nnowcasting using GOES-16 satellite infrared observations. Uncertainty was\nassessed by how well probabilistic forecasts were calibrated and how well\nuncertainty separated forecasts with large and small errors. Most of the\nBayesian deep learning methods produced probabilistic forecasts that\noutperformed the deterministic ResNet, with one, the initial-weights ensemble +\nMonte Carlo (MC) dropout, an ensemble of deterministic ResNets with different\ninitial weights to start training and dropout activated during inference,\nproducing the most skillful and well-calibrated forecasts. The initial-weights\nensemble + MC dropout benefited from generating multiple solutions that more\nthoroughly sampled the hypothesis space. The Bayesian ResNet ensemble was the\nonly one that performed worse than the deterministic ResNet at longer lead\ntimes, likely due to the challenge of optimizing a larger number of parameters.\nTo address this issue, the Bayesian-MOPED (MOdel Priors with Empirical Bayes\nusing Deep neural network) ResNet ensemble was adopted, and it enhanced\nforecast skill by constraining the hypothesis search near the deterministic\nResNet hypothesis. All Bayesian methods demonstrated well-calibrated\nuncertainty and effectively separated cases with large and small errors. In\ncase studies, the initial-weights ensemble + MC dropout demonstrated better\nforecast skill than the Bayesian-MOPED ensemble and the deterministic ResNet on\nselected CI events in clear-sky regions. However, the initial-weights ensemble\n+ MC dropout exhibited poorer generalization in clear-sky and anvil cloud\nregions without CI occurrence compared to the deterministic ResNet and\nBayesian-MOPED ensemble.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet",
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A comparison of stretched-grid and limited-area modelling for data-driven regional weather forecasting",
    "url": "http://arxiv.org/abs/2507.18378v1",
    "authors": [
      "Jasper S. Wijnands",
      "Michiel Van Ginderachter",
      "Bastien Fran\u00e7ois",
      "Sophie Buurman",
      "Piet Termonia",
      "Dieter Van den Bleeken"
    ],
    "published": "2025-07-24",
    "abstract": "Regional machine learning weather prediction (MLWP) models based on graph\nneural networks have recently demonstrated remarkable predictive accuracy,\noutperforming numerical weather prediction models at lower computational costs.\nIn particular, limited-area model (LAM) and stretched-grid model (SGM)\napproaches have emerged for generating high-resolution regional forecasts,\nbased on initial conditions from a regional (re)analysis. While LAM uses\nlateral boundaries from an external global model, SGM incorporates a global\ndomain at lower resolution. This study aims to understand how the differences\nin model design impact relative performance and potential applications.\nSpecifically, the strengths and weaknesses of these two approaches are\nidentified for generating deterministic regional forecasts over Europe. Using\nthe Anemoi framework, models of both types are built by minimally adapting a\nshared architecture and trained using global and regional reanalyses in a\nnear-identical setup. Several inference experiments have been conducted to\nexplore their relative performance and highlight key differences. Results show\nthat both LAM and SGM are competitive deterministic MLWP models with generally\naccurate and comparable forecasting performance over the regional domain.\nVarious differences were identified in the performance of the models across\napplications. LAM is able to successfully exploit high-quality boundary\nforcings to make predictions within the regional domain and is suitable in\ncontexts where global data is difficult to acquire. SGM is fully self-contained\nfor easier operationalisation, can take advantage of more training data and\nsignificantly surpasses LAM in terms of (temporal) generalisability. Our paper\ncan serve as a starting point for meteorological institutes to guide their\nchoice between LAM and SGM in developing an operational data-driven forecasting\nsystem.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "EarthLink: A Self-Evolving AI Agent for Climate Science",
    "url": "http://arxiv.org/abs/2507.17311v2",
    "authors": [
      "Zijie Guo",
      "Jiong Wang",
      "Xiaoyu Yue",
      "Wangxu Wei",
      "Zhe Jiang",
      "Wanghan Xu",
      "Ben Fei",
      "Wenlong Zhang",
      "Xinyu Gu",
      "Lijing Cheng",
      "Jing-Jia Luo",
      "Chao Li",
      "Yaqiang Wang",
      "Tao Chen",
      "Wanli Ouyang",
      "Fenghua Ling",
      "Lei Bai"
    ],
    "published": "2025-07-23",
    "abstract": "Modern Earth science is at an inflection point. The vast, fragmented, and\ncomplex nature of Earth system data, coupled with increasingly sophisticated\nanalytical demands, creates a significant bottleneck for rapid scientific\ndiscovery. Here we introduce EarthLink, the first AI agent designed as an\ninteractive copilot for Earth scientists. It automates the end-to-end research\nworkflow, from planning and code generation to multi-scenario analysis. Unlike\nstatic diagnostic tools, EarthLink can learn from user interaction,\ncontinuously refining its capabilities through a dynamic feedback loop. We\nvalidated its performance on a number of core scientific tasks of climate\nchange, ranging from model-observation comparisons to the diagnosis of complex\nphenomena. In a multi-expert evaluation, EarthLink produced scientifically\nsound analyses and demonstrated an analytical competency that was rated as\ncomparable to specific aspects of a human junior researcher's workflow.\nAdditionally, its transparent, auditable workflows and natural language\ninterface empower scientists to shift from laborious manual execution to\nstrategic oversight and hypothesis generation. EarthLink marks a pivotal step\ntowards an efficient, trustworthy, and collaborative paradigm for Earth system\nresearch in an era of accelerating global change. The system is accessible at\nour website https://earthlink.intern-ai.org.cn.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model",
    "url": "http://arxiv.org/abs/2507.22675v1",
    "authors": [
      "Meiqi Hu",
      "Lingzhi Lu",
      "Chengxi Han",
      "Xiaoping Liu"
    ],
    "published": "2025-07-30",
    "abstract": "Recently, large foundation models trained on vast datasets have demonstrated\nexceptional capabilities in feature extraction and general feature\nrepresentation. The ongoing advancements in deep learning-driven large models\nhave shown great promise in accelerating unsupervised change detection methods,\nthereby enhancing the practical applicability of change detection technologies.\nBuilding on this progress, this paper introduces MergeSAM, an innovative\nunsupervised change detection method for high-resolution remote sensing\nimagery, based on the Segment Anything Model (SAM). Two novel strategies,\nMaskMatching and MaskSplitting, are designed to address real-world complexities\nsuch as object splitting, merging, and other intricate changes. The proposed\nmethod fully leverages SAM's object segmentation capabilities to construct\nmultitemporal masks that capture complex changes, embedding the spatial\nstructure of land cover into the change detection process.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning",
    "url": "http://arxiv.org/abs/2507.20776v1",
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Yingchao Feng",
      "Kaiwen Wei",
      "Wenxin Yin",
      "Wenhui Diao",
      "Mengyu Wang",
      "Hanbo Bi",
      "Kaiyue Kang",
      "Tong Ling",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2025-07-28",
    "abstract": "Remote sensing (RS) images from multiple modalities and platforms exhibit\ndiverse details due to differences in sensor characteristics and imaging\nperspectives. Existing vision-language research in RS largely relies on\nrelatively homogeneous data sources. Moreover, they still remain limited to\nconventional visual perception tasks such as classification or captioning. As a\nresult, these methods fail to serve as a unified and standalone framework\ncapable of effectively handling RS imagery from diverse sources in real-world\napplications. To address these issues, we propose RingMo-Agent, a model\ndesigned to handle multi-modal and multi-platform data that performs perception\nand reasoning tasks based on user textual instructions. Compared with existing\nmodels, RingMo-Agent 1) is supported by a large-scale vision-language dataset\nnamed RS-VL3M, comprising over 3 million image-text pairs, spanning optical,\nSAR, and infrared (IR) modalities collected from both satellite and UAV\nplatforms, covering perception and challenging reasoning tasks; 2) learns\nmodality adaptive representations by incorporating separated embedding layers\nto construct isolated features for heterogeneous modalities and reduce\ncross-modal interference; 3) unifies task modeling by introducing task-specific\ntokens and employing a token-based high-dimensional hidden state decoding\nmechanism designed for long-horizon spatial tasks. Extensive experiments on\nvarious RS vision-language tasks demonstrate that RingMo-Agent not only proves\neffective in both visual understanding and sophisticated analytical tasks, but\nalso exhibits strong generalizability across different platforms and sensing\nmodalities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data",
    "url": "http://arxiv.org/abs/2507.22291v1",
    "authors": [
      "Christopher F. Brown",
      "Michal R. Kazmierski",
      "Valerie J. Pasquarella",
      "William J. Rucklidge",
      "Masha Samsikova",
      "Chenhui Zhang",
      "Evan Shelhamer",
      "Estefania Lahera",
      "Olivia Wiles",
      "Simon Ilyushchenko",
      "Noel Gorelick",
      "Lihui Lydia Zhang",
      "Sophia Alj",
      "Emily Schechter",
      "Sean Askay",
      "Oliver Guinan",
      "Rebecca Moore",
      "Alexis Boukouvalas",
      "Pushmeet Kohli"
    ],
    "published": "2025-07-29",
    "abstract": "Unprecedented volumes of Earth observation data are continually collected\naround the world, but high-quality labels remain scarce given the effort\nrequired to make physical measurements and observations. This has led to\nconsiderable investment in bespoke modeling efforts translating sparse labels\ninto maps. Here we introduce AlphaEarth Foundations, an embedding field model\nyielding a highly general, geospatial representation that assimilates spatial,\ntemporal, and measurement contexts across multiple sources, enabling accurate\nand efficient production of maps and monitoring systems from local to global\nscales. The embeddings generated by AlphaEarth Foundations are the only to\nconsistently outperform all previous featurization approaches tested on a\ndiverse set of mapping evaluations without re-training. We will release a\ndataset of global, annual, analysis-ready embedding field layers from 2017\nthrough 2024.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SCANet: Split Coordinate Attention Network for Building Footprint Extraction",
    "url": "http://arxiv.org/abs/2507.20809v1",
    "authors": [
      "Chunshi Wang",
      "Bin Zhao",
      "Shuxue Ding"
    ],
    "published": "2025-07-28",
    "abstract": "Building footprint extraction holds immense significance in remote sensing\nimage analysis and has great value in urban planning, land use, environmental\nprotection and disaster assessment. Despite the progress made by conventional\nand deep learning approaches in this field, they continue to encounter\nsignificant challenges. This paper introduces a novel plug-and-play attention\nmodule, Split Coordinate Attention (SCA), which ingeniously captures spatially\nremote interactions by employing two spatial range of pooling kernels,\nstrategically encoding each channel along x and y planes, and separately\nperforms a series of split operations for each feature group, thus enabling\nmore efficient semantic feature extraction. By inserting into a 2D CNN to form\nan effective SCANet, our SCANet outperforms recent SOTA methods on the public\nWuhan University (WHU) Building Dataset and Massachusetts Building Dataset in\nterms of various metrics. Particularly SCANet achieves the best IoU, 91.61% and\n75.49% for the two datasets. Our code is available at\nhttps://github.com/AiEson/SCANet",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit",
    "url": "http://arxiv.org/abs/2507.20623v1",
    "authors": [
      "Yang Zhao",
      "Shusheng Li",
      "Xueshang Feng"
    ],
    "published": "2025-07-28",
    "abstract": "As the development of lightweight deep learning algorithms, various deep\nneural network (DNN) models have been proposed for the remote sensing scene\nclassification (RSSC) application. However, it is still challenging for these\nRSSC models to achieve optimal performance among model accuracy, inference\nlatency, and energy consumption on resource-constrained edge devices. In this\npaper, we propose a lightweight RSSC framework, which includes a distilled\nglobal filter network (GFNet) model and an early-exit mechanism designed for\nedge devices to achieve state-of-the-art performance. Specifically, we first\napply frequency domain distillation on the GFNet model to reduce model size.\nThen we design a dynamic early-exit model tailored for DNN models on edge\ndevices to further improve model inference efficiency. We evaluate our E3C\nmodel on three edge devices across four datasets. Extensive experimental\nresults show that it achieves an average of 1.3x speedup on model inference and\nover 40% improvement on energy efficiency, while maintaining high\nclassification accuracy.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations",
    "url": "http://arxiv.org/abs/2507.23154v1",
    "authors": [
      "Sofiane Bouaziz",
      "Adel Hafiane",
      "Raphael Canals",
      "Rachid Nedjai"
    ],
    "published": "2025-07-30",
    "abstract": "Urban heatwaves, droughts, and land degradation are pressing and growing\nchallenges in the context of climate change. A valuable approach to studying\nthem requires accurate spatio-temporal information on land surface conditions.\nOne of the most important variables for assessing and understanding these\nphenomena is Land Surface Temperature (LST), which is derived from satellites\nand provides essential information about the thermal state of the Earth's\nsurface. However, satellite platforms inherently face a trade-off between\nspatial and temporal resolutions. To bridge this gap, we propose FuseTen, a\nnovel generative framework that produces daily LST observations at a fine 10 m\nspatial resolution by fusing spatio-temporal observations derived from\nSentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative\narchitecture trained using an averaging-based supervision strategy grounded in\nphysical principles. It incorporates attention and normalization modules within\nthe fusion process and uses a PatchGAN discriminator to enforce realism.\nExperiments across multiple dates show that FuseTen outperforms linear\nbaselines, with an average 32.06% improvement in quantitative metrics and\n31.42% in visual fidelity. To the best of our knowledge, this is the first\nnon-linear method to generate daily LST estimates at such fine spatial\nresolution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SpecBPP: A Self-Supervised Learning Approach for Hyperspectral Representation and Soil Organic Carbon Estimation",
    "url": "http://arxiv.org/abs/2507.19781v1",
    "authors": [
      "Daniel La'ah Ayuba",
      "Jean-Yves Guillemaut",
      "Belen Marti-Cardona",
      "Oscar Mendez Maldonado"
    ],
    "published": "2025-07-26",
    "abstract": "Self-supervised learning has revolutionized representation learning in vision\nand language, but remains underexplored for hyperspectral imagery (HSI), where\nthe sequential structure of spectral bands offers unique opportunities. In this\nwork, we propose Spectral Band Permutation Prediction (SpecBPP), a novel\nself-supervised learning framework that leverages the inherent spectral\ncontinuity in HSI. Instead of reconstructing masked bands, SpecBPP challenges a\nmodel to recover the correct order of shuffled spectral segments, encouraging\nglobal spectral understanding. We implement a curriculum-based training\nstrategy that progressively increases permutation difficulty to manage the\nfactorial complexity of the permutation space. Applied to Soil Organic Carbon\n(SOC) estimation using EnMAP satellite data, our method achieves\nstate-of-the-art results, outperforming both masked autoencoder (MAE) and\njoint-embedding predictive (JEPA) baselines. Fine-tuned on limited labeled\nsamples, our model yields an $R^2$ of 0.9456, RMSE of 1.1053%, and RPD of 4.19,\nsignificantly surpassing traditional and self-supervised benchmarks. Our\nresults demonstrate that spectral order prediction is a powerful pretext task\nfor hyperspectral understanding, opening new avenues for scientific\nrepresentation learning in remote sensing and beyond.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "JEPA",
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Discrete Gaussian Vector Fields On Meshes",
    "url": "http://arxiv.org/abs/2507.20024v1",
    "authors": [
      "Michael Gillan",
      "Stefan Siegert",
      "Ben Youngman"
    ],
    "published": "2025-07-26",
    "abstract": "Though the underlying fields associated with vector-valued environmental data\nare continuous, observations themselves are discrete. For example, climate\nmodels typically output grid-based representations of wind fields or ocean\ncurrents, and these are often downscaled to a discrete set of points. By\ntreating the area of interest as a two-dimensional manifold that can be\nrepresented as a triangular mesh and embedded in Euclidean space, this work\nshows that discrete intrinsic Gaussian processes for vector-valued data can be\ndeveloped from discrete differential operators defined with respect to a mesh.\nThese Gaussian processes account for the geometry and curvature of the manifold\nwhilst also providing a flexible and practical formulation that can be readily\napplied to any two-dimensional mesh. We show that these models can capture\nharmonic flows, incorporate boundary conditions, and model non-stationary data.\nFinally, we apply these models to downscaling stationary and non-stationary\ngridded wind data on the globe, and to inference of ocean currents from sparse\nobservations in bounded domains.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Machine Learning Framework for Predicting Microphysical Properties of Ice Crystals from Cloud Particle Imagery",
    "url": "http://arxiv.org/abs/2507.19759v1",
    "authors": [
      "Joseph Ko",
      "Jerry Harrington",
      "Kara Sulia",
      "Vanessa Przybylo",
      "Marcus van Lier-Walqui",
      "Kara Lamb"
    ],
    "published": "2025-07-26",
    "abstract": "The microphysical properties of ice crystals are important because they\nsignificantly alter the radiative properties and spatiotemporal distributions\nof clouds, which in turn strongly affect Earth's climate. However, it is\nchallenging to measure key properties of ice crystals, such as mass or\nmorphological features. Here, we present a framework for predicting\nthree-dimensional (3D) microphysical properties of ice crystals from in situ\ntwo-dimensional (2D) imagery. First, we computationally generate synthetic ice\ncrystals using 3D modeling software along with geometric parameters estimated\nfrom the 2021 Ice Cryo-Encapsulation Balloon (ICEBall) field campaign. Then, we\nuse synthetic crystals to train machine learning (ML) models to predict\neffective density ($\\rho_{e}$), effective surface area ($A_e$), and number of\nbullets ($N_b$) from synthetic rosette imagery. When tested on unseen synthetic\nimages, we find that our ML models can predict microphysical properties with\nhigh accuracy. For $\\rho_{e}$ and $A_e$, respectively, our best-performing\nsingle view models achieved $R^2$ values of 0.99 and 0.98. For $N_b$, our best\nsingle view model achieved a balanced accuracy and F1 score of 0.91. We also\nquantify the marginal prediction improvements from incorporating a second view.\nA stereo view ResNet-18 model reduced RMSE by 40% for both $\\rho_e$ and $A_e$,\nrelative to a single view ResNet-18 model. For $N_b$, we find that a stereo\nview ResNet-18 model improved the F1 score by 8%. This work provides a novel\nML-driven framework for estimating ice microphysical properties from in situ\nimagery, which will allow for downstream constraints on microphysical\nparameterizations, such as the mass-size relationship.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Integrating Activity Predictions in Knowledge Graphs",
    "url": "http://arxiv.org/abs/2507.19733v1",
    "authors": [
      "Alec Sculley",
      "Cameron Stockton",
      "Forrest Hare"
    ],
    "published": "2025-07-26",
    "abstract": "We argue that ontology-structured knowledge graphs can play a crucial role in\ngenerating predictions about future events. By leveraging the semantic\nframework provided by Basic Formal Ontology (BFO) and Common Core Ontologies\n(CCO), we demonstrate how data such as the movements of a fishing vessel can be\norganized in and retrieved from a knowledge graph. These query results are then\nused to create Markov chain models, allowing us to predict future states based\non the vessel's history. To fully support this process, we introduce the term\n`spatiotemporal instant' to complete the necessary structural semantics.\nAdditionally, we critique the prevailing ontological model of probability,\nwhich conflates probability with likelihood and relies on the problematic\nconcept of modal measurements: measurements of future entities. We propose an\nalternative view, where probabilities are treated as being about process\nprofiles, which better captures the dynamics of real world phenomena. Finally,\nwe demonstrate how our Markov chain based probability calculations can be\nseamlessly integrated back into the knowledge graph, enabling further analysis\nand decision-making. Keywords: predictive analytics, ontology, Markov chains,\nprobability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks",
    "url": "http://arxiv.org/abs/2508.03566v1",
    "authors": [
      "Xinyu Xiong",
      "Zihuang Wu",
      "Lei Zhang",
      "Lei Lu",
      "Ming Li",
      "Guanbin Li"
    ],
    "published": "2025-08-05",
    "abstract": "Recent studies have highlighted the potential of adapting the Segment\nAnything Model (SAM) for various downstream tasks. However, constructing a more\npowerful and generalizable encoder to further enhance performance remains an\nopen challenge. In this work, we propose SAM2-UNeXT, an advanced framework that\nbuilds upon the core principles of SAM2-UNet while extending the\nrepresentational capacity of SAM2 through the integration of an auxiliary\nDINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue\nlayer, our approach enables more accurate segmentation with a simple\narchitecture, relaxing the need for complex decoder designs. Extensive\nexperiments conducted on four benchmarks, including dichotomous image\nsegmentation, camouflaged object detection, marine animal segmentation, and\nremote sensing saliency detection, demonstrate the superior performance of our\nproposed method. The code is available at\nhttps://github.com/WZH0120/SAM2-UNeXT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models",
    "url": "http://arxiv.org/abs/2508.01731v1",
    "authors": [
      "Yuxiang Zhang",
      "Wei Li",
      "Mengmeng Zhang",
      "Jiawei Han",
      "Ran Tao",
      "Shunlin Liang"
    ],
    "published": "2025-08-03",
    "abstract": "Recent advances in Remote Sensing Foundation Models (RSFMs) have led to\nsignificant breakthroughs in the field. While many RSFMs have been pretrained\nwith massive optical imagery, more multispectral/hyperspectral data remain lack\nof the corresponding foundation models. To leverage the advantages of spectral\nimagery in earth observation, we explore whether existing RSFMs can be\neffectively adapted to process diverse spectral modalities without requiring\nextensive spectral pretraining. In response to this challenge, we proposed\nSpectralX, an innovative parameter-efficient fine-tuning framework that adapt\nexisting RSFMs as backbone while introducing a two-stage training approach to\nhandle various spectral inputs, thereby significantly improving domain\ngeneralization performance. In the first stage, we employ a\nmasked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) to\nextract attribute tokens from both spatial and spectral dimensions.\nSimultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA)\nthat dynamically aggregates multi-attribute expert knowledge while performing\nlayer-wise fine-tuning. With semantic segmentation as downstream task in the\nsecond stage, we insert an Attribute-refined Adapter (Are-adapter) into the\nfirst stage framework. By iteratively querying low-level semantic features with\nhigh-level representations, the model learns to focus on task-beneficial\nattributes, enabling customized adjustment of RSFMs. Following this two-phase\nadaptation process, SpectralX is capable of interpreting spectral imagery from\nnew regions or seasons. The codes will be available from the website:\nhttps://github.com/YuxiangZhang-BIT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.05271v1",
    "authors": [
      "Xiaoyang Zhang",
      "Guodong Fan",
      "Guang-Yong Chen",
      "Zhen Hua",
      "Jinjiang Li",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "published": "2025-08-07",
    "abstract": "Change detection in remote sensing imagery plays a vital role in various\nengineering applications, such as natural disaster monitoring, urban expansion\ntracking, and infrastructure management. Despite the remarkable progress of\ndeep learning in recent years, most existing methods still rely on\nspatial-domain modeling, where the limited diversity of feature representations\nhinders the detection of subtle change regions. We observe that\nfrequency-domain feature modeling particularly in the wavelet domain an amplify\nfine-grained differences in frequency components, enhancing the perception of\nedge changes that are challenging to capture in the spatial domain. Thus, we\npropose a method called Wavelet-Guided Dual-Frequency Encoding (WGDF).\nSpecifically, we first apply Discrete Wavelet Transform (DWT) to decompose the\ninput images into high-frequency and low-frequency components, which are used\nto model local details and global structures, respectively. In the\nhigh-frequency branch, we design a Dual-Frequency Feature Enhancement (DFFE)\nmodule to strengthen edge detail representation and introduce a\nFrequency-Domain Interactive Difference (FDID) module to enhance the modeling\nof fine-grained changes. In the low-frequency branch, we exploit Transformers\nto capture global semantic relationships and employ a Progressive Contextual\nDifference Module (PCDM) to progressively refine change regions, enabling\nprecise structural semantic characterization. Finally, the high- and\nlow-frequency features are synergistically fused to unify local sensitivity\nwith global discriminability. Extensive experiments on multiple remote sensing\ndatasets demonstrate that WGDF significantly alleviates edge ambiguity and\nachieves superior detection accuracy and robustness compared to\nstate-of-the-art methods. The code will be available at\nhttps://github.com/boshizhang123/WGDF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Deep learning framework for crater detection and identification on the Moon and Mars",
    "url": "http://arxiv.org/abs/2508.03920v1",
    "authors": [
      "Yihan Ma",
      "Zeyang Yu",
      "Rohitash Chandra"
    ],
    "published": "2025-08-05",
    "abstract": "Impact craters are among the most prominent geomorphological features on\nplanetary surfaces and are of substantial significance in planetary science\nresearch. Their spatial distribution and morphological characteristics provide\ncritical information on planetary surface composition, geological history, and\nimpact processes. In recent years, the rapid advancement of deep learning\nmodels has fostered significant interest in automated crater detection. In this\npaper, we apply advancements in deep learning models for impact crater\ndetection and identification. We use novel models, including Convolutional\nNeural Networks (CNNs) and variants such as YOLO and ResNet. We present a\nframework that features a two-stage approach where the first stage features\ncrater identification using simple classic CNN, ResNet-50 and YOLO. In the\nsecond stage, our framework employs YOLO-based detection for crater\nlocalisation. Therefore, we detect and identify different types of craters and\npresent a summary report with remote sensing data for a selected region. We\nconsider selected regions for craters and identification from Mars and the Moon\nbased on remote sensing data. Our results indicate that YOLO demonstrates the\nmost balanced crater detection performance, while ResNet-50 excels in\nidentifying large craters with high precision.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "ResNet"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling",
    "url": "http://arxiv.org/abs/2508.03774v1",
    "authors": [
      "Rui Zhu",
      "Yuexing Peng",
      "Peng Wang",
      "George C. Alexandropoulos",
      "Wenbo Wang",
      "Wei Xiang"
    ],
    "published": "2025-08-05",
    "abstract": "Electromagnetic (EM) scattering modeling is critical for radar remote\nsensing, however, its inherent complexity introduces significant computational\nchallenges. Traditional numerical solvers offer high accuracy, but suffer from\nscalability issues and substantial computational costs. Pure data-driven deep\nlearning approaches, while efficient, lack physical constraints embedding\nduring training and require extensive labeled data, limiting their\napplicability and generalization. To overcome these limitations, we propose a\nU-shaped Physics-Informed Network (U-PINet), the first fully\ndeep-learning-based, physics-informed hierarchical framework for computational\nEM designed to ensure physical consistency while maximizing computational\nefficiency. Motivated by the hierarchical decomposition strategy in EM solvers\nand the inherent sparsity of local EM coupling, the U-PINet models the\ndecomposition and coupling of near- and far-field interactions through a\nmultiscale processing neural network architecture, while employing a\nphysics-inspired sparse graph representation to efficiently model both self-\nand mutual- coupling among mesh elements of complex $3$-Dimensional (3D)\nobjects. This principled approach enables end-to-end multiscale EM scattering\nmodeling with improved efficiency, generalization, and physical consistency.\nExperimental results showcase that the U-PINet accurately predicts surface\ncurrent distributions, achieving close agreement with traditional solver, while\nsignificantly reducing computational time and outperforming conventional deep\nlearning baselines in both accuracy and robustness. Furthermore, our\nevaluations on radar cross section prediction tasks confirm the feasibility of\nthe U-PINet for downstream EM scattering applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MGCR-Net:Multimodal Graph-Conditioned Vision-Language Reconstruction Network for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.01555v1",
    "authors": [
      "Chengming Wang",
      "Guodong Fan",
      "Jinjiang Li",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "published": "2025-08-03",
    "abstract": "With the advancement of remote sensing satellite technology and the rapid\nprogress of deep learning, remote sensing change detection (RSCD) has become a\nkey technique for regional monitoring. Traditional change detection (CD)\nmethods and deep learning-based approaches have made significant contributions\nto change analysis and detection, however, many outstanding methods still face\nlimitations in the exploration and application of multimodal data. To address\nthis, we propose the multimodal graph-conditioned vision-language\nreconstruction network (MGCR-Net) to further explore the semantic interaction\ncapabilities of multimodal data. Multimodal large language models (MLLM) have\nattracted widespread attention for their outstanding performance in computer\nvision, particularly due to their powerful visual-language understanding and\ndialogic interaction capabilities. Specifically, we design a MLLM-based\noptimization strategy to generate multimodal textual data from the original CD\nimages, which serve as textual input to MGCR. Visual and textual features are\nextracted through a dual encoder framework. For the first time in the RSCD\ntask, we introduce a multimodal graph-conditioned vision-language\nreconstruction mechanism, which is integrated with graph attention to construct\na semantic graph-conditioned reconstruction module (SGCM), this module\ngenerates vision-language (VL) tokens through graph-based conditions and\nenables cross-dimensional interaction between visual and textual features via\nmultihead attention. The reconstructed VL features are then deeply fused using\nthe language vision transformer (LViT), achieving fine-grained feature\nalignment and high-level semantic interaction. Experimental results on four\npublic datasets demonstrate that MGCR achieves superior performance compared to\nmainstream CD methods. Our code is available on\nhttps://github.com/cn-xvkong/MGCR",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "CGCCE-Net:Change-Guided Cross Correlation Enhancement Network for Remote Sensing Building Change Detection",
    "url": "http://arxiv.org/abs/2508.01549v1",
    "authors": [
      "ChengMing Wang"
    ],
    "published": "2025-08-03",
    "abstract": "Change detection encompasses a variety of task types, and the goal of\nbuilding change detection (BCD) tasks is to accurately locate buildings and\ndistinguish changed building areas. In recent years, various deep\nlearning-based BCD methods have achieved significant success in detecting\ndifference regions by using different change information enhancement\ntechniques, effectively improving the precision of BCD tasks. To address the\nissue of BCD with special colors, we propose the change-guided cross\ncorrelation enhancement network (CGCCE-Net). We design the change-guided\nresidual refinement (CGRR) Branch, which focuses on extending shallow texture\nfeatures to multiple scale features obtained from PVT, enabling early attention\nand acquisition of special colors. Then, channel spatial attention is used in\nthe deep features to achieve independent information enhancement. Additionally,\nwe construct the global cross correlation module (GCCM) to facilitate semantic\ninformation interaction between bi-temporal images, establishing building and\ntarget recognition relationships between different images. Further semantic\nfeature enhancement is achieved through the semantic cognitive enhancement\nmodule (SCEM), and finally, the cross fusion decoder (CFD) is used for change\ninformation fusion and image reconstruction. Extensive experiments on three\npublic datasets demonstrate that our CGCCE-Net outperforms mainstream BCD\nmethods with outstanding performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets",
    "url": "http://arxiv.org/abs/2508.02871v1",
    "authors": [
      "J. Alex Hurt",
      "Trevor M. Bajkowski",
      "Grant J. Scott",
      "Curt H. Davis"
    ],
    "published": "2025-08-04",
    "abstract": "In 2012, AlexNet established deep convolutional neural networks (DCNNs) as\nthe state-of-the-art in CV, as these networks soon led in visual tasks for many\ndomains, including remote sensing. With the publication of Visual Transformers,\nwe are witnessing the second modern leap in computational vision, and as such,\nit is imperative to understand how various transformer-based neural networks\nperform on satellite imagery. While transformers have shown high levels of\nperformance in natural language processing and CV applications, they have yet\nto be compared on a large scale to modern remote sensing data. In this paper,\nwe explore the use of transformer-based neural networks for object detection in\nhigh-resolution electro-optical satellite imagery, demonstrating\nstate-of-the-art performance on a variety of publicly available benchmark data\nsets. We compare eleven distinct bounding-box detection and localization\nalgorithms in this study, of which seven were published since 2020, and all\neleven since 2015. The performance of five transformer-based architectures is\ncompared with six convolutional networks on three state-of-the-art opensource\nhigh-resolution remote sensing imagery datasets ranging in size and complexity.\nFollowing the training and evaluation of thirty-three deep neural models, we\nthen discuss and analyze model performance across various feature extraction\nmethodologies and detection algorithms.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2508.01941v1",
    "authors": [
      "Andrea Dosi",
      "Semanto Mondal",
      "Rajib Chandra Ghosh",
      "Massimo Brescia",
      "Giuseppe Longo"
    ],
    "published": "2025-08-03",
    "abstract": "This work presents the results of a methodological transfer from remote\nsensing to healthcare, adapting AMBER -- a transformer-based model originally\ndesigned for multiband images, such as hyperspectral data -- to the task of 3D\nmedical datacube segmentation. In this study, we use the AMBER architecture\nwith Adaptive Fourier Neural Operators (AFNO) in place of the multi-head\nself-attention mechanism. While existing models rely on various forms of\nattention to capture global context, AMBER-AFNO achieves this through\nfrequency-domain mixing, enabling a drastic reduction in model complexity. This\ndesign reduces the number of trainable parameters by over 80% compared to\nUNETR++, while maintaining a FLOPs count comparable to other state-of-the-art\narchitectures. Model performance is evaluated on two benchmark 3D medical\ndatasets -- ACDC and Synapse -- using standard metrics such as Dice Similarity\nCoefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO\nachieves competitive or superior accuracy with significant gains in training\nefficiency, inference speed, and memory usage.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Bridging ocean wave physics and deep learning: Physics-informed neural operators for nonlinear wavefield reconstruction in real-time",
    "url": "http://arxiv.org/abs/2508.03315v1",
    "authors": [
      "Svenja Ehlers",
      "Merten Stender",
      "Norbert Hoffmann"
    ],
    "published": "2025-08-05",
    "abstract": "Accurate real-time prediction of phase-resolved ocean wave fields remains a\ncritical yet largely unsolved problem, primarily due to the absence of\npractical data assimilation methods for reconstructing initial conditions from\nsparse or indirect wave measurements. While recent advances in supervised deep\nlearning have shown potential for this purpose, they require large labelled\ndatasets of ground truth wave data, which are infeasible to obtain in\nreal-world scenarios. To overcome this limitation, we propose a\nPhysics-Informed Neural Operator (PINO) framework for reconstructing spatially\nand temporally phase-resolved, nonlinear ocean wave fields from sparse\nmeasurements, without the need for ground truth data during training. This is\nachieved by embedding residuals of the free surface boundary conditions of\nocean gravity waves into the loss function of the PINO, constraining the\nsolution space in a soft manner. After training, we validate our approach using\nhighly realistic synthetic wave data and demonstrate the accurate\nreconstruction of nonlinear wave fields from both buoy time series and radar\nsnapshots. Our results indicate that PINOs enable accurate, real-time\nreconstruction and generalize robustly across a wide range of wave conditions,\nthereby paving the way for operational, data-driven wave reconstruction and\nprediction in realistic marine environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models",
    "url": "http://arxiv.org/abs/2508.05260v1",
    "authors": [
      "Zhouyao Qian",
      "Yang Chen",
      "Baodian Li",
      "Shuyi Zhang",
      "Zhen Tian",
      "Gongsen Wang",
      "Tianyue Gu",
      "Xinyu Zhou",
      "Huilin Chen",
      "Xinyi Li",
      "Hao Zhu",
      "Shuyao Zhang",
      "Zongheng Li",
      "Siyuan Wang"
    ],
    "published": "2025-08-07",
    "abstract": "Marine chlorophyll concentration is an important indicator of ecosystem\nhealth and carbon cycle strength, and its accurate prediction is crucial for\nred tide warning and ecological response. In this paper, we propose a LSTM-RF\nhybrid model that combines the advantages of LSTM and RF, which solves the\ndeficiencies of a single model in time-series modelling and nonlinear feature\nportrayal. Trained with multi-source ocean data(temperature, salinity,\ndissolved oxygen, etc.), the experimental results show that the LSTM-RF model\nhas an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test\nset, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2\n=0.4934) alone , respectively. The standardised treatment and sliding window\napproach improved the prediction accuracy of the model and provided an\ninnovative solution for high-frequency prediction of marine ecological\nvariables.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Operational convection-permitting COSMO/ICON ensemble predictions at observation sites (CIENS)",
    "url": "http://arxiv.org/abs/2508.03845v1",
    "authors": [
      "Sebastian Lerch",
      "Benedikt Schulz",
      "Reinhold Hess",
      "Annette M\u00f6ller",
      "Cristina Primo",
      "Sebastian Trepte",
      "Susanne Theis"
    ],
    "published": "2025-08-05",
    "abstract": "We present the CIENS dataset, which contains ensemble weather forecasts from\nthe operational convection-permitting numerical weather prediction model of the\nGerman Weather Service. It comprises forecasts for 55 meteorological variables\nmapped to the locations of synoptic stations, as well as additional spatially\naggregated forecasts from surrounding grid points, available for a subset of\nthese variables. Forecasts are available at hourly lead times from 0 to 21\nhours for two daily model runs initialized at 00 and 12 UTC, covering the\nperiod from December 2010 to June 2023. Additionally, the dataset provides\nstation observations for six key variables at 170 locations across Germany:\npressure, temperature, hourly precipitation accumulation, wind speed, wind\ndirection, and wind gusts. Since the forecast are mapped to the observed\nlocations, the data is delivered in a convenient format for analysis. The CIENS\ndataset complements the growing collection of benchmark datasets for weather\nand climate modeling. A key distinguishing feature is its long temporal extent,\nwhich encompasses multiple updates to the underlying numerical weather\nprediction model and thus supports investigations into how forecasting methods\ncan account for such changes. In addition to detailing the design and contents\nof the CIENS dataset, we outline potential applications in ensemble\npost-processing, forecast verification, and related research areas. A use case\nfocused on ensemble post-processing illustrates the benefits of incorporating\nthe rich set of available model predictors into machine learning-based\nforecasting models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Synthetic medical data generation: state of the art and application to trauma mechanism classification",
    "url": "http://arxiv.org/abs/2508.02771v1",
    "authors": [
      "Oc\u00e9ane Doremus",
      "Ariel Guerra-Adames",
      "Marta Avalos-Fernandez",
      "Vianney Jouhet",
      "C\u00e9dric Gil-Jardin\u00e9",
      "Emmanuel Lagarde"
    ],
    "published": "2025-08-04",
    "abstract": "Faced with the challenges of patient confidentiality and scientific\nreproducibility, research on machine learning for health is turning towards the\nconception of synthetic medical databases. This article presents a brief\noverview of state-of-the-art machine learning methods for generating synthetic\ntabular and textual data, focusing their application to the automatic\nclassification of trauma mechanisms, followed by our proposed methodology for\ngenerating high-quality, synthetic medical records combining tabular and\nunstructured text data.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "NICE^k Metrics: Unified and Multidimensional Framework for Evaluating Deterministic Solar Forecasting Accuracy",
    "url": "http://arxiv.org/abs/2508.01457v1",
    "authors": [
      "Cyril Voyant",
      "Milan Despotovic",
      "Luis Garcia-Gutierrez",
      "Rodrigo Amaro e Silva",
      "Philippe Lauret",
      "Ted Soubdhan",
      "Nadjem Bailek"
    ],
    "published": "2025-08-02",
    "abstract": "Accurate solar energy output prediction is key for integrating renewables\ninto grids, maintaining stability, and improving energy management. However,\nstandard error metrics such as Root Mean Squared Error (RMSE), Mean Absolute\nError (MAE), and Skill Scores (SS) fail to capture the multidimensional nature\nof solar irradiance forecasting. These metrics lack sensitivity to\nforecastability, rely on arbitrary baselines (e.g., clear-sky models), and are\npoorly suited for operational use.\n  To address this, we introduce the NICEk framework (Normalized Informed\nComparison of Errors, with k = 1, 2, 3, Sigma), offering a robust and\ninterpretable evaluation of forecasting models. Each NICEk score corresponds to\nan Lk norm: NICE1 targets average errors, NICE2 emphasizes large deviations,\nNICE3 highlights outliers, and NICESigma combines all.\n  Using Monte Carlo simulations and data from 68 stations in the Spanish SIAR\nnetwork, we evaluated methods including autoregressive models, extreme\nlearning, and smart persistence. Theoretical and empirical results align when\nassumptions hold (e.g., R^2 ~ 1.0 for NICE2). Most importantly, NICESigma\nconsistently shows higher discriminative power (p < 0.05), outperforming\ntraditional metrics (p > 0.05).\n  The NICEk metrics exhibit stronger statistical significance (e.g., p-values\nfrom 10^-6 to 0.004 across horizons) and greater generalizability. They offer a\nunified and operational alternative to standard error metrics in deterministic\nsolar forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.10568v1",
    "authors": [
      "Humza Naveed",
      "Xina Zeng",
      "Mitch Bryson",
      "Nagita Mehrseresht"
    ],
    "published": "2025-08-14",
    "abstract": "Foundational models have achieved significant success in diverse domains of\ncomputer vision. They learn general representations that are easily\ntransferable to tasks not seen during training. One such foundational model is\nSegment anything model (SAM), which can accurately segment objects in images.\nWe propose adapting the SAM encoder via fine-tuning for remote sensing change\ndetection (RSCD) along with spatial-temporal feature enhancement (STFE) and\nmulti-scale decoder fusion (MSDF) to detect changes robustly at multiple\nscales. Additionally, we propose a novel cross-entropy masking (CEM) loss to\nhandle high class imbalance in change detection datasets. Our method\noutperforms state-of-the-art (SOTA) methods on four change detection datasets,\nLevir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on\na large complex S2Looking dataset. The code is available at:\nhttps://github.com/humza909/SAM-CEM-CD",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss",
    "url": "http://arxiv.org/abs/2508.09453v1",
    "authors": [
      "Abdul Matin",
      "Tanjim Bin Faruk",
      "Shrideep Pallickara",
      "Sangmi Lee Pallickara"
    ],
    "published": "2025-08-13",
    "abstract": "The proliferation of foundation models, pretrained on large-scale unlabeled\ndatasets, has emerged as an effective approach in creating adaptable and\nreusable architectures that can be leveraged for various downstream tasks using\nsatellite observations. However, their direct application to hyperspectral\nremote sensing remains challenging due to inherent spectral disparities and the\nscarcity of available observations. In this work, we present HyperKD, a novel\nknowledge distillation framework that enables transferring learned\nrepresentations from a teacher model into a student model for effective\ndevelopment of a foundation model on hyperspectral images. Unlike typical\nknowledge distillation frameworks, which use a complex teacher to guide a\nsimpler student, HyperKD enables an inverse form of knowledge transfer across\ndifferent types of spectral data, guided by a simpler teacher model. Building\nupon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi\nfoundational model into a student tailored for EnMAP hyperspectral imagery.\nHyperKD addresses the inverse domain adaptation problem with spectral gaps by\nintroducing a feature-based strategy that includes spectral range-based channel\nalignment, spatial feature-guided masking, and an enhanced loss function\ntailored for hyperspectral images. HyperKD bridges the substantial spectral\ndomain gap, enabling the effective use of pretrained foundation models for\ngeospatial applications. Extensive experiments show that HyperKD significantly\nimproves representation learning in MAEs, leading to enhanced reconstruction\nfidelity and more robust performance on downstream tasks such as land cover\nclassification, crop type identification, and soil organic carbon prediction,\nunderpinning the potential of knowledge distillation frameworks in remote\nsensing analytics with hyperspectral imagery.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders",
    "url": "http://arxiv.org/abs/2508.07020v1",
    "authors": [
      "Tanjim Bin Faruk",
      "Abdul Matin",
      "Shrideep Pallickara",
      "Sangmi Lee Pallickara"
    ],
    "published": "2025-08-09",
    "abstract": "Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of\ncontiguous spectral bands, enabling fine-grained mapping of soils, crops, and\nland cover. While self-supervised Masked Autoencoders excel on RGB and low-band\nmultispectral data, they struggle to exploit the intricate spatial-spectral\ncorrelations in 200+ band hyperspectral images. We introduce TerraMAE, a novel\nHSI encoding framework specifically designed to learn highly representative\nspatial-spectral embeddings for diverse geospatial analyses. TerraMAE features\nan adaptive channel grouping strategy, based on statistical reflectance\nproperties to capture spectral similarities, and an enhanced reconstruction\nloss function that incorporates spatial and spectral quality metrics. We\ndemonstrate TerraMAE's effectiveness through superior spatial-spectral\ninformation preservation in high-fidelity image reconstruction. Furthermore, we\nvalidate its practical utility and the quality of its learned representations\nthrough strong performance on three key downstream geospatial tasks: crop\nidentification, land cover classification, and soil texture prediction.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning",
    "url": "http://arxiv.org/abs/2508.09555v1",
    "authors": [
      "Ahmet \u00d6ztel",
      "\u0130smet Karaca"
    ],
    "published": "2025-08-13",
    "abstract": "Objective - This study presents a biometric identification method based on\ntopological invariants from 2D iris images, representing iris texture via\nformally defined digital homology and evaluating classification performance.\n  Methods - Each normalized iris image (48x482 pixels) is divided into grids\n(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their\nratio using a recent algorithm for homology groups in 2D digital images. The\nresulting invariants form a feature matrix used with logistic regression, KNN,\nand SVM (with PCA and 100 randomized repetitions). A convolutional neural\nnetwork (CNN) is trained on raw images for comparison.\n  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,\noutperforming CNN (96.44 +/- 1.32%) and other feature-based models. The\ntopological features showed high accuracy with low variance.\n  Conclusion - This is the first use of topological invariants from formal\ndigital homology for iris recognition. The method offers a compact,\ninterpretable, and accurate alternative to deep learning, useful when\nexplainability or limited data is important. Beyond iris recognition, it can\napply to other biometrics, medical imaging, materials science, remote sensing,\nand interpretable AI. It runs efficiently on CPU-only systems and produces\nrobust, explainable features valuable for security-critical domains.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification",
      "Recognition",
      "Regression"
    ]
  },
  {
    "title": "Can Multitask Learning Enhance Model Explainability?",
    "url": "http://arxiv.org/abs/2508.06966v1",
    "authors": [
      "Hiba Najjar",
      "Bushra Alshbib",
      "Andreas Dengel"
    ],
    "published": "2025-08-09",
    "abstract": "Remote sensing provides satellite data in diverse types and formats. The\nusage of multimodal learning networks exploits this diversity to improve model\nperformance, except that the complexity of such networks comes at the expense\nof their interpretability. In this study, we explore how modalities can be\nleveraged through multitask learning to intrinsically explain model behavior.\nIn particular, instead of additional inputs, we use certain modalities as\nadditional targets to be predicted along with the main task. The success of\nthis approach relies on the rich information content of satellite data, which\nremains as input modalities. We show how this modeling context provides\nnumerous benefits: (1) in case of data scarcity, the additional modalities do\nnot need to be collected for model inference at deployment, (2) the model\nperformance remains comparable to the multimodal baseline performance, and in\nsome cases achieves better scores, (3) prediction errors in the main task can\nbe explained via the model behavior in the auxiliary task(s). We demonstrate\nthe efficiency of our approach on three datasets, including segmentation,\nclassification, and regression tasks. Code available at\ngit.opendfki.de/hiba.najjar/mtl_explainability/.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Leveraging GNN to Enhance MEF Method in Predicting ENSO",
    "url": "http://arxiv.org/abs/2508.07410v1",
    "authors": [
      "Saghar Ganji",
      "Mohammad Naisipour"
    ],
    "published": "2025-08-10",
    "abstract": "Reliable long-lead forecasting of the El Nino Southern Oscillation (ENSO)\nremains a long-standing challenge in climate science. The previously developed\nMultimodal ENSO Forecast (MEF) model uses 80 ensemble predictions by two\nindependent deep learning modules: a 3D Convolutional Neural Network (3D-CNN)\nand a time-series module. In their approach, outputs of the two modules are\ncombined using a weighting strategy wherein one is prioritized over the other\nas a function of global performance. Separate weighting or testing of\nindividual ensemble members did not occur, however, which may have limited the\nmodel to optimize the use of high-performing but spread-out forecasts. In this\nstudy, we propose a better framework that employs graph-based analysis to\ndirectly model similarity between all 80 members of the ensemble. By\nconstructing an undirected graph whose vertices are ensemble outputs and whose\nweights on edges measure similarity (via RMSE and correlation), we identify and\ncluster structurally similar and accurate predictions. From which we obtain an\noptimized subset of 20 members using community detection methods. The final\nprediction is then obtained by averaging this optimized subset. This method\nimproves the forecast skill through noise removal and emphasis on ensemble\ncoherence. Interestingly, our graph-based selection shows robust statistical\ncharacteristics among top performers, offering new ensemble behavior insights.\nIn addition, we observe that while the GNN-based approach does not always\noutperform the baseline MEF under every scenario, it produces more stable and\nconsistent outputs, particularly in compound long-lead situations. The approach\nis model-agnostic too, suggesting that it can be applied directly to other\nforecasting models with gargantuan ensemble outputs, such as statistical,\nphysical, or hybrid models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "GNN"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning for Cloud Detection in IASI Measurements: A Data-Driven SVM Approach with Physical Constraints",
    "url": "http://arxiv.org/abs/2508.10120v1",
    "authors": [
      "Chiara Zugarini",
      "Cristina Sgattoni",
      "Luca Sgheri"
    ],
    "published": "2025-08-13",
    "abstract": "Cloud detection is essential for atmospheric retrievals, climate studies, and\nweather forecasting. We analyze infrared radiances from the Infrared\nAtmospheric Sounding Interferometer (IASI) onboard Meteorological Operational\n(MetOp) satellites to classify scenes as clear or cloudy.\n  We apply the Support Vector Machine (SVM) approach, based on kernel methods\nfor non-separable data. In this study, the method is implemented for Cloud\nIdentification (CISVM) to classify the test set using radiances or brightness\ntemperatures, with dimensionality reduction through Principal Component\nAnalysis (PCA) and cloud-sensitive channel selection to focus on the most\ninformative features. Our best configuration achieves 88.30 percent agreement\nwith reference labels and shows strong consistency with cloud masks from the\nModerate Resolution Imaging Spectroradiometer (MODIS), with the largest\ndiscrepancies in polar regions due to sensor differences.\n  These results demonstrate that CISVM is a robust, flexible, and efficient\nmethod for automated cloud classification from infrared radiances, suitable for\noperational retrievals and future missions such as Far infrared Outgoing\nRadiation Understanding and Monitoring (FORUM), the ninth European Space Agency\nEarth Explorer Mission.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Knowledge-guided machine learning for disentangling Pacific sea surface temperature variability across timescales",
    "url": "http://arxiv.org/abs/2508.08490v1",
    "authors": [
      "Kyle J. C. Hall",
      "Maria J. Molina",
      "Emily F. Wisinski",
      "Gerald A. Meehl",
      "Antonietta Capotondi"
    ],
    "published": "2025-08-11",
    "abstract": "Global weather patterns and regimes are heavily influenced by the dominant\nmodes of Pacific sea surface temperature (SST) variability, including the El\nNi\\~no-Southern Oscillation (ENSO), Tropical Pacific Decadal Variability\n(TPDV), North Pacific Meridional Mode (NPMM), and the Pacific Decadal\nOscillation (PDO). However, separating these modes of variability remains\nchallenging due to their spatial overlap and possible nonlinear coupling, which\nviolates the assumptions of traditional linear methods. We develop a\nKnowledge-Guided AutoEncoder (KGAE) that uses spectral constraints to identify\nphysically interpretable modes, without the need for predefined temporal\nfilters or thresholds. The KGAE separates ENSO-like modes on 2- and 3-7-year\ntimescales and a decadal mode with characteristics reminiscent of the PDO and\nthe NPMM, each with distinct spatial patterns. We demonstrate that the decadal\nmode modulates ENSO diversity (central Pacific versus eastern Pacific), and\nthat a quasibiennial mode leads and follows the interannual mode, suggesting a\nrole in ENSO onset and decay. When applied to climate model output, KGAEs\nreveal model-specific biases in ENSO diversity and seasonal timing. Finally,\nresidual training isolates a primarily equatorial decadal mode, which may be a\ncomponent of TPDV-related decadal variability, likely originating from\nadvection linked to upwelling near the Gal\\'apagos Islands and the South\nEquatorial Current. Our results highlight how machine learning can uncover\nphysically meaningful modes of Earth system variability and improve the\nrepresentation and evaluation of variability across models and timescales.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "Nonparametric Reaction Coordinate Optimization with Histories: A Framework for Rare Event Dynamics",
    "url": "http://arxiv.org/abs/2508.07326v1",
    "authors": [
      "Polina V. Banushkina",
      "Sergei V. Krivov"
    ],
    "published": "2025-08-10",
    "abstract": "Rare but critical events in complex systems, such as protein folding,\nchemical reactions, disease progression, and extreme weather or climate\nphenomena, are governed by complex, high-dimensional, stochastic dynamics.\nIdentifying an optimal reaction coordinate (RC) that accurately captures the\nprogress of these dynamics is crucial for understanding and simulating such\nprocesses. This work introduces a nonparametric RC optimization framework that\nincorporates trajectory histories, enabling robust analysis even for irregular\nor incomplete data. The power of the method is demonstrated through\nincreasingly challenging analyses of protein folding dynamics, where it\nprovides accurate committor estimates that pass a stringent validation test and\nyield high-resolution free energy profiles. Its generality is further\nillustrated through applications to dynamics in phase space, a conceptual ocean\ncirculation model, and a longitudinal clinical dataset. These results\ndemonstrate that rare event dynamics can be accurately characterized without\nexhaustive sampling of the configuration space, establishing a general,\nflexible, and robust framework for analyzing complex dynamical systems and\nlongitudinal datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Taking the Garbage Out of Data-Driven Prediction Across Climate Timescales",
    "url": "http://arxiv.org/abs/2508.07062v1",
    "authors": [
      "Jason C. Furtado",
      "Maria J. Molina",
      "Marybeth C. Arcodia",
      "Weston Anderson",
      "Tom Beucler",
      "John A. Callahan",
      "Laura M. Ciasto",
      "Vittorio A. Gensini",
      "Michelle L'Heureux",
      "Kathleen Pegion",
      "Jhayron S. P\u00e9rez-Carrasquilla",
      "Maike Sonnewald",
      "Ken Takahashi",
      "Baoqiang Xiang",
      "Brian G. Zimmerman"
    ],
    "published": "2025-08-09",
    "abstract": "Artificial intelligence (AI) -- and specifically machine learning (ML) --\napplications for climate prediction across timescales are proliferating\nquickly. The emergence of these methods prompts a revisit to the impact of data\npreprocessing, a topic familiar to the climate community, as more traditional\nstatistical models work with relatively small sample sizes. Indeed, the skill\nand confidence in the forecasts produced by data-driven models are directly\ninfluenced by the quality of the datasets and how they are treated during model\ndevelopment, thus yielding the colloquialism \"garbage in, garbage out.\" As\nsuch, this article establishes protocols for the proper preprocessing of input\ndata for AI/ML models designed for climate prediction (i.e., subseasonal to\ndecadal and longer). The three aims are to: (1) educate researchers,\ndevelopers, and end users on the effects that preprocessing has on climate\npredictions; (2) provide recommended practices for data preprocessing for such\napplications; and (3) empower end users to decipher whether the models they are\nusing are properly designed for their objectives. Specific topics covered in\nthis article include the creation of (standardized) anomalies, dealing with\nnon-stationarity and the spatiotemporally correlated nature of climate data,\nand handling of extreme values and variables with potentially complex\ndistributions. Case studies will illustrate how using different preprocessing\ntechniques can produce different predictions from the same model, which can\ncreate confusion and decrease confidence in the overall process. Ultimately,\nimplementing the recommended practices set forth in this article will enhance\nthe robustness and transparency of AI/ML in climate prediction studies.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing",
    "url": "http://arxiv.org/abs/2508.12409v1",
    "authors": [
      "Liang Lv",
      "Di Wang",
      "Jing Zhang",
      "Lefei Zhang"
    ],
    "published": "2025-08-17",
    "abstract": "Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)\nanalysis by leveraging unlabeled data through pseudo-labeling and consistency\nlearning. However, existing S4 studies often rely on small-scale datasets and\nmodels, limiting their practical applicability. To address this, we propose S5,\nthe first scalable framework for semi-supervised semantic segmentation in RS,\nwhich unlocks the potential of vast unlabeled Earth observation data typically\nunderutilized due to costly pixel-level annotations. Built upon existing\nlarge-scale RS datasets, S5 introduces a data selection strategy that\nintegrates entropy-based filtering and diversity expansion, resulting in the\nRS4P-1M dataset. Using this dataset, we systematically scales S4 methods by\npre-training RS foundation models (RSFMs) of varying sizes on this extensive\ncorpus, significantly boosting their performance on land cover segmentation and\nobject detection tasks. Furthermore, during fine-tuning, we incorporate a\nMixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which\nenables efficient adaptation to multiple RS benchmarks with fewer parameters.\nThis approach improves the generalization and versatility of RSFMs across\ndiverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance\nacross all benchmarks, underscoring the viability of scaling semi-supervised\nlearning for RS applications. All datasets, code, and models will be released\nat https://github.com/MiliLab/S5",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries",
    "url": "http://arxiv.org/abs/2508.15752v1",
    "authors": [
      "Jon E. Froehlich",
      "Jared Hwang",
      "Zeyu Wang",
      "John S. O'Meara",
      "Xia Su",
      "William Huang",
      "Yang Zhang",
      "Alex Fiannaca",
      "Philip Nelson",
      "Shaun Kane"
    ],
    "published": "2025-08-21",
    "abstract": "Interactive digital maps have revolutionized how people travel and learn\nabout the world; however, they rely on pre-existing structured data in GIS\ndatabases (e.g., road networks, POI indices), limiting their ability to address\ngeo-visual questions related to what the world looks like. We introduce our\nvision for Geo-Visual Agents--multimodal AI agents capable of understanding and\nresponding to nuanced visual-spatial inquiries about the world by analyzing\nlarge-scale repositories of geospatial images, including streetscapes (e.g.,\nGoogle Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial\nimagery (e.g., satellite photos) combined with traditional GIS data sources. We\ndefine our vision, describe sensing and interaction approaches, provide three\nexemplars, and enumerate key challenges and opportunities for future work.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "CuMoLoS-MAE: A Masked Autoencoder for Remote Sensing Data Reconstruction",
    "url": "http://arxiv.org/abs/2508.14957v1",
    "authors": [
      "Anurup Naskar",
      "Nathanael Zhixin Wong",
      "Sara Shamekh"
    ],
    "published": "2025-08-20",
    "abstract": "Accurate atmospheric profiles from remote sensing instruments such as Doppler\nLidar, Radar, and radiometers are frequently corrupted by low-SNR (Signal to\nNoise Ratio) gates, range folding, and spurious discontinuities. Traditional\ngap filling blurs fine-scale structures, whereas deep models lack confidence\nestimates. We present CuMoLoS-MAE, a Curriculum-Guided Monte Carlo Stochastic\nEnsemble Masked Autoencoder designed to (i) restore fine-scale features such as\nupdraft and downdraft cores, shear lines, and small vortices, (ii) learn a\ndata-driven prior over atmospheric fields, and (iii) quantify pixel-wise\nuncertainty. During training, CuMoLoS-MAE employs a mask-ratio curriculum that\nforces a ViT decoder to reconstruct from progressively sparser context. At\ninference, we approximate the posterior predictive by Monte Carlo over random\nmask realisations, evaluating the MAE multiple times and aggregating the\noutputs to obtain the posterior predictive mean reconstruction together with a\nfinely resolved per-pixel uncertainty map. Together with high-fidelity\nreconstruction, this novel deep learning-based workflow enables enhanced\nconvection diagnostics, supports real-time data assimilation, and improves\nlong-term climate reanalysis.",
    "categories": [
      "ocean",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives",
    "url": "http://arxiv.org/abs/2508.14558v1",
    "authors": [
      "Juepeng Zheng",
      "Zi Ye",
      "Yibin Wen",
      "Jianxi Huang",
      "Zhiwei Zhang",
      "Qingmei Li",
      "Qiong Hu",
      "Baodong Xu",
      "Lingyuan Zhao",
      "Haohuan Fu"
    ],
    "published": "2025-08-20",
    "abstract": "Powered by advances in multiple remote sensing sensors, the production of\nhigh spatial resolution images provides great potential to achieve\ncost-efficient and high-accuracy agricultural inventory and analysis in an\nautomated way. Lots of studies that aim at providing an inventory of the level\nof each agricultural parcel have generated many methods for Agricultural Parcel\nand Boundary Delineation (APBD). This review covers APBD methods for detecting\nand delineating agricultural parcels and systematically reviews the past and\npresent of APBD-related research applied to remote sensing images. With the\ngoal to provide a clear knowledge map of existing APBD efforts, we conduct a\ncomprehensive review of recent APBD papers to build a meta-data analysis,\nincluding the algorithm, the study site, the crop type, the sensor type, the\nevaluation method, etc. We categorize the methods into three classes: (1)\ntraditional image processing methods (including pixel-based, edge-based and\nregion-based); (2) traditional machine learning methods (such as random forest,\ndecision tree); and (3) deep learning-based methods. With deep\nlearning-oriented approaches contributing to a majority, we further discuss\ndeep learning-based methods like semantic segmentation-based, object\ndetection-based and Transformer-based methods. In addition, we discuss five\nAPBD-related issues to further comprehend the APBD domain using remote sensing\ndata, such as multi-sensor data in APBD task, comparisons between single-task\nlearning and multi-task learning in the APBD domain, comparisons among\ndifferent algorithms and different APBD tasks, etc. Finally, this review\nproposes some APBD-related applications and a few exciting prospects and\npotential hot topics in future APBD research. We hope this review help\nresearchers who involved in APBD domain to keep track of its development and\ntendency.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Generative AI models enable efficient and physically consistent sea-ice simulations",
    "url": "http://arxiv.org/abs/2508.14984v1",
    "authors": [
      "Tobias Sebastian Finn",
      "Marc Bocquet",
      "Pierre Rampal",
      "Charlotte Durand",
      "Flavia Porro",
      "Alban Farchi",
      "Alberto Carrassi"
    ],
    "published": "2025-08-20",
    "abstract": "Sea ice is governed by highly complex, scale-invariant, and anisotropic\nprocesses that are challenging to represent in Earth system models. While\nadvanced numerical models have improved our understanding of the sea-ice\ndynamics, their computational costs often limit their application in ensemble\nforecasting and climate simulations. Here, we introduce GenSIM, the first\ngenerative AI-based pan-Arctic model that predicts the evolution of all\nrelevant key properties, including concentration, thickness, and drift, in a\n12-hour window with improved accuracy over deterministic predictions and high\ncomputational efficiency, while remaining physically consistent. Trained on a\nlong simulation from a state-of-the-art sea-ice--ocean system, GenSIM robustly\nreproduces statistics as observed in numerical models and observations,\nexhibiting brittle-like short-term dynamics while also depicting the long-term\nsea-ice decline. Driven solely by atmospheric forcings, we attribute GenSIM's\nemergent extrapolation capabilities to patterns that reflect the long-term\nimpact of the ocean: it seemingly has learned an internal ocean emulator. This\nability to infer slowly evolving climate-relevant dynamics from short-term\npredictions underlines the large potential of generative models to generalise\nfor unseen climates and to encode hidden physics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models",
    "url": "http://arxiv.org/abs/2508.14315v1",
    "authors": [
      "Pritthijit Nath",
      "Sebastian Schemm",
      "Henry Moss",
      "Peter Haynes",
      "Emily Shuckburgh",
      "Mark Webb"
    ],
    "published": "2025-08-19",
    "abstract": "Sub-grid parameterisations in climate models are traditionally static and\ntuned offline, limiting adaptability to evolving states. This work introduces\nFedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors\nthe spatial decomposition used in general circulation models (GCMs) by\nassigning agents to latitude bands, enabling local parameter learning with\nperiodic global aggregation. Using a hierarchy of simplified energy-balance\nclimate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble\n(ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under\ndifferent FedRL configurations. Results show that Deep Deterministic Policy\nGradient (DDPG) consistently outperforms both static and single-agent\nbaselines, with faster convergence and lower area-weighted RMSE in tropical and\nmid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to\ntransfer across hyperparameters and low computational cost make it well-suited\nfor geographically adaptive parameter learning. This capability offers a\nscalable pathway towards high-complexity GCMs and provides a prototype for\nphysically aligned, online-learning climate models that can evolve with a\nchanging climate. Code accessible at\nhttps://github.com/p3jitnath/climate-rl-fedrl.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network",
    "url": "http://arxiv.org/abs/2508.13099v1",
    "authors": [
      "Mingyu Kim",
      "Daniel Stilwell",
      "Jorge Jimenez"
    ],
    "published": "2025-08-18",
    "abstract": "This paper presents a framework for classifying and detecting spatial\ncommission outliers in maritime environments using seabed acoustic sensor\nnetworks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as\na mixture of normal and outlier processes, we estimate the probability that a\nnewly observed event is an outlier. We propose a second-order approximation of\nthis probability that incorporates both the mean and variance of the normal\nintensity function, providing improved classification accuracy compared to\nmean-only approaches. We analytically show that our method yields a tighter\nbound to the true probability using Jensen's inequality. To enhance detection,\nwe integrate a real-time, near-optimal sensor placement strategy that\ndynamically adjusts sensor locations based on the evolving outlier intensity.\nThe proposed framework is validated using real ship traffic data near Norfolk,\nVirginia, where numerical results demonstrate the effectiveness of our approach\nin improving both classification performance and outlier detection through\nsensor deployment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams",
    "url": "http://arxiv.org/abs/2508.12198v1",
    "authors": [
      "ChangJae Lee",
      "Heecheol Yang",
      "Jonghak Choi"
    ],
    "published": "2025-08-17",
    "abstract": "Forecasting from atmospheric soundings is a fundamental task in operational\nmeteorology, often requiring structured visual reasoning over Skew-T log-P\ndiagrams by human forecasters. While recent advances in Vision-Language Models\n(VLMs) have shown promise in other scientific domains, their application to\nmeteorological diagram interpretation remains largely unexplored. In this\nstudy, we present a lightweight AI assistant that interprets Skew-T diagrams\nusing a small language model (LM) and a small VLM fine-tuned to emulate human\nforecasters. Using a curriculum learning framework, we first train the models\nto identify key atmospheric features from diagrams through visual question\nanswering, followed by chain-of-thought reasoning tasks that estimate\nprecipitation probability based on the derived visual groundings. Model inputs\ninclude either textual summaries or generated Skew-T diagrams derived from\noperational Numerical Weather Prediction (NWP) forecasts, paired with\nthree-hour precipitation observations from South Korea's Auto Weather Stations\nnetwork. Evaluation results demonstrate that the fine-tuned VLM achieves skill\ncomparable to an operational NWP model, despite relying solely on static\natmospheric profiles. Ablation studies reveal that visual grounding and\nreasoning supervision are critical for performance, while attention map\nanalysis confirms that the model learns to focus on relevant meteorological\nfeatures. These findings highlight the potential of compact, interpretable\nmultimodal models to support weather forecasting tasks. The approach offers a\ncomputationally efficient alternative to large-scale systems, and future work\ncould extend it to more complex applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement",
    "url": "http://arxiv.org/abs/2508.20954v1",
    "authors": [
      "Amir Jmal",
      "Chaima Chtourou",
      "Mahdi Louati",
      "Abdelaziz Kallel",
      "Houda Khmila"
    ],
    "published": "2025-08-28",
    "abstract": "In the context of proven climate change, maintaining olive biodiversity\nthrough early anomaly detection and treatment using remote sensing technology\nis crucial, offering effective management solutions. This paper presents an\ninnovative approach to olive tree segmentation from satellite images. By\nleveraging foundational models and advanced segmentation techniques, the study\nintegrates the Segment Anything Model (SAM) to accurately identify and segment\nolive trees in agricultural plots. The methodology includes SAM segmentation\nand corrections based on trees alignement in the field and a learanble\nconstraint about the shape and the size. Our approach achieved a 98\\% accuracy\nrate, significantly surpassing the initial SAM performance of 82\\%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Anomaly Detection"
    ]
  },
  {
    "title": "Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory",
    "url": "http://arxiv.org/abs/2508.18829v1",
    "authors": [
      "Takayuki Ishikawa",
      "Carmelo Bonannella",
      "Bas J. W. Lerink",
      "Marc Ru\u00dfwurm"
    ],
    "published": "2025-08-26",
    "abstract": "National Forest Inventory (NFI)s serve as the primary source of forest\ninformation, providing crucial tree species distribution data. However,\nmaintaining these inventories requires labor-intensive on-site campaigns.\nRemote sensing approaches, particularly when combined with machine learning,\noffer opportunities to update NFIs more frequently and at larger scales. While\nthe use of Satellite Image Time Series has proven effective for distinguishing\ntree species through seasonal canopy reflectance patterns, current approaches\nrely primarily on Random Forest classifiers with hand-designed features and\nphenology-based metrics. Using deep features from an available pre-trained\nremote sensing foundation models offers a complementary strategy. These\npre-trained models leverage unannotated global data and are meant to used for\ngeneral-purpose applications and can then be efficiently fine-tuned with\nsmaller labeled datasets for specific classification tasks. This work\nsystematically investigates how deep features improve tree species\nclassification accuracy in the Netherlands with few annotated data. Data-wise,\nwe extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites\ndata and SRTM data using Google Earth Engine. Our results demonstrate that\nfine-tuning a publicly available remote sensing time series foundation model\noutperforms the current state-of-the-art in NFI classification in the\nNetherlands by a large margin of up to 10% across all datasets. This\ndemonstrates that classic hand-defined harmonic features are too simple for\nthis task and highlights the potential of using deep AI features for\ndata-limited application like NFI classification. By leveraging openly\navailable satellite data and pre-trained models, this approach significantly\nimproves classification accuracy compared to traditional methods and can\neffectively complement existing forest inventory processes.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images",
    "url": "http://arxiv.org/abs/2508.18067v1",
    "authors": [
      "Kaiyu Li",
      "Xiangyong Cao",
      "Ruixun Liu",
      "Shihong Wang",
      "Zixuan Jiang",
      "Zhi Wang",
      "Deyu Meng"
    ],
    "published": "2025-08-25",
    "abstract": "Semantic segmentation of remote sensing (RS) images is pivotal for\ncomprehensive Earth observation, but the demand for interpreting new object\ncategories, coupled with the high expense of manual annotation, poses\nsignificant challenges. Although open-vocabulary semantic segmentation (OVSS)\noffers a promising solution, existing frameworks designed for natural images\nare insufficient for the unique complexities of RS data. They struggle with\nvast scale variations and fine-grained details, and their adaptation often\nrelies on extensive, costly annotations. To address this critical gap, this\npaper introduces SegEarth-OV, the first framework for annotation-free\nopen-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,\na universal upsampler that robustly restores high-resolution spatial details\nfrom coarse features, correcting distorted target shapes without any\ntask-specific post-training. We also present a simple yet effective Global Bias\nAlleviation operation to subtract the inherent global context from patch\nfeatures, significantly enhancing local semantic fidelity. These components\nempower SegEarth-OV to effectively harness the rich semantics of pre-trained\nVLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the\nframework's universality to other challenging RS modalities like SAR images,\nwhere large-scale VLMs are unavailable and expensive to create, we introduce\nAlignEarth, which is a distillation-based strategy and can efficiently transfer\nsemantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the\nneed to build SAR foundation models from scratch and enabling universal OVSS\nacross diverse sensor types. Extensive experiments on both optical and SAR\ndatasets validate that SegEarth-OV can achieve dramatic improvements over the\nSOTA methods, establishing a robust foundation for annotation-free and\nopen-world Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "GRASP: Geospatial pixel Reasoning viA Structured Policy learning",
    "url": "http://arxiv.org/abs/2508.17102v1",
    "authors": [
      "Chengjie Jiang",
      "Yunqi Zhou",
      "Jiafeng Yan",
      "Jing Li"
    ],
    "published": "2025-08-23",
    "abstract": "Geospatial pixel reasoning is a nascent remote-sensing task that aims to\ngenerate segmentation masks directly from natural-language instructions.\nPrevailing MLLM-based systems co-train a language model and a mask decoder with\ndense pixel supervision, which is expensive and often weak on out-of-domain\n(OOD) data. We introduce GRASP, a structured policy-learning framework. In our\ndesign, a multimodal large language model first emits task-relevant bounding\nboxes and positive points from a vision-language instruction. These outputs are\nthen passed to a pre-trained segmentation model, which consumes them as prompts\nto generate the final mask. Instead of supervised fine-tuning, we optimize the\nsystem purely with reinforcement learning: the model is trained solely with\nGRPO, guided by format rewards and accuracy rewards computed on boxes and\npoints (no mask supervision). This leverages strong priors in foundation\nmodels, minimizes trainable parameters, and enables learning from inexpensive\nannotations. We additionally curate GRASP-1k, which contains\nreasoning-intensive queries, detailed reasoning traces, and fine-grained\nsegmentation annotations. Evaluations on both in-domain and out-of-domain test\nsets show state-of-the-art results: about 4% improvement in-domain and up to\n54% on OOD benchmarks. The experiment results evidence our model's robust\ngeneralization and demonstrate that complex geospatial segmentation behaviors\ncan be learned via RL from weak spatial cues. Code and the dataset will be\nreleased open-source.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities",
    "url": "http://arxiv.org/abs/2508.19305v1",
    "authors": [
      "Chen Chu",
      "Cyrus Shahabi"
    ],
    "published": "2025-08-26",
    "abstract": "Spatial representation learning is essential for GeoAI applications such as\nurban analytics, enabling the encoding of shapes, locations, and spatial\nrelationships (topological and distance-based) of geo-entities like points,\npolylines, and polygons. Existing methods either target a single geo-entity\ntype or, like Poly2Vec, decompose entities into simpler components to enable\nFourier transformation, introducing high computational cost. Moreover, since\nthe transformed space lacks geometric alignment, these methods rely on uniform,\nnon-adaptive sampling, which blurs fine-grained features like edges and\nboundaries. To address these limitations, we introduce Geo2Vec, a novel method\ninspired by signed distance fields (SDF) that operates directly in the original\nspace. Geo2Vec adaptively samples points and encodes their signed distances\n(positive outside, negative inside), capturing geometry without decomposition.\nA neural network trained to approximate the SDF produces compact,\ngeometry-aware, and unified representations for all geo-entity types.\nAdditionally, we propose a rotation-invariant positional encoding to model\nhigh-frequency spatial variations and construct a structured and robust\nembedding space for downstream GeoAI models. Empirical results show that\nGeo2Vec consistently outperforms existing methods in representing shape and\nlocation, capturing topological and distance relationships, and achieving\ngreater efficiency in real-world GeoAI applications. Code and Data can be found\nat: https://github.com/chuchen2017/GeoNeuralRepresentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "The point is the mask: scaling coral reef segmentation with weak supervision",
    "url": "http://arxiv.org/abs/2508.18958v1",
    "authors": [
      "Matteo Contini",
      "Victor Illien",
      "Sylvain Poulain",
      "Serge Bernard",
      "Julien Barde",
      "Sylvain Bonhommeau",
      "Alexis Joly"
    ],
    "published": "2025-08-26",
    "abstract": "Monitoring coral reefs at large spatial scales remains an open challenge,\nessential for assessing ecosystem health and informing conservation efforts.\nWhile drone-based aerial imagery offers broad spatial coverage, its limited\nresolution makes it difficult to reliably distinguish fine-scale classes, such\nas coral morphotypes. At the same time, obtaining pixel-level annotations over\nlarge spatial extents is costly and labor-intensive, limiting the scalability\nof deep learning-based segmentation methods for aerial imagery. We present a\nmulti-scale weakly supervised semantic segmentation framework that addresses\nthis challenge by transferring fine-scale ecological information from\nunderwater imagery to aerial data. Our method enables large-scale coral reef\nmapping from drone imagery with minimal manual annotation, combining\nclassification-based supervision, spatial interpolation and self-distillation\ntechniques. We demonstrate the efficacy of the approach, enabling large-area\nsegmentation of coral morphotypes and demonstrating flexibility for integrating\nnew classes. This study presents a scalable, cost-effective methodology for\nhigh-resolution reef monitoring, combining low-cost data collection, weakly\nsupervised deep learning and multi-scale remote sensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency",
    "url": "http://arxiv.org/abs/2508.18693v1",
    "authors": [
      "Zhitong Cheng",
      "Yiran Jiang",
      "Yulong Ge",
      "Yufeng Li",
      "Zhongheng Qin",
      "Rongzhi Lin",
      "Jianwei Ma"
    ],
    "published": "2025-08-26",
    "abstract": "Domain shift, characterized by degraded model performance during transition\nfrom labeled source domains to unlabeled target domains, poses a persistent\nchallenge for deploying deep learning systems. Current unsupervised domain\nadaptation (UDA) methods predominantly rely on fine-tuning feature extractors -\nan approach limited by inefficiency, reduced interpretability, and poor\nscalability to modern architectures.\n  Our analysis reveals that models pretrained on large-scale data exhibit\ndomain-invariant geometric patterns in their feature space, characterized by\nintra-class clustering and inter-class separation, thereby preserving\ntransferable discriminative structures. These findings indicate that domain\nshifts primarily manifest as boundary misalignment rather than feature\ndegradation.\n  Unlike fine-tuning entire pre-trained models - which risks introducing\nunpredictable feature distortions - we propose the Feature-space Planes\nSearcher (FPS): a novel domain adaptation framework that optimizes decision\nboundaries by leveraging these geometric patterns while keeping the feature\nencoder frozen. This streamlined approach enables interpretative analysis of\nadaptation while substantially reducing memory and computational costs through\noffline feature extraction, permitting full-dataset optimization in a single\ncomputation cycle.\n  Evaluations on public benchmarks demonstrate that FPS achieves competitive or\nsuperior performance to state-of-the-art methods. FPS scales efficiently with\nmultimodal large models and shows versatility across diverse domains including\nprotein structure prediction, remote sensing classification, and earthquake\ndetection. We anticipate FPS will provide a simple, effective, and\ngeneralizable paradigm for transfer learning, particularly in domain adaptation\ntasks. .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Inferring geometry and material properties from Mueller matrices with machine learning",
    "url": "http://arxiv.org/abs/2508.19713v1",
    "authors": [
      "Lars Doorenbos",
      "C. H. Lucas Patty",
      "Raphael Sznitman",
      "Pablo M\u00e1rquez-Neila"
    ],
    "published": "2025-08-27",
    "abstract": "Mueller matrices (MMs) encode information on geometry and material\nproperties, but recovering both simultaneously is an ill-posed problem. We\nexplore whether MMs contain sufficient information to infer surface geometry\nand material properties with machine learning. We use a dataset of spheres of\nvarious isotropic materials, with MMs captured over the full angular domain at\nfive visible wavelengths (450-650 nm). We train machine learning models to\npredict material properties and surface normals using only these MMs as input.\nWe demonstrate that, even when the material type is unknown, surface normals\ncan be predicted and object geometry reconstructed. Moreover, MMs allow models\nto identify material types correctly. Further analyses show that diagonal\nelements are key for material characterization, and off-diagonal elements are\ndecisive for normal estimation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Global Forecasting of Tropical Cyclone Intensity Using Neural Weather Models",
    "url": "http://arxiv.org/abs/2508.17903v2",
    "authors": [
      "Milton Gomez",
      "Louis Poulain--Auzeau",
      "Alexis Berne",
      "Tom Beucler"
    ],
    "published": "2025-08-25",
    "abstract": "Numerical Weather Prediction (NWP) models that integrate coupled physical\nequations forward in time are the traditional tools for simulating atmospheric\nprocesses and forecasting weather. With recent advancements in deep learning,\nNeural Weather Models (NeWMs) have emerged as competent medium-range NWP\nemulators, with performances that compare favorably to state-of-the-art NWP\nmodels. However, they are commonly trained on reanalyses with limited spatial\nresolution (e.g., 0.25{\\deg} horizontal grid spacing), which smooths out key\nfeatures of weather systems. For example, tropical cyclones (TCs)-among the\nmost impactful weather events due to their devastating effects on human\nactivities-are challenging to forecast, as extrema like wind gusts, used as\nproxies for TC intensity, are smoothed in deterministic forecasts at 0.25{\\deg}\nresolution. To address this, we use our best observational estimates of wind\ngusts and minimum sea level pressure to train a hierarchy of post-processing\nmodels on NeWM outputs. Applied to Pangu-Weather and FourCastNet v2, the\npost-processing models produce accurate and reliable forecasts of TC intensity\nup to five days ahead. Our post-processing algorithm is tracking-independent,\npreventing full misses, and we demonstrate that even linear models extract\npredictive information from NeWM outputs beyond what is encoded in their\ninitial conditions. While spatial masking improves probabilistic forecast\nconsistency, we do not find clear advantages of convolutional architectures\nover simple multilayer perceptrons for our NeWM post-processing purposes.\nOverall, by combining the efficiency of NeWMs with a lightweight,\ntracking-independent post-processing framework, our approach improves the\naccessibility of global TC intensity forecasts, marking a step toward their\ndemocratization.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "Huracan: A skillful end-to-end data-driven system for ensemble data assimilation and weather prediction",
    "url": "http://arxiv.org/abs/2508.18486v1",
    "authors": [
      "Zekun Ni",
      "Jonathan Weyn",
      "Hang Zhang",
      "Yanfei Xiang",
      "Jiang Bian",
      "Weixin Jin",
      "Kit Thambiratnam",
      "Qi Zhang",
      "Haiyu Dong",
      "Hongyu Sun"
    ],
    "published": "2025-08-25",
    "abstract": "Over the past few years, machine learning-based data-driven weather\nprediction has been transforming operational weather forecasting by providing\nmore accurate forecasts while using a mere fraction of computing power compared\nto traditional numerical weather prediction (NWP). However, those models still\nrely on initial conditions from NWP, putting an upper limit on their forecast\nabilities. A few end-to-end systems have since been proposed, but they have yet\nto match the forecast skill of state-of-the-art NWP competitors. In this work,\nwe propose Huracan, an observation-driven weather forecasting system which\ncombines an ensemble data assimilation model with a forecast model to produce\nhighly accurate forecasts relying only on observations as inputs. Huracan is\nnot only the first to provide ensemble initial conditions and end-to-end\nensemble weather forecasts, but also the first end-to-end system to achieve an\naccuracy comparable with that of ECMWF ENS, the state-of-the-art NWP\ncompetitor, despite using a smaller amount of available observation data.\nNotably, Huracan matches or exceeds the continuous ranked probability score of\nECMWF ENS on 75.4% of the variable and lead time combinations. Our work is a\nmajor step forward in end-to-end data-driven weather prediction and opens up\nopportunities for further improving and revolutionizing operational weather\nforecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Finetuning AI Foundation Models to Develop Subgrid-Scale Parameterizations: A Case Study on Atmospheric Gravity Waves",
    "url": "http://arxiv.org/abs/2509.03816v1",
    "authors": [
      "Aman Gupta",
      "Aditi Sheshadri",
      "Sujit Roy",
      "Johannes Schmude",
      "Vishal Gaur",
      "Wei Ji Leong",
      "Manil Maskey",
      "Rahul Ramachandran"
    ],
    "published": "2025-09-04",
    "abstract": "Global climate models parameterize a range of atmospheric-oceanic processes\nlike gravity waves, clouds, moist convection, and turbulence that cannot be\nsufficiently resolved. These subgrid-scale closures for unresolved processes\nare a leading source of model uncertainty. Here, we present a new approach to\ndeveloping machine learning parameterizations of small-scale climate processes\nby fine-tuning a pre-trained AI foundation model (FM). FMs are largely\nunexplored in climate research. A pre-trained encoder-decoder from a 2.3\nbillion parameter FM (NASA and IBM Research's Prithvi WxC) -- which contains a\nlatent probabilistic representation of atmospheric evolution -- is fine-tuned\n(or reused) to create a deep learning parameterization for atmospheric gravity\nwaves (GWs). The parameterization captures GW effects for a coarse-resolution\nclimate model by learning the fluxes from an atmospheric reanalysis with 10\ntimes finer resolution. A comparison of monthly averages and instantaneous\nevolution with a machine learning model baseline (an Attention U-Net) reveals\nsuperior predictive performance of the FM parameterization throughout the\natmosphere, even in regions excluded from pre-training. This performance boost\nis quantified using the Hellinger distance, which is 0.11 for the baseline and\n0.06 for the fine-tuned model. Our findings emphasize the versatility and\nreusability of FMs, which could be used to accomplish a range of atmosphere-\nand climate-related applications, leading the way for the creation of\nobservations-driven and physically accurate parameterizations for more\nearth-system processes.",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework",
    "url": "http://arxiv.org/abs/2509.01910v1",
    "authors": [
      "Furong Jia",
      "Lanxin Liu",
      "Ce Hou",
      "Fan Zhang",
      "Xinyan Liu",
      "Yu Liu"
    ],
    "published": "2025-09-02",
    "abstract": "Worldwide geo-localization involves determining the exact geographic location\nof images captured globally, typically guided by geographic cues such as\nclimate, landmarks, and architectural styles. Despite advancements in\ngeo-localization models like GeoCLIP, which leverages images and location\nalignment via contrastive learning for accurate predictions, the\ninterpretability of these models remains insufficiently explored. Current\nconcept-based interpretability methods fail to align effectively with\nGeo-alignment image-location embedding objectives, resulting in suboptimal\ninterpretability and performance. To address this gap, we propose a novel\nframework integrating global geo-localization with concept bottlenecks. Our\nmethod inserts a Concept-Aware Alignment Module that jointly projects image and\nlocation embeddings onto a shared bank of geographic concepts (e.g., tropical\nclimate, mountain, cathedral) and minimizes a concept-level loss, enhancing\nalignment in a concept-specific subspace and enabling robust interpretability.\nTo our knowledge, this is the first work to introduce interpretability into\ngeo-localization. Extensive experiments demonstrate that our approach surpasses\nGeoCLIP in geo-localization accuracy and boosts performance across diverse\ngeospatial prediction tasks, revealing richer semantic insights into geographic\ndecision-making processes.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2509.03961v1",
    "authors": [
      "Yijun Zhou",
      "Yikui Zhai",
      "Zilu Ying",
      "Tingfeng Xian",
      "Wenlve Zhou",
      "Zhiheng Zhou",
      "Xiaolin Tian",
      "Xudong Jia",
      "Hongsheng Zhang",
      "C. L. Philip Chen"
    ],
    "published": "2025-09-04",
    "abstract": "Although deep learning has advanced remote sensing change detection (RSCD),\nmost methods rely solely on image modality, limiting feature representation,\nchange pattern modeling, and generalization especially under illumination and\nnoise disturbances. To address this, we propose MMChange, a multimodal RSCD\nmethod that combines image and text modalities to enhance accuracy and\nrobustness. An Image Feature Refinement (IFR) module is introduced to highlight\nkey regions and suppress environmental noise. To overcome the semantic\nlimitations of image features, we employ a vision language model (VLM) to\ngenerate semantic descriptions of bitemporal images. A Textual Difference\nEnhancement (TDE) module then captures fine grained semantic shifts, guiding\nthe model toward meaningful changes. To bridge the heterogeneity between\nmodalities, we design an Image Text Feature Fusion (ITFF) module that enables\ndeep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and\nSYSUCD demonstrate that MMChange consistently surpasses state of the art\nmethods across multiple metrics, validating its effectiveness for multimodal\nRSCD. Code is available at: https://github.com/yikuizhai/MMChange.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing",
    "url": "http://arxiv.org/abs/2509.03376v1",
    "authors": [
      "Hui Chen",
      "Liangyu Liu",
      "Xianchao Xiu",
      "Wanquan Liu"
    ],
    "published": "2025-09-03",
    "abstract": "Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote\nsensing images into a set of endmembers and their corresponding abundances.\nDespite significant progress in this field using deep learning, most methods\nfail to simultaneously characterize global dependencies and local consistency,\nmaking it difficult to preserve both long-range interactions and boundary\ndetails. This letter proposes a novel transformer-guided content-adaptive graph\nunmixing framework (T-CAGU), which overcomes these challenges by employing a\ntransformer to capture global dependencies and introducing a content-adaptive\ngraph neural network to enhance local relationships. Unlike previous work,\nT-CAGU integrates multiple propagation orders to dynamically learn the graph\nstructure, ensuring robustness against noise. Furthermore, T-CAGU leverages a\ngraph residual mechanism to preserve global information and stabilize training.\nExperimental results demonstrate its superiority over the state-of-the-art\nmethods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Information transmission: Inferring change area from change moment in time series remote sensing images",
    "url": "http://arxiv.org/abs/2509.03112v1",
    "authors": [
      "Jialu Li",
      "Chen Wu",
      "Meiqi Hu"
    ],
    "published": "2025-09-03",
    "abstract": "Time series change detection is a critical task for exploring ecosystem\ndynamics using time series remote sensing images, because it can simultaneously\nindicate where and when change occur. While deep learning has shown excellent\nperformance in this domain, it continues to approach change area detection and\nchange moment identification as distinct tasks. Given that change area can be\ninferred from change moment, we propose a time series change detection network,\nnamed CAIM-Net (Change Area Inference from Moment Network), to ensure\nconsistency between change area and change moment results. CAIM-Net infers\nchange area from change moment based on the intrinsic relationship between time\nseries analysis and spatial change detection. The CAIM-Net comprises three key\nsteps: Difference Extraction and Enhancement, Coarse Change Moment Extraction,\nand Fine Change Moment Extraction and Change Area Inference. In the Difference\nExtraction and Enhancement, a lightweight encoder with batch dimension stacking\nis designed to rapidly extract difference features. Subsequently, boundary\nenhancement convolution is applied to amplify these difference features. In the\nCoarse Change Moment Extraction, the enhanced difference features from the\nfirst step are used to spatiotemporal correlation analysis, and then two\ndistinct methods are employed to determine coarse change moments. In the Fine\nChange Moment Extraction and Change Area Inference, a multiscale temporal Class\nActivation Mapping (CAM) module first increases the weight of the\nchange-occurring moment from coarse change moments. Then the weighted change\nmoment is used to infer change area based on the fact that pixels with the\nchange moment must have undergone a change.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision",
    "url": "http://arxiv.org/abs/2509.01882v2",
    "authors": [
      "Shubham Laxmikant Deshmukh",
      "Matthew Wilchek",
      "Feras A. Batarseh"
    ],
    "published": "2025-09-02",
    "abstract": "Ongoing advancements in computer vision, particularly in pattern recognition\nand scene classification, have enabled new applications in environmental\nmonitoring. Deep learning now offers non-contact methods for assessing water\nquality and detecting contamination, both critical for disaster response and\npublic health protection. This work introduces HydroVision, a deep\nlearning-based scene classification framework that estimates optically active\nwater quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored\nDissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and\nTurbidity from standard Red-Green-Blue (RGB) images of surface water.\nHydroVision supports early detection of contamination trends and strengthens\nmonitoring by regulatory agencies during external environmental stressors,\nindustrial activities, and force majeure events. The model is trained on more\nthan 500,000 seasonally varied images collected from the United States\nGeological Survey Hydrologic Imagery Visualization and Information System\nbetween 2022 and 2024. This approach leverages widely available RGB imagery as\na scalable, cost-effective alternative to traditional multispectral and\nhyperspectral remote sensing. Four state-of-the-art convolutional neural\nnetworks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer\nare evaluated through transfer learning to identify the best-performing\narchitecture. DenseNet121 achieves the highest validation performance, with an\nR2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for\nreal-world water quality monitoring across diverse conditions. While the\ncurrent model is optimized for well-lit imagery, future work will focus on\nimproving robustness under low-light and obstructed scenarios to expand its\noperational utility.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "VGG",
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment",
    "url": "http://arxiv.org/abs/2509.01183v1",
    "authors": [
      "Bingnan Yang",
      "Mi Zhang",
      "Zhili Zhang",
      "Zhan Zhang",
      "Yuanxin Zhao",
      "Xiangyun Hu",
      "Jianya Gong"
    ],
    "published": "2025-09-01",
    "abstract": "High-quality image segmentation is fundamental to pixel-level geospatial\nanalysis in remote sensing, necessitating robust segmentation quality\nassessment (SQA), particularly in unsupervised settings lacking ground truth.\nAlthough recent deep learning (DL) based unsupervised SQA methods show\npotential, they often suffer from coarse evaluation granularity, incomplete\nassessments, and poor transferability. To overcome these limitations, this\npaper introduces Panoramic Quality Mapping (PQM) as a new paradigm for\ncomprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning\nframework realizing this approach. SegAssess distinctively formulates SQA as a\nfine-grained, four-class panoramic segmentation task, classifying pixels within\na segmentation mask under evaluation into true positive (TP), false positive\n(FP), true negative (TN), and false negative (FN) categories, thereby\ngenerating a complete quality map. Leveraging an enhanced Segment Anything\nModel (SAM) architecture, SegAssess uniquely employs the input mask as a prompt\nfor effective feature integration via cross-attention. Key innovations include\nan Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF)\nmodule to refine predictions near challenging object edges, and an Augmented\nMixup Sampling (AMS) training strategy integrating multi-source masks to\nsignificantly boost cross-domain robustness and zero-shot transferability.\nComprehensive experiments across 32 datasets derived from 6 sources demonstrate\nthat SegAssess achieves state-of-the-art (SOTA) performance and exhibits\nremarkable zero-shot transferability to unseen masks, establishing PQM via\nSegAssess as a robust and transferable solution for unsupervised SQA. The code\nis available at https://github.com/Yangbn97/SegAssess.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification",
    "url": "http://arxiv.org/abs/2509.00677v1",
    "authors": [
      "Qingyu Wang",
      "Xue Jiang",
      "Guozheng Xu"
    ],
    "published": "2025-08-31",
    "abstract": "Multimodal fusion has made great progress in the field of remote sensing\nimage classification due to its ability to exploit the complementary\nspatial-spectral information. Deep learning methods such as CNN and Transformer\nhave been widely used in these domains. State Space Models recently highlighted\nthat prior methods suffer from quadratic computational complexity. As a result,\nmodeling longer-range dependencies of spatial-spectral features imposes an\noverwhelming burden on the network. Mamba solves this problem by incorporating\ntime-varying parameters into ordinary SSM and performing hardware optimization,\nbut it cannot perform feature fusion directly. In order to make full use of\nMamba's low computational burden and explore the potential of internal\nstructure in multimodal feature fusion, we propose Cross State Fusion Mamba\n(CSFMamba) Network. Specifically, we first design the preprocessing module of\nremote sensing image information for the needs of Mamba structure, and combine\nit with CNN to extract multi-layer features. Secondly, a cross-state module\nbased on Mamba operator is creatively designed to fully fuse the feature of the\ntwo modalities. The advantages of Mamba and CNN are combined by designing a\nmore powerful backbone. We capture the fusion relationship between HSI and\nLiDAR modalities with stronger full-image understanding. The experimental\nresults on two datasets of MUUFL and Houston2018 show that the proposed method\noutperforms the experimental results of Transformer under the premise of\nreducing the network training burden.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation",
    "url": "http://arxiv.org/abs/2509.00598v1",
    "authors": [
      "Boyi Li",
      "Ce Zhang",
      "Richard M. Timmerman",
      "Wenxuan Bao"
    ],
    "published": "2025-08-30",
    "abstract": "The emergence of vision language models (VLMs) has bridged vision and\nlanguage, enabling joint multimodal understanding beyond traditional\nvisual-only deep learning models. However, transferring VLMs from the natural\nimage domain to remote sensing (RS) segmentation remains challenging due to the\nlimited category diversity in RS datasets and the domain gap between natural\nand RS imagery. Here, we propose a training-free framework, DGL-RSIS, that\ndecouples visual and textual inputs, performing visual-language alignment at\nboth the local semantic and global contextual levels through tailored\nstrategies. Specifically, we first introduce a global-local decoupling (GLD)\nmodule, where text inputs are divided into local class nouns and global\nmodifiers using natural language processing (NLP) techniques; image inputs are\npartitioned into a set of class-agnostic mask proposals via unsupervised mask\nproposal networks. Second, visual and textual features are aligned at local\nscale, through a novel context-aware cropping strategy for extracting image\npatches with proper boundaries and introducing RS-specific knowledge to enrich\nthe text inputs. By matching the enhanced text features with mask-guided visual\nfeatures, we enable the mask classification, supporting open-vocabulary\nsemantic segmentation (OVSS). Third, at the global scale, we propose a\nCross-Scale Grad-CAM module to refine Grad-CAM maps using contextual\ninformation from global modifiers. A subsequent mask selection module\nintegrates pixel-level Grad-CAM activations into the mask-level segmentation\noutput, such that accurate and interpretable alignment can be realized across\nglobal and local dimensions for referring expression segmentation (RES).",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "DAOVI: Distortion-Aware Omnidirectional Video Inpainting",
    "url": "http://arxiv.org/abs/2509.00396v1",
    "authors": [
      "Ryosuke Seshimo",
      "Mariko Isogawa"
    ],
    "published": "2025-08-30",
    "abstract": "Omnidirectional videos that capture the entire surroundings are employed in a\nvariety of fields such as VR applications and remote sensing. However, their\nwide field of view often causes unwanted objects to appear in the videos. This\nproblem can be addressed by video inpainting, which enables the natural removal\nof such objects while preserving both spatial and temporal consistency.\nNevertheless, most existing methods assume processing ordinary videos with a\nnarrow field of view and do not tackle the distortion in equirectangular\nprojection of omnidirectional videos. To address this issue, this paper\nproposes a novel deep learning model for omnidirectional video inpainting,\ncalled Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI\nintroduces a module that evaluates temporal motion information in the image\nspace considering geodesic distance, as well as a depth-aware feature\npropagation module in the feature space that is designed to address the\ngeometric distortion inherent to omnidirectional videos. The experimental\nresults demonstrate that our proposed method outperforms existing methods both\nquantitatively and qualitatively.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena",
    "url": "http://arxiv.org/abs/2509.03500v1",
    "authors": [
      "Itai Zilberstein",
      "Alberto Candela",
      "Steve Chien"
    ],
    "published": "2025-09-03",
    "abstract": "Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Invariant Features for Global Crop Type Classification",
    "url": "http://arxiv.org/abs/2509.03497v2",
    "authors": [
      "Xin-Yi Tong",
      "Sherrie Wang"
    ],
    "published": "2025-09-03",
    "abstract": "Accurately obtaining crop type and its spatial distribution at a global scale\nis critical for food security, agricultural policy-making, and sustainable\ndevelopment. Remote sensing offers an efficient solution for large-scale crop\nclassification, but the limited availability of reliable ground samples in many\nregions constrains applicability across geographic areas. To address\nperformance declines under geospatial shifts, this study identifies remote\nsensing features that are invariant to geographic variation and proposes\nstrategies to enhance cross-regional generalization. We construct CropGlobe, a\nglobal crop type dataset with 300,000 pixel-level samples from eight countries\nacross five continents, covering six major food and industrial crops (corn,\nsoybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage,\nCropGlobe enables a systematic evaluation under cross-country, cross-continent,\nand cross-hemisphere transfer. We compare the transferability of temporal\nmulti-spectral features (Sentinel-2-based 1D/2D median features and harmonic\ncoefficients) and hyperspectral features (from EMIT). To improve generalization\nunder spectral and phenological shifts, we design CropNet, a lightweight and\nrobust CNN tailored for pixel-level crop classification, coupled with temporal\ndata augmentation (time shift, time scale, and magnitude warping) that\nsimulates realistic cross-regional phenology. Experiments show that 2D median\ntemporal features from Sentinel-2 consistently exhibit the strongest invariance\nacross all transfer scenarios, and augmentation further improves robustness,\nparticularly when training data diversity is limited. Overall, the work\nidentifies more invariant feature representations that enhance geographic\ntransferability and suggests a promising path toward scalable, low-cost crop\ntype applications across globally diverse regions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "From Image Denoisers to Regularizing Imaging Inverse Problems: An Overview",
    "url": "http://arxiv.org/abs/2509.03475v1",
    "authors": [
      "Hong Ye Tan",
      "Subhadip Mukherjee",
      "Junqi Tang"
    ],
    "published": "2025-09-03",
    "abstract": "Inverse problems lie at the heart of modern imaging science, with broad\napplications in areas such as medical imaging, remote sensing, and microscopy.\nRecent years have witnessed a paradigm shift in solving imaging inverse\nproblems, where data-driven regularizers are used increasingly, leading to\nremarkably high-fidelity reconstruction. A particularly notable approach for\ndata-driven regularization is to use learned image denoisers as implicit priors\nin iterative image reconstruction algorithms. This survey presents a\ncomprehensive overview of this powerful and emerging class of algorithms,\ncommonly referred to as plug-and-play (PnP) methods. We begin by providing a\nbrief background on image denoising and inverse problems, followed by a short\nreview of traditional regularization strategies. We then explore how proximal\nsplitting algorithms, such as the alternating direction method of multipliers\n(ADMM) and proximal gradient descent (PGD), can naturally accommodate learned\ndenoisers in place of proximal operators, and under what conditions such\nreplacements preserve convergence. The role of Tweedie's formula in connecting\noptimal Gaussian denoisers and score estimation is discussed, which lays the\nfoundation for regularization-by-denoising (RED) and more recent\ndiffusion-based posterior sampling methods. We discuss theoretical advances\nregarding the convergence of PnP algorithms, both within the RED and proximal\nsettings, emphasizing the structural assumptions that the denoiser must satisfy\nfor convergence, such as non-expansiveness, Lipschitz continuity, and local\nhomogeneity. We also address practical considerations in algorithm design,\nincluding choices of denoiser architecture and acceleration strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LUCIE-3D: A three-dimensional climate emulator for forced responses",
    "url": "http://arxiv.org/abs/2509.02061v1",
    "authors": [
      "Haiwen Guan",
      "Troy Arcomano",
      "Ashesh Chattopadhyay",
      "Romit Maulik"
    ],
    "published": "2025-09-02",
    "abstract": "We introduce LUCIE-3D, a lightweight three-dimensional climate emulator\ndesigned to capture the vertical structure of the atmosphere, respond to\nclimate change forcings, and maintain computational efficiency with long-term\nstability. Building on the original LUCIE-2D framework, LUCIE-3D employs a\nSpherical Fourier Neural Operator (SFNO) backbone and is trained on 30 years of\nERA5 reanalysis data spanning eight vertical {\\sigma}-levels. The model\nincorporates atmospheric CO2 as a forcing variable and optionally integrates\nprescribed sea surface temperature (SST) to simulate coupled ocean--atmosphere\ndynamics. Results demonstrate that LUCIE-3D successfully reproduces\nclimatological means, variability, and long-term climate change signals,\nincluding surface warming and stratospheric cooling under increasing CO2\nconcentrations. The model further captures key dynamical processes such as\nequatorial Kelvin waves, the Madden--Julian Oscillation, and annular modes,\nwhile showing credible behavior in the statistics of extreme events. Despite\nrequiring longer training than its 2D predecessor, LUCIE-3D remains efficient,\ntraining in under five hours on four GPUs. Its combination of stability,\nphysical consistency, and accessibility makes it a valuable tool for rapid\nexperimentation, ablation studies, and the exploration of coupled climate\ndynamics, with potential applications extending to paleoclimate research and\nfuture Earth system emulation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "An Observations-focused Assessment of Global AI Weather Prediction Models During the South Asian Monsoon",
    "url": "http://arxiv.org/abs/2509.01879v1",
    "authors": [
      "Aman Gupta",
      "Aditi Sheshadri",
      "Dhruv Suri"
    ],
    "published": "2025-09-02",
    "abstract": "Seven state-of-the-art AI weather models (FourCastNet, FourCastNet-SFNO,\nPangu-Weather, GraphCast, Aurora, AIFS, and GenCast) are evaluated against\nobservational data during the South Asian Monsoon. The models are tested on\ntemperature, winds, global kinetic energy spectrum, regional precipitation,\ncloud cover, cyclone trajectory prediction, and hyperlocal predictions around\nextreme weather events. The models forecast large-scale dynamics with\nreasonable accuracy, but fall short on key metrics critical to Monsoon-time\nweather prediction. The models exhibit substantially higher errors when\ncompared against ground-based weather station data than against reanalysis or\nconventional forecasts. The AI weather prediction models show key differences\nin mesoscale kinetic energy and extreme precipitation during the Monsoon, and\npredict markedly different Monsoon-time cyclone trajectories over the Indian\nsubcontinent, raising questions about their readiness for operational\napplications. Our analysis finds that ECMWF's deterministic AIFS model offers\nthe most reliable performance and usability, with GraphCast and GenCast being\nclose seconds.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AT Loss: Advanced Torrential Loss Function for Precipitation Forecasting",
    "url": "http://arxiv.org/abs/2509.01348v1",
    "authors": [
      "Jaeho Choi",
      "Hyeri Kim",
      "Kwang-Ho Kim",
      "Jaesung Lee"
    ],
    "published": "2025-09-01",
    "abstract": "Accurate precipitation forecasting is becoming increasingly important in the\ncontext of climate change. In response, machine learning-based approaches have\nrecently gained attention as an emerging alternative to traditional methods\nsuch as numerical weather prediction and climate models. Nonetheless, many\nrecent approaches still rely on off-the-shelf loss functions, and even the more\nadvanced ones merely involve optimization processes based on the critical\nsuccess index (CSI). The problem, however, is that CSI may become ineffective\nduring extended dry periods when precipitation remains below the threshold,\nrendering it less than ideal as a criterion for optimization. To address this\nlimitation, we introduce a simple penalty expression and reinterpret it as a\nquadratic unconstrained binary optimization (QUBO) formulation. Ultimately, the\nresulting QUBO formulation is relaxed into a differentiable advanced torrential\n(AT) loss function through an approximation process. The proposed AT loss\ndemonstrates its superiority through the Lipschitz constant, forecast\nperformance evaluations, consistency experiments, and ablation studies with the\noperational model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Imputing Missing Long-Term Spatiotemporal Multivariate Atmospheric Data with CNN-Transformer Machine Learning",
    "url": "http://arxiv.org/abs/2509.01141v1",
    "authors": [
      "Jiahui Hu",
      "Wenjun Dong",
      "Alan Z. Liu"
    ],
    "published": "2025-09-01",
    "abstract": "Continuous physical domains are important for scientific investigations of\ndynamical processes in the atmosphere. However, missing data arising from\noperational constraints and adverse environmental conditions pose significant\nchallenges to accurate analysis and modeling. To address this limitation, we\npropose a novel hybrid Convolutional Neural Network (CNN) Transformer machine\nlearning model for multivariable atmospheric data imputation, termed CT-MVP.\nThis framework integrates CNNs for local feature extraction with transformers\nfor capturing long-range dependencies across time and altitude. The model is\ntrained and evaluated on a testbed using the Specified Dynamics Whole\nAtmosphere Community Climate Model with thermosphere and ionosphere extension\n(SD-WACCM-X) dataset spanning 13 years, which provides continuous global\ncoverage of atmospheric variables, including temperature and zonal and\nmeridional winds. This setup ensures that the ML approach can be rigorously\nassessed under diverse data-gap conditions. The hybrid framework enables\neffective reconstruction of missing values in high-dimensional atmospheric\ndatasets, with comparative evaluations against traditional methods and a simple\ntransformer. The results demonstrate that CT-MVP achieves superior performance\ncompared with traditional approaches, particularly in cases involving extended\nperiods of missing data, and slightly outperforms a simple transformer with the\nsame hyper-parameters.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef",
    "url": "http://arxiv.org/abs/2509.01019v1",
    "authors": [
      "Scarlett Raine",
      "Benjamin Moshirian",
      "Tobias Fischer"
    ],
    "published": "2025-08-31",
    "abstract": "Coral reefs are on the brink of collapse, with climate change, ocean\nacidification, and pollution leading to a projected 70-90% loss of coral\nspecies within the next decade. Restoration efforts are crucial, but their\nsuccess hinges on introducing automation to upscale efforts. We present\nautomated deployment of coral re-seeding devices powered by artificial\nintelligence, computer vision, and robotics. Specifically, we perform automated\nsubstrate classification, enabling detection of areas of the seafloor suitable\nfor coral growth, thus significantly reducing reliance on human experts and\nincreasing the range and efficiency of restoration. Real-world testing of the\nalgorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,\nsub-image patch classification of 89.1%, and real-time model inference at 5.5\nframes per second. Further, we present and publicly contribute a large\ncollection of annotated substrate image data to foster future research in this\narea.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India",
    "url": "http://arxiv.org/abs/2509.00653v1",
    "authors": [
      "Tung Nguyen",
      "Harkanwar Singh",
      "Nilay Naharas",
      "Lucas Bandarkar",
      "Aditya Grover"
    ],
    "published": "2025-08-31",
    "abstract": "Regional weather forecasting is a critical problem for localized climate\nadaptation, disaster mitigation, and sustainable development. While machine\nlearning has shown impressive progress in global weather forecasting, regional\nforecasting remains comparatively underexplored. Existing efforts often use\ndifferent datasets and experimental setups, limiting fair comparison and\nreproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for\ndata-driven regional weather forecasting focused on the Indian subcontinent.\nIndiaWeatherBench provides a curated dataset built from high-resolution\nregional reanalysis products, along with a suite of deterministic and\nprobabilistic metrics to facilitate consistent training and evaluation. To\nestablish strong baselines, we implement and evaluate a range of models across\ndiverse architectures, including UNets, Transformers, and Graph-based networks,\nas well as different boundary conditioning strategies and training objectives.\nWhile focused on India, IndiaWeatherBench is easily extensible to other\ngeographic regions. We open-source all raw and preprocessed datasets, model\nimplementations, and evaluation pipelines to promote accessibility and future\ndevelopment. We hope IndiaWeatherBench will serve as a foundation for advancing\nregional weather forecasting research. Code is available at\nhttps://github.com/tung-nd/IndiaWeatherBench.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers",
    "url": "http://arxiv.org/abs/2509.00631v1",
    "authors": [
      "Giacomo Acciarini",
      "Simone Mestici",
      "Halil Kelebek",
      "Linnea Wolniewicz",
      "Michael Vergalla",
      "Madhulika Guhathakurta",
      "Umaa Rebbapragada",
      "Bala Poduval",
      "At\u0131l\u0131m G\u00fcne\u015f Baydin",
      "Frank Soboczenski"
    ],
    "published": "2025-08-30",
    "abstract": "The ionosphere critically influences Global Navigation Satellite Systems\n(GNSS), satellite communications, and Low Earth Orbit (LEO) operations, yet\naccurate prediction of its variability remains challenging due to nonlinear\ncouplings between solar, geomagnetic, and thermospheric drivers. Total Electron\nContent (TEC), a key ionospheric parameter, is derived from GNSS observations,\nbut its reliable forecasting is limited by the sparse nature of global\nmeasurements and the limited accuracy of empirical models, especially during\nstrong space weather conditions. In this work, we present a machine learning\nframework for ionospheric TEC forecasting that leverages Temporal Fusion\nTransformers (TFT) to predict sparse ionosphere data. Our approach accommodates\nheterogeneous input sources, including solar irradiance, geomagnetic indices,\nand GNSS-derived vertical TEC, and applies preprocessing and temporal alignment\nstrategies. Experiments spanning 2010-2025 demonstrate that the model achieves\nrobust predictions up to 24 hours ahead, with root mean square errors as low as\n3.33 TECU. Results highlight that solar EUV irradiance provides the strongest\npredictive signals. Beyond forecasting accuracy, the framework offers\ninterpretability through attention-based analysis, supporting both operational\napplications and scientific discovery. To encourage reproducibility and\ncommunity-driven development, we release the full implementation as the\nopen-source toolkit \\texttt{ionopy}.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2509.09572v1",
    "authors": [
      "Sijun Dong",
      "Yuxuan Hu",
      "LiBo Wang",
      "Geng Chen",
      "Xiaoliang Meng"
    ],
    "published": "2025-09-11",
    "abstract": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples,\nand the difficulty of cross-domain generalization in multi-temporal and\nmulti-source remote sensing imagery, we propose PeftCD, a change detection\nframework built upon Vision Foundation Models (VFMs) with Parameter-Efficient\nFine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese\nencoder derived from a VFM, into which LoRA and Adapter modules are seamlessly\nintegrated. This design enables highly efficient task adaptation by training\nonly a minimal set of additional parameters. To fully unlock the potential of\nVFMs, we investigate two leading backbones: the Segment Anything Model v2\n(SAM2), renowned for its strong segmentation priors, and DINOv3, a\nstate-of-the-art self-supervised representation learner. The framework is\ncomplemented by a deliberately lightweight decoder, ensuring the focus remains\non the powerful feature representations from the backbones. Extensive\nexperiments demonstrate that PeftCD achieves state-of-the-art performance\nacross multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD\n(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and\nLEVIR-CD (85.62%), with notably precise boundary delineation and strong\nsuppression of pseudo-changes. In summary, PeftCD presents an optimal balance\nof accuracy, efficiency, and generalization. It offers a powerful and scalable\nparadigm for adapting large-scale VFMs to real-world remote sensing change\ndetection applications. The code and pretrained models will be released at\nhttps://github.com/dyzy41/PeftCD.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia",
    "url": "http://arxiv.org/abs/2509.08303v1",
    "authors": [
      "M. Warizmi Wafiq",
      "Peter Cutter",
      "Ate Poortinga",
      "Daniel Marc G. dela Torre",
      "Karis Tenneson",
      "Vanna Teck",
      "Enikoe Bihari",
      "Chanarun Saisaward",
      "Weraphong Suaruang",
      "Andrea McMahon",
      "Andi Vika Faradiba Muin",
      "Karno B. Batiran",
      "Chairil A",
      "Nurul Qomar",
      "Arya Arismaya Metananda",
      "David Ganz",
      "David Saah"
    ],
    "published": "2025-09-10",
    "abstract": "Oil palm cultivation remains one of the leading causes of deforestation in\nIndonesia. To better track and address this challenge, detailed and reliable\nmapping is needed to support sustainability efforts and emerging regulatory\nframeworks. We present an open-access geospatial dataset of oil palm\nplantations and related land cover types in Indonesia, produced through expert\nlabeling of high-resolution satellite imagery from 2020 to 2024. The dataset\nprovides polygon-based, wall-to-wall annotations across a range of\nagro-ecological zones and includes a hierarchical typology that distinguishes\noil palm planting stages as well as similar perennial crops. Quality was\nensured through multi-interpreter consensus and field validation. The dataset\nwas created using wall-to-wall digitization over large grids, making it\nsuitable for training and benchmarking both conventional convolutional neural\nnetworks and newer geospatial foundation models. Released under a CC-BY\nlicense, it fills a key gap in training data for remote sensing and aims to\nimprove the accuracy of land cover types mapping. By supporting transparent\nmonitoring of oil palm expansion, the resource contributes to global\ndeforestation reduction goals and follows FAIR data principles.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery",
    "url": "http://arxiv.org/abs/2509.08949v1",
    "authors": [
      "Yibin Wang",
      "Wondimagegn Beshah",
      "Padmanava Dash",
      "Haifeng Wang"
    ],
    "published": "2025-09-10",
    "abstract": "The use of unmanned aerial systems (UASs) has increased tremendously in the\ncurrent decade. They have significantly advanced remote sensing with the\ncapability to deploy and image the terrain as per required spatial, spectral,\ntemporal, and radiometric resolutions for various remote sensing applications.\nOne of the major advantages of UAS imagery is that images can be acquired in\ncloudy conditions by flying the UAS under the clouds. The limitation to the\ntechnology is that the imagery is often sullied by cloud shadows. Images taken\nover water are additionally affected by sun glint. These are two pose serious\nissues for estimating water quality parameters from the UAS images. This study\nproposes a novel machine learning approach first to identify and extract\nregions with cloud shadows and sun glint and separate such regions from\nnon-obstructed clear sky regions and sun-glint unaffected regions. The data was\nextracted from the images at pixel level to train an U-Net based deep learning\nmodel and best settings for model training was identified based on the various\nevaluation metrics from test cases. Using this evaluation, a high-quality image\ncorrection model was determined, which was used to recover the cloud shadow and\nsun glint areas in the images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping",
    "url": "http://arxiv.org/abs/2509.09408v1",
    "authors": [
      "Jonas Schmidinger",
      "Viacheslav Barkov",
      "Sebastian Vogel",
      "Martin Atzmueller",
      "Gerard B M Heuvelink"
    ],
    "published": "2025-09-11",
    "abstract": "Machine learning and geostatistics are two fundamentally different frameworks\nfor predicting and spatially mapping soil properties. Geostatistics leverages\nthe spatial structure of soil properties, while machine learning captures the\nrelationship between available environmental features and soil properties. We\npropose a hybrid framework that enriches ML with spatial context through\nengineering of 'spatial lag' features from ordinary kriging. We call this\napproach 'kriging prior regression' (KpR), as it follows the inverse logic of\nregression kriging. To evaluate this approach, we assessed both the point and\nprobabilistic prediction performance of KpR, using the TabPFN model across six\nfieldscale datasets from LimeSoDa. These datasets included soil organic carbon,\nclay content, and pH, along with features derived from remote sensing and\nin-situ proximal soil sensing. KpR with TabPFN demonstrated reliable\nuncertainty estimates and more accurate predictions in comparison to several\nother spatial techniques (e.g., regression/residual kriging with TabPFN), as\nwell as to established non-spatial machine learning algorithms (e.g., random\nforest). Most notably, it significantly improved the average R2 by around 30%\ncompared to machine learning algorithms without spatial context. This\nimprovement was due to the strong prediction performance of the TabPFN\nalgorithm itself and the complementary spatial information provided by KpR\nfeatures. TabPFN is particularly effective for prediction tasks with small\nsample sizes, common in precision agriculture, whereas KpR can compensate for\nweak relationships between sensing features and soil properties when proximal\nsoil sensing data are limited. Hence, we conclude that KpR with TabPFN is a\nvery robust and versatile modelling framework for digital soil mapping in\nprecision agriculture.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "A Benchmark Dataset for Satellite-Based Estimation and Detection of Rain",
    "url": "http://arxiv.org/abs/2509.08816v2",
    "authors": [
      "Simon Pfreundschuh",
      "Malarvizhi Arulraj",
      "Ali Behrangi",
      "Linda Bogerd",
      "Alan James Peixoto Calheiros",
      "Daniele Casella",
      "Neda Dolatabadi",
      "Clement Guilloteau",
      "Jie Gong",
      "Christian D. Kummerow",
      "Pierre Kirstetter",
      "Gyuwon Lee",
      "Maximilian Maahn",
      "Lisa Milani",
      "Giulia Panegrossi",
      "Rayana Palharini",
      "Veljko Petkovi\u0107",
      "Soorok Ryu",
      "Paolo San\u00f2",
      "Jackson Tan"
    ],
    "published": "2025-09-10",
    "abstract": "Accurately tracking the global distribution and evolution of precipitation is\nessential for both research and operational meteorology. Satellite observations\nremain the only means of achieving consistent, global-scale precipitation\nmonitoring. While machine learning has long been applied to satellite-based\nprecipitation retrieval, the absence of a standardized benchmark dataset has\nhindered fair comparisons between methods and limited progress in algorithm\ndevelopment.\n  To address this gap, the International Precipitation Working Group has\ndeveloped SatRain, the first AI-ready benchmark dataset for satellite-based\ndetection and estimation of rain, snow, graupel, and hail. SatRain includes\nmulti-sensor satellite observations representative of the major platforms\ncurrently used in precipitation remote sensing, paired with high-quality\nreference estimates from ground-based radars corrected using rain gauge\nmeasurements. It offers a standardized evaluation protocol to enable robust and\nreproducible comparisons across machine learning approaches.\n  In addition to supporting algorithm evaluation, the diversity of sensors and\ninclusion of time-resolved geostationary observations make SatRain a valuable\nfoundation for developing next-generation AI models to deliver more accurate,\ndetailed, and globally consistent precipitation estimates.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Self-supervised Learning for Hyperspectral Images of Trees",
    "url": "http://arxiv.org/abs/2509.05630v1",
    "authors": [
      "Moqsadur Rahman",
      "Saurav Kumar",
      "Santosh S. Palmate",
      "M. Shahriar Hossain"
    ],
    "published": "2025-09-06",
    "abstract": "Aerial remote sensing using multispectral and RGB imagers has provided a\ncritical impetus to precision agriculture. Analysis of the hyperspectral images\nwith limited or no labels is challenging. This paper focuses on self-supervised\nlearning to create neural network embeddings reflecting vegetation properties\nof trees from aerial hyperspectral images of crop fields. Experimental results\ndemonstrate that a constructed tree representation, using a vegetation\nproperty-related embedding space, performs better in downstream machine\nlearning tasks compared to the direct use of hyperspectral vegetation\nproperties as tree representations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics",
    "url": "http://arxiv.org/abs/2509.09599v1",
    "authors": [
      "Ira J. S. Shokar",
      "Rich R. Kerswell",
      "Peter H. Haynes"
    ],
    "published": "2025-09-11",
    "abstract": "We present a deep learning emulator for stochastic and chaotic\nspatio-temporal systems, explicitly conditioned on the parameter values of the\nunderlying partial differential equations (PDEs). Our approach involves\npre-training the model on a single parameter domain, followed by fine-tuning on\na smaller, yet diverse dataset, enabling generalisation across a broad range of\nparameter values. By incorporating local attention mechanisms, the network is\ncapable of handling varying domain sizes and resolutions. This enables\ncomputationally efficient pre-training on smaller domains while requiring only\na small additional dataset to learn how to generalise to larger domain sizes.\nWe demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky\nequation and stochastically-forced beta-plane turbulence, showcasing its\nability to capture phenomena at interpolated parameter values. The emulator\nprovides significant computational speed-ups over conventional numerical\nintegration, facilitating efficient exploration of parameter space, while a\nprobabilistic variant of the emulator provides uncertainty quantification,\nallowing for the statistical study of rare events.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction",
    "url": "http://arxiv.org/abs/2509.09128v1",
    "authors": [
      "Emam Hossain",
      "Md Osman Gani"
    ],
    "published": "2025-09-11",
    "abstract": "Conventional machine learning and deep learning models typically rely on\ncorrelation-based learning, which often fails to distinguish genuine causal\nrelationships from spurious associations, limiting their robustness,\ninterpretability, and ability to generalize. To overcome these limitations, we\nintroduce a causality-aware deep learning framework that integrates\nMultivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection\nwithin a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic\nSea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily\nand monthly resolutions, the proposed method identifies causally influential\npredictors, prioritizes direct causes of SIE dynamics, reduces unnecessary\nfeatures, and enhances computational efficiency. Experimental results show that\nincorporating causal inputs leads to improved prediction accuracy and\ninterpretability across varying lead times. While demonstrated on Arctic SIE\nforecasting, the framework is broadly applicable to other dynamic,\nhigh-dimensional domains, offering a scalable approach that advances both the\ntheoretical foundations and practical performance of causality-informed\npredictive modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon",
    "url": "http://arxiv.org/abs/2509.06227v1",
    "authors": [
      "Saghar Ganji",
      "Mohammad Naisipour",
      "Alireza Hassani",
      "Arash Adib"
    ],
    "published": "2025-09-07",
    "abstract": "The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO)\nis still one of the biggest challenges in climate science. While it is true\nthat short-to medium-range performance has been improved significantly using\nthe advances in deep learning, statistical dynamical hybrids, most operational\nsystems still use the simple mean of all ensemble members, implicitly assuming\nequal skill across members. In this study, we demonstrate, through a strictly\na-posteriori evaluation , for any large enough ensemble of ENSO forecasts,\nthere is a subset of members whose skill is substantially higher than that of\nthe ensemble mean. Using a state-of-the-art ENSO forecast system\ncross-validated against the 1986-2017 observed Nino3.4 index, we identify two\nTop-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on\nhighest Pearson correlation. Generally across all leads, these outstanding\nmembers show higher correlation and lower RMSE, with the advantage rising\nenormously with lead time. Whereas at short leads (1 month) raises the mean\ncorrelation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\\deg}C\nor by 23.3% compared to the All-40 mean, at extreme leads (23 months) the\ncorrelation is raised by +0.43 (+172%) and RMSE by 0.18 {\\deg}C or by 22.5%\ndecrease. The enhancements are largest during crucial ENSO transition periods\nsuch as SON and DJF, when accurate amplitude and phase forecasting is of\ngreatest socio-economic benefit, and furthermore season-dependent e.g.,\nmid-year months such as JJA and MJJ have incredibly large RMSE reductions. This\nstudy provides a solid foundation for further investigations to identify\nreliable clues for detecting high-quality ensemble members, thereby enhancing\nforecasting skill.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MoWE : A Mixture of Weather Experts",
    "url": "http://arxiv.org/abs/2509.09052v1",
    "authors": [
      "Dibyajyoti Chakraborty",
      "Romit Maulik",
      "Peter Harrington",
      "Dallas Foster",
      "Mohammad Amin Nabian",
      "Sanjay Choudhry"
    ],
    "published": "2025-09-10",
    "abstract": "Data-driven weather models have recently achieved state-of-the-art\nperformance, yet progress has plateaued in recent years. This paper introduces\na Mixture of Experts (MoWE) approach as a novel paradigm to overcome these\nlimitations, not by creating a new forecaster, but by optimally combining the\noutputs of existing models. The MoWE model is trained with significantly lower\ncomputational resources than the individual experts. Our model employs a Vision\nTransformer-based gating network that dynamically learns to weight the\ncontributions of multiple \"expert\" models at each grid point, conditioned on\nforecast lead time. This approach creates a synthesized deterministic forecast\nthat is more accurate than any individual component in terms of Root Mean\nSquared Error (RMSE). Our results demonstrate the effectiveness of this method,\nachieving up to a 10% lower RMSE than the best-performing AI weather model on a\n2-day forecast horizon, significantly outperforming individual experts as well\nas a simple average across experts. This work presents a computationally\nefficient and scalable strategy to push the state of the art in data-driven\nweather prediction by making the most out of leading high-quality forecast\nmodels.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Using machine learning to downscale coarse-resolution environmental variables for understanding the spatial frequency of convective storms",
    "url": "http://arxiv.org/abs/2509.08802v1",
    "authors": [
      "Hungjui Yu",
      "Lander Ver Hoef",
      "Kristen L. Rasmussen",
      "Imme Ebert-Uphoff"
    ],
    "published": "2025-09-10",
    "abstract": "Global climate models (GCMs), typically run at ~100-km resolution, capture\nlarge-scale environmental conditions but cannot resolve convection and cloud\nprocesses at kilometer scales. Convection-permitting models offer\nhigher-resolution simulations that explicitly simulate convection but are\ncomputationally expensive and impractical for large ensemble runs. This study\nexplores machine learning (ML) as a bridge between these approaches. We train\nsimple, pixel-based neural networks to predict convective storm frequency from\nenvironmental variables produced by a regional convection-permitting model. The\nML models achieve promising results, with structural similarity index measure\n(SSIM) values exceeding 0.8, capturing the diurnal cycle and orographic\nconvection without explicit temporal or spatial coordinates as input. Model\nperformance declines when fewer input features are used or specific regions are\nexcluded, underscoring the role of diverse physical mechanisms in convective\nactivity. These findings highlight ML potential as a computationally efficient\ntool for representing convection and as a means of scientific discovery,\noffering insights into convective processes. Unlike convolutional neural\nnetworks, which depend on spatial structure and grid size, the pixel-based\nmodel treats each grid point independently, enabling value-to-value prediction\nwithout spatial context. This design enhances adaptability to resolution\nchanges and supports generalization to unseen environmental regimes, making it\nparticularly suited for linking environmental conditions to convective features\nand for application across diverse model grids or climate scenarios.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Seasonal forecasting using the GenCast probabilistic machine learning model",
    "url": "http://arxiv.org/abs/2509.06457v1",
    "authors": [
      "Bobby Antonio",
      "Kristian Strommen",
      "Hannah M. Christensen"
    ],
    "published": "2025-09-08",
    "abstract": "Machine-learnt weather prediction (MLWP) models are now well established as\nbeing competitive with conventional numerical weather prediction (NWP) models\nin the medium range. However, there is still much uncertainty as to how this\nperformance extends to longer timescales, where interactions with slower\ncomponents of the earth system become important. We take GenCast, a\nstate-of-the-art probabilistic MLWP model, and apply it to the task of seasonal\nforecasting with prescribed sea surface temperature (SST), by providing\nanomalies persisted over climatology (GenCast-Persisted) or forcing with\nobservations (GenCast-Forced). The forecasts are compared to the European\nCentre for Medium-Range Weather Forecasts seasonal forecasting system, SEAS5.\nOur results indicate that, despite being trained at short timescales,\nGenCast-Persisted produces much of the correct precipitation patterns in\nresponse to El Ni\\~{n}o and La Ni\\~{n}a events, with several erroneous patterns\nin GenCast-Persisted corrected with GenCast-Forced. The uncertainty in\nprecipitation response, as represented by the ensemble, compares favourably to\nSEAS5. Whilst SEAS5 achieves superior skill in the tropics for 2-metre\ntemperature and mean sea level pressure (MSLP), GenCast-Persisted achieves\nsignificantly higher skill in some areas in higher latitudes, including\nmountainous areas, with notable improvements for MSLP in particular; this is\nreflected in a higher correlation with the observed NAO index. Reliability\ndiagrams indicate that GenCast-Persisted is overconfident compared to SEAS5,\nwhilst GenCast-Forced produces well-calibrated seasonal 2-metre temperature\npredictions. These results provide an indication of the potential of MLWP\nmodels similar to GenCast for the `full' seasonal forecasting problem, where\nthe atmospheric model is coupled to ocean, land and cryosphere models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent and Independent Data under Preferential Sampling",
    "url": "http://arxiv.org/abs/2509.09336v1",
    "authors": [
      "Daniela Silva",
      "Raquel Menezes",
      "Gon\u00e7alo Ara\u00fajo",
      "Ana Machado",
      "Renato Rosa",
      "Ana Moreno",
      "Alexandra Silva",
      "Susana Garrido"
    ],
    "published": "2025-09-11",
    "abstract": "Sustainable management of marine ecosystems is vital for maintaining healthy\nfishery resources, and benefits from advanced scientific tools to accurately\nassess species distribution patterns. In fisheries science, two primary data\nsources are used: fishery-independent data (FID), collected through systematic\nsurveys, and fishery-dependent data (FDD), obtained from commercial fishing\nactivities. While these sources provide complementary information, their\ndistinct sampling schemes - systematic for FID and preferential for FDD - pose\nsignificant integration challenges. This study introduces a novel\nspatio-temporal model that integrates FID and FDD, addressing challenges\nassociated with zero-inflation and preferential sampling (PS) common in\necological data. The model employs a six-layer structure to differentiate\nbetween presence-absence and biomass observations, offering a robust framework\nfor ecological studies affected by PS biases. Simulation results demonstrate\nthe model's accuracy in parameter estimation across diverse PS scenarios and\nits ability to detect preferential signals. Application to the study of the\ndistribution patterns of the European sardine populations along the southern\nPortuguese continental shelf illustrates the model's effectiveness in\nintegrating diverse data sources and incorporating environmental and\nvessel-specific covariates. The model reveals spatio-temporal variability in\nsardine presence and biomass, providing actionable insights for fisheries\nmanagement. Beyond ecology, this framework offers broad applicability to data\nintegration challenges in other disciplines.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts",
    "url": "http://arxiv.org/abs/2509.14104v1",
    "authors": [
      "Leonard Hackel",
      "Tom Burgert",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-09-17",
    "abstract": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation",
    "url": "http://arxiv.org/abs/2509.13229v1",
    "authors": [
      "Hugo Carlesso",
      "Josiane Mothe",
      "Radu Tudor Ionescu"
    ],
    "published": "2025-09-16",
    "abstract": "Hyperspectral imaging (HSI) captures detailed spectral signatures across\nhundreds of contiguous bands per pixel, being indispensable for remote sensing\napplications such as land-cover classification, change detection, and\nenvironmental monitoring. Due to the high dimensionality of HSI data and the\nslow rate of data transfer in satellite-based systems, compact and efficient\nmodels are required to support onboard processing and minimize the transmission\nof redundant or low-value data, e.g. cloud-covered areas. To this end, we\nintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)\nframework designed for lightweight architectures for HSI analysis. CMTSSL\nintegrates masked image modeling with decoupled spatial and spectral jigsaw\npuzzle solving, guided by a curriculum learning strategy that progressively\nincreases data complexity during self-supervision. This enables the encoder to\njointly capture fine-grained spectral continuity, spatial structure, and global\nsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously\naddresses spatial and spectral reasoning within a unified and computationally\nefficient design, being particularly suitable for training lightweight models\nfor onboard satellite deployment. We validate our approach on four public\nbenchmark datasets, demonstrating consistent gains in downstream segmentation\ntasks, using architectures that are over 16,000x lighter than some\nstate-of-the-art models. These results highlight the potential of CMTSSL in\ngeneralizable representation learning with lightweight architectures for\nreal-world HSI applications. Our code is publicly available at\nhttps://github.com/hugocarlesso/CMTSSL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji",
    "url": "http://arxiv.org/abs/2509.13388v1",
    "authors": [
      "Yadvendra Gurjar",
      "Ruoni Wan",
      "Ehsan Farahbakhsh",
      "Rohitash Chandra"
    ],
    "published": "2025-09-16",
    "abstract": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Vessel Detection and Localization Using Distributed Acoustic Sensing in Submarine Optical Fiber Cables",
    "url": "http://arxiv.org/abs/2509.11614v2",
    "authors": [
      "Erick Eduardo Ramirez-Torres",
      "Javier Macias-Guarasa",
      "Daniel Pizarro-Perez",
      "Javier Tejedor",
      "Sira Elena Palazuelos-Cagigas",
      "Pedro J. Vidal-Moreno",
      "Sonia Martin-Lopez",
      "Miguel Gonzalez-Herraez",
      "Roel Vanthillo"
    ],
    "published": "2025-09-15",
    "abstract": "Submarine cables play a critical role in global internet connectivity, energy\ntransmission, and communication but remain vulnerable to accidental damage and\nsabotage. Recent incidents in the Baltic Sea highlighted the need for enhanced\nmonitoring to protect this vital infrastructure. Traditional vessel detection\nmethods, such as synthetic aperture radar, video surveillance, and\nmultispectral satellite imagery, face limitations in real-time processing,\nadverse weather conditions, and coverage range. This paper explores Distributed\nAcoustic Sensing (DAS) as an alternative by repurposing submarine\ntelecommunication cables as large-scale acoustic sensor arrays. DAS offers\ncontinuous real-time monitoring, operates independently of cooperative systems\nlike the \"Automatic Identification System\" (AIS), being largely unaffected by\nlighting or weather conditions. However, existing research on DAS for vessel\ntracking is limited in scale and lacks validation under real-world conditions.\nTo address these gaps, a general and systematic methodology is presented for\nvessel detection and distance estimation using DAS. Advanced machine learning\nmodels are applied to improve detection and localization accuracy in dynamic\nmaritime environments. The approach is evaluated over a continuous ten-day\nperiod, covering diverse ship and operational conditions, representing one of\nthe largest-scale DAS-based vessel monitoring studies to date, and for which we\nrelease the full evaluation dataset. Results demonstrate DAS as a practical\ntool for maritime surveillance, with an overall F1-score of over 90% in vessel\ndetection, and a mean average error of 141 m for vessel distance estimation,\nbridging the gap between experimental research and real-world deployment.",
    "categories": [
      "remote_sensing",
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Artificial neural networks ensemble methodology to predict significant wave height",
    "url": "http://arxiv.org/abs/2509.14020v1",
    "authors": [
      "Felipe Crivellaro Minuzzi",
      "Leandro Farina"
    ],
    "published": "2025-09-17",
    "abstract": "The forecast of wave variables are important for several applications that\ndepend on a better description of the ocean state. Due to the chaotic behaviour\nof the differential equations which model this problem, a well know strategy to\novercome the difficulties is basically to run several simulations, by for\ninstance, varying the initial condition, and averaging the result of each of\nthese, creating an ensemble. Moreover, in the last few years, considering the\namount of available data and the computational power increase, machine learning\nalgorithms have been applied as surrogate to traditional numerical models,\nyielding comparative or better results. In this work, we present a methodology\nto create an ensemble of different artificial neural networks architectures,\nnamely, MLP, RNN, LSTM, CNN and a hybrid CNN-LSTM, which aims to predict\nsignificant wave height on six different locations in the Brazilian coast. The\nnetworks are trained using NOAA's numerical reforecast data and target the\nresidual between observational data and the numerical model output. A new\nstrategy to create the training and target datasets is demonstrated. Results\nshow that our framework is capable of producing high efficient forecast, with\nan average accuracy of $80\\%$, that can achieve up to $88\\%$ in the best case\nscenario, which means $5\\%$ reduction in error metrics if compared to NOAA's\nnumerical model, and a increasingly reduction of computational cost.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM",
      "RNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SamudrACE: Fast and Accurate Coupled Climate Modeling with 3D Ocean and Atmosphere Emulators",
    "url": "http://arxiv.org/abs/2509.12490v1",
    "authors": [
      "James P. C. Duncan",
      "Elynn Wu",
      "Surya Dheeshjith",
      "Adam Subel",
      "Troy Arcomano",
      "Spencer K. Clark",
      "Brian Henn",
      "Anna Kwa",
      "Jeremy McGibbon",
      "W. Andre Perkins",
      "William Gregory",
      "Carlos Fernandez-Granda",
      "Julius Busecke",
      "Oliver Watt-Meyer",
      "William J. Hurlin",
      "Alistair Adcroft",
      "Laure Zanna",
      "Christopher Bretherton"
    ],
    "published": "2025-09-15",
    "abstract": "Traditional numerical global climate models simulate the full Earth system by\nexchanging boundary conditions between separate simulators of the atmosphere,\nocean, sea ice, land surface, and other geophysical processes. This paradigm\nallows for distributed development of individual components within a common\nframework, unified by a coupler that handles translation between realms via\nspatial or temporal alignment and flux exchange. Following a similar approach\nadapted for machine learning-based emulators, we present SamudrACE: a coupled\nglobal climate model emulator which produces centuries-long simulations at\n1-degree horizontal, 6-hourly atmospheric, and 5-daily oceanic resolution, with\n145 2D fields spanning 8 atmospheric and 19 oceanic vertical levels, plus sea\nice, surface, and top-of-atmosphere variables. SamudrACE is highly stable and\nhas low climate biases comparable to those of its components with prescribed\nboundary forcing, with realistic variability in coupled climate phenomena such\nas ENSO that is not possible to simulate in uncoupled mode.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models",
    "url": "http://arxiv.org/abs/2509.13047v1",
    "authors": [
      "Nolan Platt",
      "Pragyansmita Nayak"
    ],
    "published": "2025-09-16",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Branched Broomrape Detection in Tomato Farms Using Satellite Imagery and Time-Series Analysis",
    "url": "http://arxiv.org/abs/2509.10804v1",
    "authors": [
      "Mohammadreza Narimani",
      "Alireza Pourreza",
      "Ali Moghimi",
      "Parastoo Farajpoor",
      "Hamid Jafarbiglu",
      "Mohsen Mesgaran"
    ],
    "published": "2025-09-13",
    "abstract": "Branched broomrape (Phelipanche ramosa (L.) Pomel) is a chlorophyll-deficient\nparasitic plant that threatens tomato production by extracting nutrients from\nthe host, with reported yield losses up to 80 percent. Its mostly subterranean\nlife cycle and prolific seed production (more than 200,000 seeds per plant,\nviable for up to 20 years) make early detection essential. We present an\nend-to-end pipeline that uses Sentinel-2 imagery and time-series analysis to\nidentify broomrape-infested tomato fields in California. Regions of interest\nwere defined from farmer-reported infestations, and images with less than 10\npercent cloud cover were retained. We processed 12 spectral bands and\nsun-sensor geometry, computed 20 vegetation indices (e.g., NDVI, NDMI), and\nderived five plant traits (Leaf Area Index, Leaf Chlorophyll Content, Canopy\nChlorophyll Content, Fraction of Absorbed Photosynthetically Active Radiation,\nand Fractional Vegetation Cover) using a neural network calibrated with\nground-truth and synthetic data. Trends in Canopy Chlorophyll Content\ndelineated transplanting-to-harvest periods, and phenology was aligned using\ngrowing degree days. Vegetation pixels were segmented and used to train a Long\nShort-Term Memory (LSTM) network on 18,874 pixels across 48 growing-degree-day\ntime points. The model achieved 88 percent training accuracy and 87 percent\ntest accuracy, with precision 0.86, recall 0.92, and F1 0.89. Permutation\nfeature importance ranked NDMI, Canopy Chlorophyll Content, FAPAR, and a\nchlorophyll red-edge index as most informative, consistent with the\nphysiological effects of infestation. Results show the promise of\nsatellite-driven time-series modeling for scalable detection of parasitic\nstress in tomato farms.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Modelling species distributions using remote sensing predictors: Comparing Dynamic Habitat Index and LULC",
    "url": "http://arxiv.org/abs/2509.14862v1",
    "authors": [
      "Ma\u00efri Souza Oliveira",
      "Cl\u00e9mentine Pr\u00e9au",
      "Samuel Alleaume",
      "Maxime Lenormand",
      "Sandra Luque"
    ],
    "published": "2025-09-18",
    "abstract": "This study compares the predictive capacity of the Dynamic Habitat Index\n(DHI) - a remote sensing (RS)-based measure of habitat productivity and\nvariability - against traditional land-use/land-cover (LULC) metrics in species\ndistribution modelling (SDM) applications. RS and LULC-based SDMs were built\nusing distribution data for eleven bird, amphibian, and mammal species in\n\\^Ile-de-France. Predictor variables were derived from Sentinel-2 RS data and\nLULC classifications, with the latter incorporating Euclidean distance to\nhabitat types. Ensemble SDMs were built using nine algorithms and evaluated\nwith the Continuous Boyce Index (CBI) and a calibrated AUC. Habitat suitability\nscores and their binary transformations were assessed using niche overlap\nindices (Schoener, Warren, and Spearman rank correlation coefficient). Both RS\nand LULC approaches exhibited similar predictive accuracy overall. After\nbinarisation however, the resulting niche maps diverged significantly. While\nLULC-based models exhibited spatial constraints (habitat suitability decreased\nas distance from recorded occurrences increased), RS-based models, which used\ncontinuous data, were not affected by geographic bias or distance effects.\nThese results underscore the need to account for spatial biases in LULC-based\nSDMs. The DHI may offer a more spatially neutral alternative, making it a\npromising predictor for modelling species niches at regional scales.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Sentinel-3 foundation model for ocean colour",
    "url": "http://arxiv.org/abs/2509.21273v1",
    "authors": [
      "Geoffrey Dawson",
      "Remy Vandaele",
      "Andrew Taylor",
      "David Moffat",
      "Helen Tamura-Wicks",
      "Sarah Jackson",
      "Rosie Lickorish",
      "Paolo Fraccaro",
      "Hywel Williams",
      "Chunbo Luo",
      "Anne Jones"
    ],
    "published": "2025-09-25",
    "abstract": "Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive\nunlabelled datasets, have the potential to drastically change AI applications\nin ocean science, where labelled data are often sparse and expensive to\ncollect. In this work, we describe a new foundation model using the Prithvi-EO\nVision Transformer architecture which has been pre-trained to reconstruct data\nfrom the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the\nmodel by fine-tuning on two downstream marine earth observation tasks. We first\nassess model performance compared to current baseline models used to quantify\nchlorophyll concentration. We then evaluate the FMs ability to refine remote\nsensing-based estimates of ocean primary production. Our results demonstrate\nthe utility of self-trained FMs for marine monitoring, in particular for making\nuse of small amounts of high quality labelled data and in capturing detailed\nspatial patterns of ocean colour whilst matching point observations. We\nconclude that this new generation of geospatial AI models has the potential to\nprovide more robust, data-driven insights into ocean ecosystems and their role\nin global climate processes.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2509.18711v1",
    "authors": [
      "Ke Li",
      "Di Wang",
      "Ting Wang",
      "Fuyu Dong",
      "Yiming Zhang",
      "Luyao Zhang",
      "Xiangyu Wang",
      "Shaofeng Li",
      "Quan Wang"
    ],
    "published": "2025-09-23",
    "abstract": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Visual Instruction Pretraining for Domain-Specific Foundation Models",
    "url": "http://arxiv.org/abs/2509.17562v2",
    "authors": [
      "Yuxuan Li",
      "Yicheng Zhang",
      "Wenhao Tang",
      "Yimian Dai",
      "Ming-Ming Cheng",
      "Xiang Li",
      "Jian Yang"
    ],
    "published": "2025-09-22",
    "abstract": "Modern computer vision is converging on a closed loop in which perception,\nreasoning and generation mutually reinforce each other. However, this loop\nremains incomplete: the top-down influence of high-level reasoning on the\nfoundational learning of low-level perceptual features is not yet\nunderexplored. This paper addresses this gap by proposing a new paradigm for\npretraining foundation models in downstream domains. We introduce Visual\ninsTruction Pretraining (ViTP), a novel approach that directly leverages\nreasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)\nbackbone within a Vision-Language Model and pretrains it end-to-end using a\nrich corpus of visual instruction data curated from target downstream domains.\nViTP is powered by our proposed Visual Robustness Learning (VRL), which compels\nthe ViT to learn robust and domain-relevant features from a sparse set of\nvisual tokens. Extensive experiments on 16 challenging remote sensing and\nmedical imaging benchmarks demonstrate that ViTP establishes new\nstate-of-the-art performance across a diverse range of downstream tasks. The\ncode is available at https://github.com/zcablii/ViTP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model",
    "url": "http://arxiv.org/abs/2509.16617v1",
    "authors": [
      "David Kreismann"
    ],
    "published": "2025-09-20",
    "abstract": "As urbanization and climate change progress, urban heat island effects are\nbecoming more frequent and severe. To formulate effective mitigation plans,\ncities require detailed air temperature data. However, predictive analytics\nmethods based on conventional machine learning models and limited data\ninfrastructure often provide inaccurate predictions, especially in underserved\nareas. In this context, geospatial foundation models trained on unstructured\nglobal data demonstrate strong generalization and require minimal fine-tuning,\noffering an alternative for predictions where traditional approaches are\nlimited. This study fine-tunes a geospatial foundation model to predict urban\nland surface temperatures under future climate scenarios and explores its\nresponse to land cover changes using simulated vegetation strategies. The\nfine-tuned model achieved pixel-wise downscaling errors below 1.74 {\\deg}C and\naligned with ground truth patterns, demonstrating an extrapolation capacity up\nto 3.62 {\\deg}C.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images",
    "url": "http://arxiv.org/abs/2509.20918v1",
    "authors": [
      "Qinfeng Zhu",
      "Han Li",
      "Liang He",
      "Lei Fan"
    ],
    "published": "2025-09-25",
    "abstract": "Semantic segmentation of remote sensing imagery is a fundamental task in\ncomputer vision, supporting a wide range of applications such as land use\nclassification, urban planning, and environmental monitoring. However, this\ntask is often challenged by the high spatial resolution, complex scene\nstructures, and diverse object scales present in remote sensing data. To\naddress these challenges, various deep learning architectures have been\nproposed, including convolutional neural networks, Vision Transformers, and the\nrecently introduced Vision Mamba. Vision Mamba features a global receptive\nfield and low computational complexity, demonstrating both efficiency and\neffectiveness in image segmentation. However, its reliance on global scanning\ntends to overlook critical local features, such as textures and edges, which\nare essential for achieving accurate segmentation in remote sensing contexts.\nTo tackle this limitation, we propose SwinMamba, a novel framework inspired by\nthe Swin Transformer. SwinMamba integrates localized Mamba-style scanning\nwithin shifted windows with a global receptive field, to enhance the model's\nperception of both local and global features. Specifically, the first two\nstages of SwinMamba perform local scanning to capture fine-grained details,\nwhile its subsequent two stages leverage global scanning to fuse broader\ncontextual information. In our model, the use of overlapping shifted windows\nenhances inter-region information exchange, facilitating more robust feature\nintegration across the entire image. Extensive experiments on the LoveDA and\nISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art\nmethods, underscoring its effectiveness and potential as a superior solution\nfor semantic segmentation of remotely sensed imagery.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
    "url": "http://arxiv.org/abs/2509.20234v1",
    "authors": [
      "Tom Burgert",
      "Oliver Stoll",
      "Paolo Rota",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-09-24",
    "abstract": "The hypothesis that Convolutional Neural Networks (CNNs) are inherently\ntexture-biased has shaped much of the discourse on feature use in deep\nlearning. We revisit this hypothesis by examining limitations in the\ncue-conflict experiment by Geirhos et al. To address these limitations, we\npropose a domain-agnostic framework that quantifies feature reliance through\nsystematic suppression of shape, texture, and color cues, avoiding the\nconfounds of forced-choice conflicts. By evaluating humans and neural networks\nunder controlled suppression conditions, we find that CNNs are not inherently\ntexture-biased but predominantly rely on local shape features. Nonetheless,\nthis reliance can be substantially mitigated through modern training strategies\nor architectures (ConvNeXt, ViTs). We further extend the analysis across\ncomputer vision, medical imaging, and remote sensing, revealing that reliance\npatterns differ systematically: computer vision models prioritize shape,\nmedical imaging models emphasize color, and remote sensing models exhibit a\nstronger reliance towards texture. Code is available at\nhttps://github.com/tomburgert/feature-reliance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Optimal Transport Based Hyperspectral Unmixing for Highly Mixed Observations",
    "url": "http://arxiv.org/abs/2509.20417v1",
    "authors": [
      "D. Doutsas",
      "B. Figliuzzi"
    ],
    "published": "2025-09-24",
    "abstract": "We propose a novel approach based on optimal transport (OT) for tackling the\nproblem of highly mixed data in blind hyperspectral unmixing. Our method\nconstrains the distribution of the estimated abundance matrix to resemble a\ntargeted Dirichlet distribution more closely. The novelty lies in using OT to\nmeasure the discrepancy between the targeted and true abundance distributions,\nwhich we incorporate as a regularization term in our optimization problem. We\ndemonstrate the efficiency of our method through a case study involving an\nunsupervised deep learning approach. Our experiments show that the proposed\napproach allows for a better estimation of the endmembers in the presence of\nhighly mixed data, while displaying robustness to the choice of target\nabundance distribution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy",
    "url": "http://arxiv.org/abs/2509.19665v1",
    "authors": [
      "Manuel Perez-Carrasco",
      "Maya Nasr",
      "Sebastien Roche",
      "Chris Chan Miller",
      "Zhan Zhang",
      "Core Francisco Park",
      "Eleanor Walker",
      "Cecilia Garraffo",
      "Douglas Finkbeiner",
      "Ritesh Gautam",
      "Steven Wofsy"
    ],
    "published": "2025-09-24",
    "abstract": "Effective cloud and cloud shadow detection is a critical prerequisite for\naccurate retrieval of concentrations of atmospheric methane or other trace\ngases in hyperspectral remote sensing. This challenge is especially pertinent\nfor MethaneSAT and for its airborne companion mission, MethaneAIR. In this\nstudy, we use machine learning methods to address the cloud and cloud shadow\ndetection problem for sensors with these high spatial resolutions instruments.\nCloud and cloud shadows in remote sensing data need to be effectively screened\nout as they bias methane retrievals in remote sensing imagery and impact the\nquantification of emissions. We deploy and evaluate conventional techniques\nincluding Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),\nwith advanced deep learning architectures, namely UNet and a Spectral Channel\nAttention Network (SCAN) method. Our results show that conventional methods\nstruggle with spatial coherence and boundary definition, affecting the\ndetection of clouds and cloud shadows. Deep learning models substantially\nimprove detection quality: UNet performs best in preserving spatial structure,\nwhile SCAN excels at capturing fine boundary details. Notably, SCAN surpasses\nUNet on MethaneSAT data, underscoring the benefits of incorporating spectral\nattention for satellite specific features. This in depth assessment of various\ndisparate machine learning techniques demonstrates the strengths and\neffectiveness of advanced deep learning architectures in providing robust,\nscalable solutions for clouds and cloud shadow screening towards enhancing\nmethane emission quantification capacity of existing and next generation\nhyperspectral missions. Our data and code is publicly available at\nhttps://doi.org/10.7910/DVN/IKLZOJ",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection",
      "Regression"
    ]
  },
  {
    "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2509.19087v1",
    "authors": [
      "Ganesh Mallya",
      "Yotam Gigi",
      "Dahun Kim",
      "Maxim Neumann",
      "Genady Beryozkin",
      "Tomer Shekel",
      "Anelia Angelova"
    ],
    "published": "2025-09-23",
    "abstract": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Can multimodal representation learning by alignment preserve modality-specific information?",
    "url": "http://arxiv.org/abs/2509.17943v1",
    "authors": [
      "Romain Thoreau",
      "Jessie Levillain",
      "Dawa Derksen"
    ],
    "published": "2025-09-22",
    "abstract": "Combining multimodal data is a key issue in a wide range of machine learning\ntasks, including many remote sensing problems. In Earth observation, early\nmultimodal data fusion methods were based on specific neural network\narchitectures and supervised learning. Ever since, the scarcity of labeled data\nhas motivated self-supervised learning techniques. State-of-the-art multimodal\nrepresentation learning techniques leverage the spatial alignment between\nsatellite data from different modalities acquired over the same geographic area\nin order to foster a semantic alignment in the latent space. In this paper, we\ninvestigate how this methods can preserve task-relevant information that is not\nshared across modalities. First, we show, under simplifying assumptions, when\nalignment strategies fundamentally lead to an information loss. Then, we\nsupport our theoretical insight through numerical experiments in more realistic\nsettings. With those theoretical and empirical evidences, we hope to support\nnew developments in contrastive learning for the combination of multimodal\nsatellite data. Our code and data is publicly available at\nhttps://github.com/Romain3Ch216/alg_maclean_25.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Remote Sensing-Oriented World Model",
    "url": "http://arxiv.org/abs/2509.17808v1",
    "authors": [
      "Yuxi Lu",
      "Biao Wu",
      "Zhidong Li",
      "Kunqi Li",
      "Chenya Huang",
      "Huacan Wang",
      "Qizhen Lan",
      "Ronghao Chen",
      "Ling Chen",
      "Bin Liang"
    ],
    "published": "2025-09-22",
    "abstract": "World models have shown potential in artificial intelligence by predicting\nand reasoning about world states beyond direct observations. However, existing\napproaches are predominantly evaluated in synthetic environments or constrained\nscene settings, limiting their validation in real-world contexts with broad\nspatial coverage and complex semantics. Meanwhile, remote sensing applications\nurgently require spatial reasoning capabilities for disaster response and urban\nplanning. This paper bridges these gaps by introducing the first framework for\nworld modeling in remote sensing. We formulate remote sensing world modeling as\ndirection-conditioned spatial extrapolation, where models generate semantically\nconsistent adjacent image tiles given a central observation and directional\ninstruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing\nWorld-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks\nacross four scenarios: general, flood, urban, and rural. RSWISE combines visual\nfidelity assessment with instruction compliance evaluation using GPT-4o as a\nsemantic judge, ensuring models genuinely perform spatial reasoning rather than\nsimple replication. Afterwards, we present RemoteBAGEL, a unified multimodal\nmodel fine-tuned on remote sensing data for spatial extrapolation tasks.\nExtensive experiments demonstrate that RemoteBAGEL consistently outperforms\nstate-of-the-art baselines on RSWISE.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation",
    "url": "http://arxiv.org/abs/2509.17206v1",
    "authors": [
      "Gunner Stone",
      "Sushmita Sarker",
      "Alireza Tavakkoli"
    ],
    "published": "2025-09-21",
    "abstract": "Generating realistic 3D point clouds is a fundamental problem in computer\nvision with applications in remote sensing, robotics, and digital object\nmodeling. Existing generative approaches primarily capture geometry, and when\nsemantics are considered, they are typically imposed post hoc through external\nsegmentation or clustering rather than integrated into the generative process\nitself. We propose a diffusion-based framework that embeds per-point semantic\nconditioning directly within generation. Each point is associated with a\nconditional variable corresponding to its semantic label, which guides the\ndiffusion dynamics and enables the joint synthesis of geometry and semantics.\nThis design produces point clouds that are both structurally coherent and\nsegmentation-aware, with object parts explicitly represented during synthesis.\nThrough a comparative analysis of guided and unguided diffusion processes, we\ndemonstrate the significant impact of conditional variables on diffusion\ndynamics and generation quality. Extensive experiments validate the efficacy of\nour approach, producing detailed and accurate 3D point clouds tailored to\nspecific parts and features.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures",
    "url": "http://arxiv.org/abs/2509.17734v1",
    "authors": [
      "Pablo Rodr\u00edguez-Bocca",
      "Guillermo Pereira",
      "Diego Kiedanski",
      "Soledad Collazo",
      "Sebasti\u00e1n Basterrech",
      "Gerardo Rubino"
    ],
    "published": "2025-09-22",
    "abstract": "In recent years, great progress has been made in the field of forecasting\nmeteorological variables. Recently, deep learning architectures have made a\nmajor breakthrough in forecasting the daily average temperature over a ten-day\nhorizon. However, advances in forecasting events related to the maximum\ntemperature over short horizons remain a challenge for the community. A problem\nthat is even more complex consists in making predictions of the maximum daily\ntemperatures in the short, medium, and long term. In this work, we focus on\nforecasting events related to the maximum daily temperature over medium-term\nperiods (90 days). Therefore, instead of addressing the problem from a\nmeteorological point of view, this article tackles it from a climatological\npoint of view. Due to the complexity of this problem, a common approach is to\nframe the study as a temporal classification problem with the classes: maximum\ntemperature \"above normal\", \"normal\" or \"below normal\". From a practical point\nof view, we created a large historical dataset (from 1981 to 2018) collecting\ninformation from weather stations located in South America. In addition, we\nalso integrated exogenous information from the Pacific, Atlantic, and Indian\nOcean basins. We applied the AutoGluonTS platform to solve the above-mentioned\nproblem. This AutoML tool shows competitive forecasting performance with\nrespect to large operational platforms dedicated to tackling this\nclimatological problem; but with a \"relatively\" low computational cost in terms\nof time and resources.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations",
    "url": "http://arxiv.org/abs/2509.19384v1",
    "authors": [
      "Hongyuan Shi",
      "Yilin Zhai",
      "Ping Dong",
      "Zaijin You",
      "Chao Zhan",
      "Qing Wang"
    ],
    "published": "2025-09-21",
    "abstract": "Reconstructing high-resolution regional significant wave height fields from\nsparse and uneven buoy observations remains a core challenge for ocean\nmonitoring and risk-aware operations. We introduce AUWave, a hybrid deep\nlearning framework that fuses a station-wise sequence encoder (MLP) with a\nmulti-scale U-Net enhanced by a bottleneck self-attention layer to recover\n32$\\times$32 regional SWH fields. A systematic Bayesian hyperparameter search\nwith Optuna identifies the learning rate as the dominant driver of\ngeneralization, followed by the scheduler decay and the latent dimension. Using\nNDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave\nattains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE\ndistribution. Spatial errors are lowest near observation sites and increase\nwith distance, reflecting identifiability limits under sparse sampling.\nSensitivity experiments show that AUWave consistently outperforms a\nrepresentative baseline in data-richer configurations, while the baseline is\nonly marginally competitive in the most underdetermined single-buoy cases. The\narchitecture's multi-scale and attention components translate into accuracy\ngains when minimal but non-trivial spatial anchoring is available. Error maps\nand buoy ablations reveal key anchor stations whose removal disproportionately\ndegrades performance, offering actionable guidance for network design. AUWave\nprovides a scalable pathway for gap filling, high-resolution priors for data\nassimilation, and contingency reconstruction.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations",
    "url": "http://arxiv.org/abs/2509.20422v1",
    "authors": [
      "Yiling Ma",
      "Nathan Luke Abraham",
      "Stefan Versick",
      "Roland Ruhnke",
      "Andrea Schneidereit",
      "Ulrike Niemeier",
      "Felix Back",
      "Peter Braesicke",
      "Peer Nowack"
    ],
    "published": "2025-09-24",
    "abstract": "Atmospheric ozone is a crucial absorber of solar radiation and an important\ngreenhouse gas. However, most climate models participating in the Coupled Model\nIntercomparison Project (CMIP) still lack an interactive representation of\nozone due to the high computational costs of atmospheric chemistry schemes.\nHere, we introduce a machine learning parameterization (mloz) to interactively\nmodel daily ozone variability and trends across the troposphere and\nstratosphere in standard climate sensitivity simulations, including two-way\ninteractions of ozone with the Quasi-Biennial Oscillation. We demonstrate its\nhigh fidelity on decadal timescales and its flexible use online across two\ndifferent climate models -- the UK Earth System Model (UKESM) and the German\nICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile\ninformation as the only input, mloz produces stable ozone predictions around 31\ntimes faster than the chemistry scheme in UKESM, contributing less than 4\npercent of the respective total climate model runtimes. In particular, we also\ndemonstrate its transferability to different climate models without chemistry\nschemes by transferring the parameterization from UKESM to ICON. This\nhighlights the potential for widespread adoption in CMIP-level climate models\nthat lack interactive chemistry for future climate change assessments,\nparticularly when focusing on climate sensitivity simulations, where ozone\ntrends and variability are known to significantly modulate atmospheric feedback\nprocesses.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Discovering strategies for coastal resilience with AI-based prediction and optimization",
    "url": "http://arxiv.org/abs/2509.19263v1",
    "authors": [
      "Jared Markowitz",
      "Alexander New",
      "Jennifer Sleeman",
      "Chace Ashcraft",
      "Jay Brett",
      "Gary Collins",
      "Stella In",
      "Nathaniel Winstead"
    ],
    "published": "2025-09-23",
    "abstract": "Tropical storms cause extensive property damage and loss of life, making them\none of the most destructive types of natural hazards. The development of\npredictive models that identify interventions effective at mitigating storm\nimpacts has considerable potential to reduce these adverse outcomes. In this\nstudy, we use an artificial intelligence (AI)-driven approach for optimizing\nintervention schemes that improve resilience to coastal flooding. We combine\nthree different AI models to optimize the selection of intervention types,\nsites, and scales in order to minimize the expected cost of flooding damage in\na given region, including the cost of installing and maintaining interventions.\nOur approach combines data-driven generation of storm surge fields, surrogate\nmodeling of intervention impacts, and the solving of a continuous-armed bandit\nproblem. We applied this methodology to optimize the selection of sea wall and\noyster reef interventions near Tyndall Air Force Base (AFB) in Florida, an area\nthat was catastrophically impacted by Hurricane Michael. Our analysis predicts\nthat intervention optimization could be used to potentially save billions of\ndollars in storm damage, far outpacing greedy or non-optimal solutions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling",
    "url": "http://arxiv.org/abs/2509.19032v2",
    "authors": [
      "Kashaf Ul Emaan"
    ],
    "published": "2025-09-23",
    "abstract": "Detection of credit card fraud is an acute issue of financial security\nbecause transaction datasets are highly lopsided, with fraud cases being only a\ndrop in the ocean. Balancing datasets using the most popular methods of\ntraditional oversampling such as the Synthetic Minority Oversampling Technique\n(SMOTE) generally create simplistic synthetic samples that are not readily\napplicable to complex fraud patterns. Recent industry advances that include\nConditional Tabular Generative Adversarial Networks (CTGAN) and Tabular\nVariational Autoencoders (TVAE) have demonstrated increased efficiency in\ntabular synthesis, yet all these models still exhibit issues with\nhigh-dimensional dependence modelling. Now we will present our hybrid approach\nwhere we use a Generative Adversarial Network (GAN) with a Transformer encoder\nblock to produce realistic fraudulent transactions samples. The GAN\narchitecture allows training realistic generators adversarial, and the\nTransformer allows the model to learn rich feature interactions by\nself-attention. Such a hybrid strategy overcomes the limitations of SMOTE,\nCTGAN, and TVAE by producing a variety of high-quality synthetic minority\nclasses samples. We test our algorithm on the publicly-available Credit Card\nFraud Detection dataset and compare it to conventional and generative\nresampling strategies with a variety of classifiers, such as Logistic\nRegression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and\nSupport Vector Machine (SVM). Findings indicate that our Transformer-based GAN\nshows substantial gains in Recall, F1-score and Area Under the Receiver\nOperating Characteristic Curve (AUC), which indicates that it is effective in\novercoming the severe class imbalance inherent in the task of fraud detection.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "GAN",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Regression"
    ]
  },
  {
    "title": "An update to ECMWF's machine-learned weather forecast model AIFS",
    "url": "http://arxiv.org/abs/2509.18994v1",
    "authors": [
      "Gabriel Moldovan",
      "Ewan Pinnington",
      "Ana Prieto Nemesio",
      "Simon Lang",
      "Zied Ben Bouall\u00e8gue",
      "Jesper Dramsch",
      "Mihai Alexe",
      "Mario Santa Cruz",
      "Sara Hahner",
      "Harrison Cook",
      "Helen Theissen",
      "Mariana Clare",
      "Cathal O'Brien",
      "Jan Polster",
      "Linus Magnusson",
      "Gert Mertes",
      "Florian Pinault",
      "Baudouin Raoult",
      "Patricia de Rosnay",
      "Richard Forbes",
      "Matthew Chantry"
    ],
    "published": "2025-09-23",
    "abstract": "We present an update to ECMWF's machine-learned weather forecasting model\nAIFS Single with several key improvements. The model now incorporates physical\nconsistency constraints through bounding layers, an updated training schedule,\nand an expanded set of variables. The physical constraints substantially\nimprove precipitation forecasts and the new variables show a high level of\nskill. Upper-air headline scores also show improvement over the previous AIFS\nversion. The AIFS has been fully operational at ECMWF since the 25th of\nFebruary 2025.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Training-Free Data Assimilation with GenCast",
    "url": "http://arxiv.org/abs/2509.18811v1",
    "authors": [
      "Thomas Savary",
      "Fran\u00e7ois Rozet",
      "Gilles Louppe"
    ],
    "published": "2025-09-23",
    "abstract": "Data assimilation is widely used in many disciplines such as meteorology,\noceanography, and robotics to estimate the state of a dynamical system from\nnoisy observations. In this work, we propose a lightweight and general method\nto perform data assimilation using diffusion models pre-trained for emulating\ndynamical systems. Our method builds on particle filters, a class of data\nassimilation algorithms, and does not require any further training. As a\nguiding example throughout this work, we illustrate our methodology on GenCast,\na diffusion-based model that generates global ensemble weather forecasts.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Climate-Adaptive and Cascade-Constrained Machine Learning Prediction for Sea Surface Height under Greenhouse Warming",
    "url": "http://arxiv.org/abs/2509.18741v1",
    "authors": [
      "Tianmu Zheng",
      "Ru Chen",
      "Xin Su",
      "Gang Huang",
      "Bingzheng Yan"
    ],
    "published": "2025-09-23",
    "abstract": "Machine learning (ML) has achieved remarkable success in climate and marine\nscience. Given that greenhouse warming fundamentally reshapes ocean conditions\nsuch as stratification, circulation patterns and eddy activity, evaluating the\nclimate adaptability of the ML model is crucial. While physical constraints\nhave been shown to enhance the performance of ML models, kinetic energy (KE)\ncascade has not been used as a constraint despite its importance in regulating\nmulti-scale ocean motions. Here we develop two sea surface height (SSH)\nprediction models (with and without KE cascade constraint) and quantify their\nclimate adaptability at the Kuroshio Extension. Our results demonstrate that\nboth models exhibit only slight performance degradation under greenhouse\nwarming conditions. Incorporating the KE cascade as a physical constraint\nsignificantly improve the model performance, reducing eddy kinetic energy\nerrors by 14.7% in the present climate and 15.9% under greenhouse warming. This\nwork presents the first application of the kinetic energy (KE) cascade as a\nphysical constraint for ML based ocean state prediction and demonstrates its\nrobust adaptability across climates, offering guidance for the further\ndevelopment of global ML models for both present and future conditions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning",
    "url": "http://arxiv.org/abs/2509.18553v1",
    "authors": [
      "Richa Rawat",
      "Faisal Ahmed"
    ],
    "published": "2025-09-23",
    "abstract": "Cancer is one of the leading health challenges for women, specifically breast\nand ovarian cancer. Early detection can help improve the survival rate through\ntimely intervention and treatment. Traditional methods of detecting cancer\ninvolve manually examining mammograms, CT scans, ultrasounds, and other imaging\ntypes. However, this makes the process labor-intensive and requires the\nexpertise of trained pathologists. Hence, making it both time-consuming and\nresource-intensive. In this paper, we introduce a novel vision transformer\n(ViT)-based method for detecting and classifying breast and ovarian cancer. We\nuse a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both\nbinary and multi-class classification tasks using publicly available\nhistopathological image datasets. Further, we use a preprocessing pipeline that\nconverts raw histophological images into standardized PyTorch tensors, which\nare compatible with the ViT architecture and also help improve the model\nperformance. We evaluated the performance of our model on two benchmark\ndatasets: the BreakHis dataset for binary classification and the UBC-OCEAN\ndataset for five-class classification without any data augmentation. Our model\nsurpasses existing CNN, ViT, and topological data analysis-based approaches in\nbinary classification. For multi-class classification, it is evaluated against\nrecent topological methods and demonstrates superior performance. Our study\nhighlights the effectiveness of Vision Transformer-based transfer learning\ncombined with efficient preprocessing in oncological diagnostics.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Technical overview and architecture of the FastNet Machine Learning weather prediction model, version 1.0",
    "url": "http://arxiv.org/abs/2509.17658v1",
    "authors": [
      "Eric G. Daub",
      "Tom Dunstan",
      "Thusal Bennett",
      "Matthew Burnand",
      "James Chappell",
      "Alejandro Coca-Castro",
      "Noushin Eftekhari",
      "J. Scott Hosking",
      "Manvendra Janmaijaya",
      "Jon Lillis",
      "David Salvador-Jasin",
      "Nathan Simpson",
      "Oliver T Strickson",
      "Ryan Sze-Yin Chan",
      "Mohamad Elmasri",
      "Lydia Allegranza France",
      "Sam Madge",
      "James Robinson",
      "Adam A. Scaife",
      "David Walters",
      "Peter Yatsyshin",
      "Theo McCaie",
      "Levan Bokeria",
      "Hannah Brown",
      "Tom Dodds",
      "David Llewellyn-Jones",
      "Sophia Moreton",
      "Tom Potter",
      "Iain Stenson",
      "Louisa van Zeeland",
      "Karina Bett-Williams",
      "Kirstine Ida Dale"
    ],
    "published": "2025-09-22",
    "abstract": "We present FastNet version 1.0, a data-driven medium range numerical weather\nprediction (NWP) model based on a Graph Neural Network architecture, developed\njointly between the Alan Turing Institute and the Met Office. FastNet uses an\nencode-process-decode structure to produce deterministic global weather\npredictions out to 10 days. The architecture is independent of spatial\nresolution and we have trained models at 1$^{\\circ}$ and 0.25$^{\\circ}$\nresolution, with a six hour time step. FastNet uses a multi-level mesh in the\nprocessor, which is able to capture both short-range and long-range patterns in\nthe spatial structure of the atmosphere. The model is pre-trained on ECMWF's\nERA5 reanalysis data and then fine-tuned on additional autoregressive rollout\nsteps, which improves accuracy over longer time horizons. We evaluate the model\nperformance at 1.5$^{\\circ}$ resolution using 2022 as a hold-out year and\ncompare with the Met Office Global Model, finding that FastNet surpasses the\nskill of the current Met Office Global Model NWP system using a variety of\nevaluation metrics on a number of atmospheric variables. Our results show that\nboth our 1$^{\\circ}$ and 0.25$^{\\circ}$ FastNet models outperform the current\nGlobal Model and produce results with predictive skill approaching those of\nother data-driven models trained on 0.25$^{\\circ}$ ERA5.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FastNet: Improving the physical consistency of machine-learning weather prediction models through loss function design",
    "url": "http://arxiv.org/abs/2509.17601v1",
    "authors": [
      "Tom Dunstan",
      "Oliver Strickson",
      "Thusal Bennett",
      "Jack Bowyer",
      "Matthew Burnand",
      "James Chappell",
      "Alejandro Coca-Castro",
      "Kirstine Ida Dale",
      "Eric G. Daub",
      "Noushin Eftekhari",
      "Manvendra Janmaijaya",
      "Jon Lillis",
      "David Salvador-Jasin",
      "Nathan Simpson",
      "Ryan Sze-Yin Chan",
      "Mohamad Elmasri",
      "Lydia Allegranza France",
      "Sam Madge",
      "Levan Bokeria",
      "Hannah Brown",
      "Tom Dodds",
      "Anna-Louise Ellis",
      "David Llewellyn-Jones",
      "Theo McCaie",
      "Sophia Moreton",
      "Tom Potter",
      "James Robinson",
      "Adam A. Scaife",
      "Iain Stenson",
      "David Walters",
      "Karina Bett-Williams",
      "Louisa van Zeeland",
      "Peter Yatsyshin",
      "J. Scott Hosking"
    ],
    "published": "2025-09-22",
    "abstract": "Machine learning weather prediction (MLWP) models have demonstrated\nremarkable potential in delivering accurate forecasts at significantly reduced\ncomputational cost compared to traditional numerical weather prediction (NWP)\nsystems. However, challenges remain in ensuring the physical consistency of\nMLWP outputs, particularly in deterministic settings. This study presents\nFastNet, a graph neural network (GNN)-based global prediction model, and\ninvestigates the impact of alternative loss function designs on improving the\nphysical realism of its forecasts. We explore three key modifications to the\nstandard mean squared error (MSE) loss: (1) a modified spherical harmonic (MSH)\nloss that penalises spectral amplitude errors to reduce blurring and enhance\nsmall-scale structure retention; (2) inclusion of horizontal gradient terms in\nthe loss to suppress non-physical artefacts; and (3) an alternative wind\nrepresentation that decouples speed and direction to better capture extreme\nwind events. Results show that while the MSH and gradient-based losses\n\\textit{alone} may slightly degrade RMSE scores, when trained in combination\nthe model exhibits very similar MSE performance to an MSE-trained model while\nat the same time significantly improving spectral fidelity and physical\nconsistency. The alternative wind representation further improves wind speed\naccuracy and reduces directional bias. Collectively, these findings highlight\nthe importance of loss function design as a mechanism for embedding domain\nknowledge into MLWP models and advancing their operational readiness.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GeoLink: Empowering Remote Sensing Foundation Model with OpenStreetMap Data",
    "url": "http://arxiv.org/abs/2509.26016v1",
    "authors": [
      "Lubian Bai",
      "Xiuyuan Zhang",
      "Siqi Zhang",
      "Zepeng Zhang",
      "Haoyu Wang",
      "Wei Qin",
      "Shihong Du"
    ],
    "published": "2025-09-30",
    "abstract": "Integrating ground-level geospatial data with rich geographic context, like\nOpenStreetMap (OSM), into remote sensing (RS) foundation models (FMs) is\nessential for advancing geospatial intelligence and supporting a broad spectrum\nof tasks. However, modality gap between RS and OSM data, including differences\nin data structure, content, and spatial granularity, makes effective synergy\nhighly challenging, and most existing RS FMs focus on imagery alone. To this\nend, this study presents GeoLink, a multimodal framework that leverages OSM\ndata to enhance RS FM during both the pretraining and downstream task stages.\nSpecifically, GeoLink enhances RS self-supervised pretraining using\nmulti-granularity learning signals derived from OSM data, guided by cross-modal\nspatial correlations for information interaction and collaboration. It also\nintroduces image mask-reconstruction to enable sparse input for efficient\npretraining. For downstream tasks, GeoLink generates both unimodal and\nmultimodal fine-grained encodings to support a wide range of applications, from\ncommon RS interpretation tasks like land cover classification to more\ncomprehensive geographic tasks like urban function zone mapping. Extensive\nexperiments show that incorporating OSM data during pretraining enhances the\nperformance of the RS image encoder, while fusing RS and OSM data in downstream\ntasks improves the FM's adaptability to complex geographic scenarios. These\nresults underscore the potential of multimodal synergy in advancing high-level\ngeospatial artificial intelligence. Moreover, we find that spatial correlation\nplays a crucial role in enabling effective multimodal geospatial data\nintegration. Code, checkpoints, and using examples are released at\nhttps://github.com/bailubin/GeoLink_NeurIPS2025",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Environment-Aware Satellite Image Generation with Diffusion Models",
    "url": "http://arxiv.org/abs/2509.24875v1",
    "authors": [
      "Nikos Kostagiolas",
      "Pantelis Georgiades",
      "Yannis Panagakis",
      "Mihalis A. Nicolaou"
    ],
    "published": "2025-09-29",
    "abstract": "Diffusion-based foundation models have recently garnered much attention in\nthe field of generative modeling due to their ability to generate images of\nhigh quality and fidelity. Although not straightforward, their recent\napplication to the field of remote sensing signaled the first successful trials\ntowards harnessing the large volume of publicly available datasets containing\nmultimodal information. Despite their success, existing methods face\nconsiderable limitations: they rely on limited environmental context, struggle\nwith missing or corrupted data, and often fail to reliably reflect user\nintentions in generated outputs. In this work, we propose a novel diffusion\nmodel conditioned on environmental context, that is able to generate satellite\nimages by conditioning from any combination of three different control signals:\na) text, b) metadata, and c) visual data. In contrast to previous works, the\nproposed method is i) to our knowledge, the first of its kind to condition\nsatellite image generation on dynamic environmental conditions as part of its\ncontrol signals, and ii) incorporating a metadata fusion strategy that models\nattribute embedding interactions to account for partially corrupt and/or\nmissing observations. Our method outperforms previous methods both\nqualitatively (robustness to missing metadata, higher responsiveness to control\ninputs) and quantitatively (higher fidelity, accuracy, and quality of\ngenerations measured using 6 different metrics) in the trials of single-image\nand temporal generation. The reported results support our hypothesis that\nconditioning on environmental context can improve the performance of foundation\nmodels for satellite imagery, and render our model a promising candidate for\nusage in downstream tasks. The collected 3-modal dataset is to our knowledge,\nthe first publicly-available dataset to combine data from these three different\nmediums.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing",
    "url": "http://arxiv.org/abs/2509.23927v1",
    "authors": [
      "Yi Yang",
      "Xiaokun Zhang",
      "Qingchen Fang",
      "Ziqi Ye",
      "Rui Li",
      "Li Liu",
      "Haipeng Wang"
    ],
    "published": "2025-09-28",
    "abstract": "Cross-modal artificial intelligence has garnered widespread attention in\nrecent years, achieving significant progress in the study of natural images.\nHowever, existing methods are mostly designed for RGB imagery, leaving a\nsignificant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with\nits all-day, all-weather imaging capabilities, plays an irreplaceable role in\nremote sensing scene understanding. To address this gap, this paper proposes\nSAR-KnowLIP, the first universal SAR multimodal foundational model, along with\nreusable data and evaluation baselines. Specifically: (1) This work introduces\nthe critical yet long-overlooked attribute of geographic information into\nremote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR\ndataset with complete geographic projection properties), covering multiple\nsatellite platforms, 120,000 images, and 135 cities. (2) Aligned structured\ntext is generated through a hierarchical cognitive chain-of-thought (HCoT),\nproviding more than one million multi-dimensional semantic annotations of\nlandforms, regional functions, target attributes, and spatial relationships.\n(3) We design a Self-Consistent Iterative Optimization mechanism that\ncontinuously enhances cross-modal alignment through a self-supervised closed\nloop of contrastive, matching, and reconstruction learning on a transferable\nmultimodal encoder. (4) A unified evaluation benchmark is established across 11\nrepresentative downstream vision and vision-language tasks, with comparisons\nagainst 14 leading foundation models, where SAR-KnowLIP demonstrates leading\nperformance, particularly in object counting and land-cover classification. We\nexpect that SAR-KnowLIP's large-scale multimodal data, transferable model\narchitecture, and comprehensive experimental benchmark will significantly\nadvance the development of SAR multimodal baseline models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Learning Regional Monsoon Patterns with a Multimodal Attention U-Net",
    "url": "http://arxiv.org/abs/2509.23267v1",
    "authors": [
      "Swaib Ilias Mazumder",
      "Manish Kumar",
      "Aparajita Khan"
    ],
    "published": "2025-09-27",
    "abstract": "Accurate monsoon rainfall prediction is vital for India's agriculture, water\nmanagement, and climate risk planning, yet remains challenging due to sparse\nground observations and complex regional variability. We present a multimodal\ndeep learning framework for high-resolution precipitation classification that\nleverages satellite and Earth observation data. Unlike previous rainfall\nprediction models based on coarse 5-50 km grids, we curate a new 1 km\nresolution dataset for five Indian states, integrating seven key geospatial\nmodalities: land surface temperature, vegetation (NDVI), soil moisture,\nrelative humidity, wind speed, elevation, and land use, covering the\nJune-September 2024 monsoon season. Our approach uses an attention-guided U-Net\narchitecture to capture spatial patterns and temporal dependencies across\nmodalities, combined with focal and dice loss functions to handle rainfall\nclass imbalance defined by the India Meteorological Department (IMD).\nExperiments demonstrate that our multimodal framework consistently outperforms\nunimodal baselines and existing deep learning methods, especially in extreme\nrainfall categories. This work contributes a scalable framework, benchmark\ndataset, and state-of-the-art results for regional monsoon forecasting, climate\nresilience, and geospatial AI applications in India.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "A Weather Foundation Model for the Power Grid",
    "url": "http://arxiv.org/abs/2509.25268v1",
    "authors": [
      "Cristian Bodnar",
      "Rapha\u00ebl Rousseau-Rizzi",
      "Nikhil Shankar",
      "James Merleau",
      "Stylianos Flampouris",
      "Guillem Candille",
      "Slavica Antic",
      "Fran\u00e7ois Miralles",
      "Jayesh K. Gupta"
    ],
    "published": "2025-09-28",
    "abstract": "Weather foundation models (WFMs) have recently set new benchmarks in global\nforecast skill, yet their concrete value for the weather-sensitive\ninfrastructure that powers modern society remains largely unexplored. In this\nstudy, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting\nTransformer (GFT), on a rich archive of Hydro-Qu\\'ebec asset\nobservations--including transmission-line weather stations, wind-farm met-mast\nstreams, and icing sensors--to deliver hyper-local, asset-level forecasts for\nfive grid-critical variables: surface temperature, precipitation, hub-height\nwind speed, wind-turbine icing risk, and rime-ice accretion on overhead\nconductors. Across 6-72 h lead times, the tailored model surpasses\nstate-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE)\nby 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%.\nMost importantly, it attains an average precision score of 0.72 for day-ahead\nrime-ice detection, a capability absent from existing operational systems,\nwhich affords several hours of actionable warning for potentially catastrophic\noutage events. These results show that WFMs, when post-trained with small\namounts of high-fidelity, can serve as a practical foundation for\nnext-generation grid-resilience intelligence.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Neighbor-aware informal settlement mapping with graph convolutional networks",
    "url": "http://arxiv.org/abs/2509.26171v1",
    "authors": [
      "Thomas Hallopeau",
      "Joris Gu\u00e9rin",
      "Laurent Demagistri",
      "Christovam Barcellos",
      "Nadine Dessay"
    ],
    "published": "2025-09-30",
    "abstract": "Mapping informal settlements is crucial for addressing challenges related to\nurban planning, public health, and infrastructure in rapidly growing cities.\nGeospatial machine learning has emerged as a key tool for detecting and mapping\nthese areas from remote sensing data. However, existing approaches often treat\nspatial units independently, neglecting the relational structure of the urban\nfabric. We propose a graph-based framework that explicitly incorporates local\ngeographical context into the classification process. Each spatial unit (cell)\nis embedded in a graph structure along with its adjacent neighbors, and a\nlightweight Graph Convolutional Network (GCN) is trained to classify whether\nthe central cell belongs to an informal settlement. Experiments are conducted\non a case study in Rio de Janeiro using spatial cross-validation across five\ndistinct zones, ensuring robustness and generalizability across heterogeneous\nurban landscapes. Our method outperforms standard baselines, improving Kappa\ncoefficient by 17 points over individual cell classification. We also show that\ngraph-based modeling surpasses simple feature concatenation of neighboring\ncells, demonstrating the benefit of encoding spatial structure for urban scene\nunderstanding.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Overview of GeoLifeCLEF 2023: Species Composition Prediction with High Spatial Resolution at Continental Scale Using Remote Sensing",
    "url": "http://arxiv.org/abs/2509.25816v1",
    "authors": [
      "Christophe Botella",
      "Benjamin Deneu",
      "Diego Marcos",
      "Maximilien Servajean",
      "Theo Larcher",
      "Cesar Leblanc",
      "Joaquim Estopinan",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2025-09-30",
    "abstract": "Understanding the spatio-temporal distribution of species is a cornerstone of\necology and conservation. By pairing species observations with geographic and\nenvironmental predictors, researchers can model the relationship between an\nenvironment and the species which may be found there. To advance the\nstate-of-the-art in this area with deep learning models and remote sensing\ndata, we organized an open machine learning challenge called GeoLifeCLEF 2023.\nThe training dataset comprised 5 million plant species observations (single\npositive label per sample) distributed across Europe and covering most of its\nflora, high-resolution rasters: remote sensing imagery, land cover, elevation,\nin addition to coarse-resolution data: climate, soil and human footprint\nvariables. In this multi-label classification task, we evaluated models ability\nto predict the species composition in 22 thousand small plots based on\nstandardized surveys. This paper presents an overview of the competition,\nsynthesizes the approaches used by the participating teams, and analyzes the\nmain results. In particular, we highlight the biases faced by the methods\nfitted to single positive labels when it comes to the multi-label evaluation,\nand the new and effective learning strategy combining single and multi-label\ndata in training.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Spatial-Spectral Binarized Neural Network for Panchromatic and Multi-spectral Images Fusion",
    "url": "http://arxiv.org/abs/2509.23321v2",
    "authors": [
      "Yizhen Jiang",
      "Mengting Ma",
      "Anqi Zhu",
      "Xiaowen Ma",
      "Jiaxin Li",
      "Wei Zhang"
    ],
    "published": "2025-09-27",
    "abstract": "Remote sensing pansharpening aims to reconstruct spatial-spectral properties\nduring the fusion of panchromatic (PAN) images and low-resolution\nmulti-spectral (LR-MS) images, finally generating the high-resolution\nmulti-spectral (HR-MS) images. Although deep learning-based models have\nachieved excellent performance, they often come with high computational\ncomplexity, which hinder their applications on resource-limited devices. In\nthis paper, we explore the feasibility of applying the binary neural network\n(BNN) to pan-sharpening. Nevertheless, there are two main issues with\nbinarizing pan-sharpening models: (i) the binarization will cause serious\nspectral distortion due to the inconsistent spectral distribution of the\nPAN/LR-MS images; (ii) the common binary convolution kernel is difficult to\nadapt to the multi-scale and anisotropic spatial features of remote sensing\nobjects, resulting in serious degradation of contours. To address the above\nissues, we design the customized spatial-spectral binarized convolution\n(S2B-Conv), which is composed of the Spectral-Redistribution Mechanism (SRM)\nand Gabor Spatial Feature Amplifier (GSFA). Specifically, SRM employs an affine\ntransformation, generating its scaling and bias parameters through a dynamic\nlearning process. GSFA, which randomly selects different frequencies and angles\nwithin a preset range, enables to better handle multi-scale and-directional\nspatial features. A series of S2B-Conv form a brand-new binary network for\npan-sharpening, dubbed as S2BNet. Extensive quantitative and qualitative\nexperiments have shown our high-efficiency binarized pan-sharpening method can\nattain a promising performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification",
    "url": "http://arxiv.org/abs/2509.23310v1",
    "authors": [
      "Hao Liu",
      "Yongjie Zheng",
      "Yuhan Kang",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-09-27",
    "abstract": "Deep learning-based techniques for the analysis of multimodal remote sensing\ndata have become popular due to their ability to effectively integrate\ncomplementary spatial, spectral, and structural information from different\nsensors. Recently, denoising diffusion probabilistic models (DDPMs) have\nattracted attention in the remote sensing community due to their powerful\nability to capture robust and complex spatial-spectral distributions. However,\npre-training multimodal DDPMs may result in modality imbalance, and effectively\nleveraging diffusion features to guide complementary diversity feature\nextraction remains an open question. To address these issues, this paper\nproposes a balanced diffusion-guided fusion (BDGF) framework that leverages\nmultimodal diffusion features to guide a multi-branch network for land-cover\nclassification. Specifically, we propose an adaptive modality masking strategy\nto encourage the DDPMs to obtain a modality-balanced rather than spectral\nimage-dominated data distribution. Subsequently, these diffusion features\nhierarchically guide feature extraction among CNN, Mamba, and transformer\nnetworks by integrating feature fusion, group channel attention, and\ncross-attention mechanisms. Finally, a mutual learning strategy is developed to\nenhance inter-branch collaboration by aligning the probability entropy and\nfeature similarity of individual subnetworks. Extensive experiments on four\nmultimodal remote sensing datasets demonstrate that the proposed method\nachieves superior classification performance. The code is available at\nhttps://github.com/HaoLiu-XDU/BDGF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
    "url": "http://arxiv.org/abs/2509.26536v1",
    "authors": [
      "Yida Xue",
      "Mingjun Mao",
      "Xiangyuan Ru",
      "Yuqi Zhu",
      "Baochang Ren",
      "Shuofei Qiao",
      "Mengru Wang",
      "Shumin Deng",
      "Xinyu An",
      "Ningyu Zhang",
      "Ying Chen",
      "Huajun Chen"
    ],
    "published": "2025-09-30",
    "abstract": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "EnScale: Temporally-consistent multivariate generative downscaling via proper scoring rules",
    "url": "http://arxiv.org/abs/2509.26258v1",
    "authors": [
      "Maybritt Schillinger",
      "Maxim Samarin",
      "Xinwei Shen",
      "Reto Knutti",
      "Nicolai Meinshausen"
    ],
    "published": "2025-09-30",
    "abstract": "The practical use of future climate projections from global circulation\nmodels (GCMs) is often limited by their coarse spatial resolution, requiring\ndownscaling to generate high-resolution data. Regional climate models (RCMs)\nprovide this refinement, but are computationally expensive. To address this\nissue, machine learning models can learn the downscaling function, mapping\ncoarse GCM outputs to high-resolution fields. Among these, generative\napproaches aim to capture the full conditional distribution of RCM data given\ncoarse-scale GCM data, which is characterized by large variability and thus\nchallenging to model accurately. We introduce EnScale, a generative machine\nlearning framework that emulates the full GCM-to-RCM map by training on\nmultiple pairs of GCM and corresponding RCM data. It first adjusts large-scale\nmismatches between GCM and coarsened RCM data, followed by a super-resolution\nstep to generate high-resolution fields. Both steps employ generative models\noptimized with the energy score, a proper scoring rule. Compared to\nstate-of-the-art ML downscaling approaches, our setup reduces computational\ncost by about one order of magnitude. EnScale jointly emulates multiple\nvariables -- temperature, precipitation, solar radiation, and wind -- spatially\nconsistent over an area in Central Europe. In addition, we propose a variant\nEnScale-t that enables temporally consistent downscaling. We establish a\ncomprehensive evaluation framework across various categories including\ncalibration, spatial structure, extremes, and multivariate dependencies.\nComparison with diverse benchmarks demonstrates EnScale's strong performance\nand computational efficiency. EnScale offers a promising approach for accurate\nand temporally consistent RCM emulation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields",
    "url": "http://arxiv.org/abs/2509.26005v1",
    "authors": [
      "Rui-Yang Zhang",
      "Henry B. Moss",
      "Lachlan Astfalck",
      "Edward Cripps",
      "David S. Leslie"
    ],
    "published": "2025-09-30",
    "abstract": "We introduce a formal active learning methodology for guiding the placement\nof Lagrangian observers to infer time-dependent vector fields -- a key task in\noceanography, marine science, and ocean engineering -- using a physics-informed\nspatio-temporal Gaussian process surrogate model. The majority of existing\nplacement campaigns either follow standard `space-filling' designs or\nrelatively ad-hoc expert opinions. A key challenge to applying principled\nactive learning in this setting is that Lagrangian observers are continuously\nadvected through the vector field, so they make measurements at different\nlocations and times. It is, therefore, important to consider the likely future\ntrajectories of placed observers to account for the utility of candidate\nplacement locations. To this end, we present BALLAST: Bayesian Active Learning\nwith Look-ahead Amendment for Sea-drifter Trajectories. We observe noticeable\nbenefits of BALLAST-aided sequential observer placement strategies on both\nsynthetic and high-fidelity ocean current models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise",
    "url": "http://arxiv.org/abs/2509.25730v1",
    "authors": [
      "Indu Kant Deo",
      "Akash Venkateshwaran",
      "Rajeev K. Jaiman"
    ],
    "published": "2025-09-30",
    "abstract": "Ship traffic is an increasing source of underwater radiated noise in coastal\nwaters, motivating real-time digital twins of ocean acoustics for operational\nnoise mitigation. We present a physics-guided probabilistic framework to\npredict three-dimensional transmission loss in realistic ocean environments. As\na case study, we consider the Salish Sea along shipping routes from the Pacific\nOcean to the Port of Vancouver. A dataset of over 30 million source-receiver\npairs was generated with a Gaussian beam solver across seasonal sound speed\nprofiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We\nfirst assess sparse variational Gaussian processes (SVGP) and then incorporate\nphysics-based mean functions combining spherical spreading with\nfrequency-dependent absorption. To capture nonlinear effects, we examine deep\nsigma-point processes and stochastic variational deep kernel learning. The\nfinal framework integrates four components: (i) a learnable physics-informed\nmean that represents dominant propagation trends, (ii) a convolutional encoder\nfor bathymetry along the source-receiver track, (iii) a neural encoder for\nsource, receiver, and frequency coordinates, and (iv) a residual SVGP layer\nthat provides calibrated predictive uncertainty. This probabilistic digital\ntwin facilitates the construction of sound-exposure bounds and worst-case\nscenarios for received levels. We further demonstrate the application of the\nframework to ship speed optimization, where predicted transmission loss\ncombined with near-field source models provides sound exposure level estimates\nfor minimizing acoustic impacts on marine mammals. The proposed framework\nadvances uncertainty-aware digital twins for ocean acoustics and illustrates\nhow physics-guided machine learning can support sustainable maritime\noperations.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data",
    "url": "http://arxiv.org/abs/2509.25263v1",
    "authors": [
      "Yifang Zhang",
      "Pengfei Duan",
      "Henan Wang",
      "Shengwu Xiong"
    ],
    "published": "2025-09-28",
    "abstract": "Rainfall nowcasting, which aims to predict precipitation within the next 0 to\n3 hours, is critical for disaster mitigation and real-time response planning.\nHowever, most time series forecasting benchmarks in meteorology are evaluated\non variables with strong periodicity, such as temperature and humidity, which\nfail to reflect model capabilities in more complex and practically meteorology\nscenarios like rainfall nowcasting. To address this gap, we propose\nRainfallBench, a benchmark designed for rainfall nowcasting, a highly\nchallenging and practically relevant task characterized by zero inflation,\ntemporal decay, and non-stationarity, focused on predicting precipitation\nwithin the next 0 to 3 hours. The dataset is derived from five years of\nmeteorological observations, recorded at 15-minute intervals across six\nessential variables, and collected from more than 12,000 GNSS stations\nglobally. In particular, it incorporates precipitable water vapor (PWV), a\ncrucial indicator of rainfall that is absent in other datasets. We further\ndesign specialized evaluation strategies to assess model performance on key\nmeteorological challenges, such as multi-scale prediction and extreme rainfall\nevents, and evaluate over 20 state-of-the-art models across six major\narchitectures on RainfallBench. Additionally, to address the zero-inflation and\ntemporal decay issues overlooked by existing models, we introduce Bi-Focus\nPrecipitation Forecaster (BFPF), a plug-and-play module that incorporates\ndomain-specific priors to enhance rainfall time series forecasting. Statistical\nanalysis and ablation studies validate the comprehensiveness of our dataset as\nwell as the superiority of our methodology. Code and datasets are available at\nhttps://anonymous.4open.science/r/RainfallBench-A710.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment",
    "url": "http://arxiv.org/abs/2510.05617v1",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Iffanice Houndayi",
      "Rym Oualha",
      "Mohamed Aziz Cherif",
      "Kobby Panford-Quainoo",
      "Arnu Pretorius"
    ],
    "published": "2025-10-07",
    "abstract": "Open-access multispectral imagery from missions like Landsat 8-9 and\nSentinel-2 has fueled the development of geospatial foundation models (GFMs)\nfor humanitarian and environmental applications. Yet, their deployment remains\nlimited by (i) the absence of automated geospatial data pipelines and (ii) the\nlarge size of fine-tuned models. Existing GFMs lack workflows for processing\nraw satellite imagery, and downstream adaptations often retain the full\ncomplexity of the original encoder.\n  We present InstaGeo, an open-source, end-to-end framework that addresses\nthese challenges by integrating: (1) automated data curation to transform raw\nimagery into model-ready datasets; (2) task-specific model distillation to\nderive compact, compute-efficient models; and (3) seamless deployment as\ninteractive web-map applications. Using InstaGeo, we reproduced datasets from\nthree published studies and trained models with marginal mIoU differences of\n-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for\ndesert locust prediction. The distilled models are up to 8x smaller than\nstandard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal\naccuracy loss.\n  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger\ncrop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp\nimprovement over prior baselines. Moreover, InstaGeo enables users to progress\nfrom raw data to model deployment within a single working day.\n  By unifying data preparation, model compression, and deployment, InstaGeo\ntransforms research-grade GFMs into practical, low-carbon tools for real-time,\nlarge-scale Earth observation. This approach shifts geospatial AI toward data\nquality and application-driven innovation. Source code, datasets, and model\ncheckpoints are available at:\nhttps://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Zephyrus: An Agentic Framework for Weather Science",
    "url": "http://arxiv.org/abs/2510.04017v1",
    "authors": [
      "Sumanth Varambally",
      "Marshall Fisher",
      "Jas Thakker",
      "Yiwei Chen",
      "Zhirui Xia",
      "Yasaman Jafari",
      "Ruijia Niu",
      "Manas Jain",
      "Veeramakali Vignesh Manivannan",
      "Zachary Novack",
      "Luyu Han",
      "Srikar Eranky",
      "Salva R\u00fchling Cachay",
      "Taylor Berg-Kirkpatrick",
      "Duncan Watson-Parris",
      "Yi-An Ma",
      "Rose Yu"
    ],
    "published": "2025-10-05",
    "abstract": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Hyperspectral data augmentation with transformer-based diffusion models",
    "url": "http://arxiv.org/abs/2510.08363v1",
    "authors": [
      "Mattia Ferrari",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-09",
    "abstract": "The introduction of new generation hyperspectral satellite sensors, combined\nwith advancements in deep learning methodologies, has significantly enhanced\nthe ability to discriminate detailed land-cover classes at medium-large scales.\nHowever, a significant challenge in deep learning methods is the risk of\noverfitting when training networks with small labeled datasets. In this work,\nwe propose a data augmentation technique that leverages a guided diffusion\nmodel. To effectively train the model with a limited number of labeled samples\nand to capture complex patterns in the data, we implement a lightweight\ntransformer network. Additionally, we introduce a modified weighted loss\nfunction and an optimized cosine variance scheduler, which facilitate fast and\neffective training on small datasets. We evaluate the effectiveness of the\nproposed method on a forest classification task with 10 different forest types\nusing hyperspectral images acquired by the PRISMA satellite. The results\ndemonstrate that the proposed method outperforms other data augmentation\ntechniques in both average and weighted average accuracy. The effectiveness of\nthe method is further highlighted by the stable training behavior of the model,\nwhich addresses a common limitation in the practical application of deep\ngenerative models for data augmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models",
    "url": "http://arxiv.org/abs/2510.07008v1",
    "authors": [
      "Gianmarco Perantoni",
      "Giulio Weikmann",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-08",
    "abstract": "The temporal consistency of yearly land-cover maps is of great importance to\nmodel the evolution and change of the land cover over the years. In this paper,\nwe focus the attention on a novel approach to classification of yearly\nsatellite image time series (SITS) that combines deep learning with Bayesian\nmodelling, using Hidden Markov Models (HMMs) integrated with Transformer\nEncoder (TE) based DNNs. The proposed approach aims to capture both i)\nintricate temporal correlations in yearly SITS and ii) specific patterns in\nmultiyear crop type sequences. It leverages the cascade classification of an\nHMM layer built on top of the TE, discerning consistent yearly crop-type\nsequences. Validation on a multiyear crop type classification dataset spanning\n47 crop types and six years of Sentinel-2 acquisitions demonstrates the\nimportance of modelling temporal consistency in the predicted labels. HMMs\nenhance the overall performance and F1 scores, emphasising the effectiveness of\nthe proposed approach.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Label-frugal satellite image change detection with generative virtual exemplar learning",
    "url": "http://arxiv.org/abs/2510.06926v1",
    "authors": [
      "Hichem Sahbi"
    ],
    "published": "2025-10-08",
    "abstract": "Change detection is a major task in remote sensing which consists in finding\nall the occurrences of changes in multi-temporal satellite or aerial images.\nThe success of existing methods, and particularly deep learning ones, is\ntributary to the availability of hand-labeled training data that capture the\nacquisition conditions and the subjectivity of the user (oracle). In this\npaper, we devise a novel change detection algorithm, based on active learning.\nThe main contribution of our work resides in a new model that measures how\nimportant is each unlabeled sample, and provides an oracle with only the most\ncritical samples (also referred to as virtual exemplars) for further labeling.\nThese exemplars are generated, using an invertible graph convnet, as the\noptimum of an adversarial loss that (i) measures representativity, diversity\nand ambiguity of the data, and thereby (ii) challenges (the most) the current\nchange detection criteria, leading to a better re-estimate of these criteria in\nthe subsequent iterations of active learning. Extensive experiments show the\npositive impact of our label-efficient learning model against comparative\nmethods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Explaining raw data complexity to improve satellite onboard processing",
    "url": "http://arxiv.org/abs/2510.06858v2",
    "authors": [
      "Adrien Dorise",
      "Marjorie Bellizzi",
      "Adrien Girard",
      "Benjamin Francesconi",
      "St\u00e9phane May"
    ],
    "published": "2025-10-08",
    "abstract": "With increasing processing power, deploying AI models for remote sensing\ndirectly onboard satellites is becoming feasible. However, new constraints\narise, mainly when using raw, unprocessed sensor data instead of preprocessed\nground-based products. While current solutions primarily rely on preprocessed\nsensor images, few approaches directly leverage raw data. This study\ninvestigates the effects of utilising raw data on deep learning models for\nobject detection and classification tasks. We introduce a simulation workflow\nto generate raw-like products from high-resolution L1 imagery, enabling\nsystemic evaluation. Two object detection models (YOLOv11n and YOLOX-S) are\ntrained on both raw and L1 datasets, and their performance is compared using\nstandard detection metrics and explainability tools. Results indicate that\nwhile both models perform similarly at low to medium confidence thresholds, the\nmodel trained on raw data struggles with object boundary identification at high\nconfidence levels. It suggests that adapting AI architectures with improved\ncontouring methods can enhance object detection on raw images, improving\nonboard AI for remote sensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data",
    "url": "http://arxiv.org/abs/2510.05760v1",
    "authors": [
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-07",
    "abstract": "Deep learning has gained broad interest in remote sensing image scene\nclassification thanks to the effectiveness of deep neural networks in\nextracting the semantics from complex data. However, deep networks require\nlarge amounts of training samples to obtain good generalization capabilities\nand are sensitive to errors in the training labels. This is a problem in remote\nsensing since highly reliable labels can be obtained at high costs and in\nlimited amount. However, many sources of less reliable labeled data are\navailable, e.g., obsolete digital maps. In order to train deep networks with\nlarger datasets, we propose both the combination of single or multiple weak\nsources of labeled data with a small but reliable dataset to generate\nmultisource labeled datasets and a novel training strategy where the\nreliability of each source is taken in consideration. This is done by\nexploiting the transition matrices describing the statistics of the errors of\neach source. The transition matrices are embedded into the labels and used\nduring the training process to weigh each label according to the related\nsource. The proposed method acts as a weighting scheme at gradient level, where\neach instance contributes with different weights to the optimization of\ndifferent classes. The effectiveness of the proposed method is validated by\nexperiments on different datasets. The results proved the robustness and\ncapability of leveraging on unreliable source of labels of the proposed method.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images",
    "url": "http://arxiv.org/abs/2510.04916v1",
    "authors": [
      "Giulio Weikmann",
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-06",
    "abstract": "Deep learning has become increasingly important in remote sensing image\nclassification due to its ability to extract semantic information from complex\ndata. Classification tasks often include predefined label hierarchies that\nrepresent the semantic relationships among classes. However, these hierarchies\nare frequently overlooked, and most approaches focus only on fine-grained\nclassification schemes. In this paper, we present a novel Semantics-Aware\nHierarchical Consensus (SAHC) method for learning hierarchical features and\nrelationships by integrating hierarchy-specific classification heads within a\ndeep network architecture, each specialized in different degrees of class\ngranularity. The proposed approach employs trainable hierarchy matrices, which\nguide the network through the learning of the hierarchical structure in a\nself-supervised manner. Furthermore, we introduce a hierarchical consensus\nmechanism to ensure consistent probability distributions across different\nhierarchical levels. This mechanism acts as a weighted ensemble being able to\neffectively leverage the inherent structure of the hierarchical classification\ntask. The proposed SAHC method is evaluated on three benchmark datasets with\ndifferent degrees of hierarchical complexity on different tasks, using distinct\nbackbone architectures to effectively emphasize its adaptability. Experimental\nresults show both the effectiveness of the proposed approach in guiding network\nlearning and the robustness of the hierarchical consensus for remote sensing\nimage classification tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification",
    "url": "http://arxiv.org/abs/2510.04628v1",
    "authors": [
      "Hao Liu",
      "Yunhao Gao",
      "Wei Li",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-06",
    "abstract": "Deep learning-based methods have achieved significant success in remote\nsensing Earth observation data analysis. Numerous feature fusion techniques\naddress multimodal remote sensing image classification by integrating global\nand local features. However, these techniques often struggle to extract\nstructural and detail features from heterogeneous and redundant multimodal\nimages. With the goal of introducing frequency domain learning to model key and\nsparse detail features, this paper introduces the spatial-spectral-frequency\ninteraction network (S$^2$Fin), which integrates pairwise fusion modules across\nthe spatial, spectral, and frequency domains. Specifically, we propose a\nhigh-frequency sparse enhancement transformer that employs sparse\nspatial-spectral attention to optimize the parameters of the high-frequency\nfilter. Subsequently, a two-level spatial-frequency fusion strategy is\nintroduced, comprising an adaptive frequency channel module that fuses\nlow-frequency structures with enhanced high-frequency details, and a\nhigh-frequency resonance mask that emphasizes sharp edges via phase similarity.\nIn addition, a spatial-spectral attention fusion module further enhances\nfeature extraction at intermediate layers of the network. Experiments on four\nbenchmark multimodal datasets with limited labeled data demonstrate that\nS$^2$Fin performs superior classification, outperforming state-of-the-art\nmethods. The code is available at https://github.com/HaoLiu-XDU/SSFin.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks",
    "url": "http://arxiv.org/abs/2510.03725v1",
    "authors": [
      "Thomas Hallopeau",
      "Joris Gu\u00e9rin",
      "Laurent Demagistri",
      "Youssef Fouzai",
      "Renata Gracie",
      "Vanderlei Pascoal De Matos",
      "Helen Gurgel",
      "Nadine Dessay"
    ],
    "published": "2025-10-04",
    "abstract": "While deep learning methods for detecting informal settlements have already\nbeen developed, they have not yet fully utilized the potential offered by\nrecent pretrained neural networks. We compare two types of pretrained neural\nnetworks for detecting the favelas of Rio de Janeiro: 1. Generic networks\npretrained on large diverse datasets of unspecific images, 2. A specialized\nnetwork pretrained on satellite imagery. While the latter is more specific to\nthe target task, the former has been pretrained on significantly more images.\nHence, this research investigates whether task specificity or data volume\nyields superior performance in urban informal settlement detection.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping",
    "url": "http://arxiv.org/abs/2510.06769v1",
    "authors": [
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-08",
    "abstract": "The quantity and the quality of the training labels are central problems in\nhigh-resolution land-cover mapping with machine-learning-based solutions. In\nthis context, weak labels can be gathered in large quantities by leveraging on\nexisting low-resolution or obsolete products. In this paper, we address the\nproblem of training land-cover classifiers using high-resolution imagery (e.g.,\nSentinel-2) and weak low-resolution reference data (e.g., MODIS -derived\nland-cover maps). Inspired by recent works in Deep Multiple Instance Learning\n(DMIL), we propose a method that trains pixel-level multi-class classifiers and\npredicts low-resolution labels (i.e., patch-level classification), where the\nactual high-resolution labels are learned implicitly without direct\nsupervision. This is achieved with flexible pooling layers that are able to\nlink the semantics of the pixels in the high-resolution imagery to the\nlow-resolution reference labels. Then, the Multiple Instance Learning (MIL)\nproblem is re-framed in a multi-class and in a multi-label setting. In the\nformer, the low-resolution annotation represents the majority of the pixels in\nthe patch. In the latter, the annotation only provides us information on the\npresence of one of the land-cover classes in the patch and thus multiple labels\ncan be considered valid for a patch at a time, whereas the low-resolution\nlabels provide us only one label. Therefore, the classifier is trained with a\nPositive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020\nIEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed\nframework compared to standard training strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Do Superpixel Segmentation Methods Influence Deforestation Image Classification?",
    "url": "http://arxiv.org/abs/2510.04645v1",
    "authors": [
      "Hugo Resende",
      "Fabio A. Faria",
      "Eduardo B. Neto",
      "Isabela Borlido",
      "Victor Sundermann",
      "Silvio Jamil F. Guimar\u00e3es",
      "\u00c1lvaro L. Fazenda"
    ],
    "published": "2025-10-06",
    "abstract": "Image segmentation is a crucial step in various visual applications,\nincluding environmental monitoring through remote sensing. In the context of\nthe ForestEyes project, which combines citizen science and machine learning to\ndetect deforestation in tropical forests, image segments are used for labeling\nby volunteers and subsequent model training. Traditionally, the Simple Linear\nIterative Clustering (SLIC) algorithm is adopted as the segmentation method.\nHowever, recent studies have indicated that other superpixel-based methods\noutperform SLIC in remote sensing image segmentation, and might suggest that\nthey are more suitable for the task of detecting deforested areas. In this\nsense, this study investigated the impact of the four best segmentation\nmethods, together with SLIC, on the training of classifiers for the target\napplication. Initially, the results showed little variation in performance\namong segmentation methods, even when selecting the top five classifiers using\nthe PyCaret AutoML library. However, by applying a classifier fusion approach\n(ensemble of classifiers), noticeable improvements in balanced accuracy were\nobserved, highlighting the importance of both the choice of segmentation method\nand the combination of machine learning-based models for deforestation\ndetection tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Zeeman: A Deep Learning Regional Atmospheric Chemistry Transport Model",
    "url": "http://arxiv.org/abs/2510.06140v1",
    "authors": [
      "Mijie Pang",
      "Jianbing Jin",
      "Arjo Segers",
      "Hai Xiang Lin",
      "Guoqiang Wang",
      "Hong Liao",
      "Wei Han"
    ],
    "published": "2025-10-07",
    "abstract": "Atmospheric chemistry encapsulates the emission of various pollutants, the\ncomplex chemistry reactions, and the meteorology dominant transport, which form\na dynamic system that governs air quality. While deep learning (DL) models have\nshown promise in capturing intricate patterns for forecasting individual\natmospheric component - such as PM2.5 and ozone - the critical interactions\namong multiple pollutants and the combined influence of emissions and\nmeteorology are often overlook. This study introduces an advanced DL-based\natmospheric chemistry transport model Zeeman for multi-component atmospheric\nchemistry simulation. Leveraging an attention mechanism, our model effectively\ncaptures the nuanced relationships among these constituents. Performance\nmetrics demonstrate that our approach rivals numerical models, offering an\nefficient solution for atmospheric chemistry. In the future, this model could\nbe further integrated with data assimilation techniques to facilitate efficient\nand accurate atmospheric emission estimation and concentration forecast.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Deep Learning Reconstruction of Tropical Cyclogenesis in the Western North Pacific from Climate Reanalysis Dataset",
    "url": "http://arxiv.org/abs/2510.06118v1",
    "authors": [
      "Duc-Trong Le",
      "Tran-Binh Dang",
      "Anh-Duc Hoang Gia",
      "Duc-Hai Nguyen",
      "Minh-Hoa Tien",
      "Xuan-Truong Ngo",
      "Quang-Trung Luu",
      "Quang-Lap Luu",
      "Tai-Hung Nguyen",
      "Thanh T. N. Nguyen",
      "Chanh Kieu"
    ],
    "published": "2025-10-07",
    "abstract": "This study presents a deep learning (DL) architecture based on residual\nconvolutional neural networks (ResNet) to reconstruct the climatology of\ntropical cyclogenesis (TCG) in the Western North Pacific (WNP) basin from\nclimate reanalysis datasets. Using different TCG data labeling strategies and\ndata enrichment windows for the NASA Modern-Era Retrospective analysis for\nResearch and Applications Version 2 (MERRA2) dataset during the 1980-2020\nperiod, we demonstrate that ResNet can reasonably reproduce the overall TCG\nclimatology in the WNP, capturing both its seasonality and spatial\ndistribution. Our sensitivity analyses and optimizations show that this TCG\nreconstruction depends on both the type of TCG climatology that one wishes to\nreconstruct and the strategies used to label TCG data. Of interest, analyses of\ndifferent input features reveal that DL-based reconstruction of TCG climatology\nneeds only a subset of channels rather than all available data, which is\nconsistent with previous modeling and observational studies of TCG. These\nresults not only enhance our understanding of the TCG process but also provide\na promising pathway for predicting or downscaling TCG climatology based on\nlarge-scale environments from global model forecasts or climate output.\nOverall, our study demonstrates that DL can offer an effective approach for\nstudying TC climatology beyond the traditional physical-based simulations and\nvortex-tracking algorithms used in current climate model analyses.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "ResNet"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "Developing a Sequential Deep Learning Pipeline to Model Alaskan Permafrost Thaw Under Climate Change",
    "url": "http://arxiv.org/abs/2510.06258v1",
    "authors": [
      "Addina Rahaman"
    ],
    "published": "2025-10-05",
    "abstract": "Changing climate conditions threaten the natural permafrost thaw-freeze\ncycle, leading to year-round soil temperatures above 0{\\deg}C. In Alaska, the\nwarming of the topmost permafrost layer, known as the active layer, signals\nelevated greenhouse gas release due to high carbon storage. Accurate soil\ntemperature prediction is therefore essential for risk mitigation and stability\nassessment; however, many existing approaches overlook the numerous factors\ndriving soil thermal dynamics. This study presents a proof-of-concept\nlatitude-based deep learning pipeline for modeling yearly soil temperatures\nacross multiple depths. The framework employs dynamic reanalysis feature data\nfrom the ERA5-Land dataset, static geologic and lithological features,\nsliding-window sequences for seasonal context, a derived scenario signal\nfeature for long-term climate forcing, and latitude band embeddings for spatial\nsensitivity. Five deep learning models were tested: a Temporal Convolutional\nNetwork (TCN), a Transformer, a 1-Dimensional Convolutional Long-Short Term\nMemory (Conv1DLSTM), a Gated-Recurrent Unit (GRU), and a Bidirectional\nLong-Short Term Memory (BiLSTM). Results showed solid recognition of\nlatitudinal and depth-wise temperature discrepancies, with the GRU performing\nbest in sequential temperature pattern detection. Bias-corrected CMIP5 RCP data\nenabled recognition of sinusoidal temperature trends, though limited divergence\nbetween scenarios were observed. This study establishes an end-to-end framework\nfor adopting deep learning in active layer temperature modeling, offering\nseasonal, spatial, and vertical temperature context without intrinsic\nrestrictions on feature selection.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Recognition",
      "Forecast"
    ]
  },
  {
    "title": "Deep learning the sources of MJO predictability: a spectral view of learned features",
    "url": "http://arxiv.org/abs/2510.03582v1",
    "authors": [
      "Lin Yao",
      "Da Yang",
      "James P. C. Duncan",
      "Ashesh Chattopadhyay",
      "Pedram Hassanzadeh",
      "Wahid Bhimji",
      "Bin Yu"
    ],
    "published": "2025-10-04",
    "abstract": "The Madden-Julian oscillation (MJO) is a planetary-scale, intraseasonal\ntropical rainfall phenomenon crucial for global weather and climate; however,\nits dynamics and predictability remain poorly understood. Here, we leverage\ndeep learning (DL) to investigate the sources of MJO predictability, motivated\nby a central difference in MJO theories: which spatial scales are essential for\ndriving the MJO? We first develop a deep convolutional neural network (DCNN) to\nforecast the MJO indices (RMM and ROMI). Our model predicts RMM and ROMI up to\n21 and 33 days, respectively, achieving skills comparable to leading\nsubseasonal-to-seasonal models such as NCEP. To identify the spatial scales\nmost relevant for MJO forecasting, we conduct spectral analysis of the latent\nfeature space and find that large-scale patterns dominate the learned signals.\nAdditional experiments show that models using only large-scale signals as the\ninput have the same skills as those using all the scales, supporting the\nlarge-scale view of the MJO. Meanwhile, we find that small-scale signals remain\ninformative: surprisingly, models using only small-scale input can still\nproduce skillful forecasts up to 1-2 weeks ahead. We show that this is achieved\nby reconstructing the large-scale envelope of the small-scale activities, which\naligns with the multi-scale view of the MJO. Altogether, our findings support\nthat large-scale patterns--whether directly included or reconstructed--may be\nthe primary source of MJO predictability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Beyond the Training Data: Confidence-Guided Mixing of Parameterizations in a Hybrid AI-Climate Model",
    "url": "http://arxiv.org/abs/2510.08107v1",
    "authors": [
      "Helge Heuer",
      "Tom Beucler",
      "Mierk Schwabe",
      "Julien Savre",
      "Manuel Schlund",
      "Veronika Eyring"
    ],
    "published": "2025-10-09",
    "abstract": "Persistent systematic errors in Earth system models (ESMs) arise from\ndifficulties in representing the full diversity of subgrid, multiscale\natmospheric convection and turbulence. Machine learning (ML) parameterizations\ntrained on short high-resolution simulations show strong potential to reduce\nthese errors. However, stable long-term atmospheric simulations with hybrid\n(physics + ML) ESMs remain difficult, as neural networks (NNs) trained offline\noften destabilize online runs. Training convection parameterizations directly\non coarse-grained data is challenging, notably because scales cannot be cleanly\nseparated. This issue is mitigated using data from superparameterized\nsimulations, which provide clearer scale separation. Yet, transferring a\nparameterization from one ESM to another remains difficult due to distribution\nshifts that induce large inference errors. Here, we present a proof-of-concept\nwhere a ClimSim-trained, physics-informed NN convection parameterization is\nsuccessfully transferred to ICON-A. The scheme is (a) trained on adjusted\nClimSim data with subtracted radiative tendencies, and (b) integrated into\nICON-A. The NN parameterization predicts its own error, enabling mixing with a\nconventional convection scheme when confidence is low, thus making the hybrid\nAI-physics model tunable with respect to observations and reanalysis through\nmixing parameters. This improves process understanding by constraining\nconvective tendencies across column water vapor, lower-tropospheric stability,\nand geographical conditions, yielding interpretable regime behavior. In\nAMIP-style setups, several hybrid configurations outperform the default\nconvection scheme (e.g., improved precipitation statistics). With additive\ninput noise during training, both hybrid and pure-ML schemes lead to stable\nsimulations and remain physically consistent for at least 20 years.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Climate Knowledge in Large Language Models",
    "url": "http://arxiv.org/abs/2510.08043v1",
    "authors": [
      "Ivan Kuznetsov",
      "Jacopo Grassi",
      "Dmitrii Pantiukhin",
      "Boris Shapkin",
      "Thomas Jung",
      "Nikolay Koldunov"
    ],
    "published": "2025-10-09",
    "abstract": "Large language models (LLMs) are increasingly deployed for climate-related\napplications, where understanding internal climatological knowledge is crucial\nfor reliability and misinformation risk assessment. Despite growing adoption,\nthe capacity of LLMs to recall climate normals from parametric knowledge\nremains largely uncharacterized. We investigate the capacity of contemporary\nLLMs to recall climate normals without external retrieval, focusing on a\nprototypical query: mean July 2-m air temperature 1991-2020 at specified\nlocations. We construct a global grid of queries at 1{\\deg} resolution land\npoints, providing coordinates and location descriptors, and validate responses\nagainst ERA5 reanalysis. Results show that LLMs encode non-trivial climate\nstructure, capturing latitudinal and topographic patterns, with\nroot-mean-square errors of 3-6 {\\deg}C and biases of $\\pm$1 {\\deg}C. However,\nspatially coherent errors remain, particularly in mountains and high latitudes.\nPerformance degrades sharply above 1500 m, where RMSE reaches 5-13 {\\deg}C\ncompared to 2-4 {\\deg}C at lower elevations. We find that including geographic\ncontext (country, city, region) reduces errors by 27% on average, with larger\nmodels being most sensitive to location descriptors. While models capture the\nglobal mean magnitude of observed warming between 1950-1974 and 2000-2024, they\nfail to reproduce spatial patterns of temperature change, which directly relate\nto assessing climate change. This limitation highlights that while LLMs may\ncapture present-day climate distributions, they struggle to represent the\nregional and local expression of long-term shifts in temperature essential for\nunderstanding climate dynamics. Our evaluation framework provides a\nreproducible benchmark for quantifying parametric climate knowledge in LLMs and\ncomplements existing climate communication assessments.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Climate Model Tuning with Online Synchronization-Based Parameter Estimation",
    "url": "http://arxiv.org/abs/2510.06180v1",
    "authors": [
      "Jordan Seneca",
      "Suzanne Bintanja",
      "Frank M. Selten"
    ],
    "published": "2025-10-07",
    "abstract": "In climate science, the tuning of climate models is a computationally\nintensive problem due to the combination of the high-dimensionality of the\nsystem state and long integration times. Here we demonstrate the potential of a\nparameter estimation algorithm which makes use of synchronization to tune a\nglobal atmospheric model at modest computational costs. We first use it to\ndirectly optimize internal model parameters. We then apply the algorithm to the\nweights of each member of a supermodel ensemble to optimize the overall\npredictions. In both cases, the algorithm is able to find parameters which\nresult in reduced errors in the climatology of the model. Finally, we introduce\na novel approach which combines both methods called adaptive supermodeling,\nwhere the internal parameters of the members of a supermodel are tuned\nsimultaneously with the model weights such that the supermodel predictions are\noptimized. For a case designed to challenge the two previous methods, adaptive\nsupermodeling achieves a performance similar to a perfect model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Representing Subgrid-Scale Cloud Effects in a Radiation Parameterization using Machine Learning: MLe-radiation v1.0",
    "url": "http://arxiv.org/abs/2510.05963v2",
    "authors": [
      "Katharina Hafner",
      "Sara Shamekh",
      "Guillaume Bertoli",
      "Axel Lauer",
      "Robert Pincus",
      "Julien Savre",
      "Veronika Eyring"
    ],
    "published": "2025-10-07",
    "abstract": "Improvements of Machine Learning (ML)-based radiation emulators remain\nconstrained by the underlying assumptions to represent horizontal and vertical\nsubgrid-scale cloud distributions, which continue to introduce substantial\nuncertainties. In this study, we introduce a method to represent the impact of\nsubgrid-scale clouds by applying ML to learn processes from high-resolution\nmodel output with a horizontal grid spacing of 5km. In global storm resolving\nmodels, clouds begin to be explicitly resolved. Coarse-graining these\nhigh-resolution simulations to the resolution of coarser Earth System Models\nyields radiative heating rates that implicitly include subgrid-scale cloud\neffects, without assumptions about their horizontal or vertical distributions.\nWe define the cloud radiative impact as the difference between all-sky and\nclear-sky radiative fluxes, and train the ML component solely on this\ncloud-induced contribution to heating rates. The clear-sky tendencies remain\nbeing computed with a conventional physics-based radiation scheme. This hybrid\ndesign enhances generalization, since the machine-learned part addresses only\nsubgrid-scale cloud effects, while the clear-sky component remains responsive\nto changes in greenhouse gas or aerosol concentrations. Applied to\ncoarse-grained data offline, the ML-enhanced radiation scheme reduces errors by\na factor of 4-10 compared with a conventional coarse-scale radiation scheme.\nThis shows the potential of representing subgrid-scale cloud effects in\nradiation schemes with ML for the next generation of Earth System Models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Mass Conservation on Rails -- Rethinking Physics-Informed Learning of Ice Flow Vector Fields",
    "url": "http://arxiv.org/abs/2510.06286v1",
    "authors": [
      "Kim Bente",
      "Roman Marchant",
      "Fabio Ramos"
    ],
    "published": "2025-10-07",
    "abstract": "To reliably project future sea level rise, ice sheet models require inputs\nthat respect physics. Embedding physical principles like mass conservation into\nmodels that interpolate Antarctic ice flow vector fields from sparse & noisy\nmeasurements not only promotes physical adherence but can also improve accuracy\nand robustness. While physics-informed neural networks (PINNs) impose physics\nas soft penalties, offering flexibility but no physical guarantees, we instead\npropose divergence-free neural networks (dfNNs), which enforce local mass\nconservation exactly via a vector calculus trick. Our comparison of dfNNs,\nPINNs, and unconstrained NNs on ice flux interpolation over Byrd Glacier\nsuggests that \"mass conservation on rails\" yields more reliable estimates, and\nthat directional guidance, a learning strategy leveraging continent-wide\nsatellite velocity data, boosts performance across models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Benchmarking atmospheric circulation variability in an AI emulator, ACE2, and a hybrid model, NeuralGCM",
    "url": "http://arxiv.org/abs/2510.04466v1",
    "authors": [
      "Ian Baxter",
      "Hamid Pahlavan",
      "Pedram Hassanzadeh",
      "Katharine Rucker",
      "Tiffany Shaw"
    ],
    "published": "2025-10-06",
    "abstract": "Physics-based atmosphere-land models with prescribed sea surface temperature\nhave notable successes but also biases in their ability to represent\natmospheric variability compared to observations. Recently, AI emulators and\nhybrid models have emerged with the potential to overcome these biases, but\nstill require systematic evaluation against metrics grounded in fundamental\natmospheric dynamics. Here, we evaluate the representation of four atmospheric\nvariability benchmarking metrics in a fully data-driven AI emulator (ACE2-ERA5)\nand hybrid model (NeuralGCM). The hybrid model and emulator can capture the\nspectra of large-scale tropical waves and extratropical eddy-mean flow\ninteractions, including critical levels. However, both struggle to capture the\ntimescales associated with quasi-biennial oscillation (QBO, $\\sim 28$ months)\nand Southern annular mode propagation ($\\sim 150$ days). These dynamical\nmetrics serve as an initial benchmarking tool to inform AI model development\nand understand their limitations, which may be essential for\nout-of-distribution applications (e.g., extrapolating to unseen climates).",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Score-based generative emulation of impact-relevant Earth system model outputs",
    "url": "http://arxiv.org/abs/2510.04358v1",
    "authors": [
      "Shahine Bouabid",
      "Andre Nogueira Souza",
      "Raffaele Ferrari"
    ],
    "published": "2025-10-05",
    "abstract": "Policy targets evolve faster than the Couple Model Intercomparison Project\ncycles, complicating adaptation and mitigation planning that must often contend\nwith outdated projections. Climate model output emulators address this gap by\noffering inexpensive surrogates that can rapidly explore alternative futures\nwhile staying close to Earth System Model (ESM) behavior. We focus on emulators\ndesigned to provide inputs to impact models. Using monthly ESM fields of\nnear-surface temperature, precipitation, relative humidity, and wind speed, we\nshow that deep generative models have the potential to model jointly the\ndistribution of variables relevant for impacts. The specific model we propose\nuses score-based diffusion on a spherical mesh and runs on a single mid-range\ngraphical processing unit. We introduce a thorough suite of diagnostics to\ncompare emulator outputs with their parent ESMs, including their probability\ndensities, cross-variable correlations, time of emergence, or tail behavior. We\nevaluate performance across three distinct ESMs in both pre-industrial and\nforced regimes. The results show that the emulator produces distributions that\nclosely match the ESM outputs and captures key forced responses. They also\nreveal important failure cases, notably for variables with a strong regime\nshift in the seasonal cycle. Although not a perfect match to the ESM, the\ninaccuracies of the emulator are small relative to the scale of internal\nvariability in ESM projections. We therefore argue that it shows potential to\nbe useful in supporting impact assessment. We discuss priorities for future\ndevelopment toward daily resolution, finer spatial scales, and bias-aware\ntraining. Code is made available at https://github.com/shahineb/climemu.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints",
    "url": "http://arxiv.org/abs/2510.04006v1",
    "authors": [
      "Hang Fan",
      "Yi Xiao",
      "Yongquan Qu",
      "Fenghua Ling",
      "Ben Fei",
      "Lei Bai",
      "Pierre Gentine"
    ],
    "published": "2025-10-05",
    "abstract": "Data-driven machine learning (ML) models have recently shown promise in\nsurpassing traditional physics-based approaches for weather forecasting,\nleading to a so-called second revolution in weather forecasting. However, most\nML-based forecast models treat reanalysis as the truth and are trained under\nvariable-specific loss weighting, ignoring their physical coupling and spatial\nstructure. Over long time horizons, the forecasts become blurry and physically\nunrealistic under rollout training. To address this, we reinterpret model\ntraining as a weak-constraint four-dimensional variational data assimilation\n(WC-4DVar) problem, treating reanalysis data as imperfect observations. This\nallows the loss function to incorporate reanalysis error covariance and capture\nmultivariate dependencies. In practice, we compute the loss in a latent space\nlearned by an autoencoder (AE), where the reanalysis error covariance becomes\napproximately diagonal, thus avoiding the need to explicitly model it in the\nhigh-dimensional model space. We show that rollout training with latent-space\nconstraints improves long-term forecast skill and better preserves fine-scale\nstructures and physical realism compared to training with model-space loss.\nFinally, we extend this framework to accommodate heterogeneous data sources,\nenabling the forecast model to be trained jointly on reanalysis and\nmulti-source observations within a unified theoretical formulation.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  }
]