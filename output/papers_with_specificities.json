[
  {
    "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes",
    "url": "http://arxiv.org/abs/2504.20303v1",
    "authors": [
      "Junlin Guo",
      "James R. Zimmer-Dauphinee",
      "Jordan M. Nieusma",
      "Siqi Lu",
      "Quan Liu",
      "Ruining Deng",
      "Can Cui",
      "Jialin Yue",
      "Yizhe Lin",
      "Tianyuan Yao",
      "Juming Xiong",
      "Junchao Zhu",
      "Chongyu Qu",
      "Yuechen Yang",
      "Mitchell Wilkes",
      "Xiao Wang",
      "Parker VanValkenburgh",
      "Steven A. Wernke",
      "Yuankai Huo"
    ],
    "published": "2025-04-28",
    "abstract": "By mapping sites at large scales using remotely sensed data, archaeologists\ncan generate unique insights into long-term demographic trends, inter-regional\nsocial networks, and past adaptations to climate change. Remote sensing surveys\ncomplement field-based approaches, and their reach can be especially great when\ncombined with deep learning and computer vision techniques. However,\nconventional supervised deep learning methods face challenges in annotating\nfine-grained archaeological features at scale. While recent vision foundation\nmodels have shown remarkable success in learning large-scale remote sensing\ndata with minimal annotations, most off-the-shelf solutions are designed for\nRGB images rather than multi-spectral satellite imagery, such as the 8-band\ndata used in our study. In this paper, we introduce DeepAndes, a\ntransformer-based vision foundation model trained on three million\nmulti-spectral satellite images, specifically tailored for Andean archaeology.\nDeepAndes incorporates a customized DINOv2 self-supervised learning algorithm\noptimized for 8-band multi-spectral imagery, marking the first foundation model\ndesigned explicitly for the Andes region. We evaluate its image understanding\nperformance through imbalanced image classification, image instance retrieval,\nand pixel-level semantic segmentation tasks. Our experiments show that\nDeepAndes achieves superior F1 scores, mean average precision, and Dice scores\nin few-shot learning scenarios, significantly outperforming models trained from\nscratch or pre-trained on smaller datasets. This underscores the effectiveness\nof large-scale self-supervised pre-training in archaeological remote sensing.\nCodes will be available on https://github.com/geopacha/DeepAndes.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis",
    "url": "http://arxiv.org/abs/2504.19223v1",
    "authors": [
      "Alexander Baumann",
      "Leonardo Ayala",
      "Silvia Seidlitz",
      "Jan Sellner",
      "Alexander Studier-Fischer",
      "Berkin \u00d6zdemir",
      "Lena Maier-Hein",
      "Slobodan Ilic"
    ],
    "published": "2025-04-27",
    "abstract": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "JEPA"
    ],
    "applications": []
  },
  {
    "title": "PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data",
    "url": "http://arxiv.org/abs/2504.18770v1",
    "authors": [
      "Manuel Weber",
      "Carly Beneke"
    ],
    "published": "2025-04-26",
    "abstract": "We propose PyViT-FUSE, a foundation model for earth observation data\nexplicitly designed to handle multi-modal imagery by learning to fuse an\narbitrary number of mixed-resolution input bands into a single representation\nthrough an attention mechanism. The learned patch tokens are further processed\nby a stack of vision transformers with a novel pyramidal structure. We train\nthe model on a globally sampled dataset in a self-supervised manner, leveraging\ncore concepts of the SwAV algorithm. We show the interpretability of the fusion\nmechanism by visualization of the attention scores and the models applicability\nto downstream tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology",
    "url": "http://arxiv.org/abs/2504.18256v1",
    "authors": [
      "Elena Plekhanova",
      "Damien Robert",
      "Johannes Dollinger",
      "Emilia Arens",
      "Philipp Brun",
      "Jan Dirk Wegner",
      "Niklaus Zimmermann"
    ],
    "published": "2025-04-25",
    "abstract": "With the exacerbation of the biodiversity and climate crises, macroecological\npursuits such as global biodiversity mapping become more urgent. Remote sensing\noffers a wealth of Earth observation data for ecological studies, but the\nscarcity of labeled datasets remains a major challenge. Recently,\nself-supervised learning has enabled learning representations from unlabeled\ndata, triggering the development of pretrained geospatial models with\ngeneralizable features. However, these models are often trained on datasets\nbiased toward areas of high human activity, leaving entire ecological regions\nunderrepresented. Additionally, while some datasets attempt to address\nseasonality through multi-date imagery, they typically follow calendar seasons\nrather than local phenological cycles. To better capture vegetation seasonality\nat a global scale, we propose a simple phenology-informed sampling strategy and\nintroduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we\ntrain an existing model with a season-contrastive objective. We compare\nrepresentations learned from SSL4Eco against other datasets on diverse\necological downstream tasks and demonstrate that our straightforward sampling\nmethod consistently improves representation quality, highlighting the\nimportance of dataset construction. The model pretrained on SSL4Eco reaches\nstate of the art performance on 7 out of 8 downstream tasks spanning\n(multi-label) classification and regression. We release our code, data, and\nmodel weights to support macroecological and computer vision research at\nhttps://github.com/PlekhanovaElena/ssl4eco.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2504.17397v1",
    "authors": [
      "Francesc Marti-Escofet",
      "Benedikt Blumenstiel",
      "Linus Scheibenreif",
      "Paolo Fraccaro",
      "Konrad Schindler"
    ],
    "published": "2025-04-24",
    "abstract": "Earth observation (EO) is crucial for monitoring environmental changes,\nresponding to disasters, and managing natural resources. In this context,\nfoundation models facilitate remote sensing image analysis to retrieve relevant\ngeoinformation accurately and efficiently. However, as these models grow in\nsize, fine-tuning becomes increasingly challenging due to the associated\ncomputational resources and costs, limiting their accessibility and\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\nfeatures and even degrade model generalization. To address this,\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\nIn this paper, we conduct extensive experiments with various foundation model\narchitectures and PEFT techniques to evaluate their effectiveness on five\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\ninsights into when and how PEFT methods support the adaptation of pre-trained\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\nfull fine-tuning performance and enhance model generalisation to unseen\ngeographic regions, while reducing training time and memory requirements.\nAdditional experiments investigate the effect of architecture choices such as\nthe decoder type or the use of metadata, suggesting UNet decoders and\nfine-tuning without metadata as the recommended configuration. We have\nintegrated all evaluated foundation models and techniques into the open-source\npackage TerraTorch to support quick, scalable, and cost-effective model\nadaptation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "A Genealogy of Multi-Sensor Foundation Models in Remote Sensing",
    "url": "http://arxiv.org/abs/2504.17177v1",
    "authors": [
      "Kevin Lane",
      "Morteza Karimzadeh"
    ],
    "published": "2025-04-24",
    "abstract": "Foundation models have garnered increasing attention for representation\nlearning in remote sensing, primarily adopting approaches that have\ndemonstrated success in computer vision with minimal domain-specific\nmodification. However, the development and application of foundation models in\nthis field are still burgeoning, as there are a variety of competing approaches\nthat each come with significant benefits and drawbacks. This paper examines\nthese approaches along with their roots in the computer vision field in order\nto characterize potential advantages and pitfalls while outlining future\ndirections to further improve remote sensing-specific foundation models. We\ndiscuss the quality of the learned representations and methods to alleviate the\nneed for massive compute resources. We place emphasis on the multi-sensor\naspect of Earth observations, and the extent to which existing approaches\nleverage multiple sensors in training foundation models in relation to\nmulti-modal foundation models. Finally, we identify opportunities for further\nharnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote\nsensing observations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatelliteCalculator: A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion",
    "url": "http://arxiv.org/abs/2504.13442v1",
    "authors": [
      "Zhenyu Yu",
      "Mohd. Yamani Idna Idris",
      "Pei Wang"
    ],
    "published": "2025-04-18",
    "abstract": "Quantitative remote sensing inversion plays a critical role in environmental\nmonitoring, enabling the estimation of key ecological variables such as\nvegetation indices, canopy structure, and carbon stock. Although vision\nfoundation models have achieved remarkable progress in classification and\nsegmentation tasks, their application to physically interpretable regression\nremains largely unexplored. Furthermore, the multi-spectral nature and\ngeospatial heterogeneity of remote sensing data pose significant challenges for\ngeneralization and transferability. To address these issues, we introduce\nSatelliteCalculator, the first vision foundation model tailored for\nquantitative remote sensing inversion. By leveraging physically defined index\nformulas, we automatically construct a large-scale dataset of over one million\npaired samples across eight core ecological indicators. The model integrates a\nfrozen Swin Transformer backbone with a prompt-guided architecture, featuring\ncross-attentive adapters and lightweight task-specific MLP decoders.\nExperiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator\nachieves competitive accuracy across all tasks while significantly reducing\ninference cost. Our results validate the feasibility of applying foundation\nmodels to quantitative inversion, and provide a scalable framework for\ntask-adaptive remote sensing estimation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "SAM-Based Building Change Detection with Distribution-Aware Fourier Adaptation and Edge-Constrained Warping",
    "url": "http://arxiv.org/abs/2504.12619v1",
    "authors": [
      "Yun-Cheng Li",
      "Sen Lei",
      "Yi-Tao Zhao",
      "Heng-Chao Li",
      "Jun Li",
      "Antonio Plaza"
    ],
    "published": "2025-04-17",
    "abstract": "Building change detection remains challenging for urban development, disaster\nassessment, and military reconnaissance. While foundation models like Segment\nAnything Model (SAM) show strong segmentation capabilities, SAM is limited in\nthe task of building change detection due to domain gap issues. Existing\nadapter-based fine-tuning approaches face challenges with imbalanced building\ndistribution, resulting in poor detection of subtle changes and inaccurate edge\nextraction. Additionally, bi-temporal misalignment in change detection,\ntypically addressed by optical flow, remains vulnerable to background noises.\nThis affects the detection of building changes and compromises both detection\naccuracy and edge recognition. To tackle these challenges, we propose a new\nSAM-Based Network with Distribution-Aware Fourier Adaptation and\nEdge-Constrained Warping (FAEWNet) for building change detection. FAEWNet\nutilizes the SAM encoder to extract rich visual features from remote sensing\nimages. To guide SAM in focusing on specific ground objects in remote sensing\nscenes, we propose a Distribution-Aware Fourier Aggregated Adapter to aggregate\ntask-oriented changed information. This adapter not only effectively addresses\nthe domain gap issue, but also pays attention to the distribution of changed\nbuildings. Furthermore, to mitigate noise interference and misalignment in\nheight offset estimation, we design a novel flow module that refines building\nedge extraction and enhances the perception of changed buildings. Our\nstate-of-the-art results on the LEVIR-CD, S2Looking and WHU-CD datasets\nhighlight the effectiveness of FAEWNet. The code is available at\nhttps://github.com/SUPERMAN123000/FAEWNet.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning",
    "url": "http://arxiv.org/abs/2504.11999v1",
    "authors": [
      "Mengyu Wang",
      "Hanbo Bi",
      "Yingchao Feng",
      "Linlin Xin",
      "Shuo Gong",
      "Tianqi Wang",
      "Zhiyuan Yan",
      "Peijin Wang",
      "Wenhui Diao",
      "Xian Sun"
    ],
    "published": "2025-04-16",
    "abstract": "Vision foundation models in remote sensing have been extensively studied due\nto their superior generalization on various downstream tasks. Synthetic\nAperture Radar (SAR) offers all-day, all-weather imaging capabilities,\nproviding significant advantages for Earth observation. However, establishing a\nfoundation model for SAR image interpretation inevitably encounters the\nchallenges of insufficient information utilization and poor interpretability.\nIn this paper, we propose a remote sensing foundation model based on\ncomplex-valued SAR data, which simulates the polarimetric decomposition process\nfor pre-training, i.e., characterizing pixel scattering intensity as a weighted\ncombination of scattering bases and scattering coefficients, thereby endowing\nthe foundation model with physical interpretability. Specifically, we construct\na series of scattering queries, each representing an independent and meaningful\nscattering basis, which interact with SAR features in the scattering query\ndecoder and output the corresponding scattering coefficient. To guide the\npre-training process, polarimetric decomposition loss and power\nself-supervision loss are constructed. The former aligns the predicted\ncoefficients with Yamaguchi coefficients, while the latter reconstructs power\nfrom the predicted coefficients and compares it to the input image's power. The\nperformance of our foundation model is validated on six typical downstream\ntasks, achieving state-of-the-art results. Notably, the foundation model can\nextract stable feature representations and exhibits strong generalization, even\nin data-scarce conditions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization",
    "url": "http://arxiv.org/abs/2504.10727v1",
    "authors": [
      "Darryl Hannan",
      "John Cooper",
      "Dylan White",
      "Timothy Doster",
      "Henry Kvinge",
      "Yijing Watkins"
    ],
    "published": "2025-04-14",
    "abstract": "Multimodal large language models (MLLMs) have altered the landscape of\ncomputer vision, obtaining impressive results across a wide range of tasks,\nespecially in zero-shot settings. Unfortunately, their strong performance does\nnot always transfer to out-of-distribution domains, such as earth observation\n(EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks,\nsuch as image captioning and scene understanding, while failing at tasks that\nrequire more fine-grained spatial reasoning, such as object localization.\nHowever, MLLMs are advancing rapidly and insights quickly become out-dated. In\nthis work, we analyze more recent MLLMs that have been explicitly trained to\ninclude fine-grained spatial reasoning capabilities, benchmarking them on EO\nobject localization tasks. We demonstrate that these models are performant in\ncertain settings, making them well suited for zero-shot scenarios.\nAdditionally, we provide a detailed discussion focused on prompt selection,\nground sample distance (GSD) optimization, and analyzing failure cases. We hope\nthat this work will prove valuable as others evaluate whether an MLLM is well\nsuited for a given EO localization task and how to optimize it.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks",
    "url": "http://arxiv.org/abs/2504.10403v2",
    "authors": [
      "Yan Zhu",
      "Jingyang Zhu",
      "Ting Wang",
      "Yuanming Shi",
      "Chunxiao Jiang",
      "Khaled Ben Letaief"
    ],
    "published": "2025-04-14",
    "abstract": "Advancements in artificial intelligence (AI) and low-earth orbit (LEO)\nsatellites have promoted the application of large remote sensing foundation\nmodels for various downstream tasks. However, direct downloading of these\nmodels for fine-tuning on the ground is impeded by privacy concerns and limited\nbandwidth. Satellite federated learning (FL) offers a solution by enabling\nmodel fine-tuning directly on-board satellites and aggregating model updates\nwithout data downloading. Nevertheless, for large foundation models, the\ncomputational capacity of satellites is insufficient to support effective\non-board fine-tuning in traditional satellite FL frameworks. To address these\nchallenges, we propose a satellite-ground collaborative federated fine-tuning\nframework. The key of the framework lies in how to reasonably decompose and\nallocate model components to alleviate insufficient on-board computation\ncapabilities. During fine-tuning, satellites exchange intermediate results with\nground stations or other satellites for forward propagation and back\npropagation, which brings communication challenges due to the special\ncommunication topology of space transmission networks, such as intermittent\nsatellite-ground communication, short duration of satellite-ground\ncommunication windows, and unstable inter-orbit inter-satellite links (ISLs).\nTo reduce transmission delays, we further introduce tailored communication\nstrategies that integrate both communication and computing resources.\nSpecifically, we propose a parallel intra-orbit communication strategy, a\ntopology-aware satellite-ground communication strategy, and a\nlatency-minimalization inter-orbit communication strategy to reduce space\ncommunication costs. Simulation results demonstrate significant reductions in\ntraining time with improvements of approximately 33%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation",
    "url": "http://arxiv.org/abs/2504.06962v2",
    "authors": [
      "Thomas Kerdreux",
      "Alexandre Tuel",
      "Quentin Febvre",
      "Alexis Mouche",
      "Bertrand Chapron"
    ],
    "published": "2025-04-09",
    "abstract": "Self-supervised learning (SSL) has enabled the development of vision\nfoundation models for Earth Observation (EO), demonstrating strong\ntransferability across diverse remote sensing tasks. While prior work has\nfocused on network architectures and training strategies, the role of dataset\ncuration, especially in balancing and diversifying pre-training datasets,\nremains underexplored. In EO, this challenge is amplified by the redundancy and\nheavy-tailed distributions common in satellite imagery, which can lead to\nbiased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to\nimprove SSL pre-training by maximizing dataset diversity and balance. Our\nmethod iteratively refines the training set without requiring a pre-existing\nfeature extractor, making it well-suited for domains where curated datasets are\nlimited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode\n(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by\nocean observations. We train models from scratch on the entire Sentinel-1 WV\narchive spanning 10 years. Across three downstream tasks, our results show that\ndynamic pruning improves both computational efficiency and representation\nquality, leading to stronger transferability.\n  We also release the weights of OceanSAR-1, the first model in the OceanSAR\nfamily, a series of foundation models for ocean observation and analysis using\nSAR imagery, at github.com/galeio-research/OceanSAR-models/.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation",
    "url": "http://arxiv.org/abs/2504.06220v3",
    "authors": [
      "Xiaoxing Hu",
      "Ziyang Gong",
      "Yupei Wang",
      "Yuru Jia",
      "Gen Luo",
      "Xue Yang"
    ],
    "published": "2025-04-08",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt\npowerful Foundation Models (FMs) to diverse downstream tasks while preserving\nand unleashing their inherent capabilities. However, we have observed that\nexisting PEFT methods, which are often designed with natural imagery in mind,\nstruggle when applied to Remote Sensing (RS) scenarios. This is primarily due\nto their inability to handle artifact influences, a problem particularly severe\nin RS image features. To tackle this challenge, we introduce Earth-Adapter, the\nfirst PEFT method specifically designed for RS artifacts conquering.\nEarth-Adapter introduces a novel Mixture of Frequency Adaptation process that\ncombines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT).\nBy utilizing DFT, Earth-Adapter can decompose features into different frequency\ncomponents, precisely separating artifacts from original features. The MoA then\ndynamically assigns weights to each adapter expert, allowing for the\ncombination of features across various frequency domains. These\nsimple-yet-effective approaches enable Earth-Adapter to more efficiently\novercome the disturbances caused by artifacts than previous PEFT methods,\nsignificantly enhancing the FMs' performance on RS scenarios. Experiments on\nDomain Adaptation (DA), and Domain Generalization (DG) semantic segmentation\nbenchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline\nRein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG\nbenchmarks. Our code will be released at\nhttps://github.com/VisionXLab/Earth-Adapter.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via Eliminate Before Align and Keyword Explicit Reasoning",
    "url": "http://arxiv.org/abs/2504.05644v1",
    "authors": [
      "Yan Zhang",
      "Zhong Ji",
      "Changxu Meng",
      "Yanwei Pang",
      "Jungong Han"
    ],
    "published": "2025-04-08",
    "abstract": "Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR),\nwhich aims at searching for the corresponding targets based on the given query.\nAmong these efforts, the application of Foundation Models (FMs), such as CLIP,\nto the domain of remote sensing has yielded encouraging outcomes. However,\nexisting FM based methodologies neglect the negative impact of weakly\ncorrelated sample pairs and fail to account for the key distinctions among\nremote sensing texts, leading to biased and superficial exploration of sample\npairs. To address these challenges, we propose an approach named iEBAKER (an\nImproved Eliminate Before Align strategy with Keyword Explicit Reasoning\nframework) for RSITR. Specifically, we propose an innovative Eliminate Before\nAlign (EBA) strategy to filter out the weakly correlated sample pairs, thereby\nmitigating their deviations from optimal embedding space during\nalignment.Further, two specific schemes are introduced from the perspective of\nwhether local similarity and global similarity affect each other. On this\nbasis, we introduce an alternative Sort After Reversed Retrieval (SAR)\nstrategy, aims at optimizing the similarity matrix via reverse retrieval.\nAdditionally, we incorporate a Keyword Explicit Reasoning (KER) module to\nfacilitate the beneficial impact of subtle key concept distinctions. Without\nbells and whistles, our approach enables a direct transition from FM to RSITR\ntask, eliminating the need for additional pretraining on remote sensing data.\nExtensive experiments conducted on three popular benchmark datasets demonstrate\nthat our proposed iEBAKER method surpasses the state-of-the-art models while\nrequiring less training data. Our source code will be released at\nhttps://github.com/zhangy0822/iEBAKER.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": []
  },
  {
    "title": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation",
    "url": "http://arxiv.org/abs/2504.03166v1",
    "authors": [
      "Hanbo Bi",
      "Yingchao Feng",
      "Boyuan Tong",
      "Mengyu Wang",
      "Haichen Yu",
      "Yongqiang Mao",
      "Hao Chang",
      "Wenhui Diao",
      "Peijin Wang",
      "Yue Yu",
      "Hanyang Peng",
      "Yehong Zhang",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2025-04-04",
    "abstract": "The rapid advancement of foundation models has revolutionized visual\nrepresentation learning in a self-supervised manner. However, their application\nin remote sensing (RS) remains constrained by a fundamental gap: existing\nmodels predominantly handle single or limited modalities, overlooking the\ninherently multi-modal nature of RS observations. Optical, synthetic aperture\nradar (SAR), and multi-spectral data offer complementary insights that\nsignificantly reduce the inherent ambiguity and uncertainty in single-source\nanalysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS\nfoundation model with 14.7 billion parameters, pre-trained on 400 million\nmulti-modal RS images from nine satellites. RingMoE incorporates three key\ninnovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture\ncomprising modal-specialized, collaborative, and shared experts, effectively\nmodeling intra-modal knowledge while capturing cross-modal dependencies to\nmitigate conflicts between modal representations; (2) Physics-informed\nself-supervised learning, explicitly embedding sensor-specific radiometric\ncharacteristics into the pre-training objectives; (3) Dynamic expert pruning,\nenabling adaptive model compression from 14.7B to 1B parameters while\nmaintaining performance, facilitating efficient deployment in Earth observation\napplications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e.,\nclassification, detection, segmentation, tracking, change detection, and depth\nestimation), RingMoE outperforms existing foundation models and sets new SOTAs,\ndemonstrating remarkable adaptability from single-modal to multi-modal\nscenarios. Beyond theoretical progress, it has been deployed and trialed in\nmultiple sectors, including emergency response, land management, marine\nsciences, and urban planning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Tracking"
    ]
  },
  {
    "title": "FlexiMo: A Flexible Remote Sensing Foundation Model",
    "url": "http://arxiv.org/abs/2503.23844v1",
    "authors": [
      "Xuyang Li",
      "Chenyu Li",
      "Pedram Ghamisi",
      "Danfeng Hong"
    ],
    "published": "2025-03-31",
    "abstract": "The rapid expansion of multi-source satellite imagery drives innovation in\nEarth observation, opening unprecedented opportunities for Remote Sensing\nFoundation Models to harness diverse data. However, many existing models remain\nconstrained by fixed spatial resolutions and patch sizes, limiting their\nability to fully exploit the heterogeneous spatial characteristics inherent in\nsatellite imagery. To address these challenges, we propose FlexiMo, a flexible\nremote sensing foundation model that endows the pre-trained model with the\nflexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a\nspatial resolution-aware module that employs a parameter-free alignment\nembedding mechanism to dynamically recalibrate patch embeddings based on the\ninput image's resolution and dimensions. This design not only preserves\ncritical token characteristics and ensures multi-scale feature fidelity but\nalso enables efficient feature extraction without requiring modifications to\nthe underlying network architecture. In addition, FlexiMo incorporates a\nlightweight channel adaptation module that leverages prior spectral information\nfrom sensors. This mechanism allows the model to process images with varying\nnumbers of channels while maintaining the data's intrinsic physical properties.\nExtensive experiments on diverse multimodal, multi-resolution, and multi-scale\ndatasets demonstrate that FlexiMo significantly enhances model generalization\nand robustness. In particular, our method achieves outstanding performance\nacross a range of downstream tasks, including scene classification, land cover\nclassification, urban building segmentation, and cloud detection. By enabling\nparameter-efficient and physically consistent adaptation, FlexiMo paves the way\nfor more adaptable and effective foundation models in real-world remote sensing\napplications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Efficient Adaptation For Remote Sensing Visual Grounding",
    "url": "http://arxiv.org/abs/2503.23083v1",
    "authors": [
      "Hasan Moughnieh",
      "Mohamad Chalhoub",
      "Hasan Nasrallah",
      "Cristiano Nattero",
      "Paolo Campanella",
      "Ali J. Ghandour"
    ],
    "published": "2025-03-29",
    "abstract": "Foundation models have revolutionized artificial intelligence (AI), offering\nremarkable capabilities across multi-modal domains. Their ability to precisely\nlocate objects in complex aerial and satellite images, using rich contextual\ninformation and detailed object descriptions, is essential for remote sensing\n(RS). These models can associate textual descriptions with object positions\nthrough the Visual Grounding (VG) task, but due to domain-specific challenges,\ntheir direct application to RS produces sub-optimal results. To address this,\nwe applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these\nmodels for RS-specific VG tasks. Specifically, we evaluated LoRA placement\nacross different modules in Grounding DINO and used BitFit and adapters to\nfine-tune the OFA foundation model pre-trained on general-purpose VG datasets.\nThis approach achieved performance comparable to or surpassing current State Of\nThe Art (SOTA) models while significantly reducing computational costs. This\nstudy highlights the potential of PEFT techniques to advance efficient and\nprecise multi-modal analysis in RS, offering a practical and cost-effective\nalternative to full model training.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery",
    "url": "http://arxiv.org/abs/2503.22516v1",
    "authors": [
      "Samira Alkaee Taleghan",
      "Morteza Karimzadeh",
      "Andrew P. Barrett",
      "Walter N. Meier",
      "Farnoush Banaei-Kashani"
    ],
    "published": "2025-03-28",
    "abstract": "Accurate segmentation of sea ice types is essential for mapping and\noperational forecasting of sea ice conditions for safe navigation and resource\nextraction in ice-covered waters, as well as for understanding polar climate\nprocesses. While deep learning methods have shown promise in automating sea ice\nsegmentation, they often rely on extensive labeled datasets which require\nexpert knowledge and are time-consuming to create. Recently, foundation models\n(FMs) have shown excellent results for segmenting remote sensing images by\nutilizing pre-training on large datasets using self-supervised techniques.\nHowever, their effectiveness for sea ice segmentation remains unexplored,\nespecially given sea ice's complex structures, seasonal changes, and unique\nspectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagery\ncharacteristics including banding and scalloping noise, and varying ice\nbackscatter characteristics, which are often missing in standard remote sensing\npre-training datasets. In particular, SAR images over polar regions are\nacquired using different modes than used to capture the images at lower\nlatitudes by the same sensors that form training datasets for FMs. This study\nevaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1\nSAR imagery, focusing on their seasonal and spatial generalization. Among the\nselected models, Prithvi-600M outperforms the baseline models, while CROMA\nachieves a very similar performance in F1-score. Our contributions include\noffering a systematic methodology for selecting FMs for sea ice data analysis,\na comprehensive benchmarking study on performances of FMs for sea ice\nsegmentation with tailored performance metrics, and insights into existing gaps\nand future directions for improving domain-specific models in polar\napplications using SAR data.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "A Survey on Remote Sensing Foundation Models: From Vision to Multimodality",
    "url": "http://arxiv.org/abs/2503.22081v1",
    "authors": [
      "Ziyue Huang",
      "Hongxi Yan",
      "Qiqi Zhan",
      "Shuai Yang",
      "Mingming Zhang",
      "Chenkai Zhang",
      "YiMing Lei",
      "Zeming Liu",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "published": "2025-03-28",
    "abstract": "The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2503.21841v1",
    "authors": [
      "Jingtao Li",
      "Yingyi Liu",
      "Xinyu Wang",
      "Yunning Peng",
      "Chen Sun",
      "Shaoyu Wang",
      "Zhendong Sun",
      "Tian Ke",
      "Xiao Jiang",
      "Tangwei Lu",
      "Anran Zhao",
      "Yanfei Zhong"
    ],
    "published": "2025-03-27",
    "abstract": "Advanced interpretation of hyperspectral remote sensing images benefits many\nprecise Earth observation tasks. Recently, visual foundation models have\npromoted the remote sensing interpretation but concentrating on RGB and\nmultispectral images. Due to the varied hyperspectral channels,existing\nfoundation models would face image-by-image tuning situation, imposing great\npressure on hardware and time resources. In this paper, we propose a\ntuning-free hyperspectral foundation model called HyperFree, by adapting the\nexisting visual prompt engineering. To process varied channel numbers, we\ndesign a learned weight dictionary covering full-spectrum from $0.4 \\sim 2.5 \\,\n\\mu\\text{m}$, supporting to build the embedding layer dynamically. To make the\nprompt design more tractable, HyperFree can generate multiple semantic-aware\nmasks for one prompt by treating feature distance as semantic-similarity. After\npre-training HyperFree on constructed large-scale high-resolution hyperspectral\nimages, HyperFree (1 prompt) has shown comparable results with specialized\nmodels (5 shots) on 5 tasks and 11 datasets.Code and dataset are accessible at\nhttps://rsidea.whu.edu.cn/hyperfree.htm.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text",
    "url": "http://arxiv.org/abs/2503.19311v1",
    "authors": [
      "Weizhi Chen",
      "Jingbo Chen",
      "Yupeng Deng",
      "Jiansheng Chen",
      "Yuman Feng",
      "Zhihao Xi",
      "Diyou Liu",
      "Kai Li",
      "Yu Meng"
    ],
    "published": "2025-03-25",
    "abstract": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "CLIP"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2503.18540v1",
    "authors": [
      "Guneet Mutreja",
      "Philipp Schuegraf",
      "Ksenia Bittner"
    ],
    "published": "2025-03-24",
    "abstract": "Recent advances in self-supervised learning have led to the development of\nfoundation models that have significantly advanced performance in various\ncomputer vision tasks. However, despite their potential, these models often\noverlook the crucial role of high-resolution digital surface models (DSMs) in\nunderstanding urban environments, particularly for building-level analysis,\nwhich is essential for applications like digital twins. To address this gap, we\nintroduce HiRes-FusedMIM, a novel pre-trained model specifically designed to\nleverage the rich information contained within high-resolution RGB and DSM\ndata. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling\n(SimMIM) architecture with a multi-objective loss function that combines\nreconstruction and contrastive objectives, enabling it to learn powerful, joint\nrepresentations from both modalities. We conducted a comprehensive evaluation\nof HiRes-FusedMIM on a diverse set of downstream tasks, including\nclassification, semantic segmentation, and instance segmentation. Our results\ndemonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art\ngeospatial methods on several building-related datasets, including WHU Aerial\nand LoveDA, demonstrating its effectiveness in capturing and leveraging\nfine-grained building information; 2) Incorporating DSMs during pre-training\nconsistently improves performance compared to using RGB data alone,\nhighlighting the value of elevation information for building-level analysis; 3)\nThe dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB\nand DSM data, significantly outperforms a single-encoder model on the Vaihingen\nsegmentation task, indicating the benefits of learning specialized\nrepresentations for each modality. To facilitate further research and\napplications in this direction, we will publicly release the trained model\nweights.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations",
    "url": "http://arxiv.org/abs/2503.16683v1",
    "authors": [
      "Zeping Liu",
      "Fan Zhang",
      "Junfeng Jiao",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published": "2025-03-20",
    "abstract": "Advancements in vision and language foundation models have inspired the\ndevelopment of geo-foundation models (GeoFMs), enhancing performance across\ndiverse geospatial tasks. However, many existing GeoFMs primarily focus on\noverhead remote sensing (RS) data while neglecting other data modalities such\nas ground-level imagery. A key challenge in multimodal GeoFM development is to\nexplicitly model geospatial relationships across modalities, which enables\ngeneralizability across tasks, spatial scales, and temporal contexts. To\naddress these limitations, we propose GAIR, a novel multimodal GeoFM\narchitecture integrating overhead RS data, street view (SV) imagery, and their\ngeolocation metadata. We utilize three factorized neural encoders to project an\nSV image, its geolocation, and an RS image into the embedding space. The SV\nimage needs to be located within the RS image's spatial footprint but does not\nneed to be at its geographic center. In order to geographically align the SV\nimage and RS image, we propose a novel implicit neural representations (INR)\nmodule that learns a continuous RS image representation and looks up the RS\nembedding at the SV image's geolocation. Next, these geographically aligned SV\nembedding, RS embedding, and location embedding are trained with contrastive\nlearning objectives from unlabeled data. We evaluate GAIR across 10 geospatial\ntasks spanning RS image-based, SV image-based, and location embedding-based\nbenchmarks. Experimental results demonstrate that GAIR outperforms\nstate-of-the-art GeoFMs and other strong baselines, highlighting its\neffectiveness in learning generalizable and transferable geospatial\nrepresentations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding",
    "url": "http://arxiv.org/abs/2503.16426v1",
    "authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Bowen Chen",
      "Wenyuan Li",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-03-20",
    "abstract": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Falcon: A Remote Sensing Vision-Language Foundation Model",
    "url": "http://arxiv.org/abs/2503.11070v1",
    "authors": [
      "Kelu Yao",
      "Nuo Xu",
      "Rong Yang",
      "Yingying Xu",
      "Zhuoyan Gao",
      "Titinunt Kitrungrotsakul",
      "Yi Ren",
      "Pu Zhang",
      "Jin Wang",
      "Ning Wei",
      "Chao Li"
    ],
    "published": "2025-03-14",
    "abstract": "This paper introduces a holistic vision-language foundation model tailored\nfor remote sensing, named Falcon. Falcon offers a unified, prompt-based\nparadigm that effectively executes comprehensive and complex remote sensing\ntasks. Falcon demonstrates powerful understanding and reasoning abilities at\nthe image, region, and pixel levels. Specifically, given simple natural\nlanguage instructions and remote sensing images, Falcon can produce impressive\nresults in text form across 14 distinct tasks, i.e., image classification,\nobject detection, segmentation, image captioning, and etc. To facilitate\nFalcon's training and empower its representation capacity to encode rich\nspatial and semantic information, we developed Falcon_SFT, a large-scale,\nmulti-task, instruction-tuning dataset in the field of remote sensing. The\nFalcon_SFT dataset consists of approximately 78 million high-quality data\nsamples, covering 5.6 million multi-spatial resolution and multi-view remote\nsensing images with diverse instructions. It features hierarchical annotations\nand undergoes manual sampling verification to ensure high data quality and\nreliability. Extensive comparative experiments are conducted, which verify that\nFalcon achieves remarkable performance over 67 datasets and 14 tasks, despite\nhaving only 0.7B parameters. We release the complete dataset, code, and model\nweights at https://github.com/TianHuiLab/Falcon, hoping to help further develop\nthe open-source community.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Towards Privacy-preserved Pre-training of Remote Sensing Foundation Models with Federated Mutual-guidance Learning",
    "url": "http://arxiv.org/abs/2503.11051v1",
    "authors": [
      "Jieyi Tan",
      "Chengwei Zhang",
      "Bo Dang",
      "Yansheng Li"
    ],
    "published": "2025-03-14",
    "abstract": "Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with a\ndata-centralized paradigm, through self-supervision on large-scale curated\nremote sensing data. For each institution, however, pre-training RSFMs with\nlimited data in a standalone manner may lead to suboptimal performance, while\naggregating remote sensing data from multiple institutions for centralized\npre-training raises privacy concerns. Seeking for collaboration is a promising\nsolution to resolve this dilemma, where multiple institutions can\ncollaboratively train RSFMs without sharing private data. In this paper, we\npropose a novel privacy-preserved pre-training framework (FedSense), which\nenables multiple institutions to collaboratively train RSFMs without sharing\nprivate data. However, it is a non-trivial task hindered by a vicious cycle,\nwhich results from model drift by remote sensing data heterogeneity and high\ncommunication overhead. To break this vicious cycle, we introduce Federated\nMutual-guidance Learning. Specifically, we propose a Server-to-Clients Guidance\n(SCG) mechanism to guide clients updates towards global-flatness optimal\nsolutions. Additionally, we propose a Clients-to-Server Guidance (CSG)\nmechanism to inject local knowledge into the server by low-bit communication.\nExtensive experiments on four downstream tasks demonstrate the effectiveness of\nour FedSense in both full-precision and communication-reduced scenarios,\nshowcasing remarkable communication efficiency and performance gains.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing",
    "url": "http://arxiv.org/abs/2503.10392v1",
    "authors": [
      "Fengxiang Wang",
      "Hongzhen Wang",
      "Yulin Wang",
      "Di Wang",
      "Mingshuo Chen",
      "Haiyan Zhao",
      "Yangang Sun",
      "Shuo Wang",
      "Long Lan",
      "Wenjing Yang",
      "Jing Zhang"
    ],
    "published": "2025-03-13",
    "abstract": "Recent advances in self-supervised learning for Vision Transformers (ViTs)\nhave fueled breakthroughs in remote sensing (RS) foundation models. However,\nthe quadratic complexity of self-attention poses a significant barrier to\nscalability, particularly for large models and high-resolution images. While\nthe linear-complexity Mamba architecture offers a promising alternative,\nexisting RS applications of Mamba remain limited to supervised tasks on small,\ndomain-specific datasets. To address these challenges, we propose RoMA, a\nframework that enables scalable self-supervised pretraining of Mamba-based RS\nfoundation models using large-scale, diverse, unlabeled data. RoMA enhances\nscalability for high-resolution images through a tailored auto-regressive\nlearning strategy, incorporating two key innovations: 1) a rotation-aware\npretraining mechanism combining adaptive cropping with angular embeddings to\nhandle sparsely distributed objects with arbitrary orientations, and 2)\nmulti-scale token prediction objectives that address the extreme variations in\nobject scales inherent to RS imagery. Systematic empirical studies validate\nthat Mamba adheres to RS data and parameter scaling laws, with performance\nscaling reliably as model and data size increase. Furthermore, experiments\nacross scene classification, object detection, and semantic segmentation tasks\ndemonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based\ncounterparts in both accuracy and computational efficiency. The source code and\npretrained models will be released at https://github.com/MiliLab/RoMA.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Light-weighted foundation model for seismic data processing based on representative and non-redundant pre-training dataset",
    "url": "http://arxiv.org/abs/2503.10092v1",
    "authors": [
      "Xintong Dong",
      "Wenshuo Yu",
      "Jun Lin",
      "Zhenbo Guo",
      "Hongzhou Wang",
      "Jianhao Yang"
    ],
    "published": "2025-03-13",
    "abstract": "In the fields of computer vision (CV) and remote sensing (RS), foundational\nmodels typically follow the \"big data + large model parameters\" paradigm.\nHowever, the application of this strategy in seismic data processing faces\nseveral challenges: seismic data is difficult to obtain and the scarcity of\npublicly available datasets make it difficult to construct large-scale\ndatasets. Additionally, the high computational cost associated with a large\nnumber of model parameters restricts widespread research in this domain.\nTherefore, we propose a lightweight seismic processing foundational model\nparadigm (SPFM), which aims to overcome the limitations of traditional methods\nby data engineering and network architecture innovation. Specifically, we\npropose an innovative dataset construction strategy that generates more seismic\ndata by data augmentation techniques, including collecting publicly available\nfield data and using generative diffusion models (GDM) for data enhancement.\nFurthermore, we optimize the data distribution by employing dimensionality\nreduction, cluster analysis, and stratified sampling methods, reducing\nredundant information while preserving important seismic features, thus\nconstructing a comprehensive dataset. In terms of network architecture design,\nwe introduce the selective structured state-space model (Mamba) structure,\nwhich effectively captures global features of seismic data and alleviates the\nquadratic growth of computational complexity inherent in Transformer-based\nmodels, thereby improving computational efficiency. This model, pre-trained\nwith only four A800 GPUs, outperforms traditional methods across multiple\ntasks, including denoising, interpolation, frequency-band extrapolation, and\nresolution enhancement. The lightweight paradigm provides an solution for\nseismic data processing, advancing the generalization and accessibility of\nseismic data processing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Diffusion Models"
    ],
    "applications": []
  },
  {
    "title": "Isolated Channel Vision Transformers: From Single-Channel Pretraining to Multi-Channel Finetuning",
    "url": "http://arxiv.org/abs/2503.09826v1",
    "authors": [
      "Wenyi Lian",
      "Joakim Lindblad",
      "Patrick Micke",
      "Nata\u0161a Sladoje"
    ],
    "published": "2025-03-12",
    "abstract": "Vision Transformers (ViTs) have achieved remarkable success in standard RGB\nimage processing tasks. However, applying ViTs to multi-channel imaging (MCI)\ndata, e.g., for medical and remote sensing applications, remains a challenge.\nIn particular, MCI data often consist of layers acquired from different\nmodalities. Directly training ViTs on such data can obscure complementary\ninformation and impair the performance. In this paper, we introduce a simple\nyet effective pretraining framework for large-scale MCI datasets. Our method,\nnamed Isolated Channel ViT (IC-ViT), patchifies image channels individually and\nthereby enables pretraining for multimodal multi-channel tasks. We show that\nthis channel-wise patchifying is a key technique for MCI processing. More\nimportantly, one can pretrain the IC-ViT on single channels and finetune it on\ndownstream multi-channel datasets. This pretraining framework captures\ndependencies between patches as well as channels and produces robust feature\nrepresentation. Experiments on various tasks and benchmarks, including JUMP-CP\nand CHAMMI for cell microscopy imaging, and So2Sat-LCZ42 for satellite imaging,\nshow that the proposed IC-ViT delivers 4-14 percentage points of performance\nimprovement over existing channel-adaptive approaches. Further, its efficient\ntraining makes it a suitable candidate for large-scale pretraining of\nfoundation models on heterogeneous data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Visual and Text Prompt Segmentation: A Novel Multi-Model Framework for Remote Sensing",
    "url": "http://arxiv.org/abs/2503.07911v1",
    "authors": [
      "Xing Zi",
      "Kairui Jin",
      "Xian Tao",
      "Jun Li",
      "Ali Braytee",
      "Rajiv Ratn Shah",
      "Mukesh Prasad"
    ],
    "published": "2025-03-10",
    "abstract": "Pixel-level segmentation is essential in remote sensing, where foundational\nvision models like CLIP and Segment Anything Model(SAM) have demonstrated\nsignificant capabilities in zero-shot segmentation tasks. Despite their\nadvances, challenges specific to remote sensing remain substantial. Firstly,\nThe SAM without clear prompt constraints, often generates redundant masks, and\nmaking post-processing more complex. Secondly, the CLIP model, mainly designed\nfor global feature alignment in foundational models, often overlooks local\nobjects crucial to remote sensing. This oversight leads to inaccurate\nrecognition or misplaced focus in multi-target remote sensing imagery. Thirdly,\nboth models have not been pre-trained on multi-scale aerial views, increasing\nthe likelihood of detection failures. To tackle these challenges, we introduce\nthe innovative VTPSeg pipeline, utilizing the strengths of Grounding DINO,\nCLIP, and SAM for enhanced open-vocabulary image segmentation. The Grounding\nDINO+(GD+) module generates initial candidate bounding boxes, while the CLIP\nFilter++(CLIP++) module uses a combination of visual and textual prompts to\nrefine and filter out irrelevant object bounding boxes, ensuring that only\npertinent objects are considered. Subsequently, these refined bounding boxes\nserve as specific prompts for the FastSAM model, which executes precise\nsegmentation. Our VTPSeg is validated by experimental and ablation study\nresults on five popular remote sensing image segmentation datasets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?",
    "url": "http://arxiv.org/abs/2503.07890v1",
    "authors": [
      "Yuru Jia",
      "Valerio Marsocci",
      "Ziyang Gong",
      "Xue Yang",
      "Maarten Vergauwen",
      "Andrea Nascetti"
    ],
    "published": "2025-03-10",
    "abstract": "Self-supervised learning (SSL) has revolutionized representation learning in\nRemote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage\nvast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs\nprimarily focus on discriminative objectives, such as contrastive learning or\nmasked image modeling, owing to their proven success in learning transferable\nrepresentations. However, generative diffusion models--which demonstrate the\npotential to capture multi-grained semantics essential for RS tasks during\nimage generation--remain underexplored for discriminative applications. This\nprompts the question: can generative diffusion models also excel and serve as\nGFMs with sufficient discriminative power? In this work, we answer this\nquestion with SatDiFuser, a framework that transforms a diffusion-based\ngenerative geospatial foundation model into a powerful pretraining tool for\ndiscriminative RS. By systematically analyzing multi-stage, noise-dependent\ndiffusion features, we develop three fusion strategies to effectively leverage\nthese diverse representations. Extensive experiments on remote sensing\nbenchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving\ngains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in\nclassification, demonstrating the capacity of diffusion-based generative\nfoundation models to rival or exceed discriminative GFMs. Code will be\nreleased.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Image Generation"
    ]
  },
  {
    "title": "A Recipe for Improving Remote Sensing VLM Zero Shot Generalization",
    "url": "http://arxiv.org/abs/2503.08722v2",
    "authors": [
      "Aviad Barzilai",
      "Yotam Gigi",
      "Amr Helmy",
      "Vered Silverman",
      "Yehonathan Refael",
      "Bolous Jaber",
      "Tomer Shekel",
      "George Leifman",
      "Genady Beryozkin"
    ],
    "published": "2025-03-10",
    "abstract": "Foundation models have had a significant impact across various AI\napplications, enabling use cases that were previously impossible. Contrastive\nVisual Language Models (VLMs), in particular, have outperformed other\ntechniques in many tasks. However, their prevalence in remote sensing (RS) is\nstill limited, due to the scarcity of diverse remote-sensing visual-language\ndatasets. In this work we introduce two novel image-caption datasets for\ntraining of remote sensing foundation models. The first dataset pairs aerial\nand satellite imagery with captions generated by Gemini using landmarks\nextracted from Google Maps. The second dataset utilizes public web images and\ntheir corresponding alt-text, filtered for the remote sensing domain, resulting\nin a diverse dataset with greater breadth in image styles and subject matter.\nThese datasets are used to pre-train the\nMaMMUT~\\citep{kuo2023mammutsimplearchitecturejoint} VLM architecture, resulting\nin state-of-the-art generalization performance in zero-shot cross-modal\nretrieval on well-known public benchmarks. Finally, we present our ongoing\nresearch to distill image-level knowledge gained in the VLM contrastive\ntraining procedure to enhance the model's localization ability. Specifically,\nwe iteratively generate pseudo-labels for image regions based on the model's\nattention maps and use these labels for further training. To mitigate noisy\nattention maps and create robust segmentation masks, we introduce a novel\nattention-pooling mechanism called the Smooth-Attention-Operation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A General Purpose Spectral Foundational Model for Both Proximal and Remote Sensing Spectral Imaging",
    "url": "http://arxiv.org/abs/2503.01628v1",
    "authors": [
      "William Michael Laprade",
      "Jesper Cairo Westergaard",
      "Svend Christensen",
      "Mads Nielsen",
      "Anders Bjorholm Dahl"
    ],
    "published": "2025-03-03",
    "abstract": "Spectral imaging data acquired via multispectral and hyperspectral cameras\ncan have hundreds of channels, where each channel records the reflectance at a\nspecific wavelength and bandwidth. Time and resource constraints limit our\nability to collect large spectral datasets, making it difficult to build and\ntrain predictive models from scratch. In the RGB domain, we can often alleviate\nsome of the limitations of smaller datasets by using pretrained foundational\nmodels as a starting point. However, most existing foundation models are\npretrained on large datasets of 3-channel RGB images, severely limiting their\neffectiveness when used with spectral imaging data. The few spectral foundation\nmodels that do exist usually have one of two limitations: (1) they are built\nand trained only on remote sensing data limiting their application in proximal\nspectral imaging, (2) they utilize the more widely available multispectral\nimaging datasets with less than 15 channels restricting their use with\nhundred-channel hyperspectral images. To alleviate these issues, we propose a\nlarge-scale foundational model and dataset built upon the masked autoencoder\narchitecture that takes advantage of spectral channel encoding,\nspatial-spectral masking and ImageNet pretraining for an adaptable and robust\nmodel for downstream spectral imaging tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking",
    "url": "http://arxiv.org/abs/2502.18748v1",
    "authors": [
      "Shaheer Mohamed",
      "Tharindu Fernando",
      "Sridha Sridharan",
      "Peyman Moghadam",
      "Clinton Fookes"
    ],
    "published": "2025-02-26",
    "abstract": "Hyperspectral object tracking using snapshot mosaic cameras is emerging as it\nprovides enhanced spectral information alongside spatial data, contributing to\na more comprehensive understanding of material properties. Using transformers,\nwhich have consistently outperformed convolutional neural networks (CNNs) in\nlearning better feature representations, would be expected to be effective for\nHyperspectral object tracking. However, training large transformers\nnecessitates extensive datasets and prolonged training periods. This is\nparticularly critical for complex tasks like object tracking, and the scarcity\nof large datasets in the hyperspectral domain acts as a bottleneck in achieving\nthe full potential of powerful transformer models. This paper proposes an\neffective methodology that adapts large pretrained transformer-based foundation\nmodels for hyperspectral object tracking. We propose an adaptive, learnable\nspatial-spectral token fusion module that can be extended to any\ntransformer-based backbone for learning inherent spatial-spectral features in\nhyperspectral data. Furthermore, our model incorporates a cross-modality\ntraining pipeline that facilitates effective learning across hyperspectral\ndatasets collected with different sensor modalities. This enables the\nextraction of complementary knowledge from additional modalities, whether or\nnot they are present during testing. Our proposed model also achieves superior\nperformance with minimal training iterations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching",
    "url": "http://arxiv.org/abs/2502.18104v1",
    "authors": [
      "Han Nie",
      "Bin Luo",
      "Jun Liu",
      "Zhitao Fu",
      "Huan Zhou",
      "Shuo Zhang",
      "Weixing Liu"
    ],
    "published": "2025-02-25",
    "abstract": "The ideal goal of image matching is to achieve stable and efficient\nperformance in unseen domains. However, many existing learning-based\noptical-SAR image matching methods, despite their effectiveness in specific\nscenarios, exhibit limited generalization and struggle to adapt to practical\napplications. Repeatedly training or fine-tuning matching models to address\ndomain differences is not only not elegant enough but also introduces\nadditional computational overhead and data production costs. In recent years,\ngeneral foundation models have shown great potential for enhancing\ngeneralization. However, the disparity in visual domains between natural and\nremote sensing images poses challenges for their direct application. Therefore,\neffectively leveraging foundation models to improve the generalization of\noptical-SAR image matching remains challenge. To address the above challenges,\nwe propose PromptMID, a novel approach that constructs modality-invariant\ndescriptors using text prompts based on land use classification as priors\ninformation for optical and SAR image matching. PromptMID extracts multi-scale\nmodality-invariant features by leveraging pre-trained diffusion models and\nvisual foundation models (VFMs), while specially designed feature aggregation\nmodules effectively fuse features across different granularities. Extensive\nexperiments on optical-SAR image datasets from four diverse regions demonstrate\nthat PromptMID outperforms state-of-the-art matching methods, achieving\nsuperior results in both seen and unseen domains and exhibiting strong\ncross-domain generalization capabilities. The source code will be made publicly\navailable https://github.com/HanNieWHU/PromptMID.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "CARE: Confidence-Aware Regression Estimation of building density fine-tuning EO Foundation Models",
    "url": "http://arxiv.org/abs/2502.13734v2",
    "authors": [
      "Nikolaos Dionelis",
      "Jente Bosmans",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-02-19",
    "abstract": "Performing accurate confidence quantification and assessment in pixel-wise\nregression tasks, which are downstream applications of AI Foundation Models for\nEarth Observation (EO), is important for deep neural networks to predict their\nfailures, improve their performance and enhance their capabilities in\nreal-world applications, for their practical deployment. For pixel-wise\nregression tasks, specifically utilizing remote sensing data from satellite\nimagery in EO Foundation Models, confidence quantification is a critical\nchallenge. The focus of this research work is on developing a Foundation Model\nusing EO satellite data that computes and assigns a confidence metric alongside\nregression outputs to improve the reliability and interpretability of\npredictions generated by deep neural networks. To this end, we develop, train\nand evaluate the proposed Confidence-Aware Regression Estimation (CARE)\nFoundation Model. Our model CARE computes and assigns confidence to regression\nresults as downstream tasks of a Foundation Model for EO data, and performs a\nconfidence-aware self-corrective learning method for the low-confidence\nregions. We evaluate the model CARE, and experimental results on multi-spectral\ndata from the Copernicus Sentinel-2 satellite constellation to estimate the\nbuilding density (i.e. monitoring urban growth), show that the proposed method\ncan be successfully applied to important regression problems in EO and remote\nsensing. We also show that our model CARE outperforms other baseline methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "S2C: Learning Noise-Resistant Differences for Unsupervised Change Detection in Multimodal Remote Sensing Images",
    "url": "http://arxiv.org/abs/2502.12604v1",
    "authors": [
      "Lei Ding",
      "Xibing Zuo",
      "Danfeng Hong",
      "Haitao Guo",
      "Jun Lu",
      "Zhihui Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-02-18",
    "abstract": "Unsupervised Change Detection (UCD) in multimodal Remote Sensing (RS) images\nremains a difficult challenge due to the inherent spatio-temporal complexity\nwithin data, and the heterogeneity arising from different imaging sensors.\nInspired by recent advancements in Visual Foundation Models (VFMs) and\nContrastive Learning (CL) methodologies, this research aims to develop CL\nmethodologies to translate implicit knowledge in VFM into change\nrepresentations, thus eliminating the need for explicit supervision. To this\nend, we introduce a Semantic-to-Change (S2C) learning framework for UCD in both\nhomogeneous and multimodal RS images. Differently from existing CL\nmethodologies that typically focus on learning multi-temporal similarities, we\nintroduce a novel triplet learning strategy that explicitly models temporal\ndifferences, which are crucial to the CD task. Furthermore, random spatial and\nspectral perturbations are introduced during the training to enhance robustness\nto temporal noise. In addition, a grid sparsity regularization is defined to\nsuppress insignificant changes, and an IoU-matching algorithm is developed to\nrefine the CD results. Experiments on four benchmark CD datasets demonstrate\nthat the proposed S2C learning framework achieves significant improvements in\naccuracy, surpassing current state-of-the-art by over 31\\%, 9\\%, 23\\%, and\n15\\%, respectively. It also demonstrates robustness and sample efficiency,\nsuitable for training and adaptation of various Visual Foundation Models (VFMs)\nor backbone neural networks. The relevant code will be available at:\ngithub.com/DingLei14/S2C.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A Survey of Sample-Efficient Deep Learning for Change Detection in Remote Sensing: Tasks, Strategies, and Challenges",
    "url": "http://arxiv.org/abs/2502.02835v1",
    "authors": [
      "Lei Ding",
      "Danfeng Hong",
      "Maofan Zhao",
      "Hongruixuan Chen",
      "Chenyu Li",
      "Jie Deng",
      "Naoto Yokoya",
      "Lorenzo Bruzzone",
      "Jocelyn Chanussot"
    ],
    "published": "2025-02-05",
    "abstract": "In the last decade, the rapid development of deep learning (DL) has made it\npossible to perform automatic, accurate, and robust Change Detection (CD) on\nlarge volumes of Remote Sensing Images (RSIs). However, despite advances in CD\nmethods, their practical application in real-world contexts remains limited due\nto the diverse input data and the applicational context. For example, the\ncollected RSIs can be time-series observations, and more informative results\nare required to indicate the time of change or the specific change category.\nMoreover, training a Deep Neural Network (DNN) requires a massive amount of\ntraining samples, whereas in many cases these samples are difficult to collect.\nTo address these challenges, various specific CD methods have been developed\nconsidering different application scenarios and training resources.\nAdditionally, recent advancements in image generation, self-supervision, and\nvisual foundation models (VFMs) have opened up new approaches to address the\n'data-hungry' issue of DL-based CD. The development of these methods in broader\napplication scenarios requires further investigation and discussion. Therefore,\nthis article summarizes the literature methods for different CD tasks and the\navailable strategies and techniques to train and deploy DL-based CD methods in\nsample-limited scenarios. We expect that this survey can provide new insights\nand inspiration for researchers in this field to develop more effective CD\nmethods that can be applied in a wider range of contexts.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Detection",
      "Image Generation"
    ]
  },
  {
    "title": "SatMamba: Development of Foundation Models for Remote Sensing Imagery Using State Space Models",
    "url": "http://arxiv.org/abs/2502.00435v1",
    "authors": [
      "Chuc Man Duc",
      "Hiromichi Fukui"
    ],
    "published": "2025-02-01",
    "abstract": "Foundation models refer to deep learning models pretrained on large unlabeled\ndatasets through self-supervised algorithms. In the Earth science and remote\nsensing communities, there is growing interest in transforming the use of Earth\nobservation data, including satellite and aerial imagery, through foundation\nmodels. Various foundation models have been developed for remote sensing, such\nas those for multispectral, high-resolution, and hyperspectral images, and have\ndemonstrated superior performance on various downstream tasks compared to\ntraditional supervised models. These models are evolving rapidly, with\ncapabilities to handle multispectral, multitemporal, and multisensor data. Most\nstudies use masked autoencoders in combination with Vision Transformers (ViTs)\nas the backbone for pretraining. While the models showed promising performance,\nViTs face challenges, such as quadratic computational scaling with input\nlength, which may limit performance on multiband and multitemporal data with\nlong sequences. This research aims to address these challenges by proposing\nSatMamba, a new pretraining framework that combines masked autoencoders with\nState Space Model, offering linear computational scaling. Experiments on\nhigh-resolution imagery across various downstream tasks show promising results,\npaving the way for more efficient foundation models and unlocking the full\npotential of Earth observation data. The source code is available in\nhttps://github.com/mdchuc/HRSFM.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "A Simple Aerial Detection Baseline of Multimodal Language Models",
    "url": "http://arxiv.org/abs/2501.09720v3",
    "authors": [
      "Qingyun Li",
      "Yushi Chen",
      "Xinya Shu",
      "Dong Chen",
      "Xin He",
      "Yi Yu",
      "Xue Yang"
    ],
    "published": "2025-01-16",
    "abstract": "The multimodal language models (MLMs) based on generative pre-trained\nTransformer are considered powerful candidates for unifying various domains and\ntasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\nperformance in multiple tasks, such as visual question answering and visual\ngrounding. In addition to visual grounding that detects specific objects\ncorresponded to given instruction, aerial detection, which detects all objects\nof multiple categories, is also a valuable and challenging task for RS\nfoundation models. However, aerial detection has not been explored by existing\nRS MLMs because the autoregressive prediction mechanism of MLMs differs\nsignificantly from the detection outputs. In this paper, we present a simple\nbaseline for applying MLMs to aerial detection for the first time, named\nLMMRotate. Specifically, we first introduce a normalization method to transform\ndetection outputs into textual outputs to be compatible with the MLM framework.\nThen, we propose a evaluation method, which ensures a fair comparison between\nMLMs and conventional object detection models. We construct the baseline by\nfine-tuning open-source general-purpose MLMs and achieve impressive detection\nperformance comparable to conventional detector. We hope that this baseline\nwill serve as a reference for future MLM development, enabling more\ncomprehensive capabilities for understanding RS images. Code is available at\nhttps://github.com/Li-Qingyun/mllm-mmrotate.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation Models",
    "url": "http://arxiv.org/abs/2501.06809v1",
    "authors": [
      "Keyan Chen",
      "Jiafan Zhang",
      "Chenyang Liu",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-12",
    "abstract": "Referring remote sensing image segmentation is crucial for achieving\nfine-grained visual understanding through free-format textual input, enabling\nenhanced scene and object extraction in remote sensing applications. Current\nresearch primarily utilizes pre-trained language models to encode textual\ndescriptions and align them with visual modalities, thereby facilitating the\nexpression of relevant visual features. However, these approaches often\nstruggle to establish robust alignments between fine-grained semantic concepts,\nleading to inconsistent representations across textual and visual information.\nTo address these limitations, we introduce a referring remote sensing image\nsegmentation foundational model, RSRefSeg. RSRefSeg leverages CLIP for visual\nand textual encoding, employing both global and local textual semantics as\nfilters to generate referring-related visual activation features in the latent\nspace. These activated features then serve as input prompts for SAM, which\nrefines the segmentation masks through its robust visual generalization\ncapabilities. Experimental results on the RRSIS-D dataset demonstrate that\nRSRefSeg outperforms existing methods, underscoring the effectiveness of\nfoundational models in enhancing multimodal task comprehension. The code is\navailable at \\url{https://github.com/KyanChen/RSRefSeg}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Semantic-CD: Remote Sensing Image Semantic Change Detection towards Open-vocabulary Setting",
    "url": "http://arxiv.org/abs/2501.06808v1",
    "authors": [
      "Yongshuo Zhu",
      "Lu Li",
      "Keyan Chen",
      "Chenyang Liu",
      "Fugen Zhou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-12",
    "abstract": "Remote sensing image semantic change detection is a method used to analyze\nremote sensing images, aiming to identify areas of change as well as categorize\nthese changes within images of the same location taken at different times.\nTraditional change detection methods often face challenges in generalizing\nacross semantic categories in practical scenarios. To address this issue, we\nintroduce a novel approach called Semantic-CD, specifically designed for\nsemantic change detection in remote sensing images. This method incorporates\nthe open vocabulary semantics from the vision-language foundation model, CLIP.\nBy utilizing CLIP's extensive vocabulary knowledge, our model enhances its\nability to generalize across categories and improves segmentation through fully\ndecoupled multi-task learning, which includes both binary change detection and\nsemantic change detection tasks. Semantic-CD consists of four main components:\na bi-temporal CLIP visual encoder for extracting features from bi-temporal\nimages, an open semantic prompter for creating semantic cost volume maps with\nopen vocabulary, a binary change detection decoder for generating binary change\ndetection masks, and a semantic change detection decoder for producing semantic\nlabels. Experimental results on the SECOND dataset demonstrate that Semantic-CD\nachieves more accurate masks and reduces semantic classification errors,\nillustrating its effectiveness in applying semantic priors from vision-language\nfoundation models to SCD tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model",
    "url": "http://arxiv.org/abs/2501.00895v2",
    "authors": [
      "Chenyang Liu",
      "Keyan Chen",
      "Rui Zhao",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-01",
    "abstract": "Generative foundation models have advanced large-scale text-driven natural\nimage generation, becoming a prominent research trend across various vertical\ndomains. However, in the remote sensing field, there is still a lack of\nresearch on large-scale text-to-image (text2image) generation technology.\nExisting remote sensing image-text datasets are small in scale and confined to\nspecific geographic areas and scene types. Besides, existing text2image methods\nhave struggled to achieve global-scale, multi-resolution controllable, and\nunbounded image generation. To address these challenges, this paper presents\ntwo key contributions: the Git-10M dataset and the Text2Earth foundation model.\nGit-10M is a global-scale image-text dataset comprising 10.5 million image-text\npairs, 5 times larger than the previous largest one. The dataset covers a wide\nrange of geographic scenes and contains resolution information, significantly\nsurpassing existing datasets in both size and diversity. Building on Git-10M,\nwe propose Text2Earth, a 1.3 billion parameter generative foundation model\nbased on the diffusion framework to model global-scale remote sensing scenes.\nText2Earth integrates a resolution guidance mechanism, enabling users to\nspecify image resolutions. A dynamic condition adaptation strategy is proposed\nfor training and inference to improve image quality. Text2Earth excels in\nzero-shot text2image generation and demonstrates robust generalization and\nflexibility across multiple tasks, including unbounded scene construction,\nimage editing, and cross-modal image generation. This robust capability\nsurpasses previous models restricted to the basic fixed size and limited scene\ntypes. On the previous benchmark dataset, Text2Earth outperforms previous\nmodels with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA\nmetric.Our project page is https://chen-yang-liu.github.io/Text2Earth",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "SeaMo: A Season-Aware Multimodal Foundation Model for Remote Sensing",
    "url": "http://arxiv.org/abs/2412.19237v2",
    "authors": [
      "Xuyang Li",
      "Chenyu Li",
      "Gemine Vivone",
      "Danfeng Hong"
    ],
    "published": "2024-12-26",
    "abstract": "Remote Sensing (RS) data encapsulates rich multi-dimensional information\nessential for Earth observation. Its vast volume, diverse sources, and temporal\ncontinuity make it particularly well-suited for developing large Visual\nFoundation Models (VFMs). These models serve as powerful feature extractors,\nleveraging extensive RS data for pretraining and subsequent fine-tuning in\nvarious geoscientific applications. However, existing VFMs in the RS domain\noften concentrate on specific image characteristics, neglecting the full\nseason-aware potential of RS data. To bridge this gap, we introduce SeaMo, a\nnovel VFM that effectively integrates multimodal and multi-seasonal RS\ninformation. SeaMo leverages a masked image modeling framework to fully exploit\nthe spatial, spectral, and seasonal dimensions of RS data. Specifically, we\nemploy unaligned spatial region selection to capture spatial heterogeneity,\nincorporate multi-source inputs for enhanced multimodal integration, and\nintroduce temporal-multimodal fusion blocks to assimilate seasonal variations\neffectively. By explicitly modeling the complex, season-dependent attributes of\nRS data, SeaMo enhances generalization, robustness, and adaptability across\ngeoscientific tasks. Extensive experiments and ablation studies demonstrate its\nsuperior performance, underscoring its potential as a foundational model for\nEarth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Detect Changes like Humans: Incorporating Semantic Priors for Improved Change Detection",
    "url": "http://arxiv.org/abs/2412.16918v1",
    "authors": [
      "Yuhang Gan",
      "Wenjie Xuan",
      "Zhiming Luo",
      "Lei Fang",
      "Zengmao Wang",
      "Juhua Liu",
      "Bo Du"
    ],
    "published": "2024-12-22",
    "abstract": "When given two similar images, humans identify their differences by comparing\nthe appearance ({\\it e.g., color, texture}) with the help of semantics ({\\it\ne.g., objects, relations}). However, mainstream change detection models adopt a\nsupervised training paradigm, where the annotated binary change map is the main\nconstraint. Thus, these methods primarily emphasize the difference-aware\nfeatures between bi-temporal images and neglect the semantic understanding of\nthe changed landscapes, which undermines the accuracy in the presence of noise\nand illumination variations. To this end, this paper explores incorporating\nsemantic priors to improve the ability to detect changes. Firstly, we propose a\nSemantic-Aware Change Detection network, namely SA-CDNet, which transfers the\ncommon knowledge of the visual foundation models ({\\it i.e., FastSAM}) to\nchange detection. Inspired by the human visual paradigm, a novel dual-stream\nfeature decoder is derived to distinguish changes by combining semantic-aware\nfeatures and difference-aware features. Secondly, we design a single-temporal\nsemantic pre-training strategy to enhance the semantic understanding of\nlandscapes, which brings further increments. Specifically, we construct\npseudo-change detection data from public single-temporal remote sensing\nsegmentation datasets for large-scale pre-training, where an extra branch is\nalso introduced for the proxy semantic segmentation task. Experimental results\non five challenging benchmarks demonstrate the superiority of our method over\nthe existing state-of-the-art methods. The code is available at\n\\href{https://github.com/thislzm/SA-CD}{SA-CD}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "SAModified: A Foundation Model-Based Zero-Shot Approach for Refining Noisy Land-Use Land-Cover Maps",
    "url": "http://arxiv.org/abs/2412.12552v1",
    "authors": [
      "Sparsh Pekhale",
      "Rakshith Sathish",
      "Sathisha Basavaraju",
      "Divya Sharma"
    ],
    "published": "2024-12-17",
    "abstract": "Land-use and land cover (LULC) analysis is critical in remote sensing, with\nwide-ranging applications across diverse fields such as agriculture, utilities,\nand urban planning. However, automating LULC map generation using machine\nlearning is rendered challenging due to noisy labels. Typically, the ground\ntruths (e.g. ESRI LULC, MapBioMass) have noisy labels that hamper the model's\nability to learn to accurately classify the pixels. Further, these erroneous\nlabels can significantly distort the performance metrics of a model, leading to\nmisleading evaluations. Traditionally, the ambiguous labels are rectified using\nunsupervised algorithms. These algorithms struggle not only with scalability\nbut also with generalization across different geographies. To overcome these\nchallenges, we propose a zero-shot approach using the foundation model, Segment\nAnything Model (SAM), to automatically delineate different land parcels/regions\nand leverage them to relabel the unsure pixels by using the local label\nstatistics within each detected region. We achieve a significant reduction in\nlabel noise and an improvement in the performance of the downstream\nsegmentation model by $\\approx 5\\%$ when trained with denoised labels.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications",
    "url": "http://arxiv.org/abs/2412.02732v2",
    "authors": [
      "Daniela Szwarcman",
      "Sujit Roy",
      "Paolo Fraccaro",
      "\u00deorsteinn El\u00ed G\u00edslason",
      "Benedikt Blumenstiel",
      "Rinki Ghosal",
      "Pedro Henrique de Oliveira",
      "Joao Lucas de Sousa Almeida",
      "Rocco Sedona",
      "Yanghui Kang",
      "Srija Chakraborty",
      "Sizhe Wang",
      "Carlos Gomes",
      "Ankur Kumar",
      "Myscon Truong",
      "Denys Godwin",
      "Hyunho Lee",
      "Chia-Yu Hsu",
      "Ata Akbari Asanjan",
      "Besart Mujeci",
      "Disha Shidham",
      "Trevor Keenan",
      "Paulo Arevalo",
      "Wenwen Li",
      "Hamed Alemohammad",
      "Pontus Olofsson",
      "Christopher Hain",
      "Robert Kennedy",
      "Bianca Zadrozny",
      "David Bell",
      "Gabriele Cavallaro",
      "Campbell Watson",
      "Manil Maskey",
      "Rahul Ramachandran",
      "Juan Bernabe Moreno"
    ],
    "published": "2024-12-03",
    "abstract": "This technical report presents Prithvi-EO-2.0, a new geospatial foundation\nmodel that offers significant improvements over its predecessor,\nPrithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's\nHarmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M\nand 600M parameter models incorporate temporal and location embeddings for\nenhanced performance across various geospatial tasks. Through extensive\nbenchmarking with GEO-Bench, the 600M version outperforms the previous\nPrithvi-EO model by 8\\% across a range of tasks. It also outperforms six other\ngeospatial foundation models when benchmarked on remote sensing tasks from\ndifferent domains and resolutions (i.e. from 0.1m to 15m). The results\ndemonstrate the versatility of the model in both classical earth observation\nand high-resolution applications. Early involvement of end-users and subject\nmatter experts (SMEs) are among the key factors that contributed to the\nproject's success. In particular, SME involvement allowed for constant feedback\non model and dataset design, as well as successful customization for diverse\nSME-led applications in disaster response, land use and crop mapping, and\necosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and\nIBM terratorch, with additional resources on GitHub. The project exemplifies\nthe Trusted Open Science approach embraced by all involved organizations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model",
    "url": "http://arxiv.org/abs/2411.17984v2",
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Hanbo Bi",
      "Boyuan Tong",
      "Zhaozhi Wang",
      "Wenhui Diao",
      "Hao Chang",
      "Yingchao Feng",
      "Ziqi Zhang",
      "Yaowei Wang",
      "Qixiang Ye",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2024-11-27",
    "abstract": "Remote sensing foundation models largely break away from the traditional\nparadigm of designing task-specific models, offering greater scalability across\nmultiple tasks. However, they face challenges such as low computational\nefficiency and limited interpretability, especially when dealing with\nlarge-scale remote sensing images. To overcome these, we draw inspiration from\nheat conduction, a physical process modeling local heat diffusion. Building on\nthis idea, we are the first to explore the potential of using the parallel\ncomputing model of heat conduction to simulate the local region correlations in\nhigh-resolution remote sensing images, and introduce RS-vHeat, an efficient\nmulti-modal remote sensing foundation model. Specifically, RS-vHeat 1) applies\nthe Heat Conduction Operator (HCO) with a complexity of $O(N^{1.5})$ and a\nglobal receptive field, reducing computational overhead while capturing remote\nsensing object structure information to guide heat diffusion; 2) learns the\nfrequency distribution representations of various scenes through a\nself-supervised strategy based on frequency domain hierarchical masking and\nmulti-domain reconstruction; 3) significantly improves efficiency and\nperformance over state-of-the-art techniques across 4 tasks and 10 datasets.\nCompared to attention-based remote sensing foundation models, we reduce memory\nusage by 84\\%, FLOPs by 24\\% and improves throughput by 2.7 times. The code\nwill be made publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatVision-TOA: A Geospatial Foundation Model for Coarse-Resolution All-Sky Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2411.17000v1",
    "authors": [
      "Caleb S. Spradlin",
      "Jordan A. Caraballo-Vega",
      "Jian Li",
      "Mark L. Carroll",
      "Jie Gong",
      "Paul M. Montesano"
    ],
    "published": "2024-11-26",
    "abstract": "Foundation models have the potential to transform the landscape of remote\nsensing (RS) data analysis by enabling large computer vision models to be\npre-trained on vast amounts of remote sensing data. These models can then be\nfine-tuned with small amounts of labeled training and applied to a variety of\napplications. Most existing foundation models are designed for high spatial\nresolution, cloud-free satellite imagery or photos, limiting their\napplicability in scenarios that require frequent temporal monitoring or broad\nspectral profiles. As a result, foundation models trained solely on cloud-free\nimages have limited utility for applications that involve atmospheric variables\nor require atmospheric corrections. We introduce SatVision-TOA, a novel\nfoundation model pre-trained on 14-band MODIS L1B Top-Of-Atmosphere (TOA)\nradiance imagery, addressing the need for models pre-trained to handle\nmoderate- and coarse-resolution all-sky remote sensing data. The SatVision-TOA\nmodel is pre-trained using a Masked-Image-Modeling (MIM) framework and the\nSwinV2 architecture, and learns detailed contextual representations through\nself-supervised learning without the need for labels. It is a 3 billion\nparameter model that is trained on 100 million images. To our knowledge this is\nthe largest foundation model trained solely on satellite RS imagery. Results\nshow that SatVision-TOA achieves superior performance over baseline methods on\ndownstream tasks such as 3D cloud retrieval. Notably, the model achieves a mean\nintersection over union (mIOU) of 0.46, a substantial improvement over the\nbaseline mIOU of 0.22. Additionally, the rate of false negative results in the\nfine-tuning task were reduced by over 50% compared to the baseline. Our work\nadvances pre-trained vision modeling for multispectral RS by learning from a\nvariety of atmospheric and aerosol conditions to improve cloud and land surface\nmonitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2411.13127v2",
    "authors": [
      "Xuechao Zou",
      "Shun Zhang",
      "Kai Li",
      "Shiying Wang",
      "Junliang Xing",
      "Lei Jin",
      "Congyan Lang",
      "Pin Tao"
    ],
    "published": "2024-11-20",
    "abstract": "Cloud segmentation is a critical challenge in remote sensing image\ninterpretation, as its accuracy directly impacts the effectiveness of\nsubsequent data processing and analysis. Recently, vision foundation models\n(VFM) have demonstrated powerful generalization capabilities across various\nvisual tasks. In this paper, we present a parameter-efficient adaptive\napproach, termed Cloud-Adapter, designed to enhance the accuracy and robustness\nof cloud segmentation. Our method leverages a VFM pretrained on general domain\ndata, which remains frozen, eliminating the need for additional training.\nCloud-Adapter incorporates a lightweight spatial perception module that\ninitially utilizes a convolutional neural network (ConvNet) to extract dense\nspatial representations. These multi-scale features are then aggregated and\nserve as contextual inputs to an adapting module, which modulates the frozen\ntransformer layers within the VFM. Experimental results demonstrate that the\nCloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the\nfrozen backbone, achieves substantial performance gains. Cloud-Adapter\nconsistently achieves state-of-the-art performance across various cloud\nsegmentation datasets from multiple satellite sources, sensor series, data\nprocessing levels, land cover scenarios, and annotation granularities. We have\nreleased the code and model checkpoints at\nhttps://xavierjiezou.github.io/Cloud-Adapter/ to support further research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Aquila: A Hierarchically Aligned Visual-Language Model for Enhanced Remote Sensing Image Comprehension",
    "url": "http://arxiv.org/abs/2411.06074v1",
    "authors": [
      "Kaixuan Lu",
      "Ruiqian Zhang",
      "Xiao Huang",
      "Yuxing Xie"
    ],
    "published": "2024-11-09",
    "abstract": "Recently, large vision language models (VLMs) have made significant strides\nin visual language capabilities through visual instruction tuning, showing\ngreat promise in the field of remote sensing image interpretation. However,\nexisting remote sensing vision language models (RSVLMs) often fall short in\ncapturing the complex characteristics of remote sensing scenes, as they\ntypically rely on low resolution, single scale visual features and simplistic\nmethods to map visual features to language features. In this paper, we present\nAquila, an advanced visual language foundation model designed to enable richer\nvisual feature representation and more precise visual-language feature\nalignment for remote sensing images. Our approach introduces a learnable\nHierarchical Spatial Feature Integration (SFI) module that supports high\nresolution image inputs and aggregates multi scale visual features, allowing\nfor the detailed representation of complex visual information. Additionally,\nthe SFI module is repeatedly integrated into the layers of the large language\nmodel (LLM) to achieve deep visual language feature alignment, without\ncompromising the model's performance in natural language processing tasks.\nThese innovations, capturing detailed visual effects through higher resolution\nand multi scale input, and enhancing feature alignment significantly improve\nthe model's ability to learn from image text data. We validate the\neffectiveness of Aquila through extensive quantitative experiments and\nqualitative analyses, demonstrating its superior performance.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "STARS: Sensor-agnostic Transformer Architecture for Remote Sensing",
    "url": "http://arxiv.org/abs/2411.05714v1",
    "authors": [
      "Ethan King",
      "Jaime Rodriguez",
      "Diego Llanes",
      "Timothy Doster",
      "Tegan Emerson",
      "James Koch"
    ],
    "published": "2024-11-08",
    "abstract": "We present a sensor-agnostic spectral transformer as the basis for spectral\nfoundation models. To that end, we introduce a Universal Spectral\nRepresentation (USR) that leverages sensor meta-data, such as sensing kernel\nspecifications and sensing wavelengths, to encode spectra obtained from any\nspectral instrument into a common representation, such that a single model can\ningest data from any sensor. Furthermore, we develop a methodology for\npre-training such models in a self-supervised manner using a novel random\nsensor-augmentation and reconstruction pipeline to learn spectral features\nindependent of the sensing paradigm. We demonstrate that our architecture can\nlearn sensor independent spectral features that generalize effectively to\nsensors not seen during training. This work sets the stage for training\nfoundation models that can both leverage and be effective for the growing\ndiversity of spectral data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Joint-Optimized Unsupervised Adversarial Domain Adaptation in Remote Sensing Segmentation with Prompted Foundation Model",
    "url": "http://arxiv.org/abs/2411.05878v2",
    "authors": [
      "Shuchang Lyu",
      "Qi Zhao",
      "Guangliang Cheng",
      "Yiwei He",
      "Zheng Zhou",
      "Guangbiao Wang",
      "Zhenwei Shi"
    ],
    "published": "2024-11-08",
    "abstract": "Unsupervised Domain Adaptation for Remote Sensing Semantic Segmentation\n(UDA-RSSeg) addresses the challenge of adapting a model trained on source\ndomain data to target domain samples, thereby minimizing the need for annotated\ndata across diverse remote sensing scenes. This task presents two principal\nchallenges: (1) severe inconsistencies in feature representation across\ndifferent remote sensing domains, and (2) a domain gap that emerges due to the\nrepresentation bias of source domain patterns when translating features to\npredictive logits. To tackle these issues, we propose a joint-optimized\nadversarial network incorporating the \"Segment Anything Model (SAM)\n(SAM-JOANet)\" for UDA-RSSeg. Our approach integrates SAM to leverage its robust\ngeneralized representation capabilities, thereby alleviating feature\ninconsistencies. We introduce a finetuning decoder designed to convert\nSAM-Encoder features into predictive logits. Additionally, a feature-level\nadversarial-based prompted segmentor is employed to generate class-agnostic\nmaps, which guide the finetuning decoder's feature representations. The network\nis optimized end-to-end, combining the prompted segmentor and the finetuning\ndecoder. Extensive evaluations on benchmark datasets, including ISPRS\n(Potsdam/Vaihingen) and CITY-OSM (Paris/Chicago), demonstrate the effectiveness\nof our method. The results, supported by visualization and analysis, confirm\nthe method's interpretability and robustness. The code of this paper is\navailable at https://github.com/CV-ShuchangLyu/SAM-JOANet.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "In the Era of Prompt Learning with Vision-Language Models",
    "url": "http://arxiv.org/abs/2411.04892v1",
    "authors": [
      "Ankit Jha"
    ],
    "published": "2024-11-07",
    "abstract": "Large-scale foundation models like CLIP have shown strong zero-shot\ngeneralization but struggle with domain shifts, limiting their adaptability. In\nour work, we introduce \\textsc{StyLIP}, a novel domain-agnostic prompt learning\nstrategy for Domain Generalization (DG). StyLIP disentangles visual style and\ncontent in CLIP`s vision encoder by using style projectors to learn\ndomain-specific prompt tokens and combining them with content features. Trained\ncontrastively, this approach enables seamless adaptation across domains,\noutperforming state-of-the-art methods on multiple DG benchmarks. Additionally,\nwe propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s\nfrozen vision backbone to learn domain-invariant prompts through image style\nand content features. By aligning domains in embedding space with entropy\nminimization, AD-CLIP effectively handles domain shifts, even when only target\ndomain samples are available. Lastly, we outline future work on class discovery\nusing prompt learning for semantic segmentation in remote sensing, focusing on\nidentifying novel or rare classes in unstructured environments. This paves the\nway for more adaptive and generalizable models in complex, real-world\nscenarios.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2410.22629v2",
    "authors": [
      "Ziyang Gong",
      "Zhixiang Wei",
      "Di Wang",
      "Xianzheng Ma",
      "Hongruixuan Chen",
      "Yuru Jia",
      "Yupeng Deng",
      "Zhenming Ji",
      "Xiangwei Zhu",
      "Naoto Yokoya",
      "Jing Zhang",
      "Bo Du",
      "Liangpei Zhang"
    ],
    "published": "2024-10-30",
    "abstract": "The field of Remote Sensing Domain Generalization (RSDG) has emerged as a\ncritical and valuable research frontier, focusing on developing models that\ngeneralize effectively across diverse scenarios. Despite the substantial domain\ngaps in RS images that are characterized by variabilities such as location,\nwavelength, and sensor type, research in this area remains underexplored: (1)\nCurrent cross-domain methods primarily focus on Domain Adaptation (DA), which\nadapts models to predefined domains rather than to unseen ones; (2) Few studies\ntargeting the RSDG issue, especially for semantic segmentation tasks, where\nexisting models are developed for specific unknown domains, struggling with\nissues of underfitting on other unknown scenarios; (3) Existing RS foundation\nmodels tend to prioritize in-domain performance over cross-domain\ngeneralization. To this end, we introduce the first vision foundation model for\nRSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong\ncross-domain generalization through a specially designed data-level Earth-Style\nInjection pipeline and a model-level Multi-Task Training pipeline. In addition,\nfor the semantic segmentation task, we have curated an RSDG benchmark\ncomprising 28 cross-domain settings across various regions, spectral bands,\nplatforms, and climates, providing a comprehensive framework for testing the\ngeneralizability of future RSDG models. Extensive experiments on this benchmark\ndemonstrate the superiority of CrossEarth over existing state-of-the-art\nmethods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery",
    "url": "http://arxiv.org/abs/2410.19965v1",
    "authors": [
      "Philipe Dias",
      "Aristeidis Tsaris",
      "Jordan Bowman",
      "Abhishek Potnis",
      "Jacob Arndt",
      "H. Lexie Yang",
      "Dalton Lunga"
    ],
    "published": "2024-10-25",
    "abstract": "While the pretraining of Foundation Models (FMs) for remote sensing (RS)\nimagery is on the rise, models remain restricted to a few hundred million\nparameters. Scaling models to billions of parameters has been shown to yield\nunprecedented benefits including emergent abilities, but requires data scaling\nand computing resources typically not available outside industry R&D labs. In\nthis work, we pair high-performance computing resources including Frontier\nsupercomputer, America's first exascale system, and high-resolution optical RS\ndata to pretrain billion-scale FMs. Our study assesses performance of different\npretrained variants of vision Transformers across image classification,\nsemantic segmentation and object detection benchmarks, which highlight the\nimportance of data scaling for effective model scaling. Moreover, we discuss\nconstruction of a novel TIU pretraining dataset, model initialization, with\ndata and pretrained models intended for public release. By discussing technical\nchallenges and details often lacking in the related literature, this work is\nintended to offer best practices to the geospatial community toward efficient\ntraining and benchmarking of larger FMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Foundation Models for Remote Sensing and Earth Observation: A Survey",
    "url": "http://arxiv.org/abs/2410.16602v2",
    "authors": [
      "Aoran Xiao",
      "Weihao Xuan",
      "Junjue Wang",
      "Jiaxing Huang",
      "Dacheng Tao",
      "Shijian Lu",
      "Naoto Yokoya"
    ],
    "published": "2024-10-22",
    "abstract": "Remote Sensing (RS) is a crucial technology for observing, monitoring, and\ninterpreting our planet, with broad applications across geoscience, economics,\nhumanitarian fields, etc. While artificial intelligence (AI), particularly deep\nlearning, has achieved significant advances in RS, unique challenges persist in\ndeveloping more intelligent RS systems, including the complexity of Earth's\nenvironments, diverse sensor modalities, distinctive feature patterns, varying\nspatial and spectral resolutions, and temporal dynamics. Meanwhile, recent\nbreakthroughs in large Foundation Models (FMs) have expanded AI's potential\nacross many domains due to their exceptional generalizability and zero-shot\ntransfer capabilities. However, their success has largely been confined to\nnatural data like images and video, with degraded performance and even failures\nfor RS data of various non-optical modalities. This has inspired growing\ninterest in developing Remote Sensing Foundation Models (RSFMs) to address the\ncomplex demands of Earth Observation (EO) tasks, spanning the surface,\natmosphere, and oceans. This survey systematically reviews the emerging field\nof RSFMs. It begins with an outline of their motivation and background,\nfollowed by an introduction of their foundational concepts. It then categorizes\nand reviews existing RSFM studies including their datasets and technical\ncontributions across Visual Foundation Models (VFMs), Visual-Language Models\n(VLMs), Large Language Models (LLMs), and beyond. In addition, we benchmark\nthese models against publicly available datasets, discuss existing challenges,\nand propose future research directions in this rapidly evolving field. A\nproject associated with this survey has been built at\nhttps://github.com/xiaoaoran/awesome-RSFMs .",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "MANet: Fine-Tuning Segment Anything Model for Multimodal Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2410.11160v1",
    "authors": [
      "Xianping Ma",
      "Xiaokang Zhang",
      "Man-On Pun",
      "Bo Huang"
    ],
    "published": "2024-10-15",
    "abstract": "Multimodal remote sensing data, collected from a variety of sensors, provide\na comprehensive and integrated perspective of the Earth's surface. By employing\nmultimodal fusion techniques, semantic segmentation offers more detailed\ninsights into geographic scenes compared to single-modality approaches.\nBuilding upon recent advancements in vision foundation models, particularly the\nSegment Anything Model (SAM), this study introduces a novel Multimodal\nAdapter-based Network (MANet) for multimodal remote sensing semantic\nsegmentation. At the core of this approach is the development of a Multimodal\nAdapter (MMAdapter), which fine-tunes SAM's image encoder to effectively\nleverage the model's general knowledge for multimodal data. In addition, a\npyramid-based Deep Fusion Module (DFM) is incorporated to further integrate\nhigh-level geographic features across multiple scales before decoding. This\nwork not only introduces a novel network for multimodal fusion, but also\ndemonstrates, for the first time, SAM's powerful generalization capabilities\nwith Digital Surface Model (DSM) data. Experimental results on two\nwell-established fine-resolution multimodal remote sensing datasets, ISPRS\nVaihingen and ISPRS Potsdam, confirm that the proposed MANet significantly\nsurpasses current models in the task of multimodal semantic segmentation. The\nsource code for this work will be accessible at https://github.com/sstary/SSRS.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Exploring Foundation Models in Remote Sensing Image Change Detection: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2410.07824v1",
    "authors": [
      "Zihan Yu",
      "Tianxiao Li",
      "Yuxin Zhu",
      "Rongze Pan"
    ],
    "published": "2024-10-10",
    "abstract": "Change detection, as an important and widely applied technique in the field\nof remote sensing, aims to analyze changes in surface areas over time and has\nbroad applications in areas such as environmental monitoring, urban\ndevelopment, and land use analysis.In recent years, deep learning, especially\nthe development of foundation models, has provided more powerful solutions for\nfeature extraction and data fusion, effectively addressing these complexities.\nThis paper systematically reviews the latest advancements in the field of\nchange detection, with a focus on the application of foundation models in\nremote sensing tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2410.06194v1",
    "authors": [
      "Shiyu Miao",
      "Delong Chen",
      "Fan Liu",
      "Chuanyi Zhang",
      "Yanhui Gu",
      "Shengjie Guo",
      "Jun Zhou"
    ],
    "published": "2024-10-08",
    "abstract": "The Direct Segment Anything Model (DirectSAM) excels in class-agnostic\ncontour extraction. In this paper, we explore its use by applying it to optical\nremote sensing imagery, where semantic contour extraction-such as identifying\nbuildings, road networks, and coastlines-holds significant practical value.\nThose applications are currently handled via training specialized small models\nseparately on small datasets in each domain. We introduce a foundation model\nderived from DirectSAM, termed DirectSAM-RS, which not only inherits the strong\nsegmentation capability acquired from natural images, but also benefits from a\nlarge-scale dataset we created for remote sensing semantic contour extraction.\nThis dataset comprises over 34k image-text-contour triplets, making it at least\n30 times larger than individual dataset. DirectSAM-RS integrates a prompter\nmodule: a text encoder and cross-attention layers attached to the DirectSAM\narchitecture, which allows flexible conditioning on target class labels or\nreferring expressions. We evaluate the DirectSAM-RS in both zero-shot and\nfine-tuning setting, and demonstrate that it achieves state-of-the-art\nperformance across several downstream benchmarks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "PointSAM: Pointly-Supervised Segment Anything Model for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2409.13401v2",
    "authors": [
      "Nanqing Liu",
      "Xun Xu",
      "Yongyi Su",
      "Haojie Zhang",
      "Heng-Chao Li"
    ],
    "published": "2024-09-20",
    "abstract": "Segment Anything Model (SAM) is an advanced foundational model for image\nsegmentation, which is gradually being applied to remote sensing images (RSIs).\nDue to the domain gap between RSIs and natural images, traditional methods\ntypically use SAM as a source pre-trained model and fine-tune it with fully\nsupervised masks. Unlike these methods, our work focuses on fine-tuning SAM\nusing more convenient and challenging point annotations. Leveraging SAM's\nzero-shot capabilities, we adopt a self-training framework that iteratively\ngenerates pseudo-labels for training. However, if the pseudo-labels contain\nnoisy labels, there is a risk of error accumulation. To address this issue, we\nextract target prototypes from the target dataset and use the Hungarian\nalgorithm to match them with prediction prototypes, preventing the model from\nlearning in the wrong direction. Additionally, due to the complex backgrounds\nand dense distribution of objects in RSI, using point prompts may result in\nmultiple objects being recognized as one. To solve this problem, we propose a\nnegative prompt calibration method based on the non-overlapping nature of\ninstance masks. In brief, we use the prompts of overlapping masks as\ncorresponding negative signals, resulting in refined masks. Combining the above\nmethods, we propose a novel Pointly-supervised Segment Anything Model named\nPointSAM. We conduct experiments on RSI datasets, including WHU, HRSID, and\nNWPU VHR-10, and the results show that our method significantly outperforms\ndirect testing with SAM, SAM2, and other comparison methods. Furthermore, we\nintroduce PointSAM as a point-to-box converter and achieve encouraging results,\nsuggesting that this method can be extended to other point-supervised tasks.\nThe code is available at https://github.com/Lans1ng/PointSAM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "RingMo-Aerial: An Aerial Remote Sensing Foundation Model With A Affine Transformation Contrastive Learning",
    "url": "http://arxiv.org/abs/2409.13366v2",
    "authors": [
      "Wenhui Diao",
      "Haichen Yu",
      "Kaiyue Kang",
      "Tong Ling",
      "Di Liu",
      "Yingchao Feng",
      "Hanbo Bi",
      "Libo Ren",
      "Xuexue Li",
      "Yongqiang Mao",
      "Xian Sun"
    ],
    "published": "2024-09-20",
    "abstract": "Aerial Remote Sensing (ARS) vision tasks pose significant challenges due to\nthe unique characteristics of their viewing angles. Existing research has\nprimarily focused on algorithms for specific tasks, which have limited\napplicability in a broad range of ARS vision applications. This paper proposes\nthe RingMo-Aerial model, aiming to fill the gap in foundation model research in\nthe field of ARS vision. By introducing the Frequency-Enhanced Multi-Head\nSelf-Attention (FE-MSA) mechanism and an affine transformation-based\ncontrastive learning pre-training method, the model's detection capability for\nsmall targets is enhanced and optimized for the tilted viewing angles\ncharacteristic of ARS. Furthermore, the ARS-Adapter, an efficient parameter\nfine-tuning method, is proposed to improve the model's adaptability and\neffectiveness in various ARS vision tasks. Experimental results demonstrate\nthat RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This\nindicates the practicality and efficacy of RingMo-Aerial in enhancing the\nperformance of ARS vision tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Detecting Looted Archaeological Sites from Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2409.09432v1",
    "authors": [
      "Elliot Vincent",
      "Mehra\u00efl Saroufim",
      "Jonathan Chemla",
      "Yves Ubelmann",
      "Philippe Marquis",
      "Jean Ponce",
      "Mathieu Aubry"
    ],
    "published": "2024-09-14",
    "abstract": "Archaeological sites are the physical remains of past human activity and one\nof the main sources of information about past societies and cultures. However,\nthey are also the target of malevolent human actions, especially in countries\nhaving experienced inner turmoil and conflicts. Because monitoring these sites\nfrom space is a key step towards their preservation, we introduce the DAFA\nLooted Sites dataset, \\datasetname, a labeled multi-temporal remote sensing\ndataset containing 55,480 images acquired monthly over 8 years across 675\nAfghan archaeological sites, including 135 sites looted during the acquisition\nperiod. \\datasetname~is particularly challenging because of the limited number\nof training samples, the class imbalance, the weak binary annotations only\navailable at the level of the time series, and the subtlety of relevant changes\ncoupled with important irrelevant ones over a long time period. It is also an\ninteresting playground to assess the performance of satellite image time series\n(SITS) classification methods on a real and important use case. We evaluate a\nlarge set of baselines, outline the substantial benefits of using foundation\nmodels and show the additional boost that can be provided by using complete\ntime series instead of using a single image.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Pushing the Limits of Vision-Language Models in Remote Sensing without Human Annotations",
    "url": "http://arxiv.org/abs/2409.07048v1",
    "authors": [
      "Keumgang Cha",
      "Donggeun Yu",
      "Junghoon Seo"
    ],
    "published": "2024-09-11",
    "abstract": "The prominence of generalized foundation models in vision-language\nintegration has witnessed a surge, given their multifarious applications.\nWithin the natural domain, the procurement of vision-language datasets to\nconstruct these foundation models is facilitated by their abundant availability\nand the ease of web crawling. Conversely, in the remote sensing domain,\nalthough vision-language datasets exist, their volume is suboptimal for\nconstructing robust foundation models. This study introduces an approach to\ncurate vision-language datasets by employing an image decoding machine learning\nmodel, negating the need for human-annotated labels. Utilizing this\nmethodology, we amassed approximately 9.6 million vision-language paired\ndatasets in VHR imagery. The resultant model outperformed counterparts that did\nnot leverage publicly available vision-language datasets, particularly in\ndownstream tasks such as zero-shot classification, semantic localization, and\nimage-text retrieval. Moreover, in tasks exclusively employing vision encoders,\nsuch as linear probing and k-NN classification, our model demonstrated superior\nefficacy compared to those relying on domain-specific vision-language datasets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability",
    "url": "http://arxiv.org/abs/2409.00489v1",
    "authors": [
      "Chia-Yu Hsu",
      "Wenwen Li",
      "Sizhe Wang"
    ],
    "published": "2024-08-31",
    "abstract": "Research on geospatial foundation models (GFMs) has become a trending topic\nin geospatial artificial intelligence (AI) research due to their potential for\nachieving high generalizability and domain adaptability, reducing model\ntraining costs for individual researchers. Unlike large language models, such\nas ChatGPT, constructing visual foundation models for image analysis,\nparticularly in remote sensing, encountered significant challenges such as\nformulating diverse vision tasks into a general problem framework. This paper\nevaluates the recently released NASA-IBM GFM Prithvi for its predictive\nperformance on high-level image analysis tasks across multiple benchmark\ndatasets. Prithvi was selected because it is one of the first open-source GFMs\ntrained on time-series of high-resolution remote sensing imagery. A series of\nexperiments were designed to assess Prithvi's performance as compared to other\npre-trained task-specific AI models in geospatial image analysis. New\nstrategies, including band adaptation, multi-scale feature generation, and\nfine-tuning techniques, are introduced and integrated into an image analysis\npipeline to enhance Prithvi's domain adaptation capability and improve model\nperformance. In-depth analyses reveal Prithvi's strengths and weaknesses,\noffering insights for both improving Prithvi and developing future visual\nfoundation models for geospatial tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Tuning a SAM-Based Model with Multi-Cognitive Visual Adapter to Remote Sensing Instance Segmentation",
    "url": "http://arxiv.org/abs/2408.08576v1",
    "authors": [
      "Linghao Zheng",
      "Xinyang Pu",
      "Feng Xu"
    ],
    "published": "2024-08-16",
    "abstract": "The Segment Anything Model (SAM), a foundational model designed for\npromptable segmentation tasks, demonstrates exceptional generalization\ncapabilities, making it highly promising for natural scene image segmentation.\nHowever, SAM's lack of pretraining on massive remote sensing images and its\ninteractive structure limit its automatic mask prediction capabilities. In this\npaper, a Multi-Cognitive SAM-Based Instance Segmentation Model (MC-SAM SEG) is\nintroduced to employ SAM on remote sensing domain. The SAM-Mona encoder\nutilizing the Multi-cognitive Visual Adapter (Mona) is conducted to facilitate\nSAM's transfer learning in remote sensing applications. The proposed method\nnamed MC-SAM SEG extracts high-quality features by fine-tuning the SAM-Mona\nencoder along with a feature aggregator. Subsequently, a pixel decoder and\ntransformer decoder are designed for prompt-free mask generation and instance\nclassification. The comprehensive experiments are conducted on the HRSID and\nWHU datasets for instance segmentation tasks on Synthetic Aperture Radar (SAR)\nimages and optical remote sensing images respectively. The evaluation results\nindicate the proposed method surpasses other deep learning algorithms and\nverify its effectiveness and generalization.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "SpectralEarth: Training Hyperspectral Foundation Models at Scale",
    "url": "http://arxiv.org/abs/2408.08447v1",
    "authors": [
      "Nassim Ait Ali Braham",
      "Conrad M Albrecht",
      "Julien Mairal",
      "Jocelyn Chanussot",
      "Yi Wang",
      "Xiao Xiang Zhu"
    ],
    "published": "2024-08-15",
    "abstract": "Foundation models have triggered a paradigm shift in computer vision and are\nincreasingly being adopted in remote sensing, particularly for multispectral\nimagery. Yet, their potential in hyperspectral imaging (HSI) remains untapped\ndue to the absence of comprehensive and globally representative hyperspectral\ndatasets. To close this gap, we introduce SpectralEarth, a large-scale\nmulti-temporal dataset designed to pretrain hyperspectral foundation models\nleveraging data from the Environmental Mapping and Analysis Program (EnMAP).\nSpectralEarth comprises 538,974 image patches covering 415,153 unique locations\nfrom more than 11,636 globally distributed EnMAP scenes spanning two years of\narchive. Additionally, 17.5% of these locations include multiple timestamps,\nenabling multi-temporal HSI analysis. Utilizing state-of-the-art\nself-supervised learning (SSL) algorithms, we pretrain a series of foundation\nmodels on SpectralEarth. We integrate a spectral adapter into classical vision\nbackbones to accommodate the unique characteristics of HSI. In tandem, we\nconstruct four downstream datasets for land-cover and crop-type mapping,\nproviding benchmarks for model evaluation. Experimental results support the\nversatility of our models, showcasing their generalizability across different\ntasks and sensors. We also highlight computational efficiency during model\nfine-tuning. The dataset, models, and source code will be made publicly\navailable.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Seg-CycleGAN : SAR-to-optical image translation guided by a downstream task",
    "url": "http://arxiv.org/abs/2408.05777v1",
    "authors": [
      "Hannuo Zhang",
      "Huihui Li",
      "Jiarui Lin",
      "Yujie Zhang",
      "Jianghua Fan",
      "Hang Liu"
    ],
    "published": "2024-08-11",
    "abstract": "Optical remote sensing and Synthetic Aperture Radar(SAR) remote sensing are\ncrucial for earth observation, offering complementary capabilities. While\noptical sensors provide high-quality images, they are limited by weather and\nlighting conditions. In contrast, SAR sensors can operate effectively under\nadverse conditions. This letter proposes a GAN-based SAR-to-optical image\ntranslation method named Seg-CycleGAN, designed to enhance the accuracy of ship\ntarget translation by leveraging semantic information from a pre-trained\nsemantic segmentation model. Our method utilizes the downstream task of ship\ntarget semantic segmentation to guide the training of image translation\nnetwork, improving the quality of output Optical-styled images. The potential\nof foundation-model-annotated datasets in SAR-to-optical translation tasks is\nrevealed. This work suggests broader research and applications for\ndownstream-task-guided frameworks. The code will be available at\nhttps://github.com/NPULHH/",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height Estimation",
    "url": "http://arxiv.org/abs/2408.04523v1",
    "authors": [
      "Daniele Rege Cambrin",
      "Isaac Corley",
      "Paolo Garza"
    ],
    "published": "2024-08-08",
    "abstract": "Estimating global tree canopy height is crucial for forest conservation and\nclimate change applications. However, capturing high-resolution ground truth\ncanopy height using LiDAR is expensive and not available globally. An efficient\nalternative is to train a canopy height estimator to operate on single-view\nremotely sensed imagery. The primary obstacle to this approach is that these\nmethods require significant training data to generalize well globally and\nacross uncommon edge cases. Recent monocular depth estimation foundation models\nhave show strong zero-shot performance even for complex scenes. In this paper\nwe leverage the representations learned by these models to transfer to the\nremote sensing domain for measuring canopy height. Our findings suggest that\nour proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2\nmodel for canopy height estimation, provides a performant and efficient\nsolution, surpassing the current state-of-the-art with superior or comparable\nperformance using only a fraction of the computational resources and\nparameters. Furthermore, our approach requires less than \\$1.30 in compute and\nresults in an estimated carbon footprint of 0.14 kgCO2. Code, experimental\nresults, and model checkpoints are openly available at\nhttps://github.com/DarthReca/depth-any-canopy.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Vision Foundation Models in Remote Sensing: A Survey",
    "url": "http://arxiv.org/abs/2408.03464v2",
    "authors": [
      "Siqi Lu",
      "Junlin Guo",
      "James R Zimmer-Dauphinee",
      "Jordan M Nieusma",
      "Xiao Wang",
      "Parker VanValkenburgh",
      "Steven A Wernke",
      "Yuankai Huo"
    ],
    "published": "2024-08-06",
    "abstract": "Artificial Intelligence (AI) technologies have profoundly transformed the\nfield of remote sensing, revolutionizing data collection, processing, and\nanalysis. Traditionally reliant on manual interpretation and task-specific\nmodels, remote sensing research has been significantly enhanced by the advent\nof foundation models-large-scale, pre-trained AI models capable of performing a\nwide array of tasks with unprecedented accuracy and efficiency. This paper\nprovides a comprehensive survey of foundation models in the remote sensing\ndomain. We categorize these models based on their architectures, pre-training\ndatasets, and methodologies. Through detailed performance comparisons, we\nhighlight emerging trends and the significant advancements achieved by those\nfoundation models. Additionally, we discuss technical challenges, practical\nimplications, and future research directions, addressing the need for\nhigh-quality data, computational resources, and improved model generalization.\nOur research also finds that pre-training methods, particularly self-supervised\nlearning techniques like contrastive learning and masked autoencoders,\nremarkably enhance the performance and robustness of foundation models. This\nsurvey aims to serve as a resource for researchers and practitioners by\nproviding a panorama of advances and promising pathways for continued\ndevelopment and application of foundation models in remote sensing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "A Causally Informed Pretraining Approach for Multimodal Foundation Models: Applications in Remote Sensing",
    "url": "http://arxiv.org/abs/2407.19660v3",
    "authors": [
      "Praveen Ravirathinam",
      "Ankush Khandelwal",
      "Rahul Ghosh",
      "Vipin Kumar"
    ],
    "published": "2024-07-29",
    "abstract": "Self-supervised learning has emerged as a powerful paradigm for pretraining\nfoundation models using large-scale data. Existing pretraining approaches\npredominantly rely on masked reconstruction or next-token prediction\nstrategies, demonstrating strong performance across various downstream tasks,\nincluding geoscience applications. However, these approaches do not fully\ncapture the causal interplay between different geospatial and environmental\nvariables. To address this limitation, we propose Causally Informed\nVariable-Step Forecasting (CI-VSF), a novel pretraining task that models\nforecasting as a conditional generation task, where driver variables (e.g.,\nweather) inform the prediction of response variables (e.g., satellite imagery).\nWe demonstrate that pretraining in such a fashion leads to enhanced performance\nwhen finetuned on both prediction (e.g., crop mapping, missing image\nprediction, soil moisture estimation) and forecasting (e.g., future image\nforecasting, soil moisture forecasting) downstream tasks when compared to other\npretraining approaches. While we use remote sensing as our main application to\ndemonstrate the efficacy of our proposed pretraining strategy over existing\nparadigms, it is applicable to any domain that involves known causal\nrelationships amongst a set of variables.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Semantic-CC: Boosting Remote Sensing Image Change Captioning via Foundational Knowledge and Semantic Guidance",
    "url": "http://arxiv.org/abs/2407.14032v1",
    "authors": [
      "Yongshuo Zhu",
      "Lu Li",
      "Keyan Chen",
      "Chenyang Liu",
      "Fugen Zhou",
      "Zhenwei Shi"
    ],
    "published": "2024-07-19",
    "abstract": "Remote sensing image change captioning (RSICC) aims to articulate the changes\nin objects of interest within bi-temporal remote sensing images using natural\nlanguage. Given the limitations of current RSICC methods in expressing general\nfeatures across multi-temporal and spatial scenarios, and their deficiency in\nproviding granular, robust, and precise change descriptions, we introduce a\nnovel change captioning (CC) method based on the foundational knowledge and\nsemantic guidance, which we term Semantic-CC. Semantic-CC alleviates the\ndependency of high-generalization algorithms on extensive annotations by\nharnessing the latent knowledge of foundation models, and it generates more\ncomprehensive and accurate change descriptions guided by pixel-level semantics\nfrom change detection (CD). Specifically, we propose a bi-temporal SAM-based\nencoder for dual-image feature extraction; a multi-task semantic aggregation\nneck for facilitating information interaction between heterogeneous tasks; a\nstraightforward multi-scale change detection decoder to provide pixel-level\nsemantic guidance; and a change caption decoder based on the large language\nmodel (LLM) to generate change description sentences. Moreover, to ensure the\nstability of the joint training of CD and CC, we propose a three-stage training\nstrategy that supervises different tasks at various stages. We validate the\nproposed method on the LEVIR-CC and LEVIR-CD datasets. The experimental results\ncorroborate the complementarity of CD and CC, demonstrating that Semantic-CC\ncan generate more accurate change descriptions and achieve optimal performance\nacross both tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Paving the way toward foundation models for irregular and unaligned Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2407.08448v2",
    "authors": [
      "Iris Dumeur",
      "Silvia Valero",
      "Jordi Inglada"
    ],
    "published": "2024-07-11",
    "abstract": "Although recently several foundation models for satellite remote sensing\nimagery have been proposed, they fail to address major challenges of\nreal/operational applications. Indeed, embeddings that don't take into account\nthe spectral, spatial and temporal dimensions of the data as well as the\nirregular or unaligned temporal sampling are of little use for most real world\nuses. As a consequence, we propose an ALIgned Sits Encoder (ALISE), a novel\napproach that leverages the spatial, spectral, and temporal dimensions of\nirregular and unaligned SITS while producing aligned latent representations.\nUnlike SSL models currently available for SITS, ALISE incorporates a flexible\nquery mechanism to project the SITS into a common and learned temporal\nprojection space. Additionally, thanks to a multi-view framework, we explore\nintegration of instance discrimination along a masked autoencoding task to\nSITS. The quality of the produced representation is assessed through three\ndownstream tasks: crop segmentation (PASTIS), land cover segmentation\n(MultiSenGE), and a novel crop change detection dataset. Furthermore, the\nchange detection task is performed without supervision. The results suggest\nthat the use of aligned representations is more effective than previous SSL\nmethods for linear probing segmentation tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Evaluating and Benchmarking Foundation Models for Earth Observation and Geospatial AI",
    "url": "http://arxiv.org/abs/2406.18295v1",
    "authors": [
      "Nikolaos Dionelis",
      "Casper Fibaek",
      "Luke Camilleri",
      "Andreas Luyts",
      "Jente Bosmans",
      "Bertrand Le Saux"
    ],
    "published": "2024-06-26",
    "abstract": "When we are primarily interested in solving several problems jointly with a\ngiven prescribed high performance accuracy for each target application, then\nFoundation Models should for most cases be used rather than problem-specific\nmodels. We focus on the specific Computer Vision application of Foundation\nModels for Earth Observation (EO) and geospatial AI. These models can solve\nimportant problems we are tackling, including for example land cover\nclassification, crop type mapping, flood segmentation, building density\nestimation, and road regression segmentation. In this paper, we show that for a\nlimited number of labelled data, Foundation Models achieve improved performance\ncompared to problem-specific models. In this work, we also present our proposed\nevaluation benchmark for Foundation Models for EO. Benchmarking the\ngeneralization performance of Foundation Models is important as it has become\ndifficult to standardize a fair comparison across the many different models\nthat have been proposed recently. We present the results using our evaluation\nbenchmark for EO Foundation Models and show that Foundation Models are label\nefficient in the downstream tasks and help us solve problems we are tackling in\nEO and remote sensing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Changen2: Multi-Temporal Remote Sensing Generative Change Foundation Model",
    "url": "http://arxiv.org/abs/2406.17998v1",
    "authors": [
      "Zhuo Zheng",
      "Stefano Ermon",
      "Dongjun Kim",
      "Liangpei Zhang",
      "Yanfei Zhong"
    ],
    "published": "2024-06-26",
    "abstract": "Our understanding of the temporal dynamics of the Earth's surface has been\nadvanced by deep vision models, which often require lots of labeled\nmulti-temporal images for training. However, collecting, preprocessing, and\nannotating multi-temporal remote sensing images at scale is non-trivial since\nit is expensive and knowledge-intensive. In this paper, we present change data\ngenerators based on generative models, which are cheap and automatic,\nalleviating these data problems. Our main idea is to simulate a stochastic\nchange process over time. We describe the stochastic change process as a\nprobabilistic graphical model (GPCM), which factorizes the complex simulation\nproblem into two more tractable sub-problems, i.e., change event simulation and\nsemantic change synthesis. To solve these two problems, we present Changen2, a\nGPCM with a resolution-scalable diffusion transformer which can generate time\nseries of images and their semantic and change labels from labeled or unlabeled\nsingle-temporal images. Changen2 is a generative change foundation model that\ncan be trained at scale via self-supervision, and can produce change\nsupervisory signals from unlabeled single-temporal images. Unlike existing\nfoundation models, Changen2 synthesizes change data to train task-specific\nfoundation models for change detection. The resulting model possesses inherent\nzero-shot change detection capabilities and excellent transferability.\nExperiments suggest Changen2 has superior spatiotemporal scalability, e.g.,\nChangen2 model trained on 256$^2$ pixel single-temporal images can yield time\nseries of any length and resolutions of 1,024$^2$ pixels. Changen2 pre-trained\nmodels exhibit superior zero-shot performance (narrowing the performance gap to\n3% on LEVIR-CD and approximately 10% on both S2Looking and SECOND, compared to\nfully supervised counterparts) and transferability across multiple types of\nchange tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Towards a multimodal framework for remote sensing image change retrieval and captioning",
    "url": "http://arxiv.org/abs/2406.13424v1",
    "authors": [
      "Roger Ferrod",
      "Luigi Di Caro",
      "Dino Ienco"
    ],
    "published": "2024-06-19",
    "abstract": "Recently, there has been increasing interest in multimodal applications that\nintegrate text with other modalities, such as images, audio and video, to\nfacilitate natural language interactions with multimodal AI systems. While\napplications involving standard modalities have been extensively explored,\nthere is still a lack of investigation into specific data modalities such as\nremote sensing (RS) data. Despite the numerous potential applications of RS\ndata, including environmental protection, disaster monitoring and land\nplanning, available solutions are predominantly focused on specific tasks like\nclassification, captioning and retrieval. These solutions often overlook the\nunique characteristics of RS data, such as its capability to systematically\nprovide information on the same geographical areas over time. This ability\nenables continuous monitoring of changes in the underlying landscape. To\naddress this gap, we propose a novel foundation model for bi-temporal RS image\npairs, in the context of change detection analysis, leveraging Contrastive\nLearning and the LEVIR-CC dataset for both captioning and text-image retrieval.\nBy jointly training a contrastive encoder and captioning decoder, our model add\ntext-image retrieval capabilities, in the context of bi-temporal change\ndetection, while maintaining captioning performances that are comparable to the\nstate of the art. We release the source code and pretrained weights at:\nhttps://github.com/rogerferrod/RSICRC.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding",
    "url": "http://arxiv.org/abs/2406.12479v1",
    "authors": [
      "Linrui Xu",
      "Ling Zhao",
      "Wang Guo",
      "Qiujun Li",
      "Kewang Long",
      "Kaiqi Zou",
      "Yuhan Wang",
      "Haifeng Li"
    ],
    "published": "2024-06-18",
    "abstract": "The remote sensing image intelligence understanding model is undergoing a new\nprofound paradigm shift which has been promoted by multi-modal large language\nmodel (MLLM), i.e. from the paradigm learning a domain model (LaDM) shifts to\nparadigm learning a pre-trained general foundation model followed by an\nadaptive domain model (LaGD). Under the new LaGD paradigm, the old datasets,\nwhich have led to advances in RSI intelligence understanding in the last\ndecade, are no longer suitable for fire-new tasks. We argued that a new dataset\nmust be designed to lighten tasks with the following features: 1)\nGeneralization: training model to learn shared knowledge among tasks and to\nadapt to different tasks; 2) Understanding complex scenes: training model to\nunderstand the fine-grained attribute of the objects of interest, and to be\nable to describe the scene with natural language; 3) Reasoning: training model\nto be able to realize high-level visual reasoning. In this paper, we designed a\nhigh-quality, diversified, and unified multimodal instruction-following dataset\nfor RSI understanding produced by GPT-4V and existing datasets, which we called\nRS-GPT4V. To achieve generalization, we used a (Question, Answer) which was\ndeduced from GPT-4V via instruction-following to unify the tasks such as\ncaptioning and localization; To achieve complex scene, we proposed a\nhierarchical instruction description with local strategy in which the\nfine-grained attributes of the objects and their spatial relationships are\ndescribed and global strategy in which all the local information are integrated\nto yield detailed instruction descript; To achieve reasoning, we designed\nmultiple-turn QA pair to provide the reasoning ability for a model. The\nempirical results show that the fine-tuned MLLMs by RS-GPT4V can describe\nfine-grained information. The dataset is available at:\nhttps://github.com/GeoX-Lab/RS-GPT4V.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Scaling Efficient Masked Image Modeling on Large Remote Sensing Dataset",
    "url": "http://arxiv.org/abs/2406.11933v4",
    "authors": [
      "Fengxiang Wang",
      "Hongzhen Wang",
      "Di Wang",
      "Zonghao Guo",
      "Zhenyu Zhong",
      "Long Lan",
      "Jing Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "published": "2024-06-17",
    "abstract": "Masked Image Modeling (MIM) has become an essential method for building\nfoundational visual models in remote sensing (RS). However, the limitations in\nsize and diversity of existing RS datasets restrict the ability of MIM methods\nto learn generalizable representations. Additionally, conventional MIM\ntechniques, which require reconstructing all tokens, introduce unnecessary\ncomputational overhead. To address these issues, we present a new pre-training\npipeline for RS models, featuring the creation of a large-scale RS dataset and\nan efficient MIM approach. We curated a high-quality dataset named\nOpticalRS-13M by collecting publicly available RS datasets and processing them\nthrough exclusion, slicing, and deduplication. OpticalRS-13M comprises 13\nmillion optical images covering various RS tasks, such as object detection and\npixel segmentation. To enhance efficiency, we propose SelectiveMAE, a\npre-training method that dynamically encodes and reconstructs semantically rich\npatch tokens, thereby reducing the inefficiencies of traditional MIM models\ncaused by redundant background pixels in RS images. Extensive experiments\ndemonstrate that OpticalRS-13M significantly improves classification,\ndetection, and segmentation performance, while SelectiveMAE increases training\nefficiency over 2 times. This highlights the effectiveness and scalability of\nour pipeline in developing RS foundational models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "RS-DFM: A Remote Sensing Distributed Foundation Model for Diverse Downstream Tasks",
    "url": "http://arxiv.org/abs/2406.07032v1",
    "authors": [
      "Zhechao Wang",
      "Peirui Cheng",
      "Pengju Tian",
      "Yuchao Wang",
      "Mingxin Chen",
      "Shujing Duan",
      "Zhirui Wang",
      "Xinming Li",
      "Xian Sun"
    ],
    "published": "2024-06-11",
    "abstract": "Remote sensing lightweight foundation models have achieved notable success in\nonline perception within remote sensing. However, their capabilities are\nrestricted to performing online inference solely based on their own\nobservations and models, thus lacking a comprehensive understanding of\nlarge-scale remote sensing scenarios. To overcome this limitation, we propose a\nRemote Sensing Distributed Foundation Model (RS-DFM) based on generalized\ninformation mapping and interaction. This model can realize online\ncollaborative perception across multiple platforms and various downstream tasks\nby mapping observations into a unified space and implementing a task-agnostic\ninformation interaction strategy. Specifically, we leverage the ground-based\ngeometric prior of remote sensing oblique observations to transform the feature\nmapping from absolute depth estimation to relative depth estimation, thereby\nenhancing the model's ability to extract generalized features across diverse\nheights and perspectives. Additionally, we present a dual-branch information\ncompression module to decouple high-frequency and low-frequency feature\ninformation, achieving feature-level compression while preserving essential\ntask-agnostic details. In support of our research, we create a multi-task\nsimulation dataset named AirCo-MultiTasks for multi-UAV collaborative\nobservation. We also conduct extensive experiments, including 3D object\ndetection, instance segmentation, and trajectory prediction. The numerous\nresults demonstrate that our RS-DFM achieves state-of-the-art performance\nacross various downstream tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Multi-Label Guided Soft Contrastive Learning for Efficient Earth Observation Pretraining",
    "url": "http://arxiv.org/abs/2405.20462v2",
    "authors": [
      "Yi Wang",
      "Conrad M Albrecht",
      "Xiao Xiang Zhu"
    ],
    "published": "2024-05-30",
    "abstract": "Self-supervised pretraining on large-scale satellite data has raised great\ninterest in building Earth observation (EO) foundation models. However, many\nimportant resources beyond pure satellite imagery, such as land-cover-land-use\nproducts that provide free global semantic information, as well as vision\nfoundation models that hold strong knowledge of the natural world, are not\nwidely studied. In this work, we show these free additional resources not only\nhelp resolve common contrastive learning bottlenecks, but also significantly\nboost the efficiency and effectiveness of EO pretraining. Specifically, we\nfirst propose soft contrastive learning that optimizes cross-scene soft\nsimilarity based on land-cover-generated multi-label supervision, naturally\nsolving the issue of multiple positive samples and too strict positive matching\nin complex scenes. Second, we revisit and explore cross-domain continual\npretraining for both multispectral and SAR imagery, building efficient EO\nfoundation models from strongest vision models such as DINOv2. Adapting simple\nweight-initialization and Siamese masking strategies into our soft contrastive\nlearning framework, we demonstrate impressive continual pretraining performance\neven when the input modalities are not aligned. Without prohibitive training,\nwe produce multispectral and SAR foundation models that achieve significantly\nbetter results in 10 out of 11 downstream tasks than most existing SOTA models.\nFor example, our ResNet50/ViT-S achieve 84.8/85.0 linear probing mAP scores on\nBigEarthNet-10\\% which are better than most existing ViT-L models; under the\nsame setting, our ViT-B sets a new record of 86.8 in multispectral, and 82.5 in\nSAR, the latter even better than many multispectral models. Dataset and models\nare available at \\url{https://github.com/zhu-xlab/softcon}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "FMARS: Annotating Remote Sensing Images for Disaster Management using Foundation Models",
    "url": "http://arxiv.org/abs/2405.20109v2",
    "authors": [
      "Edoardo Arnaudo",
      "Jacopo Lungo Vaschetti",
      "Lorenzo Innocenti",
      "Luca Barco",
      "Davide Lisi",
      "Vanina Fissore",
      "Claudio Rossi"
    ],
    "published": "2024-05-30",
    "abstract": "Very-High Resolution (VHR) remote sensing imagery is increasingly accessible,\nbut often lacks annotations for effective machine learning applications. Recent\nfoundation models like GroundingDINO and Segment Anything (SAM) provide\nopportunities to automatically generate annotations. This study introduces\nFMARS (Foundation Model Annotations in Remote Sensing), a methodology\nleveraging VHR imagery and foundation models for fast and robust annotation. We\nfocus on disaster management and provide a large-scale dataset with labels\nobtained from pre-event imagery over 19 disaster events, derived from the Maxar\nOpen Data initiative. We train segmentation models on the generated labels,\nusing Unsupervised Domain Adaptation (UDA) techniques to increase\ntransferability to real-world scenarios. Our results demonstrate the\neffectiveness of leveraging foundation models to automatically annotate remote\nsensing data at scale, enabling robust downstream models for critical\napplications. Code and dataset are available at\n\\url{https://github.com/links-ads/igarss-fmars}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Research on the Spatial Data Intelligent Foundation Model",
    "url": "http://arxiv.org/abs/2405.19730v5",
    "authors": [
      "Shaohua Wang",
      "Xing Xie",
      "Yong Li",
      "Danhuai Guo",
      "Zhi Cai",
      "Yu Liu",
      "Yang Yue",
      "Xiao Pan",
      "Feng Lu",
      "Huayi Wu",
      "Zhipeng Gui",
      "Zhiming Ding",
      "Bolong Zheng",
      "Fuzheng Zhang",
      "Jingyuan Wang",
      "Zhengchao Chen",
      "Hao Lu",
      "Jiayi Li",
      "Peng Yue",
      "Wenhao Yu",
      "Yao Yao",
      "Leilei Sun",
      "Yong Zhang",
      "Longbiao Chen",
      "Xiaoping Du",
      "Xiang Li",
      "Xueying Zhang",
      "Kun Qin",
      "Zhaoya Gong",
      "Weihua Dong",
      "Xiaofeng Meng"
    ],
    "published": "2024-05-30",
    "abstract": "This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-view Remote Sensing Image Segmentation With SAM priors",
    "url": "http://arxiv.org/abs/2405.14171v1",
    "authors": [
      "Zipeng Qi",
      "Chenyang Liu",
      "Zili Liu",
      "Hao Chen",
      "Yongchang Wu",
      "Zhengxia Zou",
      "Zhenwei Sh"
    ],
    "published": "2024-05-23",
    "abstract": "Multi-view segmentation in Remote Sensing (RS) seeks to segment images from\ndiverse perspectives within a scene. Recent methods leverage 3D information\nextracted from an Implicit Neural Field (INF), bolstering result consistency\nacross multiple views while using limited accounts of labels (even within 3-5\nlabels) to streamline labor. Nonetheless, achieving superior performance within\nthe constraints of limited-view labels remains challenging due to inadequate\nscene-wide supervision and insufficient semantic features within the INF. To\naddress these. we propose to inject the prior of the visual foundation\nmodel-Segment Anything(SAM), to the INF to obtain better results under the\nlimited number of training data. Specifically, we contrast SAM features between\ntesting and training views to derive pseudo labels for each testing view,\naugmenting scene-wide labeling information. Subsequently, we introduce SAM\nfeatures via a transformer into the INF of the scene, supplementing the\nsemantic information. The experimental results demonstrate that our method\noutperforms the mainstream method, confirming the efficacy of SAM as a\nsupplement to the INF for this task.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing Image Generation",
    "url": "http://arxiv.org/abs/2405.13570v3",
    "authors": [
      "Zhiping Yu",
      "Chenyang Liu",
      "Liqin Liu",
      "Zhenwei Shi",
      "Zhengxia Zou"
    ],
    "published": "2024-05-22",
    "abstract": "The recent advancement of generative foundational models has ushered in a new\nera of image generation in the realm of natural images, revolutionizing art\ndesign, entertainment, environment simulation, and beyond. Despite producing\nhigh-quality samples, existing methods are constrained to generating images of\nscenes at a limited scale. In this paper, we present MetaEarth, a generative\nfoundation model that breaks the barrier by scaling image generation to a\nglobal level, exploring the creation of worldwide, multi-resolution, unbounded,\nand virtually limitless remote sensing images. In MetaEarth, we propose a\nresolution-guided self-cascading generative framework, which enables the\ngenerating of images at any region with a wide range of geographical\nresolutions. To achieve unbounded and arbitrary-sized image generation, we\ndesign a novel noise sampling strategy for denoising diffusion models by\nanalyzing the generation conditions and initial noise. To train MetaEarth, we\nconstruct a large dataset comprising multi-resolution optical remote sensing\nimages with geographical information. Experiments have demonstrated the\npowerful capabilities of our method in generating global-scale images.\nAdditionally, the MetaEarth serves as a data engine that can provide\nhigh-quality and rich training data for downstream tasks. Our model opens up\nnew possibilities for constructing generative world models by simulating Earth\nvisuals from an innovative overhead perspective.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "PIR: Remote Sensing Image-Text Retrieval with Prior Instruction Representation Learning",
    "url": "http://arxiv.org/abs/2405.10160v2",
    "authors": [
      "Jiancheng Pan",
      "Muyuan Ma",
      "Qing Ma",
      "Cong Bai",
      "Shengyong Chen"
    ],
    "published": "2024-05-16",
    "abstract": "Remote sensing image-text retrieval constitutes a foundational aspect of\nremote sensing interpretation tasks, facilitating the alignment of vision and\nlanguage representations. This paper introduces a prior instruction\nrepresentation (PIR) learning paradigm that draws on prior knowledge to\ninstruct adaptive learning of vision and text representations. Based on PIR, a\ndomain-adapted remote sensing image-text retrieval framework PIR-ITR is\ndesigned to address semantic noise issues in vision-language understanding\ntasks. However, with massive additional data for pre-training the\nvision-language foundation model, remote sensing image-text retrieval is\nfurther developed into an open-domain retrieval task. Continuing with the\nabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote\nsensing image-text retrieval, to address semantic noise in remote sensing\nvision-language representations and further improve open-domain retrieval\nperformance. In vision representation, we utilize the prior-guided knowledge of\nthe remote sensing scene recognition by building a belief matrix to select key\nfeatures for reducing the impact of semantic noise. In text representation, we\nuse the previous time step to cyclically activate the current time step to\nenhance text representation capability. A cluster-wise Affiliation Loss (AL) is\nproposed to constrain the inter-classes and to reduce the semantic confusion\nzones in the common subspace. Comprehensive experiments demonstrate that PIR\ncould enhance vision and text representations and outperform the\nstate-of-the-art methods of closed-domain and open-domain retrieval on two\nbenchmark datasets, RSICD and RSITMD.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Cross-sensor self-supervised training and alignment for remote sensing",
    "url": "http://arxiv.org/abs/2405.09922v1",
    "authors": [
      "Valerio Marsocci",
      "Nicolas Audebert"
    ],
    "published": "2024-05-16",
    "abstract": "Large-scale \"foundation models\" have gained traction as a way to leverage the\nvast amounts of unlabeled remote sensing data collected every day. However, due\nto the multiplicity of Earth Observation satellites, these models should learn\n\"sensor agnostic\" representations, that generalize across sensor\ncharacteristics with minimal fine-tuning. This is complicated by data\navailability, as low-resolution imagery, such as Sentinel-2 and Landsat-8 data,\nare available in large amounts, while very high-resolution aerial or satellite\ndata is less common. To tackle these challenges, we introduce cross-sensor\nself-supervised training and alignment for remote sensing (X-STARS). We design\na self-supervised training loss, the Multi-Sensor Alignment Dense loss (MSAD),\nto align representations across sensors, even with vastly different\nresolutions. Our X-STARS can be applied to train models from scratch, or to\nadapt large models pretrained on e.g low-resolution EO data to new\nhigh-resolution sensors, in a continual pretraining framework. We collect and\nrelease MSC-France, a new multi-sensor dataset, on which we train our X-STARS\nmodels, then evaluated on seven downstream classification and segmentation\ntasks. We demonstrate that X-STARS outperforms the state-of-the-art by a\nsignificant margin with less data across various conditions of data\navailability and resolutions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Many-Shot In-Context Learning in Multimodal Foundation Models",
    "url": "http://arxiv.org/abs/2405.09798v2",
    "authors": [
      "Yixing Jiang",
      "Jeremy Irvin",
      "Ji Hun Wang",
      "Muhammad Ahmed Chaudhry",
      "Jonathan H. Chen",
      "Andrew Y. Ng"
    ],
    "published": "2024-05-16",
    "abstract": "Large language models are effective at few-shot in-context learning (ICL).\nRecent advancements in multimodal foundation models have enabled\nunprecedentedly long context windows, presenting an opportunity to explore\ntheir capability to perform ICL with many more demonstrating examples. In this\nwork, we evaluate the performance of multimodal foundation models scaling from\nfew-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14\ndatasets spanning multiple domains (natural imagery, medical imagery, remote\nsensing, and molecular imagery) and tasks (image classification, visual QA, and\nobject localization). We observe that many-shot ICL, including up to almost\n2,000 demonstrating examples, leads to substantial improvements compared to\nfew-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5\nPro performance continues to improve log-linearly up to the maximum number of\ntested examples on many datasets. We also find open-weights multimodal\nfoundation models like Llama 3.2-Vision do not benefit from the demonstrating\nexamples, highlighting an important gap between open and closed multimodal\nfoundation models. Given the high inference costs required for many-shot ICL,\nwe also explore the impact of batching multiple queries in a single API call.\nWe show that batching up to 50 queries can lead to performance improvements\nunder zero-shot and many-shot ICL, with substantial gains in the zero-shot\nsetting on multiple datasets, while drastically reducing per-query cost and\nlatency. Finally, while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot\nperformance across the datasets, Gemini 1.5 Pro learns more quickly than GPT-4o\non most datasets. Our results suggest that many-shot ICL could enable users to\nefficiently adapt multimodal foundation models to new applications and domains.\nOur codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL .",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research",
    "url": "http://arxiv.org/abs/2405.08668v1",
    "authors": [
      "Qinglong Cao",
      "Yuntian Chen",
      "Lu Lu",
      "Hao Sun",
      "Zhenzhong Zeng",
      "Xiaokang Yang",
      "Dongxiao Zhang"
    ],
    "published": "2024-05-14",
    "abstract": "Large-scale Vision-Language Models (VLMs) have demonstrated exceptional\nperformance in natural vision tasks, motivating researchers across domains to\nexplore domain-specific VLMs. However, the construction of powerful\ndomain-specific VLMs demands vast amounts of annotated data, substantial\nelectrical energy, and computing resources, primarily accessible to industry,\nyet hindering VLM research in academia. To address this challenge and foster\nsustainable and equitable VLM research, we present the Generalized Domain\nPrompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs' robust\nrecognition capabilities from natural vision to specialized domains, without\nthe need for extensive data or resources. By leveraging small-scale\ndomain-specific foundation models and minimal prompt samples, GDPL empowers the\nlanguage branch with domain knowledge through quaternion networks, uncovering\ncross-modal relationships between domain-specific vision features and natural\nvision-based contextual embeddings. Simultaneously, GDPL guides the vision\nbranch into specific domains through hierarchical propagation of generated\nvision prompt features, grounded in well-matched vision-language relations.\nFurthermore, to fully harness the domain adaptation potential of VLMs, we\nintroduce a novel low-rank adaptation approach. Extensive experiments across\ndiverse domains like remote sensing, medical imaging, geology, Synthetic\nAperture Radar, and fluid dynamics, validate the efficacy of GDPL,\ndemonstrating its ability to achieve state-of-the-art domain recognition\nperformance in a prompt learning paradigm. Our framework paves the way for\nsustainable and inclusive VLM research, transcending the barriers between\nacademia and industry.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "SOAR: Advancements in Small Body Object Detection for Aerial Imagery Using State Space Models and Programmable Gradients",
    "url": "http://arxiv.org/abs/2405.01699v2",
    "authors": [
      "Tushar Verma",
      "Jyotsna Singh",
      "Yash Bhartari",
      "Rishi Jarwal",
      "Suraj Singh",
      "Shubhkarman Singh"
    ],
    "published": "2024-05-02",
    "abstract": "Small object detection in aerial imagery presents significant challenges in\ncomputer vision due to the minimal data inherent in small-sized objects and\ntheir propensity to be obscured by larger objects and background noise.\nTraditional methods using transformer-based models often face limitations\nstemming from the lack of specialized databases, which adversely affect their\nperformance with objects of varying orientations and scales. This underscores\nthe need for more adaptable, lightweight models. In response, this paper\nintroduces two innovative approaches that significantly enhance detection and\nsegmentation capabilities for small aerial objects. Firstly, we explore the use\nof the SAHI framework on the newly introduced lightweight YOLO v9 architecture,\nwhich utilizes Programmable Gradient Information (PGI) to reduce the\nsubstantial information loss typically encountered in sequential feature\nextraction processes. The paper employs the Vision Mamba model, which\nincorporates position embeddings to facilitate precise location-aware visual\nunderstanding, combined with a novel bidirectional State Space Model (SSM) for\neffective visual context modeling. This State Space Model adeptly harnesses the\nlinear complexity of CNNs and the global receptive field of Transformers,\nmaking it particularly effective in remote sensing image classification. Our\nexperimental results demonstrate substantial improvements in detection accuracy\nand processing efficiency, validating the applicability of these approaches for\nreal-time small object detection across diverse aerial scenarios. This paper\nalso discusses how these methodologies could serve as foundational models for\nfuture advancements in aerial object recognition technologies. The source code\nwill be made accessible here.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "When are Foundation Models Effective? Understanding the Suitability for Pixel-Level Classification Using Multispectral Imagery",
    "url": "http://arxiv.org/abs/2404.11797v1",
    "authors": [
      "Yiqun Xie",
      "Zhihao Wang",
      "Weiye Chen",
      "Zhili Li",
      "Xiaowei Jia",
      "Yanhua Li",
      "Ruichen Wang",
      "Kangyang Chai",
      "Ruohan Li",
      "Sergii Skakun"
    ],
    "published": "2024-04-17",
    "abstract": "Foundation models, i.e., very large deep learning models, have demonstrated\nimpressive performances in various language and vision tasks that are otherwise\ndifficult to reach using smaller-size models. The major success of GPT-type of\nlanguage models is particularly exciting and raises expectations on the\npotential of foundation models in other domains including satellite remote\nsensing. In this context, great efforts have been made to build foundation\nmodels to test their capabilities in broader applications, and examples include\nPrithvi by NASA-IBM, Segment-Anything-Model, ViT, etc. This leads to an\nimportant question: Are foundation models always a suitable choice for\ndifferent remote sensing tasks, and when or when not? This work aims to enhance\nthe understanding of the status and suitability of foundation models for\npixel-level classification using multispectral imagery at moderate resolution,\nthrough comparisons with traditional machine learning (ML) and regular-size\ndeep learning models. Interestingly, the results reveal that in many scenarios\ntraditional ML models still have similar or better performance compared to\nfoundation models, especially for tasks where texture is less useful for\nclassification. On the other hand, deep learning models did show more promising\nresults for tasks where labels partially depend on texture (e.g., burn scar),\nwhile the difference in performance between foundation models and deep learning\nmodels is not obvious. The results conform with our analysis: The suitability\nof foundation models depend on the alignment between the self-supervised\nlearning tasks and the real downstream tasks, and the typical masked\nautoencoder paradigm is not necessarily suitable for many remote sensing\nproblems.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "RSMamba: Remote Sensing Image Classification with State Space Model",
    "url": "http://arxiv.org/abs/2403.19654v1",
    "authors": [
      "Keyan Chen",
      "Bowen Chen",
      "Chenyang Liu",
      "Wenyuan Li",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2024-03-28",
    "abstract": "Remote sensing image classification forms the foundation of various\nunderstanding tasks, serving a crucial function in remote sensing image\ninterpretation. The recent advancements of Convolutional Neural Networks (CNNs)\nand Transformers have markedly enhanced classification accuracy. Nonetheless,\nremote sensing scene classification remains a significant challenge, especially\ngiven the complexity and diversity of remote sensing scenarios and the\nvariability of spatiotemporal resolutions. The capacity for whole-image\nunderstanding can provide more precise semantic cues for scene discrimination.\nIn this paper, we introduce RSMamba, a novel architecture for remote sensing\nimage classification. RSMamba is based on the State Space Model (SSM) and\nincorporates an efficient, hardware-aware design known as the Mamba. It\nintegrates the advantages of both a global receptive field and linear modeling\ncomplexity. To overcome the limitation of the vanilla Mamba, which can only\nmodel causal sequences and is not adaptable to two-dimensional image data, we\npropose a dynamic multi-path activation mechanism to augment Mamba's capacity\nto model non-causal data. Notably, RSMamba maintains the inherent modeling\nmechanism of the vanilla Mamba, yet exhibits superior performance across\nmultiple remote sensing image classification datasets. This indicates that\nRSMamba holds significant potential to function as the backbone of future\nvisual foundation models. The code will be available at\n\\url{https://github.com/KyanChen/RSMamba}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining",
    "url": "http://arxiv.org/abs/2403.13430v2",
    "authors": [
      "Di Wang",
      "Jing Zhang",
      "Minqiang Xu",
      "Lin Liu",
      "Dongsheng Wang",
      "Erzhong Gao",
      "Chengxi Han",
      "Haonan Guo",
      "Bo Du",
      "Dacheng Tao",
      "Liangpei Zhang"
    ],
    "published": "2024-03-20",
    "abstract": "Foundation models have reshaped the landscape of Remote Sensing (RS) by\nenhancing various image interpretation tasks. Pretraining is an active research\ntopic, encompassing supervised and self-supervised learning methods to\ninitialize model weights effectively. However, transferring the pretrained\nmodels to downstream tasks may encounter task discrepancy due to their\nformulation of pretraining as image classification or object discrimination\ntasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for\nRS foundation models to address this issue. Using a shared encoder and\ntask-specific decoder architecture, we conduct multi-task supervised\npretraining on the SAMRS dataset, encompassing semantic segmentation, instance\nsegmentation, and rotated object detection. MTP supports both convolutional\nneural networks and vision transformer foundation models with over 300 million\nparameters. The pretrained models are finetuned on various RS downstream tasks,\nsuch as scene classification, horizontal and rotated object detection, semantic\nsegmentation, and change detection. Extensive experiments across 14 datasets\ndemonstrate the superiority of our models over existing ones of similar size\nand their competitive performance compared to larger state-of-the-art models,\nthus validating the effectiveness of MTP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model",
    "url": "http://arxiv.org/abs/2403.07564v2",
    "authors": [
      "Mingze Wang",
      "Lili Su",
      "Cilin Yan",
      "Sheng Xu",
      "Pengcheng Yuan",
      "Xiaolong Jiang",
      "Baochang Zhang"
    ],
    "published": "2024-03-12",
    "abstract": "The intelligent interpretation of buildings plays a significant role in urban\nplanning and management, macroeconomic analysis, population dynamics, etc.\nRemote sensing image building interpretation primarily encompasses building\nextraction and change detection. However, current methodologies often treat\nthese two tasks as separate entities, thereby failing to leverage shared\nknowledge. Moreover, the complexity and diversity of remote sensing image\nscenes pose additional challenges, as most algorithms are designed to model\nindividual small datasets, thus lacking cross-scene generalization. In this\npaper, we propose a comprehensive remote sensing image building understanding\nmodel, termed RSBuilding, developed from the perspective of the foundation\nmodel. RSBuilding is designed to enhance cross-scene generalization and task\nuniversality. Specifically, we extract image features based on the prior\nknowledge of the foundation model and devise a multi-level feature sampler to\naugment scale information. To unify task representation and integrate image\nspatiotemporal clues, we introduce a cross-attention decoder with task prompts.\nAddressing the current shortage of datasets that incorporate annotations for\nboth tasks, we have developed a federated training strategy to facilitate\nsmooth model convergence even when supervision for some tasks is missing,\nthereby bolstering the complementarity of different tasks. Our model was\ntrained on a dataset comprising up to 245,000 images and validated on multiple\nbuilding extraction and change detection datasets. The experimental results\nsubstantiate that RSBuilding can concurrently handle two structurally distinct\ntasks and exhibits robust zero-shot generalization capabilities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa",
    "url": "http://arxiv.org/abs/2403.06860v2",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Mukhtar Opeyemi Yusuf",
      "Kobby Panford-Quainoo",
      "Arnu Pretorius"
    ],
    "published": "2024-03-11",
    "abstract": "Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2403.02059v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Viktoria Moor",
      "Romeo Kienzler",
      "Thomas Brunschwiler"
    ],
    "published": "2024-03-04",
    "abstract": "Image retrieval enables an efficient search through vast amounts of satellite\nimagery and returns similar images to a query. Deep learning models can\nidentify images across various semantic concepts without the need for\nannotations. This work proposes to use Geospatial Foundation Models, like\nPrithvi, for remote sensing image retrieval with multiple benefits: i) the\nmodels encode multi-spectral satellite data and ii) generalize without further\nfine-tuning. We introduce two datasets to the retrieval task and observe a\nstrong performance: Prithvi processes six bands and achieves a mean Average\nPrecision of 97.62% on BigEarthNet-43 and 44.51% on ForestNet-12, outperforming\nother RGB-based models. Further, we evaluate three compression methods with\nbinarized embeddings balancing retrieval speed and accuracy. They match the\nretrieval speed of much shorter hash codes while maintaining the same accuracy\nas floating-point embeddings but with a 32-fold compression. The code is\navailable at https://github.com/IBM/remote-sensing-image-retrieval.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ChatEarthNet: A Global-Scale Image-Text Dataset Empowering Vision-Language Geo-Foundation Models",
    "url": "http://arxiv.org/abs/2402.11325v2",
    "authors": [
      "Zhenghang Yuan",
      "Zhitong Xiong",
      "Lichao Mou",
      "Xiao Xiang Zhu"
    ],
    "published": "2024-02-17",
    "abstract": "An in-depth comprehension of global land cover is essential in Earth\nobservation, forming the foundation for a multitude of applications. Although\nremote sensing technology has advanced rapidly, leading to a proliferation of\nsatellite imagery, the inherent complexity of these images often makes them\ndifficult for non-expert users to understand. Natural language, as a carrier of\nhuman knowledge, can be a bridge between common users and complicated satellite\nimagery. In this context, we introduce a global-scale, high-quality image-text\ndataset for remote sensing, providing natural language descriptions for\nSentinel-2 data to facilitate the understanding of satellite imagery for common\nusers. Specifically, we utilize Sentinel-2 data for its global coverage as the\nfoundational image source, employing semantic segmentation labels from the\nEuropean Space Agency's (ESA) WorldCover project to enrich the descriptions of\nland covers. By conducting in-depth semantic analysis, we formulate detailed\nprompts to elicit rich descriptions from ChatGPT. To enhance the dataset's\nquality, we introduce the manual verification process. This step involves\nmanual inspection and correction to refine the dataset, thus significantly\nimproving its accuracy and quality. Finally, we offer the community\nChatEarthNet, a large-scale image-text dataset characterized by global\ncoverage, high quality, wide-ranging diversity, and detailed descriptions.\nChatEarthNet consists of 163,488 image-text pairs with captions generated by\nChatGPT-3.5 and an additional 10,000 image-text pairs with captions generated\nby ChatGPT-4V(ision). This dataset has significant potential for training\nvision-language geo-foundation models and evaluating large vision-language\nmodels for remote sensing. The dataset will be made publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment",
    "url": "http://arxiv.org/abs/2402.09816v1",
    "authors": [
      "Angelos Zavras",
      "Dimitrios Michail",
      "Beg\u00fcm Demir",
      "Ioannis Papoutsis"
    ],
    "published": "2024-02-15",
    "abstract": "Deep Learning (DL) is undergoing a paradigm shift with the emergence of\nfoundation models, aptly named by their crucial, yet incomplete nature. In this\nwork, we focus on Contrastive Language-Image Pre-training (CLIP), an\nopen-vocabulary foundation model, which achieves high accuracy across many\nimage classification tasks and is often competitive with a fully supervised\nbaseline without being explicitly trained. Nevertheless, there are still\ndomains where zero-shot CLIP performance is far from optimal, such as Remote\nSensing (RS) and medical imagery. These domains do not only exhibit\nfundamentally different distributions compared to natural images, but also\ncommonly rely on complementary modalities, beyond RGB, to derive meaningful\ninsights. To this end, we propose a methodology for the purpose of aligning\ndistinct RS imagery modalities with the visual and textual modalities of CLIP.\nOur two-stage procedure, comprises of robust fine-tuning CLIP in order to deal\nwith the distribution shift, accompanied by the cross-modal alignment of a RS\nmodality encoder, in an effort to extend the zero-shot capabilities of CLIP. We\nultimately demonstrate our method on the tasks of RS imagery classification and\ncross-modal retrieval. We empirically show that both robust fine-tuning and\ncross-modal alignment translate to significant performance gains, across\nseveral RS benchmark datasets. Notably, these enhancements are achieved without\nthe reliance on textual descriptions, without introducing any task-specific\nparameters, without training from scratch and without catastrophic forgetting.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models",
    "url": "http://arxiv.org/abs/2401.09083v1",
    "authors": [
      "Haonan Guo",
      "Xin Su",
      "Chen Wu",
      "Bo Du",
      "Liangpei Zhang",
      "Deren Li"
    ],
    "published": "2024-01-17",
    "abstract": "Recently, the flourishing large language models(LLM), especially ChatGPT,\nhave shown exceptional performance in language understanding, reasoning, and\ninteraction, attracting users and researchers from multiple fields and domains.\nAlthough LLMs have shown great capacity to perform human-like task\naccomplishment in natural language and natural image, their potential in\nhandling remote sensing interpretation tasks has not yet been fully explored.\nMoreover, the lack of automation in remote sensing task planning hinders the\naccessibility of remote sensing interpretation techniques, especially to\nnon-remote sensing experts from multiple research fields. To this end, we\npresent Remote Sensing ChatGPT, an LLM-powered agent that utilizes ChatGPT to\nconnect various AI-based remote sensing models to solve complicated\ninterpretation tasks. More specifically, given a user request and a remote\nsensing image, we utilized ChatGPT to understand user requests, perform task\nplanning according to the tasks' functions, execute each subtask iteratively,\nand generate the final response according to the output of each subtask.\nConsidering that LLM is trained with natural language and is not capable of\ndirectly perceiving visual concepts as contained in remote sensing images, we\ndesigned visual cues that inject visual information into ChatGPT. With Remote\nSensing ChatGPT, users can simply send a remote sensing image with the\ncorresponding request, and get the interpretation results as well as language\nfeedback from Remote Sensing ChatGPT. Experiments and examples show that Remote\nSensing ChatGPT can tackle a wide range of remote sensing tasks and can be\nextended to more tasks with more sophisticated models such as the remote\nsensing foundation model. The code and demo of Remote Sensing ChatGPT is\npublicly available at https://github.com/HaonanGuo/Remote-Sensing-ChatGPT .",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM)",
    "url": "http://arxiv.org/abs/2401.09019v1",
    "authors": [
      "Hongruixuan Chen",
      "Jian Song",
      "Naoto Yokoya"
    ],
    "published": "2024-01-17",
    "abstract": "Unsupervised multimodal change detection is pivotal for time-sensitive tasks\nand comprehensive multi-temporal Earth monitoring. In this study, we explore\nunsupervised multimodal change detection between two key remote sensing data\nsources: optical high-resolution imagery and OpenStreetMap (OSM) data.\nSpecifically, we propose to utilize the vision foundation model Segmentation\nAnything Model (SAM), for addressing our task. Leveraging SAM's exceptional\nzero-shot transfer capability, high-quality segmentation maps of optical images\ncan be obtained. Thus, we can directly compare these two heterogeneous data\nforms in the so-called segmentation domain. We then introduce two strategies\nfor guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt'\nmethods. The two strategies are designed to detect land-cover changes in\ngeneral scenarios and to identify new land-cover objects within existing\nbackgrounds, respectively. Experimental results on three datasets indicate that\nthe proposed approach can achieve more competitive results compared to\nrepresentative unsupervised multimodal change detection methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "OBSeg: Accurate and Fast Instance Segmentation Framework Using Segmentation Foundation Models with Oriented Bounding Box Prompts",
    "url": "http://arxiv.org/abs/2401.08174v6",
    "authors": [
      "Zhen Zhou",
      "Junfeng Fan",
      "Yunkai Ma",
      "Sihan Zhao",
      "Fengshui Jing",
      "Min Tan"
    ],
    "published": "2024-01-16",
    "abstract": "Instance segmentation in remote sensing images is a long-standing challenge.\nSince horizontal bounding boxes introduce many interference objects, oriented\nbounding boxes (OBBs) are usually used for instance identification. However,\nbased on ``segmentation within bounding box'' paradigm, current instance\nsegmentation methods using OBBs are overly dependent on bounding box detection\nperformance. To tackle this problem, this paper proposes OBSeg, an accurate and\nfast instance segmentation framework using OBBs. OBSeg is based on box\nprompt-based segmentation foundation models (BSMs), e.g., Segment Anything\nModel. Specifically, OBSeg first detects OBBs to distinguish instances and\nprovide coarse localization information. Then, it predicts OBB prompt-related\nmasks for fine segmentation. Since OBBs only serve as prompts, OBSeg alleviates\nthe over-dependence on bounding box detection performance of current instance\nsegmentation methods using OBBs. Thanks to OBB prompts, OBSeg outperforms other\ncurrent BSM-based methods using HBBs. In addition, to enable BSMs to handle OBB\nprompts, we propose a novel OBB prompt encoder. To make OBSeg more lightweight\nand further improve the performance of lightweight distilled BSMs, a Gaussian\nsmoothing-based knowledge distillation method is introduced. Experiments\ndemonstrate that OBSeg outperforms current instance segmentation methods on\nmultiple datasets in terms of instance segmentation accuracy and has\ncompetitive inference speed. The code is available at\nhttps://github.com/zhen6618/OBBInstanceSegmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
    "url": "http://arxiv.org/abs/2504.11171v1",
    "authors": [
      "Johannes Jakubik",
      "Felix Yang",
      "Benedikt Blumenstiel",
      "Erik Scheurer",
      "Rocco Sedona",
      "Stefano Maurogiovanni",
      "Jente Bosmans",
      "Nikolaos Dionelis",
      "Valerio Marsocci",
      "Niklas Kopp",
      "Rahul Ramachandran",
      "Paolo Fraccaro",
      "Thomas Brunschwiler",
      "Gabriele Cavallaro",
      "Juan Bernabe-Moreno",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-04-15",
    "abstract": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "TerraTorch: The Geospatial Foundation Models Toolkit",
    "url": "http://arxiv.org/abs/2503.20563v1",
    "authors": [
      "Carlos Gomes",
      "Benedikt Blumenstiel",
      "Joao Lucas de Sousa Almeida",
      "Pedro Henrique de Oliveira",
      "Paolo Fraccaro",
      "Francesc Marti Escofet",
      "Daniela Szwarcman",
      "Naomi Simumba",
      "Romeo Kienzler",
      "Bianca Zadrozny"
    ],
    "published": "2025-03-26",
    "abstract": "TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial\nFoundation Models built on PyTorch Lightning and tailored for satellite,\nweather, and climate data. It integrates domain-specific data modules,\npre-defined tasks, and a modular model factory that pairs any backbone with\ndiverse decoder heads. These components allow researchers and practitioners to\nfine-tune supported models in a no-code fashion by simply editing a training\nconfiguration. By consolidating best practices for model development and\nincorporating the automated hyperparameter optimization extension Iterate,\nTerraTorch reduces the expertise and time required to fine-tune or benchmark\nmodels on new Earth Observation use cases. Furthermore, TerraTorch directly\nintegrates with GEO-Bench, allowing for systematic and reproducible\nbenchmarking of Geospatial Foundation Models. TerraTorch is open sourced under\nApache 2.0, available at https://github.com/IBM/terratorch, and can be\ninstalled via pip install terratorch.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data",
    "url": "http://arxiv.org/abs/2503.12843v3",
    "authors": [
      "Haozhe Si",
      "Yuxuan Wan",
      "Minh Do",
      "Deepak Vasisht",
      "Han Zhao",
      "Hendrik F. Hamann"
    ],
    "published": "2025-03-17",
    "abstract": "Geospatial raster data, such as that collected by satellite-based imaging\nsystems at different times and spectral bands, hold immense potential for\nenabling a wide range of high-impact applications. This potential stems from\nthe rich information that is spatially and temporally contextualized across\nmultiple channels and sensing modalities. Recent work has adapted existing\nself-supervised learning approaches for such geospatial data. However, they\nfall short of scalable model architectures, leading to inflexibility and\ncomputational inefficiencies when faced with an increasing number of channels\nand modalities. To address these limitations, we introduce Low-rank Efficient\nSpatial-Spectral Vision Transformer with three key innovations: i) the LESS\nAttention Block that approximates high-dimensional spatial-spectral attention\nthrough Kronecker's product of the low-dimensional spatial and spectral\nattention components; ii) the Continuous Positional-Channel Embedding Layer\nthat preserves both the continuity and physical characteristics of each\nspatial-spectral patch; and iii) the Perception Field Mask that exploits local\nspatial dependencies by constraining attention to neighboring patches. To\nevaluate the proposed innovations, we construct GFM-Bench, which serves as a\ncomprehensive benchmark for such geospatial raster data. We pretrain LESS ViT\nusing a Hyperspectral Masked Autoencoder framework with integrated positional\nand channel masking strategies. Experimental results demonstrate that our\nproposed method achieves competitive performance against state-of-the-art\nmulti-modal geospatial foundation models while outperforming them on\ncross-satellite generalization tasks with higher computational efficiency. The\nflexibility and extensibility of our framework make it a promising direction\nfor future geospatial data analysis tasks that involve a wide range of\nmodalities and channels.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection",
    "url": "http://arxiv.org/abs/2503.09493v1",
    "authors": [
      "Romain Thoreau",
      "Valerio Marsocci",
      "Dawa Derksen"
    ],
    "published": "2025-03-12",
    "abstract": "As large-scale heterogeneous data sets become increasingly available,\nadapting foundation models at low cost has become a key issue. Seminal works in\nnatural language processing, e.g. Low-Rank Adaptation (LoRA), leverage the low\n\"intrinsic rank\" of parameter updates during adaptation. In this paper, we\nargue that incorporating stronger inductive biases in both data and models can\nenhance the adaptation of Geospatial Foundation Models (GFMs), pretrained on\nRGB satellite images, to other types of optical satellite data. Specifically,\nthe pretrained parameters of GFMs serve as a strong prior for the spatial\nstructure of multispectral images. For this reason, we introduce DEFLECT\n(Deflecting Embeddings for Finetuning Latent representations for Earth and\nClimate Tasks), a novel strategy for adapting GFMs to multispectral satellite\nimagery with very few additional parameters. DEFLECT improves the\nrepresentation capabilities of the extracted features, particularly enhancing\nspectral information, which is essential for geoscience and\nenvironmental-related tasks. We demonstrate the effectiveness of our method\nacross three different GFMs and five diverse datasets, ranging from forest\nmonitoring to marine environment segmentation. Compared to competing methods,\nDEFLECT achieves on-par or higher accuracy with 5-10$\\times$ fewer parameters\nfor classification and segmentation tasks. The code will be made publicly\navailable.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Lossy Neural Compression for Geospatial Analytics: A Review",
    "url": "http://arxiv.org/abs/2503.01505v1",
    "authors": [
      "Carlos Gomes",
      "Isabelle Wittmann",
      "Damien Robert",
      "Johannes Jakubik",
      "Tim Reichelt",
      "Michele Martone",
      "Stefano Maurogiovanni",
      "Rikard Vinge",
      "Jonas Hurst",
      "Erik Scheurer",
      "Rocco Sedona",
      "Thomas Brunschwiler",
      "Stefan Kesselheim",
      "Matej Batic",
      "Philip Stier",
      "Jan Dirk Wegner",
      "Gabriele Cavallaro",
      "Edzer Pebesma",
      "Michael Marszalek",
      "Miguel A Belenguer-Plomer",
      "Kennedy Adriko",
      "Paolo Fraccaro",
      "Romeo Kienzler",
      "Rania Briq",
      "Sabrina Benassou",
      "Michele Lazzarini",
      "Conrad M Albrecht"
    ],
    "published": "2025-03-03",
    "abstract": "Over the past decades, there has been an explosion in the amount of available\nEarth Observation (EO) data. The unprecedented coverage of the Earth's surface\nand atmosphere by satellite imagery has resulted in large volumes of data that\nmust be transmitted to ground stations, stored in data centers, and distributed\nto end users. Modern Earth System Models (ESMs) face similar challenges,\noperating at high spatial and temporal resolutions, producing petabytes of data\nper simulated day. Data compression has gained relevance over the past decade,\nwith neural compression (NC) emerging from deep learning and information\ntheory, making EO data and ESM outputs ideal candidates due to their abundance\nof unlabeled data. In this review, we outline recent developments in NC applied\nto geospatial data. We introduce the fundamental concepts of NC including\nseminal works in its traditional applications to image and video compression\ndomains with focus on lossy compression. We discuss the unique characteristics\nof EO and ESM data, contrasting them with \"natural images\", and explain the\nadditional challenges and opportunities they present. Moreover, we review\ncurrent applications of NC across various EO modalities and explore the limited\nefforts in ESM compression to date. The advent of self-supervised learning\n(SSL) and foundation models (FM) has advanced methods to efficiently distill\nrepresentations from vast unlabeled data. We connect these developments to NC\nfor EO, highlighting the similarities between the two fields and elaborate on\nthe potential of transferring compressed feature representations for\nmachine--to--machine communication. Based on insights drawn from this review,\nwe devise future directions relevant to applications in EO and ESM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SSL4EO-S12 v1.1: A Multimodal, Multiseasonal Dataset for Pretraining, Updated",
    "url": "http://arxiv.org/abs/2503.00168v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Nassim Ait Ali Braham",
      "Conrad M Albrecht",
      "Stefano Maurogiovanni",
      "Paolo Fraccaro"
    ],
    "published": "2025-02-28",
    "abstract": "This technical report presents SSL4EO-S12 v1.1, a multimodal, multitemporal\nEarth Observation dataset designed for pretraining large-scale foundation\nmodels. Building on the success of SSL4EO-S12 v1.0, the new version addresses\nthe previous challenges of data misalignment and a limited data structure for\nlow-barrier, analysis-ready EO processing. SSL4EO-S12 v1.1 covers the world's\n10,000 largest cities and its surroundings within a 50 km radius across four\nseasons, resulting in a diverse collection of nearly one million patches.\nSSL4EO-S12 v1.1 packages the data in Zarr file format for cloud-efficient\nloading and representation of meta-information such as including cloud masks\nand geolocation. Released under the CC-BY-4.0 license, SSL4EO-S12 v1.1\nfacilitates open research and provides a robust foundation for future\nadvancements in self-supervised learning and geospatial analysis. The dataset\nis available online through https://datapub.fz-juelich.de/ssl4eo-s12, and we\nprovided additional resources at https://github.com/DLR-MF-DAS/SSL4EO-S12-v1.1.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "How Does the Spatial Distribution of Pre-training Data Affect Geospatial Foundation Models?",
    "url": "http://arxiv.org/abs/2501.12535v1",
    "authors": [
      "Mirali Purohit",
      "Gedeon Muhawenayo",
      "Esther Rolf",
      "Hannah Kerner"
    ],
    "published": "2025-01-21",
    "abstract": "Foundation models have made rapid advances in many domains including Earth\nobservation, where Geospatial Foundation Models (GFMs) can help address global\nchallenges such as climate change, agriculture, and disaster response. Previous\nwork on GFMs focused on tailoring model architecture and pre-text tasks, and\ndid not investigate the impact of pre-training data selection on model\nperformance. However, recent works from other domains show that the\npre-training data distribution is an important factor influencing the\nperformance of the foundation models. With this motivation, our research\nexplores how the geographic distribution of pre-training data affects the\nperformance of GFMs. We evaluated several pre-training data distributions by\nsampling different compositions from a global data pool. Our experiments with\ntwo GFMs on downstream tasks indicate that balanced and globally representative\ndata compositions often outperform region-specific sampling, highlighting the\nimportance of diversity and global coverage in pre-training data. Our results\nsuggest that the most appropriate data sampling technique may depend on the\nspecific GFM architecture. These findings will support the development of\nrobust GFMs by incorporating quality pre-training data distributions,\nultimately improving machine learning solutions for Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2412.04204v2",
    "authors": [
      "Valerio Marsocci",
      "Yuru Jia",
      "Georges Le Bellier",
      "David Kerekes",
      "Liang Zeng",
      "Sebastian Hafner",
      "Sebastian Gerard",
      "Eric Brune",
      "Ritu Yadav",
      "Ali Shibli",
      "Heng Fang",
      "Yifang Ban",
      "Maarten Vergauwen",
      "Nicolas Audebert",
      "Andrea Nascetti"
    ],
    "published": "2024-12-05",
    "abstract": "Geospatial Foundation Models (GFMs) have emerged as powerful tools for\nextracting representations from Earth observation data, but their evaluation\nremains inconsistent and narrow. Existing works often evaluate on suboptimal\ndownstream datasets and tasks, that are often too easy or too narrow, limiting\nthe usefulness of the evaluations to assess the real-world applicability of\nGFMs. Additionally, there is a distinct lack of diversity in current evaluation\nprotocols, which fail to account for the multiplicity of image resolutions,\nsensor types, and temporalities, which further complicates the assessment of\nGFM performance. In particular, most existing benchmarks are geographically\nbiased towards North America and Europe, questioning the global applicability\nof GFMs. To overcome these challenges, we introduce PANGAEA, a standardized\nevaluation protocol that covers a diverse set of datasets, tasks, resolutions,\nsensor modalities, and temporalities. It establishes a robust and widely\napplicable benchmark for GFMs. We evaluate the most popular GFMs openly\navailable on this benchmark and analyze their performance across several\ndomains. In particular, we compare these models to supervised baselines (e.g.\nUNet and vanilla ViT), and assess their effectiveness when faced with limited\nlabeled data. Our findings highlight the limitations of GFMs, under different\nscenarios, showing that they do not consistently outperform supervised models.\nPANGAEA is designed to be highly extensible, allowing for the seamless\ninclusion of new datasets, models, and tasks in future research. By releasing\nthe evaluation code and benchmark, we aim to enable other researchers to\nreplicate our experiments and build upon our work, fostering a more principled\nevaluation protocol for large pre-trained geospatial models. The code is\navailable at https://github.com/VMarsocci/pangaea-bench.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "General Geospatial Inference with a Population Dynamics Foundation Model",
    "url": "http://arxiv.org/abs/2411.07207v4",
    "authors": [
      "Mohit Agarwal",
      "Mimi Sun",
      "Chaitanya Kamath",
      "Arbaaz Muslim",
      "Prithul Sarker",
      "Joydeep Paul",
      "Hector Yee",
      "Marcin Sieniek",
      "Kim Jablonski",
      "Yael Mayer",
      "David Fork",
      "Sheila de Guia",
      "Jamie McPike",
      "Adam Boulanger",
      "Tomer Shekel",
      "David Schottlander",
      "Yao Xiao",
      "Manjit Chakravarthy Manukonda",
      "Yun Liu",
      "Neslihan Bulut",
      "Sami Abu-el-haija",
      "Bryan Perozzi",
      "Monica Bharel",
      "Von Nguyen",
      "Luke Barrington",
      "Niv Efron",
      "Yossi Matias",
      "Greg Corrado",
      "Krish Eswaran",
      "Shruthi Prabhakara",
      "Shravya Shetty",
      "Gautam Prasad"
    ],
    "published": "2024-11-11",
    "abstract": "Supporting the health and well-being of dynamic populations around the world\nrequires governmental agencies, organizations and researchers to understand and\nreason over complex relationships between human behavior and local contexts in\norder to identify high-risk groups and strategically allocate limited\nresources. Traditional approaches to these classes of problems often entail\ndeveloping manually curated, task-specific features and models to represent\nhuman behavior and the natural and built environment, which can be challenging\nto adapt to new, or even, related tasks. To address this, we introduce a\nPopulation Dynamics Foundation Model (PDFM) that aims to capture the\nrelationships between diverse data modalities and is applicable to a broad\nrange of geospatial tasks. We first construct a geo-indexed dataset for postal\ncodes and counties across the United States, capturing rich aggregated\ninformation on human behavior from maps, busyness, and aggregated search\ntrends, and environmental factors such as weather and air quality. We then\nmodel this data and the complex relationships between locations using a graph\nneural network, producing embeddings that can be adapted to a wide range of\ndownstream tasks using relatively simple models. We evaluate the effectiveness\nof our approach by benchmarking it on 27 downstream tasks spanning three\ndistinct domains: health indicators, socioeconomic factors, and environmental\nmeasurements. The approach achieves state-of-the-art performance on all 27\ngeospatial interpolation tasks, and on 25 out of the 27 extrapolation and\nsuper-resolution tasks. We combined the PDFM with a state-of-the-art\nforecasting foundation model, TimesFM, to predict unemployment and poverty,\nachieving performance that surpasses fully supervised forecasting. The full set\nof embeddings and sample code are publicly available for researchers.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "MapSAM: Adapting Segment Anything Model for Automated Feature Detection in Historical Maps",
    "url": "http://arxiv.org/abs/2411.06971v1",
    "authors": [
      "Xue Xia",
      "Daiwei Zhang",
      "Wenxuan Song",
      "Wei Huang",
      "Lorenz Hurni"
    ],
    "published": "2024-11-11",
    "abstract": "Automated feature detection in historical maps can significantly accelerate\nthe reconstruction of the geospatial past. However, this process is often\nconstrained by the time-consuming task of manually digitizing sufficient\nhigh-quality training data. The emergence of visual foundation models, such as\nthe Segment Anything Model (SAM), offers a promising solution due to their\nremarkable generalization capabilities and rapid adaptation to new data\ndistributions. Despite this, directly applying SAM in a zero-shot manner to\nhistorical map segmentation poses significant challenges, including poor\nrecognition of certain geospatial features and a reliance on input prompts,\nwhich limits its ability to be fully automated. To address these challenges, we\nintroduce MapSAM, a parameter-efficient fine-tuning strategy that adapts SAM\ninto a prompt-free and versatile solution for various downstream historical map\nsegmentation tasks. Specifically, we employ Weight-Decomposed Low-Rank\nAdaptation (DoRA) to integrate domain-specific knowledge into the image\nencoder. Additionally, we develop an automatic prompt generation process,\neliminating the need for manual input. We further enhance the positional prompt\nin SAM, transforming it into a higher-level positional-semantic prompt, and\nmodify the cross-attention mechanism in the mask decoder with masked attention\nfor more effective feature aggregation. The proposed MapSAM framework\ndemonstrates promising performance across two distinct historical map\nsegmentation tasks: one focused on linear features and the other on areal\nfeatures. Experimental results show that it adapts well to various features,\neven when fine-tuned with extremely limited data (e.g. 10 shots).",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Multimodal Contrastive Learning of Urban Space Representations from POI Data",
    "url": "http://arxiv.org/abs/2411.06229v1",
    "authors": [
      "Xinglei Wang",
      "Tao Cheng",
      "Stephen Law",
      "Zichao Zeng",
      "Lu Yin",
      "Junyuan Liu"
    ],
    "published": "2024-11-09",
    "abstract": "Existing methods for learning urban space representations from\nPoint-of-Interest (POI) data face several limitations, including issues with\ngeographical delineation, inadequate spatial information modelling,\nunderutilisation of POI semantic attributes, and computational inefficiencies.\nTo address these issues, we propose CaLLiPer (Contrastive Language-Location\nPre-training), a novel representation learning model that directly embeds\ncontinuous urban spaces into vector representations that can capture the\nspatial and semantic distribution of urban environment. This model leverages a\nmultimodal contrastive learning objective, aligning location embeddings with\ntextual POI descriptions, thereby bypassing the need for complex training\ncorpus construction and negative sampling. We validate CaLLiPer's effectiveness\nby applying it to learning urban space representations in London, UK, where it\ndemonstrates 5-15% improvement in predictive performance for land use\nclassification and socioeconomic mapping tasks compared to state-of-the-art\nmethods. Visualisations of the learned representations further illustrate our\nmodel's advantages in capturing spatial variations in urban semantics with high\naccuracy and fine resolution. Additionally, CaLLiPer achieves reduced training\ntime, showcasing its efficiency and scalability. This work provides a promising\npathway for scalable, semantically rich urban space representation learning\nthat can support the development of geospatial foundation models. The\nimplementation code is available at https://github.com/xlwang233/CaLLiPer.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "On the Generalizability of Foundation Models for Crop Type Mapping",
    "url": "http://arxiv.org/abs/2409.09451v3",
    "authors": [
      "Yi-Chia Chang",
      "Adam J. Stewart",
      "Favyen Bastani",
      "Piper Wolters",
      "Shreya Kannan",
      "George R. Huber",
      "Jingtong Wang",
      "Arindam Banerjee"
    ],
    "published": "2024-09-14",
    "abstract": "Foundation models pre-trained using self-supervised learning have shown\npowerful transfer learning capabilities on various downstream tasks, including\nlanguage understanding, text generation, and image recognition. The Earth\nobservation (EO) field has produced several foundation models pre-trained\ndirectly on multispectral satellite imagery for applications like precision\nagriculture, wildfire and drought monitoring, and natural disaster response.\nHowever, few studies have investigated the ability of these models to\ngeneralize to new geographic locations, and potential concerns of geospatial\nbias -- models trained on data-rich developed nations not transferring well to\ndata-scarce developing nations -- remain. We investigate the ability of popular\nEO foundation models to transfer to new geographic regions in the agricultural\ndomain, where differences in farming practices and class imbalance make\ntransfer learning particularly challenging. We first select five crop\nclassification datasets across five continents, normalizing for dataset size\nand harmonizing classes to focus on four major cereal grains: maize, soybean,\nrice, and wheat. We then compare three popular foundation models, pre-trained\non SSL4EO-S12, SatlasPretrain, and ImageNet, using in-distribution (ID) and\nout-of-distribution (OOD) evaluation. Experiments show that pre-trained weights\ndesigned explicitly for Sentinel-2, such as SSL4EO-S12, outperform general\npre-trained weights like ImageNet. Furthermore, while only 100 labeled images\nare sufficient for achieving high overall accuracy, 900 images are required to\nachieve high average accuracy due to class imbalance. All harmonized datasets\nand experimental code are open-source and available for download.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Evaluating the Effectiveness of Large Language Models in Representing and Understanding Movement Trajectories",
    "url": "http://arxiv.org/abs/2409.00335v1",
    "authors": [
      "Yuhan Ji",
      "Song Gao"
    ],
    "published": "2024-08-31",
    "abstract": "This research focuses on assessing the ability of AI foundation models in\nrepresenting the trajectories of movements. We utilize one of the large\nlanguage models (LLMs) (i.e., GPT-J) to encode the string format of\ntrajectories and then evaluate the effectiveness of the LLM-based\nrepresentation for trajectory data analysis. The experiments demonstrate that\nwhile the LLM-based embeddings can preserve certain trajectory distance metrics\n(i.e., the correlation coefficients exceed 0.74 between the Cosine distance\nderived from GPT-J embeddings and the Hausdorff and Dynamic Time Warping\ndistances on raw trajectories), challenges remain in restoring numeric values\nand retrieving spatial neighbors in movement trajectory analytics. In addition,\nthe LLMs can understand the spatiotemporal dependency contained in trajectories\nand have good accuracy in location prediction tasks. This research highlights\nthe need for improvement in terms of capturing the nuances and complexities of\nthe underlying geospatial data and integrating domain knowledge to support\nvarious GeoAI applications using LLMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Self-Supervised Representation Learning for Geospatial Objects: A Survey",
    "url": "http://arxiv.org/abs/2408.12133v2",
    "authors": [
      "Yile Chen",
      "Weiming Huang",
      "Kaiqi Zhao",
      "Yue Jiang",
      "Gao Cong"
    ],
    "published": "2024-08-22",
    "abstract": "The proliferation of various data sources in urban and territorial\nenvironments has significantly facilitated the development of geospatial\nartificial intelligence (GeoAI) across a wide range of geospatial applications.\nHowever, geospatial data, which is inherently linked to geospatial objects,\noften exhibits data heterogeneity that necessitates specialized fusion and\nrepresentation strategies while simultaneously being inherently sparse in\nlabels for downstream tasks. Consequently, there is a growing demand for\ntechniques that can effectively leverage geospatial data without heavy reliance\non task-specific labels and model designs. This need aligns with the principles\nof self-supervised learning (SSL), which has garnered increasing attention for\nits ability to learn effective and generalizable representations directly from\ndata without extensive labeled supervision. This paper presents a comprehensive\nand up-to-date survey of SSL techniques specifically applied to or developed\nfor geospatial objects in three primary vector geometric types: Point,\nPolyline, and Polygon. We systematically categorize various SSL techniques into\npredictive and contrastive methods, and analyze their adaptation to different\ndata types for representation learning across various downstream tasks.\nFurthermore, we examine the emerging trends in SSL for geospatial objects,\nparticularly the gradual advancements towards geospatial foundation models.\nFinally, we discuss key challenges in current research and outline promising\ndirections for future investigation. By offering a structured analysis of\nexisting studies, this paper aims to inspire continued progress in integrating\nSSL with geospatial objects, and the development of geospatial foundation\nmodels in a longer term.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Fine-tuning of Geospatial Foundation Models for Aboveground Biomass Estimation",
    "url": "http://arxiv.org/abs/2406.19888v1",
    "authors": [
      "Michal Muszynski",
      "Levente Klein",
      "Ademir Ferreira da Silva",
      "Anjani Prasad Atluri",
      "Carlos Gomes",
      "Daniela Szwarcman",
      "Gurkanwar Singh",
      "Kewen Gu",
      "Maciel Zortea",
      "Naomi Simumba",
      "Paolo Fraccaro",
      "Shraddha Singh",
      "Steve Meliksetian",
      "Campbell Watson",
      "Daiki Kimura",
      "Harini Srinivasan"
    ],
    "published": "2024-06-28",
    "abstract": "Global vegetation structure mapping is critical for understanding the global\ncarbon cycle and maximizing the efficacy of nature-based carbon sequestration\ninitiatives. Moreover, vegetation structure mapping can help reduce the impacts\nof climate change by, for example, guiding actions to improve water security,\nincrease biodiversity and reduce flood risk. Global satellite measurements\nprovide an important set of observations for monitoring and managing\ndeforestation and degradation of existing forests, natural forest regeneration,\nreforestation, biodiversity restoration, and the implementation of sustainable\nagricultural practices. In this paper, we explore the effectiveness of\nfine-tuning of a geospatial foundation model to estimate above-ground biomass\n(AGB) using space-borne data collected across different eco-regions in Brazil.\nThe fine-tuned model architecture consisted of a Swin-B transformer as the\nencoder (i.e., backbone) and a single convolutional layer for the decoder head.\nAll results were compared to a U-Net which was trained as the baseline model\nExperimental results of this sparse-label prediction task demonstrate that the\nfine-tuned geospatial foundation model with a frozen encoder has comparable\nperformance to a U-Net trained from scratch. This is despite the fine-tuned\nmodel having 13 times less parameters requiring optimization, which saves both\ntime and compute resources. Further, we explore the transfer-learning\ncapabilities of the geospatial foundation models by fine-tuning on satellite\nimagery with sparse labels from different eco-regions in Brazil.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Geode: A Zero-shot Geospatial Question-Answering Agent with Explicit Reasoning and Precise Spatio-Temporal Retrieval",
    "url": "http://arxiv.org/abs/2407.11014v1",
    "authors": [
      "Devashish Vikas Gupta",
      "Azeez Syed Ali Ishaqui",
      "Divya Kiran Kadiyala"
    ],
    "published": "2024-06-26",
    "abstract": "Large language models (LLMs) have shown promising results in learning and\ncontextualizing information from different forms of data. Recent advancements\nin foundational models, particularly those employing self-attention mechanisms,\nhave significantly enhanced our ability to comprehend the semantics of diverse\ndata types. One such area that could highly benefit from multi-modality is in\nunderstanding geospatial data, which inherently has multiple modalities.\nHowever, current Natural Language Processing (NLP) mechanisms struggle to\neffectively address geospatial queries. Existing pre-trained LLMs are\ninadequately equipped to meet the unique demands of geospatial data, lacking\nthe ability to retrieve precise spatio-temporal data in real-time, thus leading\nto significantly reduced accuracy in answering complex geospatial queries. To\naddress these limitations, we introduce Geode--a pioneering system designed to\ntackle zero-shot geospatial question-answering tasks with high precision using\nspatio-temporal data retrieval. Our approach represents a significant\nimprovement in addressing the limitations of current LLM models, demonstrating\nremarkable improvement in geospatial question-answering abilities compared to\nexisting state-of-the-art pre-trained models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GFM4MPM: Towards Geospatial Foundation Models for Mineral Prospectivity Mapping",
    "url": "http://arxiv.org/abs/2406.12756v1",
    "authors": [
      "Angel Daruna",
      "Vasily Zadorozhnyy",
      "Georgina Lukoczki",
      "Han-Pang Chiu"
    ],
    "published": "2024-06-18",
    "abstract": "Machine Learning (ML) for Mineral Prospectivity Mapping (MPM) remains a\nchallenging problem as it requires the analysis of associations between\nlarge-scale multi-modal geospatial data and few historical mineral commodity\nobservations (positive labels). Recent MPM works have explored Deep Learning\n(DL) as a modeling tool with more representation capacity. However, these\noverparameterized methods may be more prone to overfitting due to their\nreliance on scarce labeled data. While a large quantity of unlabeled geospatial\ndata exists, no prior MPM works have considered using such information in a\nself-supervised manner. Our MPM approach uses a masked image modeling framework\nto pretrain a backbone neural network in a self-supervised manner using\nunlabeled geospatial data alone. After pretraining, the backbone network\nprovides feature extraction for downstream MPM tasks. We evaluated our approach\nalongside existing methods to assess mineral prospectivity of Mississippi\nValley Type (MVT) and Clastic-Dominated (CD) Lead-Zinc deposits in North\nAmerica and Australia. Our results demonstrate that self-supervision promotes\nrobustness in learned features, improving prospectivity predictions.\nAdditionally, we leverage explainable artificial intelligence techniques to\ndemonstrate that individual predictions can be interpreted from a geological\nperspective.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Towards Vision-Language Geo-Foundation Model: A Survey",
    "url": "http://arxiv.org/abs/2406.09385v1",
    "authors": [
      "Yue Zhou",
      "Litong Feng",
      "Yiping Ke",
      "Xue Jiang",
      "Junchi Yan",
      "Xue Yang",
      "Wayne Zhang"
    ],
    "published": "2024-06-13",
    "abstract": "Vision-Language Foundation Models (VLFMs) have made remarkable progress on\nvarious multimodal tasks, such as image captioning, image-text retrieval,\nvisual question answering, and visual grounding. However, most methods rely on\ntraining with general image datasets, and the lack of geospatial data leads to\npoor performance on earth observation. Numerous geospatial image-text pair\ndatasets and VLFMs fine-tuned on them have been proposed recently. These new\napproaches aim to leverage large-scale, multimodal geospatial data to build\nversatile intelligent models with diverse geo-perceptive capabilities, which we\nrefer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper\nthoroughly reviews VLGFMs, summarizing and analyzing recent developments in the\nfield. In particular, we introduce the background and motivation behind the\nrise of VLGFMs, highlighting their unique research significance. Then, we\nsystematically summarize the core technologies employed in VLGFMs, including\ndata construction, model architectures, and applications of various multimodal\ngeospatial tasks. Finally, we conclude with insights, issues, and discussions\nregarding future research directions. To the best of our knowledge, this is the\nfirst comprehensive literature review of VLGFMs. We keep tracing related works\nat https://github.com/zytx121/Awesome-VLGFM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SeeFar: Satellite Agnostic Multi-Resolution Dataset for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2406.06776v1",
    "authors": [
      "James Lowman",
      "Kelly Liu Zheng",
      "Roydon Fraser",
      "Jesse Van Griensven The",
      "Mojtaba Valipour"
    ],
    "published": "2024-06-10",
    "abstract": "SeeFar is an evolving collection of multi-resolution satellite images from\npublic and commercial satellites. We specifically curated this dataset for\ntraining geospatial foundation models, unconstrained by satellite type. In\nrecent years, advances in technology have made satellite imagery more\naccessible than ever. More earth-observing satellites have been launched in the\nlast five years than in the previous fifty. Modern commercial satellites now\noffer up to 100 times the spatial resolution of public access satellites.\nHowever, the high cost and limited historical availability of commercial\nsatellite imagery is a barrier to the training of foundational models,\nimpacting what images can be used during inference. The SeeFar dataset\nrepresents a step towards training models that are satellite-agnostic by\ncombining multi-resolution commercial and public access pre-processed images.\nThis will enable users to utilize historical data alongside higher-resolution,\nmore expensive satellite imagery, offering greater flexibility during\ninference. To achieve this, we describe a process for standardizing data from\ndiverse satellite sources, normalizing different data formats, and aligning\nspectral bands to enhance interoperability. The SeeFar dataset includes images\nat a resolution of 384x384 pixels, spanning four spectral bands (Blue, Green,\nRed, and Near-Infrared) and expanding spatial resolutions (starting with 30,\n10, 1.5, and 1.0 meters), all in cloud-optimized GeoTIFF format. It also\nprovides consistent and comprehensive metadata to enhance data transparency and\nreliability. By aggregating data from multiple sources, SeeFar makes processed\nand consistent satellite data accessible to a wider range of users - from\nresearchers to policymakers - fostering competition and innovation in satellite\nimagery analysis. The dataset is available at \\url{coastalcarbon.ai/seefar}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery",
    "url": "http://arxiv.org/abs/2405.02512v2",
    "authors": [
      "Yohei Nakayama",
      "Jiawei Su",
      "Luis M. Pazos-Out\u00f3n"
    ],
    "published": "2024-05-03",
    "abstract": "Recent advancements in foundation models have significantly impacted various\nfields, including natural language processing, computer vision, and multi-modal\ntasks. One area that stands to benefit greatly is Earth observation, where\nthese models can efficiently process large-scale, unlabeled geospatial data. In\nthis work we extend the SwinMAE model to integrate temporal information for\nsatellite time-series data. The architecture employs a hierarchical 3D Masked\nAutoencoder (MAE) with Video Swin Transformer blocks to effectively capture\nmulti-scale spatio-temporal dependencies in satellite imagery. To enhance\ntransfer learning, we incorporate both encoder and decoder pretrained weights,\nalong with skip connections to preserve scale-specific information. This forms\nan architecture similar to SwinUNet with an additional temporal component. Our\napproach shows significant performance improvements over existing\nstate-of-the-art foundation models for all the evaluated downstream tasks: land\ncover segmentation, building density prediction, flood mapping, wildfire scar\nmapping and multi-temporal crop segmentation. Particularly, in the land cover\nsegmentation task of the PhilEO Bench dataset, it outperforms other geospatial\nfoundation models with a 10.4% higher accuracy.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "NLP-enabled Trajectory Map-matching in Urban Road Networks using a Transformer-based Encoder-decoder",
    "url": "http://arxiv.org/abs/2404.12460v4",
    "authors": [
      "Sevin Mohammadi",
      "Andrew W. Smyth"
    ],
    "published": "2024-04-18",
    "abstract": "Vehicular trajectory data from geolocation telematics is vital for analyzing\nurban mobility patterns. Map-matching aligns noisy, sparsely sampled GPS\ntrajectories with digital road maps to reconstruct accurate vehicle paths.\nTraditional methods rely on geometric proximity, topology, and shortest-path\nheuristics, but they overlook two key factors: (1) drivers may prefer routes\nbased on local road characteristics rather than shortest paths, revealing\nlearnable shared preferences, and (2) GPS noise varies spatially due to\nmultipath effects. These factors can reduce the effectiveness of conventional\nmethods in complex scenarios and increase the effort required for\nheuristic-based implementations. This study introduces a data-driven, deep\nlearning-based map-matching framework, formulating the task as machine\ntranslation, inspired by NLP. Specifically, a transformer-based encoder-decoder\nmodel learns contextual representations of noisy GPS points to infer trajectory\nbehavior and road structures in an end-to-end manner. Trained on large-scale\ntrajectory data, the method improves path estimation accuracy. Experiments on\nsynthetic trajectories show that this approach outperforms conventional methods\nby integrating contextual awareness. Evaluation on real-world GPS traces from\nManhattan, New York, achieves 75% accuracy in reconstructing navigated routes.\nThese results highlight the effectiveness of transformers in capturing drivers'\ntrajectory behaviors, spatial dependencies, and noise patterns, offering a\nscalable, robust solution for map-matching. This work contributes to advancing\ntrajectory-driven foundation models for geospatial modeling and urban mobility\napplications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Pretraining Billion-scale Geospatial Foundational Models on Frontier",
    "url": "http://arxiv.org/abs/2404.11706v1",
    "authors": [
      "Aristeidis Tsaris",
      "Philipe Ambrozio Dias",
      "Abhishek Potnis",
      "Junqi Yin",
      "Feiyi Wang",
      "Dalton Lunga"
    ],
    "published": "2024-04-17",
    "abstract": "As AI workloads increase in scope, generalization capability becomes\nchallenging for small task-specific models and their demand for large amounts\nof labeled training samples increases. On the contrary, Foundation Models (FMs)\nare trained with internet-scale unlabeled data via self-supervised learning and\nhave been shown to adapt to various tasks with minimal fine-tuning. Although\nlarge FMs have demonstrated significant impact in natural language processing\nand computer vision, efforts toward FMs for geospatial applications have been\nrestricted to smaller size models, as pretraining larger models requires very\nlarge computing resources equipped with state-of-the-art hardware accelerators.\nCurrent satellite constellations collect 100+TBs of data a day, resulting in\nimages that are billions of pixels and multimodal in nature. Such geospatial\ndata poses unique challenges opening up new opportunities to develop FMs. We\ninvestigate billion scale FMs and HPC training profiles for geospatial\napplications by pretraining on publicly available data. We studied from\nend-to-end the performance and impact in the solution by scaling the model\nsize. Our larger 3B parameter size model achieves up to 30% improvement in top1\nscene classification accuracy when comparing a 100M parameter model. Moreover,\nwe detail performance experiments on the Frontier supercomputer, America's\nfirst exascale system, where we study different model and data parallel\napproaches using PyTorch's Fully Sharded Data Parallel library. Specifically,\nwe study variants of the Vision Transformer architecture (ViT), conducting\nperformance analysis for ViT models with size up to 15B parameters. By\ndiscussing throughput and performance bottlenecks under different parallelism\nconfigurations, we offer insights on how to leverage such leadership-class HPC\nresources when developing large models for geospatial imagery applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2404.01260v1",
    "authors": [
      "Boran Han",
      "Shuai Zhang",
      "Xingjian Shi",
      "Markus Reichstein"
    ],
    "published": "2024-04-01",
    "abstract": "In the realm of geospatial analysis, the diversity of remote sensors,\nencompassing both optical and microwave technologies, offers a wealth of\ndistinct observational capabilities. Recognizing this, we present msGFM, a\nmultisensor geospatial foundation model that effectively unifies data from four\nkey sensor modalities. This integration spans an expansive dataset of two\nmillion multisensor images. msGFM is uniquely adept at handling both paired and\nunpaired sensor data. For data originating from identical geolocations, our\nmodel employs an innovative cross-sensor pretraining approach in masked image\nmodeling, enabling the synthesis of joint representations from diverse sensors.\nmsGFM, incorporating four remote sensors, upholds strong performance, forming a\ncomprehensive model adaptable to various sensor types. msGFM has demonstrated\nenhanced proficiency in a range of both single-sensor and multisensor\ndownstream tasks. These include scene classification, segmentation, cloud\nremoval, and pan-sharpening. A key discovery of our research is that\nrepresentations derived from natural images are not always compatible with the\ndistinct characteristics of geospatial remote sensors, underscoring the\nlimitations of existing representations in this field. Our work can serve as a\nguide for developing multisensor geospatial pretraining models, paving the way\nfor more advanced geospatial capabilities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Large Language Models are Geographically Biased",
    "url": "http://arxiv.org/abs/2402.02680v2",
    "authors": [
      "Rohin Manvi",
      "Samar Khanna",
      "Marshall Burke",
      "David Lobell",
      "Stefano Ermon"
    ],
    "published": "2024-02-05",
    "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their\ntraining corpora, which can lead to the perpetuation of societal harm. As the\nimpact of these foundation models grows, understanding and evaluating their\nbiases becomes crucial to achieving fairness and accuracy. We propose to study\nwhat LLMs know about the world we live in through the lens of geography. This\napproach is particularly powerful as there is ground truth for the numerous\naspects of human life that are meaningfully projected onto geographic space\nsuch as culture, race, language, politics, and religion. We show various\nproblematic geographic biases, which we define as systemic errors in geospatial\npredictions. Initially, we demonstrate that LLMs are capable of making accurate\nzero-shot geospatial predictions in the form of ratings that show strong\nmonotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We\nthen show that LLMs exhibit common biases across a range of objective and\nsubjective topics. In particular, LLMs are clearly biased against locations\nwith lower socioeconomic conditions (e.g. most of Africa) on a variety of\nsensitive subjective topics such as attractiveness, morality, and intelligence\n(Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to\nquantify this and find that there is significant variation in the magnitude of\nbias across existing LLMs. Code is available on the project website:\nhttps://rohinmanvi.github.io/GeoLLM",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping",
    "url": "http://arxiv.org/abs/2401.08787v1",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Sizhe Wang",
      "Yezhou Yang",
      "Hyunho Lee",
      "Anna Liljedahl",
      "Chandi Witharana",
      "Yili Yang",
      "Brendan M. Rogers",
      "Samantha T. Arundel",
      "Matthew B. Jones",
      "Kenton McHenry",
      "Patricia Solis"
    ],
    "published": "2024-01-16",
    "abstract": "This paper assesses trending AI foundation models, especially emerging\ncomputer vision foundation models and their performance in natural landscape\nfeature segmentation. While the term foundation model has quickly garnered\ninterest from the geospatial domain, its definition remains vague. Hence, this\npaper will first introduce AI foundation models and their defining\ncharacteristics. Built upon the tremendous success achieved by Large Language\nModels (LLMs) as the foundation models for language tasks, this paper discusses\nthe challenges of building foundation models for geospatial artificial\nintelligence (GeoAI) vision tasks. To evaluate the performance of large AI\nvision models, especially Meta's Segment Anything Model (SAM), we implemented\ndifferent instance segmentation pipelines that minimize the changes to SAM to\nleverage its power as a foundation model. A series of prompt strategies was\ndeveloped to test SAM's performance regarding its theoretical upper bound of\npredictive accuracy, zero-shot performance, and domain adaptability through\nfine-tuning. The analysis used two permafrost feature datasets, ice-wedge\npolygons and retrogressive thaw slumps because (1) these landform features are\nmore challenging to segment than manmade features due to their complicated\nformation mechanisms, diverse forms, and vague boundaries; (2) their presence\nand changes are important indicators for Arctic warming and climate change. The\nresults show that although promising, SAM still has room for improvement to\nsupport AI-augmented terrain mapping. The spatial and domain generalizability\nof this finding is further validated using a more general dataset EuroCrop for\nagricultural field mapping. Finally, we discuss future research directions that\nstrengthen SAM's applicability in challenging geospatial domains.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies",
    "url": "http://arxiv.org/abs/2504.20203v1",
    "authors": [
      "Vladyslav Polushko",
      "Damjan Hatic",
      "Ronald R\u00f6sch",
      "Thomas M\u00e4rz",
      "Markus Rauhut",
      "Andreas Weinmann"
    ],
    "published": "2025-04-28",
    "abstract": "Floods cause serious problems around the world. Responding quickly and\neffectively requires accurate and timely information about the affected areas.\nThe effective use of Remote Sensing images for accurate flood detection\nrequires specific detection methods. Typically, Deep Neural Networks are\nemployed, which are trained on specific datasets. For the purpose of river\nflood detection in RGB imagery, we use the BlessemFlood21 dataset. We here\nexplore the use of different augmentation strategies, ranging from basic\napproaches to more complex techniques, including optical distortion. By\nidentifying effective strategies, we aim to refine the training process of\nstate-of-the-art Deep Learning segmentation networks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2504.19598v1",
    "authors": [
      "Dou Quan",
      "Rufan Zhou",
      "Shuang Wang",
      "Ning Huyan",
      "Dong Zhao",
      "Yunan Li",
      "Licheng Jiao"
    ],
    "published": "2025-04-28",
    "abstract": "Deep learning methods have shown promising performances in remote sensing\nimage change detection (CD). However, existing methods usually train a\ndataset-specific deep network for each dataset. Due to the significant\ndifferences in the data distribution and labeling between various datasets, the\ntrained dataset-specific deep network has poor generalization performances on\nother datasets. To solve this problem, this paper proposes a change adapter\nnetwork (CANet) for a more universal and generalized CD. CANet contains\ndataset-shared and dataset-specific learning modules. The former explores the\ndiscriminative features of images, and the latter designs a lightweight adapter\nmodel, to deal with the characteristics of different datasets in data\ndistribution and labeling. The lightweight adapter can quickly generalize the\ndeep network for new CD tasks with a small computation cost. Specifically, this\npaper proposes an interesting change region mask (ICM) in the adapter, which\ncan adaptively focus on interested change objects and decrease the influence of\nlabeling differences in various datasets. Moreover, CANet adopts a unique batch\nnormalization layer for each dataset to deal with data distribution\ndifferences. Compared with existing deep learning methods, CANet can achieve\nsatisfactory CD performances on various datasets simultaneously. Experimental\nresults on several public datasets have verified the effectiveness and\nadvantages of the proposed CANet on CD. CANet has a stronger generalization\nability, smaller training costs (merely updating 4.1%-7.7% parameters), and\nbetter performances under limited training datasets than other deep learning\nmethods, which also can be flexibly inserted with existing deep models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Photon Absorption Remote Sensing Virtual Histopathology: Diagnostic Equivalence to Gold-Standard H&E Staining in Skin Cancer Excisional Biopsies",
    "url": "http://arxiv.org/abs/2504.18737v1",
    "authors": [
      "Benjamin R. Ecclestone",
      "James E. D. Tweel",
      "Marie Abi Daoud",
      "Hager Gaouda",
      "Deepak Dinakaran",
      "Michael P. Wallace",
      "Ally Khan Somani",
      "Gilbert Bigras",
      "John R. Mackey",
      "Parsin Haji Reza"
    ],
    "published": "2025-04-25",
    "abstract": "Photon Absorption Remote Sensing (PARS) enables label-free imaging of\nsubcellular morphology by observing biomolecule specific absorption\ninteractions. Coupled with deep-learning, PARS produces label-free virtual\nHematoxylin and Eosin (H&E) stained images in unprocessed tissues. This study\nevaluates the diagnostic performance of these PARS-derived virtual H&E images\nin benign and malignant excisional skin biopsies, including Squamous (SCC),\nBasal (BCC) Cell Carcinoma, and normal skin. Sixteen unstained formalin-fixed\nparaffin-embedded skin excisions were PARS imaged, virtually H&E stained, then\nchemically stained and imaged at 40x. Seven fellowship trained\ndermatopathologists assessed all 32 images in a masked randomized fashion.\nConcordance analysis indicates 95.5% agreement between primary diagnoses\nrendered on PARS versus H&E images (Cohen's k=0.93). Inter-rater reliability\nwas near-perfect for both image types (Fleiss' k=0.89 for PARS, k=0.80 for\nH&E). For subtype classification, agreement was near-perfect 91% (k=0.73) for\nSCC and was perfect for BCC. When assessing malignancy confinement (e.g.,\ncancer margins), agreement was 92% between PARS and H&E (k=0.718). During\nassessment dermatopathologists could not reliably distinguish image origin\n(PARS vs. H&E), and diagnostic confidence was equivalent between the\nmodalities. Inter-rater reliability for PARS virtual H&E was consistent with\nreported benchmarks for histologic evaluation. These results indicate that PARS\nvirtual histology may be diagnostically equivalent to traditional H&E staining\nin dermatopathology diagnostics, while enabling assessment directly from\nunlabeled, or unprocessed slides. In turn, the label-free PARS virtual H&E\nimaging workflow may preserve tissue for downstream analysis while producing\ndata well-suited for AI integration potentially accelerating and enhancing the\naccuracy of skin cancer diagnostics.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition",
    "url": "http://arxiv.org/abs/2504.16467v1",
    "authors": [
      "Qishan He",
      "Lingjun Zhao",
      "Ru Luo",
      "Siqian Zhang",
      "Lin Lei",
      "Kefeng Ji",
      "Gangyao Kuang"
    ],
    "published": "2025-04-23",
    "abstract": "Aircraft recognition in synthetic aperture radar (SAR) imagery is a\nfundamental mission in both military and civilian applications. Recently deep\nlearning (DL) has emerged a dominant paradigm for its explosive performance on\nextracting discriminative features. However, current classification algorithms\nfocus primarily on learning decision hyperplane without enough comprehension on\naircraft structural knowledge. Inspired by the fined aircraft annotation\nmethods for optical remote sensing images (RSI), we first introduce a\nstructure-based SAR aircraft annotations approach to provide structural and\ncompositional supplement information. On this basis, we propose a multi-task\nstructure guided learning (MTSGL) network for robust and interpretable SAR\naircraft recognition. Besides the classification task, MTSGL includes a\nstructural semantic awareness (SSA) module and a structural consistency\nregularization (SCR) module. The SSA is designed to capture structure semantic\ninformation, which is conducive to gain human-like comprehension of aircraft\nknowledge. The SCR helps maintain the geometric consistency between the\naircraft structure in SAR imagery and the proposed annotation. In this process,\nthe structural attribute can be disentangled in a geometrically meaningful\nmanner. In conclusion, the MTSGL is presented with the expert-level aircraft\nprior knowledge and structure guided learning paradigm, aiming to comprehend\nthe aircraft concept in a way analogous to the human cognitive process.\nExtensive experiments are conducted on a self-constructed multi-task SAR\naircraft recognition dataset (MT-SARD) and the effective results illustrate the\nsuperiority of robustness and interpretation ability of the proposed MTSGL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "DAM-Net: Domain Adaptation Network with Micro-Labeled Fine-Tuning for Change Detection",
    "url": "http://arxiv.org/abs/2504.13748v1",
    "authors": [
      "Hongjia Chen",
      "Xin Xu",
      "Fangling Pu"
    ],
    "published": "2025-04-18",
    "abstract": "Change detection (CD) in remote sensing imagery plays a crucial role in\nvarious applications such as urban planning, damage assessment, and resource\nmanagement. While deep learning approaches have significantly advanced CD\nperformance, current methods suffer from poor domain adaptability, requiring\nextensive labeled data for retraining when applied to new scenarios. This\nlimitation severely restricts their practical applications across different\ndatasets. In this work, we propose DAM-Net: a Domain Adaptation Network with\nMicro-Labeled Fine-Tuning for CD. Our network introduces adversarial domain\nadaptation to CD for, utilizing a specially designed segmentation-discriminator\nand alternating training strategy to enable effective transfer between domains.\nAdditionally, we propose a novel Micro-Labeled Fine-Tuning approach that\nstrategically selects and labels a minimal amount of samples (less than 1%) to\nenhance domain adaptation. The network incorporates a Multi-Temporal\nTransformer for feature fusion and optimized backbone structure based on\nprevious research. Experiments conducted on the LEVIR-CD and WHU-CD datasets\ndemonstrate that DAM-Net significantly outperforms existing domain adaptation\nmethods, achieving comparable performance to semi-supervised approaches that\nrequire 10% labeled data while using only 0.3% labeled samples. Our approach\nsignificantly advances cross-dataset CD applications and provides a new\nparadigm for efficient domain adaptation in remote sensing. The source code of\nDAM-Net will be made publicly available upon publication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation",
    "url": "http://arxiv.org/abs/2504.11949v1",
    "authors": [
      "Jie Wang",
      "Chen Ye Gan",
      "Caoqi Wei",
      "Jiangtao Wen",
      "Yuxing Han"
    ],
    "published": "2025-04-16",
    "abstract": "Feature matching across video streams remains a cornerstone challenge in\ncomputer vision. Increasingly, robust multimodal matching has garnered interest\nin robotics, surveillance, remote sensing, and medical imaging. While\ntraditional rely on detecting and matching spatial features, they break down\nwhen faced with noisy, misaligned, or cross-modal data. Recent deep learning\nmethods have improved robustness through learned representations, but remain\nconstrained by their dependence on extensive training data and computational\ndemands. We present Flow Intelligence, a paradigm-shifting approach that moves\nbeyond spatial features by focusing on temporal motion patterns exclusively.\nInstead of detecting traditional keypoints, our method extracts motion\nsignatures from pixel blocks across consecutive frames and extract temporal\nmotion signatures between videos. These motion-based descriptors achieve\nnatural invariance to translation, rotation, and scale variations while\nremaining robust across different imaging modalities. This novel approach also\nrequires no pretraining data, eliminates the need for spatial feature\ndetection, enables cross-modal matching using only temporal motion, and it\noutperforms existing methods in challenging scenarios where traditional\napproaches fail. By leveraging motion rather than appearance, Flow Intelligence\nenables robust, real-time video feature matching in diverse environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps",
    "url": "http://arxiv.org/abs/2504.11416v1",
    "authors": [
      "Panagiotis Agrafiotis",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-04-15",
    "abstract": "Accurate, detailed, and high-frequent bathymetry is crucial for shallow\nseabed areas facing intense climatological and anthropogenic pressures. Current\nmethods utilizing airborne or satellite optical imagery to derive bathymetry\nprimarily rely on either SfM-MVS with refraction correction or Spectrally\nDerived Bathymetry (SDB). However, SDB methods often require extensive manual\nfieldwork or costly reference data, while SfM-MVS approaches face challenges\neven after refraction correction. These include depth data gaps and noise in\nenvironments with homogeneous visual textures, which hinder the creation of\naccurate and complete Digital Surface Models (DSMs) of the seabed. To address\nthese challenges, this work introduces a methodology that combines the\nhigh-fidelity 3D reconstruction capabilities of the SfM-MVS methods with\nstate-of-the-art refraction correction techniques, along with the spectral\nanalysis capabilities of a new deep learning-based method for bathymetry\nprediction. This integration enables a synergistic approach where SfM-MVS\nderived DSMs with data gaps are used as training data to generate complete\nbathymetric maps. In this context, we propose Swin-BathyUNet that combines\nU-Net with Swin Transformer self-attention layers and a cross-attention\nmechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve\nbathymetric accuracy by capturing long-range spatial relationships and can also\nfunction as a standalone solution for standard SDB with various training depth\ndata, independent of the SfM-MVS output. Experimental results in two completely\ndifferent test sites in the Mediterranean and Baltic Seas demonstrate the\neffectiveness of the proposed approach through extensive experiments that\ndemonstrate improvements in bathymetric accuracy, detail, coverage, and noise\nreduction in the predicted DSM. The code is available at\nhttps://github.com/pagraf/Swin-BathyUNet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "YOLO-RS: Remote Sensing Enhanced Crop Detection Methods",
    "url": "http://arxiv.org/abs/2504.11165v1",
    "authors": [
      "Linlin Xiao",
      "Zhang Tiancong",
      "Yutong Jia",
      "Xinyu Nie",
      "Mengyao Wang",
      "Xiaohang Shao"
    ],
    "published": "2025-04-15",
    "abstract": "With the rapid development of remote sensing technology, crop classification\nand health detection based on deep learning have gradually become a research\nhotspot. However, the existing target detection methods show poor performance\nwhen dealing with small targets in remote sensing images, especially in the\ncase of complex background and image mixing, which is difficult to meet the\npractical application requirementsite. To address this problem, a novel target\ndetection model YOLO-RS is proposed in this paper. The model is based on the\nlatest Yolov11 which significantly enhances the detection of small targets by\nintroducing the Context Anchor Attention (CAA) mechanism and an efficient\nmulti-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional\nfeature fusion strategy in the feature fusion process, which effectively\nenhances the model's performance in the detection of small targets. Small\ntarget detection. Meanwhile, the ACmix module at the end of the model backbone\nnetwork solves the category imbalance problem by adaptively adjusting the\ncontrast and sample mixing, thus enhancing the detection accuracy in complex\nscenes. In the experiments on the PDT remote sensing crop health detection\ndataset and the CWC crop classification dataset, YOLO-RS improves both the\nrecall and the mean average precision (mAP) by about 2-3\\% or so compared with\nthe existing state-of-the-art methods, while the F1-score is also significantly\nimproved. Moreover, the computational complexity of the model only increases by\nabout 5.2 GFLOPs, indicating its significant advantages in both performance and\nefficiency. The experimental results validate the effectiveness and application\npotential of YOLO-RS in the task of detecting small targets in remote sensing\nimages.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion",
    "url": "http://arxiv.org/abs/2504.10974v2",
    "authors": [
      "Zhisheng Zhang",
      "Peng Zhang",
      "Fengxiang Wang",
      "Liangli Ma",
      "Fuchun Sun"
    ],
    "published": "2025-04-15",
    "abstract": "Enhancing forward-looking sonar images is critical for accurate underwater\ntarget detection. Current deep learning methods mainly rely on supervised\ntraining with simulated data, but the difficulty in obtaining high-quality\nreal-world paired data limits their practical use and generalization. Although\nself-supervised approaches from remote sensing partially alleviate data\nshortages, they neglect the cross-modal degradation gap between sonar and\nremote sensing images. Directly transferring pretrained weights often leads to\noverly smooth sonar images, detail loss, and insufficient brightness. To\naddress this, we propose a feature-space transformation that maps sonar images\nfrom the pixel domain to a robust feature domain, effectively bridging the\ndegradation gap. Additionally, our self-supervised multi-frame fusion strategy\nleverages complementary inter-frame information to naturally remove speckle\nnoise and enhance target-region brightness. Experiments on three self-collected\nreal-world forward-looking sonar datasets show that our method significantly\noutperforms existing approaches, effectively suppressing noise, preserving\ndetailed edges, and substantially improving brightness, demonstrating strong\npotential for underwater target detection applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "LightFormer: A lightweight and efficient decoder for remote sensing image segmentation",
    "url": "http://arxiv.org/abs/2504.10834v1",
    "authors": [
      "Sihang Chen",
      "Lijun Yun",
      "Ze Liu",
      "JianFeng Zhu",
      "Jie Chen",
      "Hui Wang",
      "Yueping Nie"
    ],
    "published": "2025-04-15",
    "abstract": "Deep learning techniques have achieved remarkable success in the semantic\nsegmentation of remote sensing images and in land-use change detection.\nNevertheless, their real-time deployment on edge platforms remains constrained\nby decoder complexity. Herein, we introduce LightFormer, a lightweight decoder\nfor time-critical tasks that involve unstructured targets, such as disaster\nassessment, unmanned aerial vehicle search-and-rescue, and cultural heritage\nmonitoring. LightFormer employs a feature-fusion and refinement module built on\nchannel processing and a learnable gating mechanism to aggregate multi-scale,\nmulti-range information efficiently, which drastically curtails model\ncomplexity. Furthermore, we propose a spatial information selection module\n(SISM) that integrates long-range attention with a detail preservation branch\nto capture spatial dependencies across multiple scales, thereby substantially\nimproving the recognition of unstructured targets in complex scenes. On the\nISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9%\nvs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters,\nthus achieving an excellent accuracy-efficiency trade-off. Consistent results\non LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its\nrobustness and superior perception of unstructured objects. These findings\nhighlight LightFormer as a practical solution for remote sensing applications\nwhere both computational economy and high-precision segmentation are\nimperative.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Rainy: Unlocking Satellite Calibration for Deep Learning in Precipitation",
    "url": "http://arxiv.org/abs/2504.10776v1",
    "authors": [
      "Zhenyu Yu",
      "Hanqing Chen",
      "Mohd Yamani Idna Idris",
      "Pei Wang"
    ],
    "published": "2025-04-15",
    "abstract": "Precipitation plays a critical role in the Earth's hydrological cycle,\ndirectly affecting ecosystems, agriculture, and water resource management.\nAccurate precipitation estimation and prediction are crucial for understanding\nclimate dynamics, disaster preparedness, and environmental monitoring. In\nrecent years, artificial intelligence (AI) has gained increasing attention in\nquantitative remote sensing (QRS), enabling more advanced data analysis and\nimproving precipitation estimation accuracy. Although traditional methods have\nbeen widely used for precipitation estimation, they face limitations due to the\ndifficulty of data acquisition and the challenge of capturing complex feature\nrelationships. Furthermore, the lack of standardized multi-source satellite\ndatasets, and in most cases, the exclusive reliance on station data,\nsignificantly hinders the effective application of advanced AI models. To\naddress these challenges, we propose the Rainy dataset, a multi-source\nspatio-temporal dataset that integrates pure satellite data with station data,\nand propose Taper Loss, designed to fill the gap in tasks where only in-situ\ndata is available without area-wide support. The Rainy dataset supports five\nmain tasks: (1) satellite calibration, (2) precipitation event prediction, (3)\nprecipitation level prediction, (4) spatiotemporal prediction, and (5)\nprecipitation downscaling. For each task, we selected benchmark models and\nevaluation metrics to provide valuable references for researchers. Using\nprecipitation as an example, the Rainy dataset and Taper Loss demonstrate the\nseamless collaboration between QRS and computer vision, offering data support\nfor AI for Science in the field of QRS and providing valuable insights for\ninterdisciplinary collaboration and integration.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "DiffMOD: Progressive Diffusion Point Denoising for Moving Object Detection in Remote Sensing",
    "url": "http://arxiv.org/abs/2504.10278v1",
    "authors": [
      "Jinyue Zhang",
      "Xiangrong Zhang",
      "Zhongjian Huang",
      "Tianyang Zhang",
      "Yifei Jiang",
      "Licheng Jiao"
    ],
    "published": "2025-04-14",
    "abstract": "Moving object detection (MOD) in remote sensing is significantly challenged\nby low resolution, extremely small object sizes, and complex noise\ninterference. Current deep learning-based MOD methods rely on probability\ndensity estimation, which restricts flexible information interaction between\nobjects and across temporal frames. To flexibly capture high-order inter-object\nand temporal relationships, we propose a point-based MOD in remote sensing.\nInspired by diffusion models, the network optimization is formulated as a\nprogressive denoising process that iteratively recovers moving object centers\nfrom sparse noisy points. Specifically, we sample scattered features from the\nbackbone outputs as atomic units for subsequent processing, while global\nfeature embeddings are aggregated to compensate for the limited coverage of\nsparse point features. By modeling spatial relative positions and semantic\naffinities, Spatial Relation Aggregation Attention is designed to enable\nhigh-order interactions among point-level features for enhanced object\nrepresentation. To enhance temporal consistency, the Temporal Propagation and\nGlobal Fusion module is designed, which leverages an implicit memory reasoning\nmechanism for robust cross-frame feature integration. To align with the\nprogressive denoising process, we propose a progressive MinK optimal transport\nassignment strategy that establishes specialized learning objectives at each\ndenoising level. Additionally, we introduce a missing loss function to\ncounteract the clustering tendency of denoised points around salient objects.\nExperiments on the RsData remote sensing MOD dataset show that our MOD method\nbased on scattered point denoising can more effectively explore potential\nrelationships between sparse moving objects and improve the detection\ncapability and temporal consistency.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective Instance-Specific Pansharpening on Real-World Data",
    "url": "http://arxiv.org/abs/2504.10242v1",
    "authors": [
      "Tianyu Xin",
      "Jin-Liang Xiao",
      "Zeyu Xia",
      "Shan Yin",
      "Liang-Jian Deng"
    ],
    "published": "2025-04-14",
    "abstract": "Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images",
    "url": "http://arxiv.org/abs/2504.09621v1",
    "authors": [
      "Jiuchen Chen",
      "Xinyu Yan",
      "Qizhi Xu",
      "Kaiqi Li"
    ],
    "published": "2025-04-13",
    "abstract": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 $\\times$ 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 $\\times$\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Remote Sensing Based Crop Health Classification Using NDVI and Fully Connected Neural Networks",
    "url": "http://arxiv.org/abs/2504.10522v1",
    "authors": [
      "J. Judith",
      "R. Tamilselvi",
      "M. Parisa Beham",
      "S. Sathiya Pandiya Lakshmi",
      "Alavikunhu Panthakkan",
      "Saeed Al Mansoori",
      "Hussain Al Ahmad"
    ],
    "published": "2025-04-11",
    "abstract": "Accurate crop health monitoring is not only essential for improving\nagricultural efficiency but also for ensuring sustainable food production in\nthe face of environmental challenges. Traditional approaches often rely on\nvisual inspection or simple NDVI measurements, which, though useful, fall short\nin detecting nuanced variations in crop stress and disease conditions. In this\nresearch, we propose a more sophisticated method that leverages NDVI data\ncombined with a Fully Connected Neural Network (FCNN) to classify crop health\nwith greater precision. The FCNN, trained using satellite imagery from various\nagricultural regions, is capable of identifying subtle distinctions between\nhealthy crops, rust-affected plants, and other stressed conditions. Our\napproach not only achieved a remarkable classification accuracy of 97.80% but\nit also significantly outperformed conventional models in terms of precision,\nrecall, and F1-scores. The ability to map the relationship between NDVI values\nand crop health using deep learning presents new opportunities for real-time,\nlarge-scale monitoring of agricultural fields, reducing manual efforts, and\noffering a scalable solution to address global food security.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Few-Shot Adaptation of Grounding DINO for Agricultural Domain",
    "url": "http://arxiv.org/abs/2504.07252v1",
    "authors": [
      "Rajhans Singh",
      "Rafael Bidese Puhl",
      "Kshitiz Dhakal",
      "Sudhir Sornapudi"
    ],
    "published": "2025-04-09",
    "abstract": "Deep learning models are transforming agricultural applications by enabling\nautomated phenotyping, monitoring, and yield estimation. However, their\neffectiveness heavily depends on large amounts of annotated training data,\nwhich can be labor and time intensive. Recent advances in open-set object\ndetection, particularly with models like Grounding-DINO, offer a potential\nsolution to detect regions of interests based on text prompt input. Initial\nzero-shot experiments revealed challenges in crafting effective text prompts,\nespecially for complex objects like individual leaves and visually similar\nclasses. To address these limitations, we propose an efficient few-shot\nadaptation method that simplifies the Grounding-DINO architecture by removing\nthe text encoder module (BERT) and introducing a randomly initialized trainable\ntext embedding. This method achieves superior performance across multiple\nagricultural datasets, including plant-weed detection, plant counting, insect\nidentification, fruit counting, and remote sensing tasks. Specifically, it\ndemonstrates up to a $\\sim24\\%$ higher mAP than fully fine-tuned YOLO models on\nagricultural datasets and outperforms previous state-of-the-art methods by\n$\\sim10\\%$ in remote sensing, under few-shot learning conditions. Our method\noffers a promising solution for automating annotation and accelerating the\ndevelopment of specialized agricultural AI solutions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2504.05062v2",
    "authors": [
      "Chenfeng Xu"
    ],
    "published": "2025-04-07",
    "abstract": "With the rapid advancement of deep learning, the field of change detection\n(CD) in remote sensing imagery has achieved remarkable progress. Existing\nchange detection methods primarily focus on achieving higher accuracy with\nincreased computational costs and parameter sizes, leaving development of\nlightweight methods for rapid real-world processing an underexplored challenge.\nTo address this challenge, we propose a Lightweight Difference Guiding Network\n(LDGNet), leveraging absolute difference image to guide optical remote sensing\nchange detection. First, to enhance the feature representation capability of\nthe lightweight backbone network, we propose the Difference Guiding Module\n(DGM), which leverages multi-scale features extracted from the absolute\ndifference image to progressively influence the original image encoder at each\nlayer, thereby reinforcing feature extraction. Second, we propose the\nDifference-Aware Dynamic Fusion (DADF) module with Visual State Space Model\n(VSSM) for lightweight long-range dependency modeling. The module first uses\nfeature absolute differences to guide VSSM's global contextual modeling of\nchange regions, then employs difference attention to dynamically fuse these\nlong-range features with feature differences, enhancing change semantics while\nsuppressing noise and background. Extensive experiments on multiple datasets\ndemonstrate that our method achieves comparable or superior performance to\ncurrent state-of-the-art (SOTA) methods requiring several times more\ncomputation, while maintaining only 3.43M parameters and 1.12G FLOPs.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Joint Retrieval of Cloud properties using Attention-based Deep Learning Models",
    "url": "http://arxiv.org/abs/2504.03133v2",
    "authors": [
      "Zahid Hassan Tushar",
      "Adeleke Ademakinwa",
      "Jianwu Wang",
      "Zhibo Zhang",
      "Sanjay Purushotham"
    ],
    "published": "2025-04-04",
    "abstract": "Accurate cloud property retrieval is vital for understanding cloud behavior\nand its impact on climate, including applications in weather forecasting,\nclimate modeling, and estimating Earth's radiation balance. The Independent\nPixel Approximation (IPA), a widely used physics-based approach, simplifies\nradiative transfer calculations by assuming each pixel is independent of its\nneighbors. While computationally efficient, IPA has significant limitations,\nsuch as inaccuracies from 3D radiative effects, errors at cloud edges, and\nineffectiveness for overlapping or heterogeneous cloud fields. Recent\nAI/ML-based deep learning models have improved retrieval accuracy by leveraging\nspatial relationships across pixels. However, these models are often\nmemory-intensive, retrieve only a single cloud property, or struggle with joint\nproperty retrievals. To overcome these challenges, we introduce CloudUNet with\nAttention Module (CAM), a compact UNet-based model that employs attention\nmechanisms to reduce errors in thick, overlapping cloud regions and a\nspecialized loss function for joint retrieval of Cloud Optical Thickness (COT)\nand Cloud Effective Radius (CER). Experiments on a Large Eddy Simulation (LES)\ndataset show that our CAM model outperforms state-of-the-art deep learning\nmethods, reducing mean absolute errors (MAE) by 34% for COT and 42% for CER,\nand achieving 76% and 86% lower MAE for COT and CER retrievals compared to the\nIPA method.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Adaptive Frequency Enhancement Network for Remote Sensing Image Semantic Segmentation",
    "url": "http://arxiv.org/abs/2504.02647v1",
    "authors": [
      "Feng Gao",
      "Miao Fu",
      "Jingchao Cao",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2025-04-03",
    "abstract": "Semantic segmentation of high-resolution remote sensing images plays a\ncrucial role in land-use monitoring and urban planning. Recent remarkable\nprogress in deep learning-based methods makes it possible to generate\nsatisfactory segmentation results. However, existing methods still face\nchallenges in adapting network parameters to various land cover distributions\nand enhancing the interaction between spatial and frequency domain features. To\naddress these challenges, we propose the Adaptive Frequency Enhancement Network\n(AFENet), which integrates two key components: the Adaptive Frequency and\nSpatial feature Interaction Module (AFSIM) and the Selective feature Fusion\nModule (SFM). AFSIM dynamically separates and modulates high- and low-frequency\nfeatures according to the content of the input image. It adaptively generates\ntwo masks to separate high- and low-frequency components, therefore providing\noptimal details and contextual supplementary information for ground object\nfeature representation. SFM selectively fuses global context and local detailed\nfeatures to enhance the network's representation capability. Hence, the\ninteractions between frequency and spatial features are further enhanced.\nExtensive experiments on three publicly available datasets demonstrate that the\nproposed AFENet outperforms state-of-the-art methods. In addition, we also\nvalidate the effectiveness of AFSIM and SFM in managing diverse land cover\ntypes and complex scenarios. Our codes are available at\nhttps://github.com/oucailab/AFENet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A Decade of Deep Learning for Remote Sensing Spatiotemporal Fusion: Advances, Challenges, and Opportunities",
    "url": "http://arxiv.org/abs/2504.00901v1",
    "authors": [
      "Enzhe Sun",
      "Yongchuan Cui",
      "Peng Liu",
      "Jining Yan"
    ],
    "published": "2025-04-01",
    "abstract": "Hardware limitations and satellite launch costs make direct acquisition of\nhigh temporal-spatial resolution remote sensing imagery challenging. Remote\nsensing spatiotemporal fusion (STF) technology addresses this problem by\nmerging high temporal but low spatial resolution imagery with high spatial but\nlow temporal resolution imagery to efficiently generate high spatiotemporal\nresolution satellite images. STF provides unprecedented observational\ncapabilities for land surface change monitoring, agricultural management, and\nenvironmental research. Deep learning (DL) methods have revolutionized the\nremote sensing spatiotemporal fusion field over the past decade through\npowerful automatic feature extraction and nonlinear modeling capabilities,\nsignificantly outperforming traditional methods in handling complex\nspatiotemporal data. Despite the rapid development of DL-based remote sensing\nSTF, the community lacks a systematic review of this quickly evolving field.\nThis paper comprehensively reviews DL developments in remote sensing STF over\nthe last decade, analyzing key research trends, method classifications,\ncommonly used datasets, and evaluation metrics. It discusses major challenges\nin existing research and identifies promising future research directions as\nreferences for researchers in this field to inspire new ideas. The specific\nmodels, datasets, and other information mentioned in this article have been\ncollected in:\nhttps://github.com/yc-cui/Deep-Learning-Spatiotemporal-Fusion-Survey.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
    "url": "http://arxiv.org/abs/2503.23793v1",
    "authors": [
      "Zhongnan Cai",
      "Yingying Wang",
      "Yunlong Lin",
      "Hui Zheng",
      "Ge Meng",
      "Zixu Lin",
      "Jiaxin Xie",
      "Junbin Lu",
      "Yue Huang",
      "Xinghao Ding"
    ],
    "published": "2025-03-31",
    "abstract": "Recently, deep learning-based pan-sharpening algorithms have achieved notable\nadvancements over traditional methods. However, many deep learning-based\napproaches incur substantial computational overhead during inference,\nespecially with high-resolution images. This excessive computational demand\nlimits the applicability of these methods in real-world scenarios, particularly\nin the absence of dedicated computing devices such as GPUs and TPUs. To address\nthese challenges, we propose Pan-LUT, a novel learnable look-up table (LUT)\nframework for pan-sharpening that strikes a balance between performance and\ncomputational efficiency for high-resolution remote sensing images. To finely\ncontrol the spectral transformation, we devise the PAN-guided look-up table\n(PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained\nspatial details and adaptively learn local contexts, we introduce the spatial\ndetails look-up table (SDLUT) and adaptive aggregation look-up table (AALUT).\nOur proposed method contains fewer than 300K parameters and processes a 8K\nresolution image in under 1 ms using a single NVIDIA GeForce RTX 2080 Ti GPU,\ndemonstrating significantly faster performance compared to other methods.\nExperiments reveal that Pan-LUT efficiently processes large remote sensing\nimages in a lightweight manner, bridging the gap to real-world applications.\nFurthermore, our model surpasses SOTA methods in full-resolution scenes under\nreal-world conditions, highlighting its effectiveness and efficiency.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A large-scale image-text dataset benchmark for farmland segmentation",
    "url": "http://arxiv.org/abs/2503.23106v1",
    "authors": [
      "Chao Tao",
      "Dandan Zhong",
      "Weiliang Mu",
      "Zhuofei Du",
      "Haiyang Wu"
    ],
    "published": "2025-03-29",
    "abstract": "The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A Deep Learning Framework for Boundary-Aware Semantic Segmentation",
    "url": "http://arxiv.org/abs/2503.22050v1",
    "authors": [
      "Tai An",
      "Weiqiang Huang",
      "Da Xu",
      "Qingyuan He",
      "Jiacheng Hu",
      "Yujia Lou"
    ],
    "published": "2025-03-28",
    "abstract": "As a fundamental task in computer vision, semantic segmentation is widely\napplied in fields such as autonomous driving, remote sensing image analysis,\nand medical image processing. In recent years, Transformer-based segmentation\nmethods have demonstrated strong performance in global feature modeling.\nHowever, they still struggle with blurred target boundaries and insufficient\nrecognition of small targets. To address these issues, this study proposes a\nMask2Former-based semantic segmentation algorithm incorporating a boundary\nenhancement feature bridging module (BEFBM). The goal is to improve target\nboundary accuracy and segmentation consistency. Built upon the Mask2Former\nframework, this method constructs a boundary-aware feature map and introduces a\nfeature bridging mechanism. This enables effective cross-scale feature fusion,\nenhancing the model's ability to focus on target boundaries. Experiments on the\nCityscapes dataset demonstrate that, compared to mainstream segmentation\nmethods, the proposed approach achieves significant improvements in metrics\nsuch as mIOU, mDICE, and mRecall. It also exhibits superior boundary retention\nin complex scenes. Visual analysis further confirms the model's advantages in\nfine-grained regions. Future research will focus on optimizing computational\nefficiency and exploring its potential in other high-precision segmentation\ntasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Recognition"
    ]
  },
  {
    "title": "Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid Self-Attention U-Nets in Aerial Imagery",
    "url": "http://arxiv.org/abs/2503.21438v1",
    "authors": [
      "Anis Ur Rahman",
      "Einari Heinaro",
      "Mete Ahishali",
      "Samuli Junttila"
    ],
    "published": "2025-03-27",
    "abstract": "Mapping standing dead trees is critical for assessing forest health,\nmonitoring biodiversity, and mitigating wildfire risks, for which aerial\nimagery has proven useful. However, dense canopy structures, spectral overlaps\nbetween living and dead vegetation, and over-segmentation errors limit the\nreliability of existing methods. This study introduces a hybrid postprocessing\nframework that refines deep learning-based tree segmentation by integrating\nwatershed algorithms with adaptive filtering, enhancing boundary delineation,\nand reducing false positives in complex forest environments. Tested on\nhigh-resolution aerial imagery from boreal forests, the framework improved\ninstance-level segmentation accuracy by 41.5% and reduced positional errors by\n57%, demonstrating robust performance in densely vegetated regions. By\nbalancing detection accuracy and over-segmentation artifacts, the method\nenabled the precise identification of individual dead trees, which is critical\nfor ecological monitoring. The framework's computational efficiency supports\nscalable applications, such as wall-to-wall tree mortality mapping over large\ngeographic regions using aerial or satellite imagery. These capabilities\ndirectly benefit wildfire risk assessment (identifying fuel accumulations),\ncarbon stock estimation (tracking emissions from decaying biomass), and\nprecision forestry (targeting salvage loggings). By bridging advanced remote\nsensing techniques with practical forest management needs, this work advances\ntools for large-scale ecological conservation and climate resilience planning.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "SChanger: Change Detection from a Semantic Change and Spatial Consistency Perspective",
    "url": "http://arxiv.org/abs/2503.20734v1",
    "authors": [
      "Ziyu Zhou",
      "Keyan Hu",
      "Yutian Fang",
      "Xiaoping Rui"
    ],
    "published": "2025-03-26",
    "abstract": "Change detection is a key task in Earth observation applications. Recently,\ndeep learning methods have demonstrated strong performance and widespread\napplication. However, change detection faces data scarcity due to the\nlabor-intensive process of accurately aligning remote sensing images of the\nsame area, which limits the performance of deep learning algorithms. To address\nthe data scarcity issue, we develop a fine-tuning strategy called the Semantic\nChange Network (SCN). We initially pre-train the model on single-temporal\nsupervised tasks to acquire prior knowledge of instance feature extraction. The\nmodel then employs a shared-weight Siamese architecture and extended Temporal\nFusion Module (TFM) to preserve this prior knowledge and is fine-tuned on\nchange detection tasks. The learned semantics for identifying all instances is\nchanged to focus on identifying only the changes. Meanwhile, we observe that\nthe locations of changes between the two images are spatially identical, a\nconcept we refer to as spatial consistency. We introduce this inductive bias\nthrough an attention map that is generated by large-kernel convolutions and\napplied to the features from both time points. This enhances the modeling of\nmulti-scale changes and helps capture underlying relationships in change\ndetection semantics. We develop a binary change detection model utilizing these\ntwo strategies. The model is validated against state-of-the-art methods on six\ndatasets, surpassing all benchmark methods and achieving F1 scores of 92.87%,\n86.43%, 68.95%, 97.62%, 84.58%, and 93.20% on the LEVIR-CD, LEVIR-CD+,\nS2Looking, CDD, SYSU-CD, and WHU-CD datasets, respectively.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Small Object Detection: A Comprehensive Survey on Challenges, Techniques and Real-World Applications",
    "url": "http://arxiv.org/abs/2503.20516v1",
    "authors": [
      "Mahya Nikouei",
      "Bita Baroutian",
      "Shahabedin Nabavi",
      "Fateme Taraghi",
      "Atefe Aghaei",
      "Ayoob Sajedi",
      "Mohsen Ebrahimi Moghaddam"
    ],
    "published": "2025-03-26",
    "abstract": "Small object detection (SOD) is a critical yet challenging task in computer\nvision, with applications like spanning surveillance, autonomous systems,\nmedical imaging, and remote sensing. Unlike larger objects, small objects\ncontain limited spatial and contextual information, making accurate detection\ndifficult. Challenges such as low resolution, occlusion, background\ninterference, and class imbalance further complicate the problem. This survey\nprovides a comprehensive review of recent advancements in SOD using deep\nlearning, focusing on articles published in Q1 journals during 2024-2025. We\nanalyzed challenges, state-of-the-art techniques, datasets, evaluation metrics,\nand real-world applications. Recent advancements in deep learning have\nintroduced innovative solutions, including multi-scale feature extraction,\nSuper-Resolution (SR) techniques, attention mechanisms, and transformer-based\narchitectures. Additionally, improvements in data augmentation, synthetic data\ngeneration, and transfer learning have addressed data scarcity and domain\nadaptation issues. Furthermore, emerging trends such as lightweight neural\nnetworks, knowledge distillation (KD), and self-supervised learning offer\npromising directions for improving detection efficiency, particularly in\nresource-constrained environments like Unmanned Aerial Vehicles (UAV)-based\nsurveillance and edge computing. We also review widely used datasets, along\nwith standard evaluation metrics such as mean Average Precision (mAP) and\nsize-specific AP scores. The survey highlights real-world applications,\nincluding traffic monitoring, maritime surveillance, industrial defect\ndetection, and precision agriculture. Finally, we discuss open research\nchallenges and future directions, emphasizing the need for robust domain\nadaptation techniques, better feature fusion strategies, and real-time\nperformance optimization.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Super-Resolution"
    ]
  },
  {
    "title": "Leveraging Land Cover Priors for Isoprene Emission Super-Resolution",
    "url": "http://arxiv.org/abs/2503.18658v1",
    "authors": [
      "Christopher Ummerle",
      "Antonio Giganti",
      "Sara Mandelli",
      "Paolo Bestagini",
      "Stefano Tubaro"
    ],
    "published": "2025-03-24",
    "abstract": "Remote sensing plays a crucial role in monitoring Earth's ecosystems, yet\nsatellite-derived data often suffer from limited spatial resolution,\nrestricting their applicability in atmospheric modeling and climate research.\nIn this work, we propose a deep learning-based Super-Resolution (SR) framework\nthat leverages land cover information to enhance the spatial accuracy of\nBiogenic Volatile Organic Compounds (BVOCs) emissions, with a particular focus\non isoprene. Our approach integrates land cover priors as emission drivers,\ncapturing spatial patterns more effectively than traditional methods. We\nevaluate the model's performance across various climate conditions and analyze\nstatistical correlations between isoprene emissions and key environmental\ninformation such as cropland and tree cover data. Additionally, we assess the\ngeneralization capabilities of our SR model by applying it to unseen climate\nzones and geographical regions. Experimental results demonstrate that\nincorporating land cover data significantly improves emission SR accuracy,\nparticularly in heterogeneous landscapes. This study contributes to atmospheric\nchemistry and climate modeling by providing a cost-effective, data-driven\napproach to refining BVOC emission maps. The proposed method enhances the\nusability of satellite-based emissions data, supporting applications in air\nquality forecasting, climate impact assessments, and environmental studies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "MapGlue: Multimodal Remote Sensing Image Matching",
    "url": "http://arxiv.org/abs/2503.16185v1",
    "authors": [
      "Peihao Wu",
      "Yongxiang Yao",
      "Wenfei Zhang",
      "Dong Wei",
      "Yi Wan",
      "Yansheng Li",
      "Yongjun Zhang"
    ],
    "published": "2025-03-20",
    "abstract": "Multimodal remote sensing image (MRSI) matching is pivotal for cross-modal\nfusion, localization, and object detection, but it faces severe challenges due\nto geometric, radiometric, and viewpoint discrepancies across imaging\nmodalities. Existing unimodal datasets lack scale and diversity, limiting deep\nlearning solutions. This paper proposes MapGlue, a universal MRSI matching\nframework, and MapData, a large-scale multimodal dataset addressing these gaps.\nOur contributions are twofold. MapData, a globally diverse dataset spanning 233\nsampling points, offers original images (7,000x5,000 to 20,000x15,000 pixels).\nAfter rigorous cleaning, it provides 121,781 aligned electronic map-visible\nimage pairs (512x512 pixels) with hybrid manual-automated ground truth,\naddressing the scarcity of scalable multimodal benchmarks. MapGlue integrates\nsemantic context with a dual graph-guided mechanism to extract cross-modal\ninvariant features. This structure enables global-to-local interaction,\nenhancing descriptor robustness against modality-specific distortions.\nExtensive evaluations on MapData and five public datasets demonstrate MapGlue's\nsuperiority in matching accuracy under complex conditions, outperforming\nstate-of-the-art methods. Notably, MapGlue generalizes effectively to unseen\nmodalities without retraining, highlighting its adaptability. This work\naddresses longstanding challenges in MRSI matching by combining scalable\ndataset construction with a robust, semantics-driven framework. Furthermore,\nMapGlue shows strong generalization capabilities on other modality matching\ntasks for which it was not specifically trained. The dataset and code are\navailable at https://github.com/PeihaoWu/MapGlue.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Toward task-driven satellite image super-resolution",
    "url": "http://arxiv.org/abs/2503.15474v1",
    "authors": [
      "Maciej Ziaja",
      "Pawel Kowaleczko",
      "Daniel Kostrzewa",
      "Nicolas Long\u00e9p\u00e9",
      "Michal Kawulok"
    ],
    "published": "2025-03-19",
    "abstract": "Super-resolution is aimed at reconstructing high-resolution images from\nlow-resolution observations. State-of-the-art approaches underpinned with deep\nlearning allow for obtaining outstanding results, generating images of high\nperceptual quality. However, it often remains unclear whether the reconstructed\ndetails are close to the actual ground-truth information and whether they\nconstitute a more valuable source for image analysis algorithms. In the\nreported work, we address the latter problem, and we present our efforts toward\nlearning super-resolution algorithms in a task-driven way to make them suitable\nfor generating high-resolution images that can be exploited for automated image\nanalysis. In the reported initial research, we propose a methodological\napproach for assessing the existing models that perform computer vision tasks\nin terms of whether they can be used for evaluating super-resolution\nreconstruction algorithms, as well as training them in a task-driven way. We\nsupport our analysis with experimental study and we expect it to establish a\nsolid foundation for selecting appropriate computer vision tasks that will\nadvance the capabilities of real-world super-resolution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation Performance on RGB Images of Closed Canopy: Validation Using TLS",
    "url": "http://arxiv.org/abs/2503.14273v2",
    "authors": [
      "Matthew J. Allen",
      "Harry J. F. Owen",
      "Stuart W. D. Grieve",
      "Emily R. Lines"
    ],
    "published": "2025-03-18",
    "abstract": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Comparative and Interpretative Analysis of CNN and Transformer Models in Predicting Wildfire Spread Using Remote Sensing Data",
    "url": "http://arxiv.org/abs/2503.14150v1",
    "authors": [
      "Yihang Zhou",
      "Ruige Kong",
      "Zhengsen Xu",
      "Linlin Xu",
      "Sibo Cheng"
    ],
    "published": "2025-03-18",
    "abstract": "Facing the escalating threat of global wildfires, numerous computer vision\ntechniques using remote sensing data have been applied in this area. However,\nthe selection of deep learning methods for wildfire prediction remains\nuncertain due to the lack of comparative analysis in a quantitative and\nexplainable manner, crucial for improving prevention measures and refining\nmodels. This study aims to thoroughly compare the performance, efficiency, and\nexplainability of four prevalent deep learning architectures: Autoencoder,\nResNet, UNet, and Transformer-based Swin-UNet. Employing a real-world dataset\nthat includes nearly a decade of remote sensing data from California, U.S.,\nthese models predict the spread of wildfires for the following day. Through\ndetailed quantitative comparison analysis, we discovered that Transformer-based\nSwin-UNet and UNet generally outperform Autoencoder and ResNet, particularly\ndue to the advanced attention mechanisms in Transformer-based Swin-UNet and the\nefficient use of skip connections in both UNet and Transformer-based Swin-UNet,\nwhich contribute to superior predictive accuracy and model interpretability.\nThen we applied XAI techniques on all four models, this not only enhances the\nclarity and trustworthiness of models but also promotes focused improvements in\nwildfire prediction capabilities. The XAI analysis reveals that UNet and\nTransformer-based Swin-UNet are able to focus on critical features such as\n'Previous Fire Mask', 'Drought', and 'Vegetation' more effectively than the\nother two models, while also maintaining balanced attention to the remaining\nfeatures, leading to their superior performance. The insights from our thorough\ncomparative analysis offer substantial implications for future model design and\nalso provide guidance for model selection in different scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "ResNet",
      "Transformer",
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening",
    "url": "http://arxiv.org/abs/2503.13214v3",
    "authors": [
      "Jie Huang",
      "Haorui Chen",
      "Jiaxuan Ren",
      "Siran Peng",
      "Liangjian Deng"
    ],
    "published": "2025-03-17",
    "abstract": "Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Ship Detection in Remote Sensing Imagery for Arbitrarily Oriented Object Detection",
    "url": "http://arxiv.org/abs/2503.14534v1",
    "authors": [
      "Bibi Erum Ayesha",
      "T. Satyanarayana Murthy",
      "Palamakula Ramesh Babu",
      "Ramu Kuchipudi"
    ],
    "published": "2025-03-17",
    "abstract": "This research paper presents an innovative ship detection system tailored for\napplications like maritime surveillance and ecological monitoring. The study\nemploys YOLOv8 and repurposed U-Net, two advanced deep learning models, to\nsignificantly enhance ship detection accuracy. Evaluation metrics include Mean\nAverage Precision (mAP), processing speed, and overall accuracy. The research\nutilizes the \"Airbus Ship Detection\" dataset, featuring diverse remote sensing\nimages, to assess the models' versatility in detecting ships with varying\norientations and environmental contexts. Conventional ship detection faces\nchallenges with arbitrary orientations, complex backgrounds, and obscured\nperspectives. Our approach incorporates YOLOv8 for real-time processing and\nU-Net for ship instance segmentation. Evaluation focuses on mAP, processing\nspeed, and overall accuracy. The dataset is chosen for its diverse images,\nmaking it an ideal benchmark. Results demonstrate significant progress in ship\ndetection. YOLOv8 achieves an 88% mAP, excelling in accurate and rapid ship\ndetection. U Net, adapted for ship instance segmentation, attains an 89% mAP,\nimproving boundary delineation and handling occlusions. This research enhances\nmaritime surveillance, disaster response, and ecological monitoring,\nexemplifying the potential of deep learning models in ship detection.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Early Detection of Forest Calamities in Homogeneous Stands -- Deep Learning Applied to Bark-Beetle Outbreaks",
    "url": "http://arxiv.org/abs/2503.12883v1",
    "authors": [
      "Maximilian Kirsch",
      "Jakob Wernicke",
      "Pawan Datta",
      "Christine Preisach"
    ],
    "published": "2025-03-17",
    "abstract": "Climate change has increased the vulnerability of forests to insect-related\ndamage, resulting in widespread forest loss in Central Europe and highlighting\nthe need for effective, continuous monitoring systems. Remote sensing based\nforest health monitoring, oftentimes, relies on supervised machine learning\nalgorithms that require labeled training data. Monitoring temporal patterns\nthrough time series analysis offers a potential alternative for earlier\ndetection of disturbance but requires substantial storage resources. This study\ninvestigates the potential of a Deep Learning algorithm based on a Long Short\nTerm Memory (LSTM) Autoencoder for the detection of anomalies in forest health\n(e.g. bark beetle outbreaks), utilizing Sentinel-2 time series data. This\napproach is an alternative to supervised machine learning methods, avoiding the\nnecessity for labeled training data. Furthermore, it is more memory-efficient\nthan other time series analysis approaches, as a robust model can be created\nusing only a 26-week-long time series as input. In this study, we monitored\npure stands of spruce in Thuringia, Germany, over a 7-year period from 2018 to\nthe end of 2024. Our best model achieved a detection accuracy of 87% on test\ndata and was able to detect 61% of all anomalies at a very early stage (more\nthan a month before visible signs of forest degradation). Compared to another\nwidely used time series break detection algorithm - BFAST (Breaks For Additive\nSeason and Trend), our approach consistently detected higher percentage of\nanomalies at an earlier stage. These findings suggest that LSTM-based\nAutoencoders could provide a promising, resource-efficient approach to forest\nhealth monitoring, enabling more timely responses to emerging threats.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LSTM",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Time Series Analysis"
    ]
  },
  {
    "title": "Observation-only learning of neural mapping schemes for gappy satellite-derived ocean colour parameters",
    "url": "http://arxiv.org/abs/2503.11532v1",
    "authors": [
      "Cl\u00e9ment Dorffer",
      "Fr\u00e9d\u00e9ric Jourdin",
      "Thi Thuy Nga Nguyen",
      "Rodolphe Devillers",
      "David Mouillot",
      "Ronan Fablet"
    ],
    "published": "2025-03-14",
    "abstract": "Monitoring optical properties of coastal and open ocean waters is crucial to\nassessing the health of marine ecosystems. Deep learning offers a promising\napproach to address these ecosystem dynamics, especially in scenarios where\ngap-free ground-truth data is lacking, which poses a challenge for designing\neffective training frameworks. Using an advanced neural variational data\nassimilation scheme (called 4DVarNet), we introduce a comprehensive training\nframework designed to effectively train directly on gappy data sets. Using the\nMediterranean Sea as a case study, our experiments not only highlight the high\nperformance of the chosen neural network in reconstructing gap-free images from\ngappy datasets but also demonstrate its superior performance over\nstate-of-the-art algorithms such as DInEOF and Direct Inversion, whether using\nCNN or UNet architectures.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "ChromaFormer: A Scalable and Accurate Transformer Architecture for Land Cover Classification",
    "url": "http://arxiv.org/abs/2503.08534v1",
    "authors": [
      "Mingshi Li",
      "Dusan Grujicic",
      "Ben Somers",
      "Stien Heremans",
      "Steven De Saeger",
      "Matthew B. Blaschko"
    ],
    "published": "2025-03-11",
    "abstract": "Remote sensing imagery from systems such as Sentinel provides full coverage\nof the Earth's surface at around 10-meter resolution. The remote sensing\ncommunity has transitioned to extensive use of deep learning models due to\ntheir high performance on benchmarks such as the UCMerced and ISPRS Vaihingen\ndatasets. Convolutional models such as UNet and ResNet variations are commonly\nemployed for remote sensing but typically only accept three channels, as they\nwere developed for RGB imagery, while satellite systems provide more than ten.\nRecently, several transformer architectures have been proposed for remote\nsensing, but they have not been extensively benchmarked and are typically used\non small datasets such as Salinas Valley. Meanwhile, it is becoming feasible to\nobtain dense spatial land-use labels for entire first-level administrative\ndivisions of some countries. Scaling law observations suggest that\nsubstantially larger multi-spectral transformer models could provide a\nsignificant leap in remote sensing performance in these settings.\n  In this work, we propose ChromaFormer, a family of multi-spectral transformer\nmodels, which we evaluate across orders of magnitude differences in model\nparameters to assess their performance and scaling effectiveness on a densely\nlabeled imagery dataset of Flanders, Belgium, covering more than 13,500 km^2\nand containing 15 classes. We propose a novel multi-spectral attention strategy\nand demonstrate its effectiveness through ablations. Furthermore, we show that\nmodels many orders of magnitude larger than conventional architectures, such as\nUNet, lead to substantial accuracy improvements: a UNet++ model with 23M\nparameters achieves less than 65% accuracy, while a multi-spectral transformer\nwith 655M parameters achieves over 95% accuracy on the Biological Valuation Map\nof Flanders.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "ResNet",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "i-WiViG: Interpretable Window Vision GNN",
    "url": "http://arxiv.org/abs/2503.08321v1",
    "authors": [
      "Ivica Obadic",
      "Dmitry Kangin",
      "Dario Oliveira",
      "Plamen P Angelov",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-03-11",
    "abstract": "Deep learning models based on graph neural networks have emerged as a popular\napproach for solving computer vision problems. They encode the image into a\ngraph structure and can be beneficial for efficiently capturing the long-range\ndependencies typically present in remote sensing imagery. However, an important\ndrawback of these methods is their black-box nature which may hamper their\nwider usage in critical applications. In this work, we tackle the\nself-interpretability of the graph-based vision models by proposing our\nInterpretable Window Vision GNN (i-WiViG) approach, which provides explanations\nby automatically identifying the relevant subgraphs for the model prediction.\nThis is achieved with window-based image graph processing that constrains the\nnode receptive field to a local image region and by using a self-interpretable\ngraph bottleneck that ranks the importance of the long-range relations between\nthe image regions. We evaluate our approach to remote sensing classification\nand regression tasks, showing it achieves competitive performance while\nproviding inherent and faithful explanations through the identified relations.\nFurther, the quantitative evaluation reveals that our model reduces the\ninfidelity of post-hoc explanations compared to other Vision GNN models,\nwithout sacrificing explanation sparsity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "A Deep Learning Architecture for Land Cover Mapping Using Spatio-Temporal Sentinel-1 Features",
    "url": "http://arxiv.org/abs/2503.07230v1",
    "authors": [
      "Luigi Russo",
      "Antonietta Sorriso",
      "Silvia Liberata Ullo",
      "Paolo Gamba"
    ],
    "published": "2025-03-10",
    "abstract": "Land Cover (LC) mapping using satellite imagery is critical for environmental\nmonitoring and management. Deep Learning (DL), particularly Convolutional\nNeural Networks (CNNs) and Vision Transformers (ViTs), have revolutionized this\nfield by enhancing the accuracy of classification tasks. In this work, a novel\napproach combining a transformer-based Swin-Unet architecture with seasonal\nsynthesized spatio-temporal images has been employed to classify LC types using\nspatio-temporal features extracted from Sentinel-1 (S1) Synthetic Aperture\nRadar (SAR) data, organized into seasonal clusters. The study focuses on three\ndistinct regions - Amazonia, Africa, and Siberia - and evaluates the model\nperformance across diverse ecoregions within these areas. By utilizing seasonal\nfeature sequences instead of dense temporal sequences, notable performance\nimprovements have been achieved, especially in regions with temporal data gaps\nlike Siberia, where S1 data distribution is uneven and non-uniform. The results\ndemonstrate the effectiveness and the generalization capabilities of the\nproposed methodology in achieving high overall accuracy (O.A.) values, even in\nregions with limited training data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Spatiotemporal Deep Learning Network for Photon-Level Block Compressed Sensing Imaging",
    "url": "http://arxiv.org/abs/2503.07143v2",
    "authors": [
      "Changzhi Yu",
      "Shuangping Han",
      "Kai Song",
      "Liantuan Xiao"
    ],
    "published": "2025-03-10",
    "abstract": "In this paper, we propose a spatiotemporal deep learning network for\nphoton-level Block Compressed Sensing Imaging, aimed to address challenges such\nas signal loss, artifacts, and noise interference in large-pixel dynamic\nimaging and tracking at the photon level. This approach combines information in\nthe time and frequency domains with a U-Net-LSTM deep learning model,\nsignificantly improving the restoration quality of dynamic target images at\nhigh frame rates. The experimental results demonstrate that dynamic target\nimaging with an average photon number of less than 10 per pixel can be achieved\nusing 16-channel parallel detection, where the pixel size is 256*256 and the\nframe rate is 200 fps.. Compared to conventional Block Compressed Sensing\nImaging, this method increases the peak signal-to-noise ratio to 38.66 dB and\nimproves the structural similarity index to 0.96. In the presence of a dynamic\nscattering medium and a static complex background, we successfully achieved\nimaging and tracking of two targets undergoing complex motion. Even in\nscenarios where the targets overlap or obstruct each other, we can still\nreconstruct clear images of each individual target separately.. This method\nprovides an effective solution for large-pixel dynamic target recognition,\ntracking, and real-time imaging in complex environments, offering promising\napplications in remote sensing, military reconnaissance, and beyond.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "LSTM"
    ],
    "applications": [
      "Detection",
      "Recognition",
      "Tracking"
    ]
  },
  {
    "title": "Exploring FMCW Radars and Feature Maps for Activity Recognition: A Benchmark Study",
    "url": "http://arxiv.org/abs/2503.05629v1",
    "authors": [
      "Ali Samimi Fard",
      "Mohammadreza Mashhadigholamali",
      "Samaneh Zolfaghari",
      "Hajar Abedi",
      "Mainak Chakraborty",
      "Luigi Borz\u00ec",
      "Masoud Daneshtalab",
      "George Shaker"
    ],
    "published": "2025-03-07",
    "abstract": "Human Activity Recognition has gained significant attention due to its\ndiverse applications, including ambient assisted living and remote sensing.\nWearable sensor-based solutions often suffer from user discomfort and\nreliability issues, while video-based methods raise privacy concerns and\nperform poorly in low-light conditions or long ranges. This study introduces a\nFrequency-Modulated Continuous Wave radar-based framework for human activity\nrecognition, leveraging a 60 GHz radar and multi-dimensional feature maps.\nUnlike conventional approaches that process feature maps as images, this study\nfeeds multi-dimensional feature maps -- Range-Doppler, Range-Azimuth, and\nRange-Elevation -- as data vectors directly into the machine learning (SVM,\nMLP) and deep learning (CNN, LSTM, ConvLSTM) models, preserving the spatial and\ntemporal structures of the data. These features were extracted from a novel\ndataset with seven activity classes and validated using two different\nvalidation approaches. The ConvLSTM model outperformed conventional machine\nlearning and deep learning models, achieving an accuracy of 90.51% and an\nF1-score of 87.31% on cross-scene validation and an accuracy of 89.56% and an\nF1-score of 87.15% on leave-one-person-out cross-validation. The results\nhighlight the approach's potential for scalable, non-intrusive, and\nprivacy-preserving activity monitoring in real-world scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "HoloMine: A Synthetic Dataset for Buried Landmines Recognition using Microwave Holographic Imaging",
    "url": "http://arxiv.org/abs/2502.21054v1",
    "authors": [
      "Emanuele Vivoli",
      "Lorenzo Capineri",
      "Marco Bertini"
    ],
    "published": "2025-02-28",
    "abstract": "The detection and removal of landmines is a complex and risky task that\nrequires advanced remote sensing techniques to reduce the risk for the\nprofessionals involved in this task. In this paper, we propose a novel\nsynthetic dataset for buried landmine detection to provide researchers with a\nvaluable resource to observe, measure, locate, and address issues in landmine\ndetection. The dataset consists of 41,800 microwave holographic images (2D) and\ntheir holographic inverted scans (3D) of different types of buried objects,\nincluding landmines, clutter, and pottery objects, and is collected by means of\na microwave holography sensor.\n  We evaluate the performance of several state-of-the-art deep learning models\ntrained on our synthetic dataset for various classification tasks. While the\nresults do not yield yet high performances, showing the difficulty of the\nproposed task, we believe that our dataset has significant potential to drive\nprogress in the field of landmine detection thanks to the accuracy and\nresolution obtainable using holographic radars.\n  To the best of our knowledge, our dataset is the first of its kind and will\nhelp drive further research on computer vision methods to automatize mine\ndetection, with the overall goal of reducing the risks and the costs of the\ndemining process.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "LMHLD: A Large-scale Multi-source High-resolution Landslide Dataset for Landslide Detection based on Deep Learning",
    "url": "http://arxiv.org/abs/2502.19866v1",
    "authors": [
      "Guanting Liu",
      "Yi Wang",
      "Xi Chen",
      "Baoyu Du",
      "Penglei Li",
      "Yuan Wu",
      "Zhice Fang"
    ],
    "published": "2025-02-27",
    "abstract": "Landslides are among the most common natural disasters globally, posing\nsignificant threats to human society. Deep learning (DL) has proven to be an\neffective method for rapidly generating landslide inventories in large-scale\ndisaster areas. However, DL models rely heavily on high-quality labeled\nlandslide data for strong feature extraction capabilities. And landslide\ndetection using DL urgently needs a benchmark dataset to evaluate the\ngeneralization ability of the latest models. To solve the above problems, we\nconstruct a Large-scale Multi-source High-resolution Landslide Dataset (LMHLD)\nfor Landslide Detection based on DL. LMHLD collects remote sensing images from\nfive different satellite sensors across seven study areas worldwide: Wenchuan,\nChina (2008); Rio de Janeiro, Brazil (2011); Gorkha, Nepal (2015); Jiuzhaigou,\nChina (2015); Taiwan, China (2018); Hokkaido, Japan (2018); Emilia-Romagna,\nItaly (2023). The dataset includes a total of 25,365 patches, with different\npatch sizes to accommodate different landslide scales. Additionally, a training\nmodule, LMHLDpart, is designed to accommodate landslide detection tasks at\nvarying scales and to alleviate the issue of catastrophic forgetting in\nmulti-task learning. Furthermore, the models trained by LMHLD is applied in\nother datasets to highlight the robustness of LMHLD. Five dataset quality\nevaluation experiments designed by using seven DL models from the U-Net family\ndemonstrate that LMHLD has the potential to become a benchmark dataset for\nlandslide detection. LMHLD is open access and can be accessed through the link:\nhttps://doi.org/10.5281/zenodo.11424988. This dataset provides a strong\nfoundation for DL models, accelerates the development of DL in landslide\ndetection, and serves as a valuable resource for landslide prevention and\nmitigation efforts.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "From underwater to aerial: a novel multi-scale knowledge distillation approach for coral reef monitoring",
    "url": "http://arxiv.org/abs/2502.17883v1",
    "authors": [
      "Matteo Contini",
      "Victor Illien",
      "Julien Barde",
      "Sylvain Poulain",
      "Serge Bernard",
      "Alexis Joly",
      "Sylvain Bonhommeau"
    ],
    "published": "2025-02-25",
    "abstract": "Drone-based remote sensing combined with AI-driven methodologies has shown\ngreat potential for accurate mapping and monitoring of coral reef ecosystems.\nThis study presents a novel multi-scale approach to coral reef monitoring,\nintegrating fine-scale underwater imagery with medium-scale aerial imagery.\nUnderwater images are captured using an Autonomous Surface Vehicle (ASV), while\naerial images are acquired with an aerial drone. A transformer-based\ndeep-learning model is trained on underwater images to detect the presence of\n31 classes covering various coral morphotypes, associated fauna, and habitats.\nThese predictions serve as annotations for training a second model applied to\naerial images. The transfer of information across scales is achieved through a\nweighted footprint method that accounts for partial overlaps between underwater\nimage footprints and aerial image tiles. The results show that the multi-scale\nmethodology successfully extends fine-scale classification to larger reef\nareas, achieving a high degree of accuracy in predicting coral morphotypes and\nassociated habitats. The method showed a strong alignment between\nunderwater-derived annotations and ground truth data, reflected by an AUC (Area\nUnder the Curve) score of 0.9251. This shows that the integration of underwater\nand aerial imagery, supported by deep-learning models, can facilitate scalable\nand accurate reef assessments. This study demonstrates the potential of\ncombining multi-scale imaging and AI to facilitate the monitoring and\nconservation of coral reefs. Our approach leverages the strengths of underwater\nand aerial imagery, ensuring the precision of fine-scale analysis while\nextending it to cover a broader reef area.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Convolutional neural networks for mineral prospecting through alteration mapping with remote sensing data",
    "url": "http://arxiv.org/abs/2502.18533v1",
    "authors": [
      "Ehsan Farahbakhsh",
      "Dakshi Goel",
      "Dhiraj Pimparkar",
      "R. Dietmar Muller",
      "Rohitash Chandra"
    ],
    "published": "2025-02-25",
    "abstract": "Traditional geological mapping, based on field observations and rock sample\nanalysis, is inefficient for continuous spatial mapping of features like\nalteration zones. Deep learning models, such as convolutional neural networks\n(CNNs), have revolutionised remote sensing data analysis by automatically\nextracting features for classification and regression tasks. CNNs can detect\nspecific mineralogical changes linked to mineralisation by identifying subtle\nfeatures in remote sensing data. This study uses CNNs with Landsat 8, Landsat\n9, and ASTER data to map alteration zones north of Broken Hill, New South\nWales, Australia. The model is trained using ground truth data and an automated\napproach with selective principal component analysis (PCA). We compare CNNs\nwith traditional machine learning models, including k-nearest neighbours,\nsupport vector machines, and multilayer perceptron. Results show that ground\ntruth-based training yields more reliable maps, with CNNs slightly\noutperforming conventional models in capturing spatial patterns. Landsat 9\noutperforms Landsat 8 in mapping iron oxide areas using ground truth-trained\nCNNs, while ASTER data provides the most accurate argillic and propylitic\nalteration maps. This highlights CNNs' effectiveness in improving geological\nmapping precision, especially for identifying subtle mineralisation-related\nalterations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "A Deep Learning Framework with Geographic Information Adaptive Loss for Remote Sensing Images based UAV Self-Positioning",
    "url": "http://arxiv.org/abs/2502.16164v1",
    "authors": [
      "Mingkun Li",
      "Ziming Wang",
      "Guang Huo",
      "Wei Chen",
      "Xiaoning Zhao"
    ],
    "published": "2025-02-22",
    "abstract": "With the expanding application scope of unmanned aerial vehicles (UAVs), the\ndemand for stable UAV control has significantly increased. However, in complex\nenvironments, GPS signals are prone to interference, resulting in ineffective\nUAV positioning. Therefore, self-positioning of UAVs in GPS-denied environments\nhas become a critical objective. Some methods obtain geolocation information in\nGPS-denied environments by matching ground objects in the UAV viewpoint with\nremote sensing images. However, most of these methods only provide coarse-level\npositioning, which satisfies cross-view geo-localization but cannot support\nprecise UAV positioning tasks. Consequently, this paper focuses on a newer and\nmore challenging task: precise UAV self-positioning based on remote sensing\nimages. This approach not only considers the features of ground objects but\nalso accounts for the spatial distribution of objects in the images. To address\nthis challenge, we present a deep learning framework with geographic\ninformation adaptive loss, which achieves precise localization by aligning UAV\nimages with corresponding satellite imagery in fine detail through the\nintegration of geographic information from multiple perspectives. To validate\nthe effectiveness of the proposed method, we conducted a series of experiments.\nThe results demonstrate the method's efficacy in enabling UAVs to achieve\nprecise self-positioning using remote sensing imagery.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
    "url": "http://arxiv.org/abs/2502.13407v2",
    "authors": [
      "Ziyuan Liu",
      "Ruifei Zhu",
      "Long Gao",
      "Yuanxiu Zhou",
      "Jingyu Ma",
      "Yuantao Gu"
    ],
    "published": "2025-02-19",
    "abstract": "Deep learning has achieved significant success in the field of remote sensing\nimage change detection (CD), yet two major challenges remain: the scarcity of\nsub-meter, comprehensive open-source CD datasets, and the difficulty of\nachieving consistent and satisfactory detection results across images with\nvarying change areas. To address these issues, we introduce the JL1-CD dataset,\nwhich consists of 5,000 pairs of 512 x 512 pixel images with a resolution of\n0.5 to 0.75 meters. This all-inclusive dataset covers a wide range of\nhuman-induced and natural changes, including buildings, roads, hardened\nsurfaces, woodlands, grasslands, croplands, water bodies, and photovoltaic\npanels, among others. Additionally, we propose a novel multi-teacher knowledge\ndistillation (MTKD) framework that leverages the Origin-Partition (O-P)\nstrategy to enhance CD performance. In the O-P strategy, we partition the\ntraining data based on the Change Area Ratio (CAR) to train separate models for\nsmall, medium, and large CAR values, alleviating the learning burden on each\nmodel and improving their performance within their respective partitions.\nBuilding upon this, our MTKD framework distills knowledge from multiple teacher\nmodels trained on different CAR partitions into a single student model,enabling\nthe student model to achieve superior detection results across diverse CAR\nscenarios without incurring additional computational or time overhead during\nthe inference phase. Experimental results on the JL1-CD and SYSU-CD datasets\ndemonstrate that the MTKD framework significantly improves the performance of\nCD models with various network architectures and parameter sizes, achieving new\nstate-of-the-art results. The JL1-CD dataset and code are available at\nhttps://github.com/circleLZY/MTKD-CD.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "End-to-end pipeline for simultaneous temperature estimation and super resolution of low-cost uncooled infrared camera frames for precision agriculture applications",
    "url": "http://arxiv.org/abs/2502.13985v1",
    "authors": [
      "Navot Oz",
      "Nir Sochen",
      "David Mendlovic",
      "Iftach Klapp"
    ],
    "published": "2025-02-18",
    "abstract": "Radiometric infrared (IR) imaging is a valuable technique for remote-sensing\napplications in precision agriculture, such as irrigation monitoring, crop\nhealth assessment, and yield estimation. Low-cost uncooled non-radiometric IR\ncameras offer new implementations in agricultural monitoring. However, these\ncameras have inherent drawbacks that limit their usability, such as low spatial\nresolution, spatially variant nonuniformity, and lack of radiometric\ncalibration. In this article, we present an end-to-end pipeline for temperature\nestimation and super resolution of frames captured by a low-cost uncooled IR\ncamera. The pipeline consists of two main components: a deep-learning-based\ntemperature-estimation module, and a deep-learning-based super-resolution\nmodule. The temperature-estimation module learns to map the raw gray level IR\nimages to the corresponding temperature maps while also correcting for\nnonuniformity. The super-resolution module uses a deep-learning network to\nenhance the spatial resolution of the IR images by scale factors of x2 and x4.\nWe evaluated the performance of the pipeline on both simulated and real-world\nagricultural datasets composing of roughly 20,000 frames of various crops. For\nthe simulated data, the results were on par with the real-world data with\nsub-degree accuracy. For the real data, the proposed pipeline was compared to a\nhigh-end radiometric thermal camera, and achieved sub-degree accuracy. The\nresults of the real data are on par with the simulated data. The proposed\npipeline can enable various applications in precision agriculture that require\nhigh quality thermal information from low-cost IR cameras.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "When Segmentation Meets Hyperspectral Image: New Paradigm for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2502.12541v1",
    "authors": [
      "Weilian Zhou",
      "Weixuan Xie",
      "Sei-ichiro Kamata",
      "Man Sing Wong",
      "Huiying",
      "Hou",
      "Haipeng Wang"
    ],
    "published": "2025-02-18",
    "abstract": "Hyperspectral image (HSI) classification is a cornerstone of remote sensing,\nenabling precise material and land-cover identification through rich spectral\ninformation. While deep learning has driven significant progress in this task,\nsmall patch-based classifiers, which account for over 90% of the progress, face\nlimitations: (1) the small patch (e.g., 7x7, 9x9)-based sampling approach\nconsiders a limited receptive field, resulting in insufficient spatial\nstructural information critical for object-level identification and noise-like\nmisclassifications even within uniform regions; (2) undefined optimal patch\nsizes lead to coarse label predictions, which degrade performance; and (3) a\nlack of multi-shape awareness around objects. To address these challenges, we\ndraw inspiration from large-scale image segmentation techniques, which excel at\nhandling object boundaries-a capability essential for semantic labeling in HSI\nclassification. However, their application remains under-explored in this task\ndue to (1) the prevailing notion that larger patch sizes degrade performance,\n(2) the extensive unlabeled regions in HSI groundtruth, and (3) the\nmisalignment of input shapes between HSI data and segmentation models. Thus, in\nthis study, we propose a novel paradigm and baseline, HSIseg, for HSI\nclassification that leverages segmentation techniques combined with a novel\nDynamic Shifted Regional Transformer (DSRT) to overcome these challenges. We\nalso introduce an intuitive progressive learning framework with adaptive\npseudo-labeling to iteratively incorporate unlabeled regions into the training\nprocess, thereby advancing the application of segmentation techniques.\nAdditionally, we incorporate auxiliary data through multi-source data\ncollaboration, promoting better feature interaction. Validated on five public\nHSI datasets, our proposal outperforms state-of-the-art methods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Multispectral Remote Sensing for Weed Detection in West Australian Agricultural Lands",
    "url": "http://arxiv.org/abs/2502.08678v1",
    "authors": [
      "Haitian Wang",
      "Muhammad Ibrahim",
      "Yumeng Miao",
      "D ustin Severtson",
      "Atif Mansoor",
      "Ajmal S. Mian"
    ],
    "published": "2025-02-12",
    "abstract": "The Kondinin region in Western Australia faces significant agricultural\nchallenges due to pervasive weed infestations, causing economic losses and\necological impacts. This study constructs a tailored multispectral remote\nsensing dataset and an end-to-end framework for weed detection to advance\nprecision agriculture practices. Unmanned aerial vehicles were used to collect\nraw multispectral data from two experimental areas (E2 and E8) over four years,\ncovering 0.6046 km^{2} and ground truth annotations were created with\nGPS-enabled vehicles to manually label weeds and crops. The dataset is\nspecifically designed for agricultural applications in Western Australia. We\npropose an end-to-end framework for weed detection that includes extensive\npreprocessing steps, such as denoising, radiometric calibration, image\nalignment, orthorectification, and stitching. The proposed method combines\nvegetation indices (NDVI, GNDVI, EVI, SAVI, MSAVI) with multispectral channels\nto form classification features, and employs several deep learning models to\nidentify weeds based on the input features. Among these models, ResNet achieves\nthe highest performance, with a weed detection accuracy of 0.9213, an F1-Score\nof 0.8735, an mIOU of 0.7888, and an mDC of 0.8865, validating the efficacy of\nthe dataset and the proposed weed detection method.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Simultaneous Automatic Picking and Manual Picking Refinement for First-Break",
    "url": "http://arxiv.org/abs/2502.01474v1",
    "authors": [
      "Haowen Bai",
      "Zixiang Zhao",
      "Jiangshe Zhang",
      "Yukun Cui",
      "Chunxia Zhang",
      "Zhenbo Guo",
      "Yongjun Wang"
    ],
    "published": "2025-02-03",
    "abstract": "First-break picking is a pivotal procedure in processing microseismic data\nfor geophysics and resource exploration. Recent advancements in deep learning\nhave catalyzed the evolution of automated methods for identifying first-break.\nNevertheless, the complexity of seismic data acquisition and the requirement\nfor detailed, expert-driven labeling often result in outliers and potential\nmislabeling within manually labeled datasets. These issues can negatively\naffect the training of neural networks, necessitating algorithms that handle\noutliers or mislabeled data effectively. We introduce the Simultaneous Picking\nand Refinement (SPR) algorithm, designed to handle datasets plagued by outlier\nsamples or even noisy labels. Unlike conventional approaches that regard manual\npicks as ground truth, our method treats the true first-break as a latent\nvariable within a probabilistic model that includes a first-break labeling\nprior. SPR aims to uncover this variable, enabling dynamic adjustments and\nimproved accuracy across the dataset. This strategy mitigates the impact of\noutliers or inaccuracies in manual labels. Intra-site picking experiments and\ncross-site generalization experiments on publicly available data confirm our\nmethod's performance in identifying first-break and its generalization across\ndifferent sites. Additionally, our investigations into noisy signals and labels\nunderscore SPR's resilience to both types of noise and its capability to refine\nmisaligned manual annotations. Moreover, the flexibility of SPR, not being\nlimited to any single network architecture, enhances its adaptability across\nvarious deep learning-based picking methods. Focusing on learning from data\nthat may contain outliers or partial inaccuracies, SPR provides a robust\nsolution to some of the principal obstacles in automatic first-break picking.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "General Feature Extraction In SAR Target Classification: A Contrastive Learning Approach Across Sensor Types",
    "url": "http://arxiv.org/abs/2502.01162v1",
    "authors": [
      "M. Muzeau",
      "J. Frontera-Pons",
      "Chengfang Ren",
      "J. -P. Ovarlez"
    ],
    "published": "2025-02-03",
    "abstract": "The increased availability of SAR data has raised a growing interest in\napplying deep learning algorithms. However, the limited availability of labeled\ndata poses a significant challenge for supervised training. This article\nintroduces a new method for classifying SAR data with minimal labeled images.\nThe method is based on a feature extractor Vit trained with contrastive\nlearning. It is trained on a dataset completely different from the one on which\nclassification is made. The effectiveness of the method is assessed through 2D\nvisualization using t-SNE for qualitative evaluation and k-NN classification\nwith a small number of labeled data for quantitative evaluation. Notably, our\nresults outperform a k-NN on data processed with PCA and a ResNet-34\nspecifically trained for the task, achieving a 95.9% accuracy on the MSTAR\ndataset with just ten labeled images per class.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "ResNet",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Multi-Resolution SAR and Optical Remote Sensing Image Registration Methods: A Review, Datasets, and Future Perspectives",
    "url": "http://arxiv.org/abs/2502.01002v1",
    "authors": [
      "Wenfei Zhang",
      "Ruipeng Zhao",
      "Yongxiang Yao",
      "Yi Wan",
      "Peihao Wu",
      "Jiayuan Li",
      "Yansheng Li",
      "Yongjun Zhang"
    ],
    "published": "2025-02-03",
    "abstract": "Synthetic Aperture Radar (SAR) and optical image registration is essential\nfor remote sensing data fusion, with applications in military reconnaissance,\nenvironmental monitoring, and disaster management. However, challenges arise\nfrom differences in imaging mechanisms, geometric distortions, and radiometric\nproperties between SAR and optical images. As image resolution increases, fine\nSAR textures become more significant, leading to alignment issues and 3D\nspatial discrepancies. Two major gaps exist: the lack of a publicly available\nmulti-resolution, multi-scene registration dataset and the absence of\nsystematic analysis of current methods. To address this, the MultiResSAR\ndataset was created, containing over 10k pairs of multi-source,\nmulti-resolution, and multi-scene SAR and optical images. Sixteen\nstate-of-the-art algorithms were tested. Results show no algorithm achieves\n100% success, and performance decreases as resolution increases, with most\nfailing on sub-meter data. XoFTR performs best among deep learning methods\n(40.58%), while RIFT performs best among traditional methods (66.51%). Future\nresearch should focus on noise suppression, 3D geometric fusion, cross-view\ntransformation modeling, and deep learning optimization for robust registration\nof high-resolution SAR and optical images. The dataset is available at\nhttps://github.com/betterlll/Multi-Resolution-SAR-dataset-.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Exploring Linear Attention Alternative for Single Image Super-Resolution",
    "url": "http://arxiv.org/abs/2502.00404v1",
    "authors": [
      "Rongchang Lu",
      "Changyu Li",
      "Donghang Li",
      "Guojing Zhang",
      "Jianqiang Huang",
      "Xilai Li"
    ],
    "published": "2025-02-01",
    "abstract": "Deep learning-based single-image super-resolution (SISR) technology focuses\non enhancing low-resolution (LR) images into high-resolution (HR) ones.\nAlthough significant progress has been made, challenges remain in computational\ncomplexity and quality, particularly in remote sensing image processing. To\naddress these issues, we propose our Omni-Scale RWKV Super-Resolution\n(OmniRWKVSR) model which presents a novel approach that combines the Receptance\nWeighted Key Value (RWKV) architecture with feature extraction techniques such\nas Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM),\naiming to overcome the limitations of existing methods and achieve superior\nSISR performance. This work has proved able to provide effective solutions for\nhigh-quality image reconstruction. Under the 4x Super-Resolution tasks,\ncompared to the MambaIR model, we achieved an average improvement of 0.26% in\nPSNR and 0.16% in SSIM.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Ground Awareness in Deep Learning for Large Outdoor Point Cloud Segmentation",
    "url": "http://arxiv.org/abs/2501.18246v1",
    "authors": [
      "Kevin Qiu",
      "Dimitri Bulatov",
      "Dorota Iwaszczuk"
    ],
    "published": "2025-01-30",
    "abstract": "This paper presents an analysis of utilizing elevation data to aid outdoor\npoint cloud semantic segmentation through existing machine-learning networks in\nremote sensing, specifically in urban, built-up areas. In dense outdoor point\nclouds, the receptive field of a machine learning model may be too small to\naccurately determine the surroundings and context of a point. By computing\nDigital Terrain Models (DTMs) from the point clouds, we extract the relative\nelevation feature, which is the vertical distance from the terrain to a point.\nRandLA-Net is employed for efficient semantic segmentation of large-scale point\nclouds. We assess its performance across three diverse outdoor datasets\ncaptured with varying sensor technologies and sensor locations. Integration of\nrelative elevation data leads to consistent performance improvements across all\nthree datasets, most notably in the Hessigheim dataset, with an increase of 3.7\npercentage points in average F1 score from 72.35% to 76.01%, by establishing\nlong-range dependencies between ground and objects. We also explore additional\nlocal features such as planarity, normal vectors, and 2D features, but their\nefficacy varied based on the characteristics of the point cloud. Ultimately,\nthis study underscores the important role of the non-local relative elevation\nfeature for semantic segmentation of point clouds in remote sensing\napplications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "AdaSemSeg: An Adaptive Few-shot Semantic Segmentation of Seismic Facies",
    "url": "http://arxiv.org/abs/2501.16760v1",
    "authors": [
      "Surojit Saha",
      "Ross Whitaker"
    ],
    "published": "2025-01-28",
    "abstract": "Automated interpretation of seismic images using deep learning methods is\nchallenging because of the limited availability of training data. Few-shot\nlearning is a suitable learning paradigm in such scenarios due to its ability\nto adapt to a new task with limited supervision (small training budget).\nExisting few-shot semantic segmentation (FSSS) methods fix the number of target\nclasses. Therefore, they do not support joint training on multiple datasets\nvarying in the number of classes. In the context of the interpretation of\nseismic facies, fixing the number of target classes inhibits the generalization\ncapability of a model trained on one facies dataset to another, which is likely\nto have a different number of facies. To address this shortcoming, we propose a\nfew-shot semantic segmentation method for interpreting seismic facies that can\nadapt to the varying number of facies across the dataset, dubbed the AdaSemSeg.\nIn general, the backbone network of FSSS methods is initialized with the\nstatistics learned from the ImageNet dataset for better performance. The lack\nof such a huge annotated dataset for seismic images motivates using a\nself-supervised algorithm on seismic datasets to initialize the backbone\nnetwork. We have trained the AdaSemSeg on three public seismic facies datasets\nwith different numbers of facies and evaluated the proposed method on multiple\nmetrics. The performance of the AdaSemSeg on unseen datasets (not used in\ntraining) is better than the prototype-based few-shot method and baselines.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP",
    "url": "http://arxiv.org/abs/2501.16222v2",
    "authors": [
      "Li Pang",
      "Jing Yao",
      "Kaiyu Li",
      "Xiangyong Cao"
    ],
    "published": "2025-01-27",
    "abstract": "Hyperspectral image (HSI) classification aims at categorizing each pixel in\nan HSI into a specific land cover class, which is crucial for applications like\nremote sensing, environmental monitoring, and agriculture. Although deep\nlearning-based HSI classification methods have achieved significant\nadvancements, existing methods still rely on manually labeled data for\ntraining, which is both time-consuming and labor-intensive. To address this\nlimitation, we introduce a novel zero-shot hyperspectral image classification\nframework based on CLIP (SPECIAL), aiming to eliminate the need for manual\nannotations. The SPECIAL framework consists of two main stages: (1) CLIP-based\npseudo-label generation, and (2) noisy label learning. In the first stage, HSI\nis spectrally interpolated to produce RGB bands. These bands are subsequently\nclassified using CLIP, resulting in noisy pseudo-labels that are accompanied by\nconfidence scores. To improve the quality of these labels, we propose a scaling\nstrategy that fuses predictions from multiple spatial scales. In the second\nstage, spectral information and a label refinement technique are incorporated\nto mitigate label noise and further enhance classification accuracy.\nExperimental results on three benchmark datasets demonstrate that our SPECIAL\noutperforms existing methods in zero-shot HSI classification, showing its\npotential for more practical applications. The code is available at\nhttps://github.com/LiPang/SPECIAL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Semi-Supervised Learning for AVO Inversion with Strong Spatial Feature Constraints",
    "url": "http://arxiv.org/abs/2501.15473v2",
    "authors": [
      "Yingtian Liu",
      "Yong Li",
      "Junheng Peng",
      "Mingwei Wang"
    ],
    "published": "2025-01-26",
    "abstract": "One-dimensional convolution is a widely used deep learning technique in\nprestack amplitude variation with offset (AVO) inversion; however, it lacks\nlateral continuity. Although two-dimensional convolution improves lateral\ncontinuity, due to the sparsity of well-log data, the model only learns weak\nspatial features and fails to explore the spatial correlations in seismic data\nfully. To overcome these challenges, we propose a novel AVO inversion method\nbased on semi-supervised learning with strong spatial feature constraints\n(SSFC-SSL). First, two-dimensional predicted values are obtained through the\ninversion network, and the predicted values at well locations are sparsely\nrepresented using well-log labels. Subsequently, a label-annihilation operator\nis introduced, enabling the predicted values at non-well locations to learn the\nspatial features of well locations through the neural network. Ultimately, a\ntwo-way strong spatial feature mapping between non-well locations and well\nlocations is achieved. Additionally, to reduce the dependence on well-log\nlabels, we combine the semi-supervised learning strategy with a low-frequency\nmodel, further enhancing the robustness of the method. Experimental results on\nboth synthetic example and field data demonstrate that the proposed method\nsignificantly improves lateral continuity and inversion accuracy compared to\none- and two-dimensional deep learning techniques.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void Filling",
    "url": "http://arxiv.org/abs/2501.15440v2",
    "authors": [
      "Daniel Panangian",
      "Ksenia Bittner"
    ],
    "published": "2025-01-26",
    "abstract": "Digital Surface Models (DSMs) are essential for accurately representing\nEarth's topography in geospatial analyses. DSMs capture detailed elevations of\nnatural and manmade features, crucial for applications like urban planning,\nvegetation studies, and 3D reconstruction. However, DSMs derived from stereo\nsatellite imagery often contain voids or missing data due to occlusions,\nshadows, and lowsignal areas. Previous studies have primarily focused on void\nfilling for digital elevation models (DEMs) and Digital Terrain Models (DTMs),\nemploying methods such as inverse distance weighting (IDW), kriging, and spline\ninterpolation. While effective for simpler terrains, these approaches often\nfail to handle the intricate structures present in DSMs. To overcome these\nlimitations, we introduce Dfilled, a guided DSM void filling method that\nleverages optical remote sensing images through edge-enhancing diffusion.\nDfilled repurposes deep anisotropic diffusion models, which originally designed\nfor super-resolution tasks, to inpaint DSMs. Additionally, we utilize Perlin\nnoise to create inpainting masks that mimic natural void patterns in DSMs.\nExperimental evaluations demonstrate that Dfilled surpasses traditional\ninterpolation methods and deep learning approaches in DSM void filling tasks.\nBoth quantitative and qualitative assessments highlight the method's ability to\nmanage complex features and deliver accurate, visually coherent results.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "CP2M: Clustered-Patch-Mixed Mosaic Augmentation for Aerial Image Segmentation",
    "url": "http://arxiv.org/abs/2501.15389v1",
    "authors": [
      "Yijie Li",
      "Hewei Wang",
      "Jinfeng Xu",
      "Zixiao Ma",
      "Puzhen Wu",
      "Shaofan Wang",
      "Soumyabrata Dev"
    ],
    "published": "2025-01-26",
    "abstract": "Remote sensing image segmentation is pivotal for earth observation,\nunderpinning applications such as environmental monitoring and urban planning.\nDue to the limited annotation data available in remote sensing images, numerous\nstudies have focused on data augmentation as a means to alleviate overfitting\nin deep learning networks. However, some existing data augmentation strategies\nrely on simple transformations that may not sufficiently enhance data diversity\nor model generalization capabilities. This paper proposes a novel augmentation\nstrategy, Clustered-Patch-Mixed Mosaic (CP2M), designed to address these\nlimitations. CP2M integrates a Mosaic augmentation phase with a clustered patch\nmix phase. The former stage constructs a new sample from four random samples,\nwhile the latter phase uses the connected component labeling algorithm to\nensure the augmented data maintains spatial coherence and avoids introducing\nirrelevant semantics when pasting random patches. Our experiments on the ISPRS\nPotsdam dataset demonstrate that CP2M substantially mitigates overfitting,\nsetting new benchmarks for segmentation accuracy and model robustness in remote\nsensing tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Snapshot multi-spectral imaging through defocusing and a Fourier imager network",
    "url": "http://arxiv.org/abs/2501.14287v1",
    "authors": [
      "Xilin Yang",
      "Michael John Fanous",
      "Hanlong Chen",
      "Ryan Lee",
      "Paloma Casteleiro Costa",
      "Yuhang Li",
      "Luzhe Huang",
      "Yijie Zhang",
      "Aydogan Ozcan"
    ],
    "published": "2025-01-24",
    "abstract": "Multi-spectral imaging, which simultaneously captures the spatial and\nspectral information of a scene, is widely used across diverse fields,\nincluding remote sensing, biomedical imaging, and agricultural monitoring.\nHere, we introduce a snapshot multi-spectral imaging approach employing a\nstandard monochrome image sensor with no additional spectral filters or\ncustomized components. Our system leverages the inherent chromatic aberration\nof wavelength-dependent defocusing as a natural source of physical encoding of\nmulti-spectral information; this encoded image information is rapidly decoded\nvia a deep learning-based multi-spectral Fourier Imager Network (mFIN). We\nexperimentally tested our method with six illumination bands and demonstrated\nan overall accuracy of 92.98% for predicting the illumination channels at the\ninput and achieved a robust multi-spectral image reconstruction on various test\nobjects. This deep learning-powered framework achieves high-quality\nmulti-spectral image reconstruction using snapshot image acquisition with a\nmonochrome image sensor and could be useful for applications in biomedicine,\nindustrial quality control, and agriculture, among others.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ATRNet-STAR: A Large Dataset and Benchmark Towards Remote Sensing Object Recognition in the Wild",
    "url": "http://arxiv.org/abs/2501.13354v4",
    "authors": [
      "Yongxiang Liu",
      "Weijie Li",
      "Li Liu",
      "Jie Zhou",
      "Bowen Peng",
      "Yafei Song",
      "Xuying Xiong",
      "Wei Yang",
      "Tianpeng Liu",
      "Zhen Liu",
      "Xiang Li"
    ],
    "published": "2025-01-23",
    "abstract": "The absence of publicly available, large-scale, high-quality datasets for\nSynthetic Aperture Radar Automatic Target Recognition (SAR ATR) has\nsignificantly hindered the application of rapidly advancing deep learning\ntechniques, which hold huge potential to unlock new capabilities in this field.\nThis is primarily because collecting large volumes of diverse target samples\nfrom SAR images is prohibitively expensive, largely due to privacy concerns,\nthe characteristics of microwave radar imagery perception, and the need for\nspecialized expertise in data annotation. Throughout the history of SAR ATR\nresearch, there have been only a number of small datasets, mainly including\ntargets like ships, airplanes, buildings, etc. There is only one vehicle\ndataset MSTAR collected in the 1990s, which has been a valuable source for SAR\nATR. To fill this gap, this paper introduces a large-scale, new dataset named\nATRNet-STAR with 40 different vehicle categories collected under various\nrealistic imaging conditions and scenes. It marks a substantial advancement in\ndataset scale and diversity, comprising over 190,000 well-annotated samples, 10\ntimes larger than its predecessor, the famous MSTAR. Building such a large\ndataset is a challenging task, and the data collection scheme will be detailed.\nSecondly, we illustrate the value of ATRNet-STAR via extensively evaluating the\nperformance of 15 representative methods with 7 different experimental settings\non challenging classification and detection benchmarks derived from the\ndataset. Finally, based on our extensive experiments, we identify valuable\ninsights for SAR ATR and discuss potential future research directions in this\nfield. We hope that the scale, diversity, and benchmark of ATRNet-STAR can\nsignificantly facilitate the advancement of SAR ATR.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Progressive Cross Attention Network for Flood Segmentation using Multispectral Satellite Imagery",
    "url": "http://arxiv.org/abs/2501.11923v1",
    "authors": [
      "Vicky Feliren",
      "Fithrothul Khikmah",
      "Irfan Dwiki Bhaswara",
      "Bahrul I. Nasution",
      "Alex M. Lechner",
      "Muhamad Risqi U. Saputra"
    ],
    "published": "2025-01-21",
    "abstract": "In recent years, the integration of deep learning techniques with remote\nsensing technology has revolutionized the way natural hazards, such as floods,\nare monitored and managed. However, existing methods for flood segmentation\nusing remote sensing data often overlook the utility of correlative features\namong multispectral satellite information. In this study, we introduce a\nprogressive cross attention network (ProCANet), a deep learning model that\nprogressively applies both self- and cross-attention mechanisms to\nmultispectral features, generating optimal feature combinations for flood\nsegmentation. The proposed model was compared with state-of-the-art approaches\nusing Sen1Floods11 dataset and our bespoke flood data generated for the Citarum\nRiver basin, Indonesia. Our model demonstrated superior performance with the\nhighest Intersection over Union (IoU) score of 0.815. Our results in this\nstudy, coupled with the ablation assessment comparing scenarios with and\nwithout attention across various modalities, opens a promising path for\nenhancing the accuracy of flood analysis using remote sensing technology.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "On the Adversarial Vulnerabilities of Transfer Learning in Remote Sensing",
    "url": "http://arxiv.org/abs/2501.11462v1",
    "authors": [
      "Tao Bai",
      "Xingjian Tian",
      "Yonghao Xu",
      "Bihan Wen"
    ],
    "published": "2025-01-20",
    "abstract": "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Few-shot Human Motion Recognition through Multi-Aspect mmWave FMCW Radar Data",
    "url": "http://arxiv.org/abs/2501.11028v1",
    "authors": [
      "Hao Fan",
      "Lingfeng Chen",
      "Chengbai Xu",
      "Jiadong Zhou",
      "Yongpeng Dai",
      "Panhe HU"
    ],
    "published": "2025-01-19",
    "abstract": "Radar human motion recognition methods based on deep learning models has been\na heated spot of remote sensing in recent years, yet the existing methods are\nmostly radial-oriented. In practical application, the test data could be\nmulti-aspect and the sample number of each motion could be very limited,\ncausing model overfitting and reduced recognition accuracy. This paper proposed\nchannel-DN4, a multi-aspect few-shot human motion recognition method. First,\nlocal descriptors are introduced for a precise classification metric. Moreover,\nepisodic training strategy was adopted to reduce model overfitting. To utilize\nthe invariant sematic information in multi-aspect conditions, we considered\nchannel attention after the embedding network to obtain precise implicit\nhigh-dimensional representation of sematic information. We tested the\nperformance of channel-DN4 and methods for comparison on measured mmWave FMCW\nradar data. The proposed channel-DN4 produced competitive and convincing\nresults, reaching the highest 87.533% recognition accuracy in 3-way 10-shot\ncondition while other methods suffer from overfitting. Codes are available at:\nhttps://github.com/MountainChenCad/channel-DN4",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "A Remote Sensing Image Change Detection Method Integrating Layer Exchange and Channel-Spatial Differences",
    "url": "http://arxiv.org/abs/2501.10905v1",
    "authors": [
      "Sijun Dong",
      "Fangcheng Zuo",
      "Geng Chen",
      "Siming Fu",
      "Xiaoliang Meng"
    ],
    "published": "2025-01-19",
    "abstract": "Change detection in remote sensing imagery is a critical technique for Earth\nobservation, primarily focusing on pixel-level segmentation of change regions\nbetween bi-temporal images. The essence of pixel-level change detection lies in\ndetermining whether corresponding pixels in bi-temporal images have changed. In\ndeep learning, the spatial and channel dimensions of feature maps represent\ndifferent information from the original images. In this study, we found that in\nchange detection tasks, difference information can be computed not only from\nthe spatial dimension of bi-temporal features but also from the channel\ndimension. Therefore, we designed the Channel-Spatial Difference Weighting\n(CSDW) module as an aggregation-distribution mechanism for bi-temporal features\nin change detection. This module enhances the sensitivity of the change\ndetection model to difference features. Additionally, bi-temporal images share\nthe same geographic location and exhibit strong inter-image correlations. To\nconstruct the correlation between bi-temporal images, we designed a decoding\nstructure based on the Layer-Exchange (LE) method to enhance the interaction of\nbi-temporal features. Comprehensive experiments on the CLCD, PX-CLCD, LEVIR-CD,\nand S2Looking datasets demonstrate that the proposed LENet model significantly\nimproves change detection performance. The code and pre-trained models will be\navailable at: https://github.com/dyzy41/lenet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "EarthView: A Large Scale Remote Sensing Dataset for Self-Supervision",
    "url": "http://arxiv.org/abs/2501.08111v1",
    "authors": [
      "Diego Velazquez",
      "Pau Rodriguez L\u00f3pez",
      "Sergio Alonso",
      "Josep M. Gonfaus",
      "Jordi Gonzalez",
      "Gerardo Richarte",
      "Javier Marin",
      "Yoshua Bengio",
      "Alexandre Lacoste"
    ],
    "published": "2025-01-14",
    "abstract": "This paper presents EarthView, a comprehensive dataset specifically designed\nfor self-supervision on remote sensing data, intended to enhance deep learning\napplications on Earth monitoring tasks. The dataset spans 15 tera pixels of\nglobal remote-sensing data, combining imagery from a diverse range of sources,\nincluding NEON, Sentinel, and a novel release of 1m spatial resolution data\nfrom Satellogic. Our dataset provides a wide spectrum of image data with\nvarying resolutions, harnessed from different sensors and organized coherently\ninto an accessible HuggingFace dataset in parquet format. This data spans five\nyears, from 2017 to 2022. Accompanying the dataset, we introduce EarthMAE, a\ntailored Masked Autoencoder, developed to tackle the distinct challenges of\nremote sensing data. Trained in a self-supervised fashion, EarthMAE effectively\nprocesses different data modalities such as hyperspectral, multispectral,\ntopographical data, segmentation maps, and temporal structure. This model helps\nus show that pre-training on Satellogic data improves performance on downstream\ntasks. While there is still a gap to fill in MAE for heterogeneous data, we\nregard this innovative combination of an expansive, diverse dataset and a\nversatile model adapted for self-supervised learning as a stride forward in\ndeep learning for Earth monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "UCloudNet: A Residual U-Net with Deep Supervision for Cloud Image Segmentation",
    "url": "http://arxiv.org/abs/2501.06440v1",
    "authors": [
      "Yijie Li",
      "Hewei Wang",
      "Shaofan Wang",
      "Yee Hui Lee",
      "Muhammad Salman Pathan",
      "Soumyabrata Dev"
    ],
    "published": "2025-01-11",
    "abstract": "Recent advancements in meteorology involve the use of ground-based sky\ncameras for cloud observation. Analyzing images from these cameras helps in\ncalculating cloud coverage and understanding atmospheric phenomena.\nTraditionally, cloud image segmentation relied on conventional computer vision\ntechniques. However, with the advent of deep learning, convolutional neural\nnetworks (CNNs) are increasingly applied for this purpose. Despite their\neffectiveness, CNNs often require many epochs to converge, posing challenges\nfor real-time processing in sky camera systems. In this paper, we introduce a\nresidual U-Net with deep supervision for cloud segmentation which provides\nbetter accuracy than previous approaches, and with less training consumption.\nBy utilizing residual connection in encoders of UCloudNet, the feature\nextraction ability is further improved.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction",
    "url": "http://arxiv.org/abs/2501.04487v1",
    "authors": [
      "Guofeng Yang",
      "Nanfei Jin",
      "Wenjie Ai",
      "Zhonghua Zheng",
      "Yuhong He",
      "Yong He"
    ],
    "published": "2025-01-08",
    "abstract": "Yield is one of the core goals of crop breeding. By predicting the potential\nyield of different breeding materials, breeders can screen these materials at\nvarious growth stages to select the best performing. Based on unmanned aerial\nvehicle remote sensing technology, high-throughput crop phenotyping data in\nbreeding areas is collected to provide data support for the breeding decisions\nof breeders. However, the accuracy of current yield predictions still requires\nimprovement, and the usability and user-friendliness of yield forecasting tools\nremain suboptimal. To address these challenges, this study introduces a hybrid\nmethod and tool for crop yield prediction, designed to allow breeders to\ninteractively and accurately predict wheat yield by chatting with a large\nlanguage model (LLM). First, the newly designed data assimilation algorithm is\nused to assimilate the leaf area index into the WOFOST model. Then, selected\noutputs from the assimilation process, along with remote sensing inversion\nresults, are used to drive the time-series temporal fusion transformer model\nfor wheat yield prediction. Finally, based on this hybrid method and leveraging\nan LLM with retrieval augmented generation technology, we developed an\ninteractive yield prediction Web tool that is user-friendly and supports\nsustainable data updates. This tool integrates multi-source data to assist\nbreeding decision-making. This study aims to accelerate the identification of\nhigh-yield materials in the breeding process, enhance breeding efficiency, and\nenable more scientific and smart breeding decisions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Rapid Automated Mapping of Clouds on Titan With Instance Segmentation",
    "url": "http://arxiv.org/abs/2501.04459v1",
    "authors": [
      "Zachary Yahn",
      "Douglas M Trent",
      "Ethan Duncan",
      "Beno\u00eet Seignovert",
      "John Santerre",
      "Conor Nixon"
    ],
    "published": "2025-01-08",
    "abstract": "Despite widespread adoption of deep learning models to address a variety of\ncomputer vision tasks, planetary science has yet to see extensive utilization\nof such tools to address its unique problems. On Titan, the largest moon of\nSaturn, tracking seasonal trends and weather patterns of clouds provides\ncrucial insights into one of the most complex climates in the Solar System, yet\nmuch of the available image data are still analyzed in a conventional way. In\nthis work, we apply a Mask R-CNN trained via transfer learning to perform\ninstance segmentation of clouds in Titan images acquired by the Cassini\nspacecraft - a previously unexplored approach to a big data problem in\nplanetary science. We demonstrate that an automated technique can provide\nquantitative measures for clouds, such as areas and centroids, that may\notherwise be prohibitively time-intensive to produce by human mapping.\nFurthermore, despite Titan specific challenges, our approach yields accuracy\ncomparable to contemporary cloud identification studies on Earth and other\nworlds. We compare the efficiencies of human-driven versus algorithmic\napproaches, showing that transfer learning provides speed-ups that may open new\nhorizons for data investigation for Titan. Moreover, we suggest that such\napproaches have broad potential for application to similar problems in\nplanetary science where they are currently under-utilized. Future planned\nmissions to the planets and remote sensing initiatives for the Earth promise to\nprovide a deluge of image data in the coming years that will benefit strongly\nfrom leveraging machine learning approaches to perform the analysis.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Segmentation",
      "Tracking"
    ]
  },
  {
    "title": "Generalization-Enhanced Few-Shot Object Detection in Remote Sensing",
    "url": "http://arxiv.org/abs/2501.02474v1",
    "authors": [
      "Hui Lin",
      "Nan Li",
      "Pengjuan Yao",
      "Kexin Dong",
      "Yuhan Guo",
      "Danfeng Hong",
      "Ying Zhang",
      "Congcong Wen"
    ],
    "published": "2025-01-05",
    "abstract": "Remote sensing object detection is particularly challenging due to the high\nresolution, multi-scale features, and diverse ground object characteristics\ninherent in satellite and UAV imagery. These challenges necessitate more\nadvanced approaches for effective object detection in such environments. While\ndeep learning methods have achieved remarkable success in remote sensing object\ndetection, they typically rely on large amounts of labeled data. Acquiring\nsufficient labeled data, particularly for novel or rare objects, is both\nchallenging and time-consuming in remote sensing scenarios, limiting the\ngeneralization capabilities of existing models. To address these challenges,\nfew-shot learning (FSL) has emerged as a promising approach, aiming to enable\nmodels to learn new classes from limited labeled examples. Building on this\nconcept, few-shot object detection (FSOD) specifically targets object detection\nchallenges in data-limited conditions. However, the generalization capability\nof FSOD models, particularly in remote sensing, is often constrained by the\ncomplex and diverse characteristics of the objects present in such\nenvironments. In this paper, we propose the Generalization-Enhanced Few-Shot\nObject Detection (GE-FSOD) model to improve the generalization capability in\nremote sensing FSOD tasks. Our model introduces three key innovations: the\nCross-Level Fusion Pyramid Attention Network (CFPAN) for enhanced multi-scale\nfeature representation, the Multi-Stage Refinement Region Proposal Network\n(MRRPN) for more accurate region proposals, and the Generalized Classification\nLoss (GCL) for improved classification performance in few-shot scenarios.\nExtensive experiments on the DIOR and NWPU VHR-10 datasets show that our model\nachieves state-of-the-art performance for few-shot object detection in remote\nsensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Deep learning selection of analogues for Mars landing sites in the Qaidam Basin, Qinghai-Tibet Plateau",
    "url": "http://arxiv.org/abs/2501.08584v1",
    "authors": [
      "Fanwei Meng",
      "Xiaopeng Wang",
      "Andr\u00e9 Antunes",
      "Jie Zhao",
      "Guoliang Zhou",
      "Biqiong Wu",
      "Tianqi Hao"
    ],
    "published": "2024-12-31",
    "abstract": "Remote sensing observations and Mars rover missions have recorded the\npresence of beaches, salt lakes, and wind erosion landforms in Martian\nsediments. All these observations indicate that Mars was hydrated in its early\nhistory. There used to be oceans on Mars, but they have now dried up.\nTherefore, signs of previous life on Mars could be preserved in the evaporites\nformed during this process. The study of evaporite regions has thus become a\npriority area for Mars' life exploration. This study proposes a method for\ntraining similarity metrics from surface land image data of Earth and Mars,\nwhich can be used for recognition or validation applications. The method will\nbe applied in simulating tasks to select Mars landing sites using a selecting\nsmall-scale area of the Mars analaogue the evaporite region of Qaidam Basin,\nQinghai-Tibet Plateau. This learning process minimizes discriminative loss\nfunction, which makes the similarity measure smaller for images from the same\nlocation and larger for images from different locations. This study selected a\nConvolutional Neural Networks (CNN) based model, which has been trained to\nexplain various changes in image appearance and identify different landforms in\nMars. By identifying different landforms, priority landing sites on Mars can be\nselected.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation",
    "url": "http://arxiv.org/abs/2412.19492v1",
    "authors": [
      "Chengyang Ye",
      "Yunzhi Zhuge",
      "Pingping Zhang"
    ],
    "published": "2024-12-27",
    "abstract": "Recently, deep learning based methods have revolutionized remote sensing\nimage segmentation. However, these methods usually rely on a pre-defined\nsemantic class set, thus needing additional image annotation and model training\nwhen adapting to new classes. More importantly, they are unable to segment\narbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote\nSensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary\nsemantic classes in remote sensing images. To address the lack of OVRSISS\ndatasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images\ncovering 40 diverse semantic classes. In addition, we propose a novel framework\nnamed GSNet that integrates domain priors from special remote sensing models\nand versatile capabilities of general vision-language models. Technically,\nGSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature\nFusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE\nfirst captures comprehensive features from both special models and general\nmodels in dual streams. Then, with the guidance of variable vocabularies, QGFF\nintegrates specialist and generalist features, enabling them to complement each\nother. Finally, RIPD is proposed to aggregate multi-source features for more\naccurate mask predictions. Experiments show that our method outperforms other\nmethods by a large margin, and our proposed LandDiscover50K improves the\nperformance of OVRSISS methods. The proposed dataset and method will be made\npublicly available at https://github.com/yecy749/GSNet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Mask Approximation Net: A Novel Diffusion Model Approach for Remote Sensing Change Captioning",
    "url": "http://arxiv.org/abs/2412.19179v2",
    "authors": [
      "Dongwei Sun",
      "Jing Yao",
      "Changsheng Zhou",
      "Xiangyong Cao",
      "Pedram Ghamisi"
    ],
    "published": "2024-12-26",
    "abstract": "Remote sensing image change description represents an innovative multimodal\ntask within the realm of remote sensing processing. This task not only\nfacilitates the detection of alterations in surface conditions, but also\nprovides comprehensive descriptions of these changes, thereby improving human\ninterpretability and interactivity.Generally, existing deep-learning-based\nmethods predominantly utilized a three-stage framework that successively\nperform feature extraction, feature fusion, and localization from bitemporal\nimages before text generation. However, this reliance often leads to an\nexcessive focus on the design of specific network architectures and restricts\nthe feature distributions to the dataset at hand, which in turn results in\nlimited generalizability and robustness during application.To address these\nlimitations, this paper proposes a novel approach for remote sensing image\nchange detection and description that incorporates diffusion models, aiming to\ntransition the emphasis of modeling paradigms from conventional feature\nlearning to data distribution learning. The proposed method primarily includes\na simple multi-scale change detection module, whose output features are\nsubsequently refined by an well-designed diffusion model. Furthermore, we\nintroduce a frequency-guided complex filter module to boost the model\nperformance by managing high-frequency noise throughout the diffusion process.\nWe validate the effectiveness of our proposed method across several datasets\nfor remote sensing change detection and description, showcasing its superior\nperformance compared to existing techniques. The code will be available at\n\\href{https://github.com/sundongwei}{MaskApproxNet} after a possible\npublication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Fusion of Deep Learning and GIS for Advanced Remote Sensing Image Analysis",
    "url": "http://arxiv.org/abs/2412.19856v1",
    "authors": [
      "Sajjad Afroosheh",
      "Mohammadreza Askari"
    ],
    "published": "2024-12-25",
    "abstract": "This paper presents an innovative framework for remote sensing image analysis\nby fusing deep learning techniques, specifically Convolutional Neural Networks\n(CNNs) and Long Short-Term Memory (LSTM) networks, with Geographic Information\nSystems (GIS). The primary objective is to enhance the accuracy and efficiency\nof spatial data analysis by overcoming challenges associated with high\ndimensionality, complex patterns, and temporal data processing. We implemented\noptimization algorithms, namely Particle Swarm Optimization (PSO) and Genetic\nAlgorithms (GA), to fine-tune model parameters, resulting in improved\nperformance metrics. Our findings reveal a significant increase in\nclassification accuracy from 78% to 92% and a reduction in prediction error\nfrom 12% to 6% after optimization. Additionally, the temporal accuracy of the\nmodels improved from 75% to 88%, showcasing the frameworks capability to\nmonitor dynamic changes effectively. The integration of GIS not only enriched\nthe spatial analysis but also facilitated a deeper understanding of the\nrelationships between geographical features. This research demonstrates that\ncombining advanced deep learning methods with GIS and optimization strategies\ncan significantly advance remote sensing applications, paving the way for\nfuture developments in environmental monitoring, urban planning, and resource\nmanagement.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends",
    "url": "http://arxiv.org/abs/2412.16631v1",
    "authors": [
      "Sofiane Bouaziz",
      "Adel Hafiane",
      "Raphael Canals",
      "Rachid Nedjai"
    ],
    "published": "2024-12-21",
    "abstract": "The rapid advancements in satellite remote sensing have enhanced the\ncapability to monitor and analyze the Earth's surface. Among the many variables\ncaptured through satellite sensors, Land Surface Temperature (LST) plays a\ncritical role in understanding key environmental processes. However, obtaining\nhigh-resolution LST data remains a challenge, as satellite sensors often face a\ntrade-off between spatial and temporal resolutions. In response,\nSpatio-Temporal Fusion (STF) has emerged as a powerful method to integrate two\nsatellite data sources, one providing high spatial but low temporal resolution,\nand the other offering high temporal but low spatial resolution. Although a\nrange of STF techniques have been proposed, from traditional methods to\ncutting-edge deep learning (DL) models, most have focused on surface\nreflectance, with limited application to LST estimation. DL approaches, in\nparticular, show promise in improving the spatial and temporal resolutions of\nLST by capturing complex, non-linear relationships between input and output LST\ndata. This paper offers a comprehensive review of the latest advancements in\nDL-based STF techniques for LST estimation. We analyze key research\ndevelopments, mathematically formulate the STF problem, and introduce a novel\ntaxonomy for DL-based STF methods. Furthermore, we discuss the challenges faced\nby current methods and highlight future research directions. In addition, we\npresent the first open-source benchmark STF dataset for LST estimation,\nconsisting of 51 pairs of MODIS-Landsat images spanning from 2013 to 2024. To\nsupport our findings, we conduct extensive experiments on state-of-the-art\nmethods and present both quantitative and qualitative assessments. This is the\nfirst survey paper focused on DL-based STF for LST estimation. We hope it\nserves as a valuable reference for researchers and paves the way for future\nresearch in this field.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Segmentation of arbitrary features in very high resolution remote sensing imagery",
    "url": "http://arxiv.org/abs/2412.16046v1",
    "authors": [
      "Henry Cording",
      "Yves Plancherel",
      "Pablo Brito-Parada"
    ],
    "published": "2024-12-20",
    "abstract": "Very high resolution (VHR) mapping through remote sensing (RS) imagery\npresents a new opportunity to inform decision-making and sustainable practices\nin countless domains. Efficient processing of big VHR data requires automated\ntools applicable to numerous geographic regions and features. Contemporary RS\nstudies address this challenge by employing deep learning (DL) models for\nspecific datasets or features, which limits their applicability across\ncontexts.\n  The present research aims to overcome this limitation by introducing\nEcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.\nEcoMapper fully automates processing of geospatial data, DL model training, and\ninference. Models trained with EcoMapper successfully segmented two distinct\nfeatures in a real-world UAV dataset, achieving scores competitive with prior\nstudies which employed context-specific models.\n  To evaluate EcoMapper, many additional models were trained on permutations of\nprincipal field survey characteristics (FSCs). A relationship was discovered\nallowing derivation of optimal ground sampling distance from feature size,\ntermed Cording Index (CI). A comprehensive methodology for field surveys was\ndeveloped to ensure DL methods can be applied effectively to collected data.\n  The EcoMapper code accompanying this work is available at\nhttps://github.com/hcording/ecomapper .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DroughtSet: Understanding Drought Through Spatial-Temporal Learning",
    "url": "http://arxiv.org/abs/2412.15075v1",
    "authors": [
      "Xuwei Tan",
      "Qian Zhao",
      "Yanlan Liu",
      "Xueru Zhang"
    ],
    "published": "2024-12-19",
    "abstract": "Drought is one of the most destructive and expensive natural disasters,\nseverely impacting natural resources and risks by depleting water resources and\ndiminishing agricultural yields. Under climate change, accurately predicting\ndrought is critical for mitigating drought-induced risks. However, the\nintricate interplay among the physical and biological drivers that regulate\ndroughts limits the predictability and understanding of drought, particularly\nat a subseasonal to seasonal (S2S) time scale. While deep learning has been\ndemonstrated with potential in addressing climate forecasting challenges, its\napplication to drought prediction has received relatively less attention. In\nthis work, we propose a new dataset, DroughtSet, which integrates relevant\npredictive features and three drought indices from multiple remote sensing and\nreanalysis datasets across the contiguous United States (CONUS). DroughtSet\nspecifically provides the machine learning community with a new real-world\ndataset to benchmark drought prediction models and more generally, time-series\nforecasting methods. Furthermore, we propose a spatial-temporal model SPDrought\nto predict and interpret S2S droughts. Our model learns from the spatial and\ntemporal information of physical and biological features to predict three types\nof droughts simultaneously. Multiple strategies are employed to quantify the\nimportance of physical and biological features for drought prediction. Our\nresults provide insights for researchers to better understand the\npredictability and sensitivity of drought to biological and physical\nconditions. We aim to contribute to the climate field by proposing a new tool\nto predict and understand the occurrence of droughts and provide the AI\ncommunity with a new benchmark to study deep learning applications in climate\nscience.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "TS-SatFire: A Multi-Task Satellite Image Time-Series Dataset for Wildfire Detection and Prediction",
    "url": "http://arxiv.org/abs/2412.11555v1",
    "authors": [
      "Yu Zhao",
      "Sebastian Gerard",
      "Yifang Ban"
    ],
    "published": "2024-12-16",
    "abstract": "Wildfire monitoring and prediction are essential for understanding wildfire\nbehaviour. With extensive Earth observation data, these tasks can be integrated\nand enhanced through multi-task deep learning models. We present a\ncomprehensive multi-temporal remote sensing dataset for active fire detection,\ndaily wildfire monitoring, and next-day wildfire prediction. Covering wildfire\nevents in the contiguous U.S. from January 2017 to October 2021, the dataset\nincludes 3552 surface reflectance images and auxiliary data such as weather,\ntopography, land cover, and fuel information, totalling 71 GB. The lifecycle of\neach wildfire is documented, with labels for active fires (AF) and burned areas\n(BA), supported by manual quality assurance of AF and BA test labels. The\ndataset supports three tasks: a) active fire detection, b) daily burned area\nmapping, and c) wildfire progression prediction. Detection tasks use pixel-wise\nclassification of multi-spectral, multi-temporal images, while prediction tasks\nintegrate satellite and auxiliary data to model fire dynamics. This dataset and\nits benchmarks provide a foundation for advancing wildfire research using deep\nlearning.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Deep Random Features for Scalable Interpolation of Spatiotemporal Data",
    "url": "http://arxiv.org/abs/2412.11350v1",
    "authors": [
      "Weibin Chen",
      "Azhir Mahmood",
      "Michel Tsamados",
      "So Takao"
    ],
    "published": "2024-12-16",
    "abstract": "The rapid growth of earth observation systems calls for a scalable approach\nto interpolate remote-sensing observations. These methods in principle, should\nacquire more information about the observed field as data grows. Gaussian\nprocesses (GPs) are candidate model choices for interpolation. However, due to\ntheir poor scalability, they usually rely on inducing points for inference,\nwhich restricts their expressivity. Moreover, commonly imposed assumptions such\nas stationarity prevents them from capturing complex patterns in the data.\nWhile deep GPs can overcome this issue, training and making inference with them\nare difficult, again requiring crude approximations via inducing points. In\nthis work, we instead approach the problem through Bayesian deep learning,\nwhere spatiotemporal fields are represented by deep neural networks, whose\nlayers share the inductive bias of stationary GPs on the plane/sphere via\nrandom feature expansions. This allows one to (1) capture high frequency\npatterns in the data, and (2) use mini-batched gradient descent for large scale\ntraining. We experiment on various remote sensing data at local/global scales,\nshowing that our approach produce competitive or superior results to existing\nmethods, with well-calibrated uncertainties.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": []
  },
  {
    "title": "Super-Resolution for Remote Sensing Imagery via the Coupling of a Variational Model and Deep Learning",
    "url": "http://arxiv.org/abs/2412.09841v1",
    "authors": [
      "Jing Sun",
      "Huanfeng Shen",
      "Qiangqiang Yuan",
      "Liangpei Zhang"
    ],
    "published": "2024-12-13",
    "abstract": "Image super-resolution (SR) is an effective way to enhance the spatial\nresolution and detail information of remote sensing images, to obtain a\nsuperior visual quality. As SR is severely ill-conditioned, effective image\npriors are necessary to regularize the solution space and generate the\ncorresponding high-resolution (HR) image. In this paper, we propose a novel\ngradient-guided multi-frame super-resolution (MFSR) framework for remote\nsensing imagery reconstruction. The framework integrates a learned gradient\nprior as the regularization term into a model-based optimization method.\nSpecifically, the local gradient regularization (LGR) prior is derived from the\ndeep residual attention network (DRAN) through gradient profile transformation.\nThe non-local total variation (NLTV) prior is characterized using the spatial\nstructure similarity of the gradient patches with the maximum a posteriori\n(MAP) model. The modeled prior performs well in preserving edge smoothness and\nsuppressing visual artifacts, while the learned prior is effective in enhancing\nsharp edges and recovering fine structures. By incorporating the two\ncomplementary priors into an adaptive norm based reconstruction framework, the\nmixed L1 and L2 regularization minimization problem is optimized to achieve the\nrequired HR remote sensing image. Extensive experimental results on remote\nsensing data demonstrate that the proposed method can produce visually pleasant\nimages and is superior to several of the state-of-the-art SR algorithms in\nterms of the quantitative evaluation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "CrossVIT-augmented Geospatial-Intelligence Visualization System for Tracking Economic Development Dynamics",
    "url": "http://arxiv.org/abs/2412.10474v1",
    "authors": [
      "Yanbing Bai",
      "Jinhua Su",
      "Bin Qiao",
      "Xiaoran Ma"
    ],
    "published": "2024-12-13",
    "abstract": "Timely and accurate economic data is crucial for effective policymaking.\nCurrent challenges in data timeliness and spatial resolution can be addressed\nwith advancements in multimodal sensing and distributed computing. We introduce\nSenseconomic, a scalable system for tracking economic dynamics via multimodal\nimagery and deep learning. Built on the Transformer framework, it integrates\nremote sensing and street view images using cross-attention, with nighttime\nlight data as weak supervision. The system achieved an R-squared value of\n0.8363 in county-level economic predictions and halved processing time to 23\nminutes using distributed computing. Its user-friendly design includes a\nVue3-based front end with Baidu maps for visualization and a Python-based back\nend automating tasks like image downloads and preprocessing. Senseconomic\nempowers policymakers and researchers with efficient tools for resource\nallocation and economic planning.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "A Progressive Image Restoration Network for High-order Degradation Imaging in Remote Sensing",
    "url": "http://arxiv.org/abs/2412.07195v1",
    "authors": [
      "Yujie Feng",
      "Yin Yang",
      "Xiaohong Fan",
      "Zhengpeng Zhang",
      "Lijing Bu",
      "Jianping Zhang"
    ],
    "published": "2024-12-10",
    "abstract": "Recently, deep learning methods have gained remarkable achievements in the\nfield of image restoration for remote sensing (RS). However, most existing RS\nimage restoration methods focus mainly on conventional first-order degradation\nmodels, which may not effectively capture the imaging mechanisms of remote\nsensing images. Furthermore, many RS image restoration approaches that use deep\nlearning are often criticized for their lacks of architecture transparency and\nmodel interpretability. To address these problems, we propose a novel\nprogressive restoration network for high-order degradation imaging (HDI-PRNet),\nto progressively restore different image degradation. HDI-PRNet is developed\nbased on the theoretical framework of degradation imaging, offering the benefit\nof mathematical interpretability within the unfolding network. The framework is\ncomposed of three main components: a module for image denoising that relies on\nproximal mapping prior learning, a module for image deblurring that integrates\nNeumann series expansion with dual-domain degradation learning, and a module\nfor super-resolution. Extensive experiments demonstrate that our method\nachieves superior performance on both synthetic and real remote sensing images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Comprehensive Analysis and Improvements in Pansharpening Using Deep Learning",
    "url": "http://arxiv.org/abs/2412.04896v1",
    "authors": [
      "Mahek Kantharia",
      "Neeraj Badal",
      "Zankhana Shah"
    ],
    "published": "2024-12-06",
    "abstract": "Pansharpening is a crucial task in remote sensing, enabling the generation of\nhigh-resolution multispectral images by fusing low-resolution multispectral\ndata with high-resolution panchromatic images. This paper provides a\ncomprehensive analysis of traditional and deep learning-based pansharpening\nmethods. While state-of-the-art deep learning methods have significantly\nimproved image quality, issues like spectral distortions persist. To address\nthis, we propose enhancements to the PSGAN framework by introducing novel\nregularization techniques for the generator loss function. Experimental results\non images from the Worldview-3 dataset demonstrate that the proposed\nmodifications improve spectral fidelity and achieve superior performance across\nmultiple quantitative metrics while delivering visually superior results.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Dual-Branch Subpixel-Guided Network for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2412.03893v1",
    "authors": [
      "Zhu Han",
      "Jin Yang",
      "Lianru Gao",
      "Zhiqiang Zeng",
      "Bing Zhang",
      "Jocelyn Chanussot"
    ],
    "published": "2024-12-05",
    "abstract": "Deep learning (DL) has been widely applied into hyperspectral image (HSI)\nclassification owing to its promising feature learning and representation\ncapabilities. However, limited by the spatial resolution of sensors, existing\nDL-based classification approaches mainly focus on pixel-level spectral and\nspatial information extraction through complex network architecture design,\nwhile ignoring the existence of mixed pixels in actual scenarios. To tackle\nthis difficulty, we propose a novel dual-branch subpixel-guided network for HSI\nclassification, called DSNet, which automatically integrates subpixel\ninformation and convolutional class features by introducing a deep autoencoder\nunmixing architecture to enhance classification performance. DSNet is capable\nof fully considering physically nonlinear properties within subpixels and\nadaptively generating diagnostic abundances in an unsupervised manner to\nachieve more reliable decision boundaries for class label distributions. The\nsubpixel fusion module is designed to ensure high-quality information fusion\nacross pixel and subpixel features, further promoting stable joint\nclassification. Experimental results on three benchmark datasets demonstrate\nthe effectiveness and superiority of DSNet compared with state-of-the-art\nDL-based HSI classification approaches. The codes will be available at\nhttps://github.com/hanzhu97702/DSNet, contributing to the remote sensing\ncommunity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion",
    "url": "http://arxiv.org/abs/2412.03413v1",
    "authors": [
      "Andrea Asperti",
      "Ali Aydogdu",
      "Emanuela Clementi",
      "Angelo Greco",
      "Lorenzo Mentaschi",
      "Fabio Merizzi",
      "Pietro Miraglio",
      "Paolo Oddo",
      "Nadia Pinardi",
      "Alessandro Testa"
    ],
    "published": "2024-12-04",
    "abstract": "Sea Surface Temperature (SST) is crucial for understanding Earth's oceans and\nclimate, significantly influencing weather patterns, ocean currents, marine\necosystem health, and the global energy balance. Large-scale SST monitoring\nrelies on satellite infrared radiation detection, but cloud cover presents a\nmajor challenge, creating extensive observational gaps and hampering our\nability to fully capture large-scale ocean temperature patterns. Efforts to\naddress these gaps in existing L4 datasets have been made, but they often\nexhibit notable local and seasonal biases, compromising data reliability and\naccuracy. To tackle this challenge, we employed deep neural networks to\nreconstruct cloud-covered portions of satellite imagery while preserving the\nintegrity of observed values in cloud-free areas, using MODIS satellite derived\nobservations of SST. Our best-performing architecture showed significant skill\nimprovements over established methodologies, achieving substantial reductions\nin error metrics when benchmarked against widely used approaches and datasets.\nThese results underscore the potential of advanced AI techniques to enhance the\ncompleteness of satellite observations in Earth-science remote sensing,\nproviding more accurate and reliable datasets for environmental assessments,\ndata-driven model training, climate research, and seamless integration into\nmodel data assimilation workflows.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Fire-Image-DenseNet (FIDN) for predicting wildfire burnt area using remote sensing data",
    "url": "http://arxiv.org/abs/2412.01400v1",
    "authors": [
      "Bo Pang",
      "Sibo Cheng",
      "Yuhan Huang",
      "Yufang Jin",
      "Yike Guo",
      "I. Colin Prentice",
      "Sandy P. Harrison",
      "Rossella Arcucci"
    ],
    "published": "2024-12-02",
    "abstract": "Predicting the extent of massive wildfires once ignited is essential to\nreduce the subsequent socioeconomic losses and environmental damage, but\nchallenging because of the complexity of fire behaviour. Existing physics-based\nmodels are limited in predicting large or long-duration wildfire events. Here,\nwe develop a deep-learning-based predictive model, Fire-Image-DenseNet (FIDN),\nthat uses spatial features derived from both near real-time and reanalysis data\non the environmental and meteorological drivers of wildfire. We trained and\ntested this model using more than 300 individual wildfires that occurred\nbetween 2012 and 2019 in the western US. In contrast to existing models, the\nperformance of FIDN does not degrade with fire size or duration. Furthermore,\nit predicts final burnt area accurately even in very heterogeneous landscapes\nin terms of fuel density and flammability. The FIDN model showed higher\naccuracy, with a mean squared error (MSE) about 82% and 67% lower than those of\nthe predictive models based on cellular automata (CA) and the minimum travel\ntime (MTT) approaches, respectively. Its structural similarity index measure\n(SSIM) averages 97%, outperforming the CA and FlamMap MTT models by 6% and 2%,\nrespectively. Additionally, FIDN is approximately three orders of magnitude\nfaster than both CA and MTT models. The enhanced computational efficiency and\naccuracy advancements offer vital insights for strategic planning and resource\nallocation for firefighting operations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Tomographic SAR Reconstruction for Forest Height Estimation",
    "url": "http://arxiv.org/abs/2412.00903v2",
    "authors": [
      "Grace Colverd",
      "Jumpei Takami",
      "Laura Schade",
      "Karol Bot",
      "Joseph A. Gallego-Mejia"
    ],
    "published": "2024-12-01",
    "abstract": "Tree height estimation serves as an important proxy for biomass estimation in\necological and forestry applications. While traditional methods such as\nphotogrammetry and Light Detection and Ranging (LiDAR) offer accurate height\nmeasurements, their application on a global scale is often cost-prohibitive and\nlogistically challenging. In contrast, remote sensing techniques, particularly\n3D tomographic reconstruction from Synthetic Aperture Radar (SAR) imagery,\nprovide a scalable solution for global height estimation. SAR images have been\nused in earth observation contexts due to their ability to work in all\nweathers, unobscured by clouds. In this study, we use deep learning to estimate\nforest canopy height directly from 2D Single Look Complex (SLC) images, a\nderivative of SAR. Our method attempts to bypass traditional tomographic signal\nprocessing, potentially reducing latency from SAR capture to end product. We\nalso quantify the impact of varying numbers of SLC images on height estimation\naccuracy, aiming to inform future satellite operations and optimize data\ncollection strategies. Compared to full tomographic processing combined with\ndeep learning, our minimal method (partial processing + deep learning) falls\nshort, with an error 16-21\\% higher, highlighting the continuing relevance of\ngeometric signal processing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Gated-Attention Feature-Fusion Based Framework for Poverty Prediction",
    "url": "http://arxiv.org/abs/2411.19690v1",
    "authors": [
      "Muhammad Umer Ramzan",
      "Wahab Khaddim",
      "Muhammad Ehsan Rana",
      "Usman Ali",
      "Manohar Ali",
      "Fiaz ul Hassan",
      "Fatima Mehmood"
    ],
    "published": "2024-11-29",
    "abstract": "This research paper addresses the significant challenge of accurately\nestimating poverty levels using deep learning, particularly in developing\nregions where traditional methods like household surveys are often costly,\ninfrequent, and quickly become outdated. To address these issues, we propose a\nstate-of-the-art Convolutional Neural Network (CNN) architecture, extending the\nResNet50 model by incorporating a Gated-Attention Feature-Fusion Module (GAFM).\nOur architecture is designed to improve the model's ability to capture and\ncombine both global and local features from satellite images, leading to more\naccurate poverty estimates. The model achieves a 75% R2 score, significantly\noutperforming existing leading methods in poverty mapping. This improvement is\ndue to the model's capacity to focus on and refine the most relevant features,\nfiltering out unnecessary data, which makes it a powerful tool for remote\nsensing and poverty estimation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "Weakly Supervised Framework Considering Multi-temporal Information for Large-scale Cropland Mapping with Satellite Imagery",
    "url": "http://arxiv.org/abs/2411.18475v1",
    "authors": [
      "Yuze Wang",
      "Aoran Hu",
      "Ji Qi",
      "Yang Liu",
      "Chao Tao"
    ],
    "published": "2024-11-27",
    "abstract": "Accurately mapping large-scale cropland is crucial for agricultural\nproduction management and planning. Currently, the combination of remote\nsensing data and deep learning techniques has shown outstanding performance in\ncropland mapping. However, those approaches require massive precise labels,\nwhich are labor-intensive. To reduce the label cost, this study presented a\nweakly supervised framework considering multi-temporal information for\nlarge-scale cropland mapping. Specifically, we extract high-quality labels\naccording to their consistency among global land cover (GLC) products to\nconstruct the supervised learning signal. On the one hand, to alleviate the\noverfitting problem caused by the model's over-trust of remaining errors in\nhigh-quality labels, we encode the similarity/aggregation of cropland in the\nvisual/spatial domain to construct the unsupervised learning signal, and take\nit as the regularization term to constrain the supervised part. On the other\nhand, to sufficiently leverage the plentiful information in the samples without\nhigh-quality labels, we also incorporate the unsupervised learning signal in\nthese samples, enriching the diversity of the feature space. After that, to\ncapture the phenological features of croplands, we introduce dense satellite\nimage time series (SITS) to extend the proposed framework in the temporal\ndimension. We also visualized the high dimensional phenological features to\nuncover how multi-temporal information benefits cropland extraction, and\nassessed the method's robustness under conditions of data scarcity. The\nproposed framework has been experimentally validated for strong adaptability\nacross three study areas (Hunan Province, Southeast France, and Kansas) in\nlarge-scale cropland mapping, and the internal mechanism and temporal\ngeneralizability are also investigated.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "\u03a9SFormer: Dual-Modal \u03a9-like Super-Resolution Transformer Network for Cross-scale and High-accuracy Terraced Field Vectorization Extraction",
    "url": "http://arxiv.org/abs/2411.17088v1",
    "authors": [
      "Chang Li",
      "Yu Wang",
      "Ce Zhang",
      "Yongjun Zhang"
    ],
    "published": "2024-11-26",
    "abstract": "Terraced field is a significant engineering practice for soil and water\nconservation (SWC). Terraced field extraction from remotely sensed imagery is\nthe foundation for monitoring and evaluating SWC. This study is the first to\npropose a novel dual-modal {\\Omega}-like super-resolution Transformer network\nfor intelligent TFVE, offering the following advantages: (1) reducing edge\nsegmentation error from conventional multi-scale downsampling encoder, through\nfusing original high-resolution features with downsampling features at each\nstep of encoder and leveraging a multi-head attention mechanism; (2) improving\nthe accuracy of TFVE by proposing a {\\Omega}-like network structure, which\nfully integrates rich high-level features from both spectral and terrain data\nto form cross-scale super-resolution features; (3) validating an optimal fusion\nscheme for cross-modal and cross-scale (i.e., inconsistent spatial resolution\nbetween remotely sensed imagery and DEM) super-resolution feature extraction;\n(4) mitigating uncertainty between segmentation edge pixels by a coarse-to-fine\nand spatial topological semantic relationship optimization (STSRO) segmentation\nstrategy; (5) leveraging contour vibration neural network to continuously\noptimize parameters and iteratively vectorize terraced fields from semantic\nsegmentation results. Moreover, a DMRVD for deep-learning-based TFVE was\ncreated for the first time, which covers nine study areas in four provinces of\nChina, with a total coverage area of 22441 square kilometers. To assess the\nperformance of {\\Omega}SFormer, classic and SOTA networks were compared. The\nmIOU of {\\Omega}SFormer has improved by 0.165, 0.297 and 0.128 respectively,\nwhen compared with best accuracy single-modal remotely sensed imagery,\nsingle-modal DEM and dual-modal result.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Super-Resolution"
    ]
  },
  {
    "title": "CMAViT: Integrating Climate, Managment, and Remote Sensing Data for Crop Yield Estimation with Multimodel Vision Transformers",
    "url": "http://arxiv.org/abs/2411.16989v1",
    "authors": [
      "Hamid Kamangir",
      "Brent. S. Sams",
      "Nick Dokoozlian",
      "Luis Sanchez",
      "J. Mason. Earles"
    ],
    "published": "2024-11-25",
    "abstract": "Crop yield prediction is essential for agricultural planning but remains\nchallenging due to the complex interactions between weather, climate, and\nmanagement practices. To address these challenges, we introduce a deep\nlearning-based multi-model called Climate-Management Aware Vision Transformer\n(CMAViT), designed for pixel-level vineyard yield predictions. CMAViT\nintegrates both spatial and temporal data by leveraging remote sensing imagery\nand short-term meteorological data, capturing the effects of growing season\nvariations. Additionally, it incorporates management practices, which are\nrepresented in text form, using a cross-attention encoder to model their\ninteraction with time-series data. This innovative multi-modal transformer\ntested on a large dataset from 2016-2019 covering 2,200 hectares and eight\ngrape cultivars including more than 5 million vines, outperforms traditional\nmodels like UNet-ConvLSTM, excelling in spatial variability capture and yield\nprediction, particularly for extreme values in vineyards. CMAViT achieved an R2\nof 0.84 and a MAPE of 8.22% on an unseen test dataset. Masking specific\nmodalities lowered performance: excluding management practices, climate data,\nand both reduced R2 to 0.73, 0.70, and 0.72, respectively, and raised MAPE to\n11.92%, 12.66%, and 12.39%, highlighting each modality's importance for\naccurate yield prediction. Code is available at\nhttps://github.com/plant-ai-biophysics-lab/CMAViT.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Resolution-Adaptive Micro-Doppler Spectrogram for Human Activity Recognition",
    "url": "http://arxiv.org/abs/2411.15057v2",
    "authors": [
      "Do-Hyun Park",
      "Min-Wook Jeon",
      "Hyoung-Nam Kim"
    ],
    "published": "2024-11-22",
    "abstract": "The rising demand for remote-sensing systems for detecting hazardous\nsituations has led to increased interest in radar-based human activity\nrecognition (HAR). Conventional radar-based HAR methods predominantly rely on\nmicro-Doppler spectrograms for recognition tasks. However, spectrograms\nfrequently fail to effectively capture micro-Doppler signatures because of\ntheir limited linear resolution. To address this limitation, we propose a\ntime--frequency domain representation method that adaptively adjusts the\nresolution based on activity characteristics. This approach nonlinearly\ntransforms the resolution to focus on the most relevant frequency range for\nmicro-Doppler signatures. We validate the proposed method by training\ndeep-learning-based HAR models on datasets generated using the adaptive\nrepresentation method. Experimental results demonstrate that the models trained\nusing the proposed method achieve superior recognition accuracy compared with\nthose trained using conventional methods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Attentive Contextual Attention for Cloud Removal",
    "url": "http://arxiv.org/abs/2411.13042v1",
    "authors": [
      "Wenli Huang",
      "Ye Deng",
      "Yang Wu",
      "Jinjun Wang"
    ],
    "published": "2024-11-20",
    "abstract": "Cloud cover can significantly hinder the use of remote sensing images for\nEarth observation, prompting urgent advancements in cloud removal technology.\nRecently, deep learning strategies have shown strong potential in restoring\ncloud-obscured areas. These methods utilize convolution to extract intricate\nlocal features and attention mechanisms to gather long-range information,\nimproving the overall comprehension of the scene. However, a common drawback of\nthese approaches is that the resulting images often suffer from blurriness,\nartifacts, and inconsistencies. This is partly because attention mechanisms\napply weights to all features based on generalized similarity scores, which can\ninadvertently introduce noise and irrelevant details from cloud-covered areas.\nTo overcome this limitation and better capture relevant distant context, we\nintroduce a novel approach named Attentive Contextual Attention (AC-Attention).\nThis method enhances conventional attention mechanisms by dynamically learning\ndata-driven attentive selection scores, enabling it to filter out noise and\nirrelevant features effectively. By integrating the AC-Attention module into\nthe DSen2-CR cloud removal framework, we significantly improve the model's\nability to capture essential distant information, leading to more effective\ncloud removal. Our extensive evaluation of various datasets shows that our\nmethod outperforms existing ones regarding image reconstruction quality.\nAdditionally, we conducted ablation studies by integrating AC-Attention into\nmultiple existing methods and widely used network architectures. These studies\ndemonstrate the effectiveness and adaptability of AC-Attention and reveal its\nability to focus on relevant features, thereby improving the overall\nperformance of the networks. The code is available at\n\\url{https://github.com/huangwenwenlili/ACA-CRNet}.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Cuvis.Ai: An Open-Source, Low-Code Software Ecosystem for Hyperspectral Processing and Classification",
    "url": "http://arxiv.org/abs/2411.11324v1",
    "authors": [
      "Nathaniel Hanson",
      "Philip Manke",
      "Simon Birkholz",
      "Maximilian M\u00fchlbauer",
      "Rene Heine",
      "Arnd Brandes"
    ],
    "published": "2024-11-18",
    "abstract": "Machine learning is an important tool for analyzing high-dimension\nhyperspectral data; however, existing software solutions are either\nclosed-source or inextensible research products. In this paper, we present\ncuvis.ai, an open-source and low-code software ecosystem for data acquisition,\npreprocessing, and model training. The package is written in Python and\nprovides wrappers around common machine learning libraries, allowing both\nclassical and deep learning models to be trained on hyperspectral data. The\ncodebase abstracts processing interconnections and data dependencies between\noperations to minimize code complexity for users. This software package\ninstantiates nodes in a directed acyclic graph to handle all stages of a\nmachine learning ecosystem, from data acquisition, including live or static\ndata sources, to final class assignment or property prediction. User-created\nmodels contain convenient serialization methods to ensure portability and\nincrease sharing within the research community. All code and data are available\nonline: https://github.com/cubert-hyperspectral/cuvis.ai",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Program Evaluation with Remotely Sensed Outcomes",
    "url": "http://arxiv.org/abs/2411.10959v2",
    "authors": [
      "Ashesh Rambachan",
      "Rahul Singh",
      "Davide Viviano"
    ],
    "published": "2024-11-17",
    "abstract": "Economists often estimate treatment effects in experiments using remotely\nsensed variables (RSVs), e.g. satellite images or mobile phone activity, in\nplace of directly measured economic outcomes. A common practice is to use an\nobservational sample to train a predictor of the economic outcome from the RSV,\nand then to use its predictions as the outcomes in the experiment. We show that\nthis method is biased whenever the RSV is post-outcome, i.e. if variation in\nthe economic outcome causes variation in the RSV. In program evaluation,\nchanges in poverty or environmental quality cause changes in satellite images,\nbut not vice versa. As our main result, we nonparametrically identify the\ntreatment effect by formalizing the intuition that underlies common practice:\nthe conditional distribution of the RSV given the outcome and treatment is\nstable across the samples.Based on our identifying formula, we find that the\nefficient representation of RSVs for causal inference requires three\npredictions rather than one. Valid inference does not require any rate\nconditions on RSV predictions, justifying the use of complex deep learning\nalgorithms with unknown statistical properties. We re-analyze the effect of an\nanti-poverty program in India using satellite images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Slender Object Scene Segmentation in Remote Sensing Image Based on Learnable Morphological Skeleton with Segment Anything Model",
    "url": "http://arxiv.org/abs/2411.08592v2",
    "authors": [
      "Jun Xie",
      "Wenxiao Li",
      "Faqiang Wang",
      "Liqiang Zhang",
      "Zhengyang Hou",
      "Jun Liu"
    ],
    "published": "2024-11-13",
    "abstract": "Morphological methods play a crucial role in remote sensing image processing,\ndue to their ability to capture and preserve small structural details. However,\nmost of the existing deep learning models for semantic segmentation are based\non the encoder-decoder architecture including U-net and Segment Anything Model\n(SAM), where the downsampling process tends to discard fine details. In this\npaper, we propose a new approach that integrates learnable morphological\nskeleton prior into deep neural networks using the variational method. To\naddress the difficulty in backpropagation in neural networks caused by the\nnon-differentiability presented in classical morphological operations, we\nprovide a smooth representation of the morphological skeleton and design a\nvariational segmentation model integrating morphological skeleton prior by\nemploying operator splitting and dual methods. Then, we integrate this model\ninto the network architecture of SAM, which is achieved by adding a token to\nmask decoder and modifying the final sigmoid layer, ensuring the final\nsegmentation results preserve the skeleton structure as much as possible.\nExperimental results on remote sensing datasets, including buildings, roads and\nwater, demonstrate that our method outperforms the original SAM on slender\nobject segmentation and exhibits better generalization capability.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "Deep Neural Network"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal",
    "url": "http://arxiv.org/abs/2504.20620v1",
    "authors": [
      "Abhishek Pasula",
      "Deepak N. Subramani"
    ],
    "published": "2025-04-29",
    "abstract": "Climate change alters ocean conditions, notably temperature and sea level. In\nthe Bay of Bengal, these changes influence monsoon precipitation and marine\nproductivity, critical to the Indian economy. In Phase 6 of the Coupled Model\nIntercomparison Project (CMIP6), Global Climate Models (GCMs) use different\nshared socioeconomic pathways (SSPs) to obtain future climate projections.\nHowever, significant discrepancies are observed between these models and the\nreanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean\nsquare error (RMSE) between the climate model output and the Ocean Reanalysis\nSystem (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the\ndynamic sea level (DSL). We introduce a new data-driven deep learning model to\ncorrect for this bias. The deep neural model for each variable is trained using\npairs of climatology-removed monthly climate projections as input and the\ncorresponding month's ORAS5 as output. This model is trained with historical\ndata (1950 to 2014), validated with future projection data from 2015 to 2020,\nand tested with future projections from 2021 to 2023. Compared to the\nconventional EquiDistant Cumulative Distribution Function (EDCDF) statistical\nmethod for bias correction in climate models, our approach decreases RMSE by\n0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the\nprojections for 2024-2100. A detailed analysis of the monthly, seasonal, and\ndecadal means and variability is performed to underscore the implications of\nthe novel dynamics uncovered in our corrected projections.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Mj\u00f6lnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density",
    "url": "http://arxiv.org/abs/2504.19822v1",
    "authors": [
      "Minjong Cheon"
    ],
    "published": "2025-04-28",
    "abstract": "Recent advances in AI-based weather forecasting models, such as FourCastNet,\nPangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep\nlearning to emulate complex atmospheric dynamics. Building on this momentum, we\npropose Mj\\\"olnir, a novel deep learning-based framework for global lightning\nflash density parameterization. Trained on ERA5 atmospheric predictors and\nWorld Wide Lightning Location Network (WWLLN) observations at a daily temporal\nresolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear\nmapping between large-scale environmental conditions and lightning activity.\nThe model architecture is based on the InceptionNeXt backbone with SENet, and a\nmulti-task learning strategy to simultaneously predict lightning occurrence and\nmagnitude. Extensive evaluations yield that Mollnir accurately reproduces the\nglobal distribution, seasonal variability, and regional characteristics of\nlightning activity, achieving a global Pearson correlation coefficient of 0.96\nfor annual mean fields. These results suggest that Mj\\\"olnir serves not only as\nan effective data-driven global lightning parameterization but also as a\npromising AI-based scheme for next-generation Earth system models (AI-ESMs).",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Global Climate Model Bias Correction Using Deep Learning",
    "url": "http://arxiv.org/abs/2504.19145v1",
    "authors": [
      "Abhishek Pasula",
      "Deepak N. Subramani"
    ],
    "published": "2025-04-27",
    "abstract": "Climate change affects ocean temperature, salinity and sea level, impacting\nmonsoons and ocean productivity. Future projections by Global Climate Models\nbased on shared socioeconomic pathways from the Coupled Model Intercomparison\nProject (CMIP) are widely used to understand the effects of climate change.\nHowever, CMIP models have significant bias compared to reanalysis in the Bay of\nBengal for the time period when both projections and reanalysis are available.\nFor example, there is a 1.5C root mean square error (RMSE) in the sea surface\ntemperature (SST) projections of the climate model CNRM-CM6 compared to the\nOcean Reanalysis System (ORAS5). We develop a suite of data-driven deep\nlearning models for bias correction of climate model projections and apply it\nto correct SST projections of the Bay of Bengal. We propose the use of three\ndifferent deep neural network architectures: convolutional encoder-decoder\nUNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression\nmodel and the Equi-Distant Cumulative Density Function (EDCDF) bias correction\nmethod for comparison and evaluating the impact of the new deep learning\nmodels. All bias correction models are trained using pairs of monthly CMIP6\nprojections and the corresponding month's ORAS5 as input and output. Historical\ndata (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used\nfor training and validation, including hyperparameter tuning. Testing is\nperformed on future projection data from 2021 to 2024. Detailed analysis of the\nthree deep neural models has been completed. We found that the UNet\narchitecture trained using a climatology-removed CNRM-CM6 projection as input\nand climatology-removed ORAS5 as output gives the best bias-corrected\nprojections. Our novel deep learning-based method for correcting CNRM-CM6 data\nhas a 15% reduction in RMSE compared EDCDF.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "LSTM",
      "Deep Neural Network"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence",
    "url": "http://arxiv.org/abs/2504.19036v1",
    "authors": [
      "Henry Herzog",
      "Joshua Hansen",
      "Yawen Zhang",
      "Patrick Beukema"
    ],
    "published": "2025-04-26",
    "abstract": "Unsustainable exploitation of the oceans exacerbated by global warming is\nthreatening coastal communities worldwide. Accurate and timely monitoring of\nmaritime activity is an essential step to effective governance and to inform\nfuture policy. In support of this complex global-scale effort, we built\nAtlantes, a deep learning based system that provides the first-ever real-time\nview of vessel behavior at global scale. Atlantes leverages a series of bespoke\ntransformers to distill a high volume, continuous stream of GPS messages\nemitted by hundreds of thousands of vessels into easily quantifiable behaviors.\nThe combination of low latency and high performance enables operationally\nrelevant decision-making and successful interventions on the high seas where\nillegal and exploitative activity is too common. Atlantes is already in use by\nhundreds of organizations worldwide. Here we provide an overview of the model\nand infrastructure that enables this system to function efficiently and\ncost-effectively at global-scale and in real-time.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Appa: Bending Weather Dynamics with Latent Diffusion Models for Global Data Assimilation",
    "url": "http://arxiv.org/abs/2504.18720v1",
    "authors": [
      "G\u00e9r\u00f4me Andry",
      "Fran\u00e7ois Rozet",
      "Sacha Lewin",
      "Omer Rochman",
      "Victor Mangeleer",
      "Matthias Pirlet",
      "Elise Faulx",
      "Marilaure Gr\u00e9goire",
      "Gilles Louppe"
    ],
    "published": "2025-04-25",
    "abstract": "Deep learning has transformed weather forecasting by improving both its\naccuracy and computational efficiency. However, before any forecast can begin,\nweather centers must identify the current atmospheric state from vast amounts\nof observational data. To address this challenging problem, we introduce Appa,\na score-based data assimilation model producing global atmospheric trajectories\nat 0.25-degree resolution and 1-hour intervals. Powered by a 1.5B-parameter\nspatio-temporal latent diffusion model trained on ERA5 reanalysis data, Appa\ncan be conditioned on any type of observations to infer the posterior\ndistribution of plausible state trajectories, without retraining. Our unified\nprobabilistic framework flexibly tackles multiple inference tasks --\nreanalysis, filtering, and forecasting -- using the same model, eliminating the\nneed for task-specific architectures or training procedures. Experiments\ndemonstrate physical consistency on a global scale and good reconstructions\nfrom observations, while showing competitive forecasting skills. Our results\nestablish latent score-based data assimilation as a promising foundation for\nfuture global atmospheric modeling systems.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Last-layer committee machines for uncertainty estimations of benthic imagery",
    "url": "http://arxiv.org/abs/2504.16952v1",
    "authors": [
      "H. Martin Gillis",
      "Isaac Xu",
      "Benjamin Misiuk",
      "Craig J. Brown",
      "Thomas Trappenberg"
    ],
    "published": "2025-04-22",
    "abstract": "Automating the annotation of benthic imagery (i.e., images of the seafloor\nand its associated organisms, habitats, and geological features) is critical\nfor monitoring rapidly changing ocean ecosystems. Deep learning approaches have\nsucceeded in this purpose; however, consistent annotation remains challenging\ndue to ambiguous seafloor images, potential inter-user annotation\ndisagreements, and out-of-distribution samples. Marine scientists implementing\ndeep learning models often obtain predictions based on one-hot representations\ntrained using a cross-entropy loss objective with softmax normalization,\nresulting with a single set of model parameters. While efficient, this approach\nmay lead to overconfident predictions for context-challenging datasets, raising\nreliability concerns that present risks for downstream tasks such as benthic\nhabitat mapping and marine spatial planning. In this study, we investigated\nclassification uncertainty as a tool to improve the labeling of benthic habitat\nimagery. We developed a framework for two challenging sub-datasets of the\nrecently publicly available BenthicNet dataset using Bayesian neural networks,\nMonte Carlo dropout inference sampling, and a proposed single last-layer\ncommittee machine. This approach resulted with a > 95% reduction of network\nparameters to obtain per-sample uncertainties while obtaining near-identical\nperformance compared to computationally more expensive strategies such as\nBayesian neural networks, Monte Carlo dropout, and deep ensembles. The method\nproposed in this research provides a strategy for obtaining prioritized lists\nof uncertain samples for human-in-the-loop interventions to identify ambiguous,\nmislabeled, out-of-distribution, and/or difficult images for enhancing existing\nannotation tools for benthic mapping and other applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping",
    "url": "http://arxiv.org/abs/2504.14372v1",
    "authors": [
      "Jose Marie Antonio Minoza"
    ],
    "published": "2025-04-19",
    "abstract": "Accurate ocean modeling and coastal hazard prediction depend on\nhigh-resolution bathymetric data; yet, current worldwide datasets are too\ncoarse for exact numerical simulations. While recent deep learning advances\nhave improved earth observation data resolution, existing methods struggle with\nthe unique challenges of producing detailed ocean floor maps, especially in\nmaintaining physical structure consistency and quantifying uncertainties. This\nwork presents a novel uncertainty-aware mechanism using spatial blocks to\nefficiently capture local bathymetric complexity based on block-based conformal\nprediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)\narchitecture, the integration of this uncertainty quantification framework\nyields spatially adaptive confidence estimates while preserving topographical\nfeatures via discrete latent representations. With smaller uncertainty widths\nin well-characterized areas and appropriately larger bounds in areas of complex\nseafloor structures, the block-based design adapts uncertainty estimates to\nlocal bathymetric complexity. Compared to conventional techniques, experimental\nresults over several ocean regions show notable increases in both\nreconstruction quality and uncertainty estimation reliability. This framework\nincreases the reliability of bathymetric reconstructions by preserving\nstructural integrity while offering spatially adaptive uncertainty estimates,\nso opening the path for more solid climate modeling and coastal hazard\nassessment.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enhancing Deterministic Freezing Level Predictions in the Northern Sierra Nevada Through Deep Neural Networks",
    "url": "http://arxiv.org/abs/2504.11560v1",
    "authors": [
      "Vesta Afzali Gorooh",
      "Agniv Sengupta",
      "Shawn Roj",
      "Rachel Weihs",
      "Brian Kawzenuk",
      "Luca Delle Monache",
      "F. Martin Ralph"
    ],
    "published": "2025-04-15",
    "abstract": "Accurate prediction of the freezing level (FZL) is essential for\nhydrometeorological forecasting systems and precipitation phase estimation, and\nit influences runoff generation and reservoir management decisions. In this\nstudy, we develop a deep learning based postprocessing framework using the Unet\nconvolutional neural network (CNN) architecture to refine the FZL forecasts\nfrom the West Weather Research and Forecasting (West-WRF) model. The proposed\nframework leverages reforecast data from West WRF and FZL estimates from the\nCalifornia Nevada River Forecast Center (CNRFC) to train a deterministic Unet\nmodel over the Yuba-Feather watershed, a hydrologically critical basin in\nnorthern California. We introduce two variants of our model, Unet-Log and\nUnet-GMM, which utilize the logarithm of the hyperbolic cosine of Error and\nGaussian Mixture Model loss functions, respectively, to enhance FZL forecast\naccuracy beyond an RMSE based benchmark. Results indicate that the Unet based\npostprocessing framework significantly improves FZL forecast skill across\ndiverse atmospheric conditions and complex topography. Compared to the raw\nWest-WRF output, our model achieves reductions in RMSE of up to 25% and\nincreases the forecast observation correlation by about 10% over the\nYuba-Feather watershed. Furthermore, it effectively captures the spatiotemporal\nvariability of the FZL across different elevations, mitigating systematic\nbiases inherent in the West-WRF model. This novel deep learning based\npostprocessing approach demonstrates a promising pathway for integrating\nmachine learning into hydrometeorological forecasting and decision support\nwithin the Forecast Informed Reservoir Operations (FIRO) framework.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "NWP-based deep learning for tropical cyclone intensity prediction",
    "url": "http://arxiv.org/abs/2504.09143v1",
    "authors": [
      "Chanh Kieu",
      "Khanh Luong",
      "Tri Nguyen"
    ],
    "published": "2025-04-12",
    "abstract": "Global artificial intelligence (AI) models are rapidly advancing and\nbeginning to outperform traditional numerical weather prediction (NWP) models\nacross metrics, yet predicting regional extreme weather such as tropical\ncyclone (TC) intensity presents unique spatial and temporal challenges that\nglobal AI models cannot capture. This study presents a new approach to train\ndeep learning (DL) models specifically for regional extreme weather prediction.\nBy leveraging physics-based NWP models to generate high-resolution data, we\ndemonstrate that DL models can better predict or downscale TC intensity and\nstructure when fine-scale processes are properly accounted for. Furthermore, by\ntraining DL models on different resolution outputs from physics-based\nsimulations, we highlight the critical role of fine-scale processes in larger\nstorm-scale dynamics, an aspect that current climate datasets used to train\nmost global DL models cannot fully represent. These findings underscore the\nchallenges in predicting or downscaling extreme weather with data-driven\nmodels, thus proposing the new role of NWP models as data generators for\ntraining DL models in the future AI model development for weather applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A physics informed neural network approach to simulating ice dynamics governed by the shallow ice approximation",
    "url": "http://arxiv.org/abs/2504.08136v1",
    "authors": [
      "Kapil Chawla",
      "William Holmes"
    ],
    "published": "2025-04-10",
    "abstract": "In this article we develop a Physics Informed Neural Network (PINN) approach\nto simulate ice sheet dynamics governed by the Shallow Ice Approximation. This\nproblem takes the form of a time-dependent parabolic obstacle problem. Prior\nwork has used this approach to address the stationary obstacle problem and here\nwe extend it to the time dependent problem. Through comprehensive 1D and 2D\nsimulations, we validate the model's effectiveness in capturing complex\nfree-boundary conditions. By merging traditional mathematical modeling with\ncutting-edge deep learning methods, this approach provides a scalable and\nrobust solution for predicting temporal variations in ice thickness. To\nillustrate this approach in a real world setting, we simulate the dynamics of\nthe Devon Ice Cap, incorporating aerogeophysical data from 2000 and 2018.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Improving Predictions of Convective Storm Wind Gusts through Statistical Post-Processing of Neural Weather Models",
    "url": "http://arxiv.org/abs/2504.00128v1",
    "authors": [
      "Antoine Leclerc",
      "Erwan Koch",
      "Monika Feldmann",
      "Daniele Nerini",
      "Tom Beucler"
    ],
    "published": "2025-03-31",
    "abstract": "Issuing timely severe weather warnings helps mitigate potentially disastrous\nconsequences. Recent advancements in Neural Weather Models (NWMs) offer a\ncomputationally inexpensive and fast approach for forecasting atmospheric\nenvironments on a 0.25{\\deg} global grid. For thunderstorms, these environments\ncan be empirically post-processed to predict wind gust distributions at\nspecific locations. With the Pangu-Weather NWM, we apply a hierarchy of\nstatistical and deep learning post-processing methods to forecast hourly wind\ngusts up to three days ahead. To ensure statistical robustness, we constrain\nour probabilistic forecasts using generalised extreme-value distributions\nacross five regions in Switzerland. Using a convolutional neural network to\npost-process the predicted atmospheric environment's spatial patterns yields\nthe best results, outperforming direct forecasting approaches across lead times\nand wind gust speeds. Our results confirm the added value of NWMs for extreme\nwind forecasting, especially for designing more responsive early-warning\nsystems.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Joint Source-Environment Adaptation for Deep Learning-Based Underwater Acoustic Source Ranging",
    "url": "http://arxiv.org/abs/2503.23262v1",
    "authors": [
      "Dariush Kari",
      "Andrew C. Singer"
    ],
    "published": "2025-03-30",
    "abstract": "In this paper, we propose a method to adapt a pre-trained deep-learning-based\nmodel for underwater acoustic localization to a new environment. We use\nunsupervised domain adaptation to improve the generalization performance of the\nmodel, i.e., using an unsupervised loss, fine-tune the pre-trained network\nparameters without access to any labels of the target environment or any data\nused to pre-train the model. This method improves the pre-trained model\nprediction by coupling that with an almost independent estimation based on the\nreceived signal energy (that depends on the source). We show the effectiveness\nof this approach on Bellhop generated data in an environment similar to that of\nthe SWellEx-96 experiment contaminated with real ocean noise from the KAM11\nexperiment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Joint Source-Environment Adaptation of Data-Driven Underwater Acoustic Source Ranging Based on Model Uncertainty",
    "url": "http://arxiv.org/abs/2503.23258v1",
    "authors": [
      "Dariush Kari",
      "Hari Vishnu",
      "Andrew C. Singer"
    ],
    "published": "2025-03-30",
    "abstract": "Adapting pre-trained deep learning models to new and unknown environments is\na difficult challenge in underwater acoustic localization. We show that\nalthough pre-trained models have performance that suffers from mismatch between\nthe training and test data, they generally exhibit a higher ``implied\nuncertainty'' in environments where there is more mismatch. Leveraging this\nnotion of implied uncertainty, we partition the test samples into more certain\nand less certain sets, and implement an estimation method using the certain\nsamples to improve the labeling for uncertain samples, which helps to adapt the\nmodel. We use an efficient method to quantify model prediction uncertainty, and\nan innovative approach to adapt a pre-trained model to unseen underwater\nenvironments at test time. This eliminates the need for labeled data from the\ntarget environment or the original training data. This adaptation is enhanced\nby integrating an independent estimate based on the received signal energy. We\nvalidate the approach extensively using real experimental data, as well as\nsynthetic data consisting of model-generated signals with real ocean noise. The\nresults demonstrate significant improvements in model prediction accuracy,\nunderscoring the potential of the method to enhance underwater acoustic\nlocalization in diverse, noisy, and unknown environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Simulation-informed deep learning for enhanced SWOT observations of fine-scale ocean dynamics",
    "url": "http://arxiv.org/abs/2503.21303v1",
    "authors": [
      "Eugenio Cutolo",
      "Carlos Granero-Belinchon",
      "Ptashanna Thiraux",
      "Jinbo Wang",
      "Ronan Fablet"
    ],
    "published": "2025-03-27",
    "abstract": "Oceanic processes at fine scales are crucial yet difficult to observe\naccurately due to limitations in satellite and in-situ measurements. The\nSurface Water and Ocean Topography (SWOT) mission provides high-resolution Sea\nSurface Height (SSH) data, though noise patterns often obscure fine scale\nstructures. Current methods struggle with noisy data or require extensive\nsupervised training, limiting their effectiveness on real-world observations.\nWe introduce SIMPGEN (Simulation-Informed Metric and Prior for Generative\nEnsemble Networks), an unsupervised adversarial learning framework combining\nreal SWOT observations with simulated reference data. SIMPGEN leverages\nwavelet-informed neural metrics to distinguish noisy from clean fields, guiding\nrealistic SSH reconstructions. Applied to SWOT data, SIMPGEN effectively\nremoves noise, preserving fine-scale features better than existing neural\nmethods. This robust, unsupervised approach not only improves SWOT SSH data\ninterpretation but also demonstrates strong potential for broader oceanographic\napplications, including data assimilation and super-resolution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years",
    "url": "http://arxiv.org/abs/2503.21211v1",
    "authors": [
      "Rixu Hao",
      "Yuxin Zhao",
      "Shaoqing Zhang",
      "Guihua Wang",
      "Xiong Deng"
    ],
    "published": "2025-03-27",
    "abstract": "El Ni\\~no-Southern Oscillation (ENSO) exerts global climate and societal\nimpacts, but real-time prediction with lead times beyond one year remains\nchallenging. Dynamical models suffer from large biases and uncertainties, while\ndeep learning struggles with interpretability and multi-scale dynamics. Here,\nwe introduce PTSTnet, an interpretable model that unifies dynamical processes\nand cross-scale spatiotemporal learning in an innovative neural-network\nframework with physics-encoding learning. PTSTnet produces interpretable\npredictions significantly outperforming state-of-the-art benchmarks with lead\ntimes beyond 24 months, providing physical insights into error propagation in\nocean-atmosphere interactions. PTSTnet learns feature representations with\nphysical consistency from sparse data to tackle inherent multi-scale and\nmulti-physics challenges underlying ocean-atmosphere processes, thereby\ninherently enhancing long-term prediction skill. Our successful realizations\nmark substantial steps forward in interpretable insights into innovative neural\nocean modelling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
    "url": "http://arxiv.org/abs/2503.19502v1",
    "authors": [
      "Qi Chen",
      "Yinghao Cui",
      "Guobin Hong",
      "Karumuri Ashok",
      "Yuchun Pu",
      "Xiaogu Zheng",
      "Xuanze Zhang",
      "Wei Zhong",
      "Peng Zhan",
      "Zhonglei Wang"
    ],
    "published": "2025-03-25",
    "abstract": "El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling",
    "url": "http://arxiv.org/abs/2503.19940v1",
    "authors": [
      "Qiusheng Huang",
      "Xiaohui Zhong",
      "Xu Fan",
      "Lei Chen",
      "Hao Li"
    ],
    "published": "2025-03-25",
    "abstract": "Similar to conventional video generation, current deep learning-based weather\nprediction frameworks often lack explicit physical constraints, leading to\nunphysical outputs that limit their reliability for operational forecasting.\nAmong various physical processes requiring proper representation, radiation\nplays a fundamental role as it drives Earth's weather and climate systems.\nHowever, accurate simulation of radiative transfer processes remains\nchallenging for traditional numerical weather prediction (NWP) models due to\ntheir inherent complexity and high computational costs. Here, we propose\nFuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance\nweather forecast accuracy while enforcing physical consistency. FuXi-RTM\nintegrates a primary forecasting model (FuXi) with a fixed deep learning-based\nradiative transfer model (DLRTM) surrogate that efficiently replaces\nconventional radiation parameterization schemes. This represents the first deep\nlearning-based weather forecasting framework to explicitly incorporate physical\nprocess modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM\noutperforms its unconstrained counterpart in 88.51% of 3320 variable and lead\ntime combinations, with improvements in radiative flux predictions. By\nincorporating additional physical processes, FuXi-RTM paves the way for\nnext-generation weather forecasting systems that are both accurate and\nphysically consistent.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Deep learning in the abyss: a stratified Physics Informed Neural Network for data assimilation",
    "url": "http://arxiv.org/abs/2503.19160v1",
    "authors": [
      "Vadim Limousin",
      "Nelly Pustelnik",
      "Bruno Deremble",
      "Antoine Venaille"
    ],
    "published": "2025-03-24",
    "abstract": "The reconstruction of deep ocean currents is a major challenge in data\nassimilation due to the scarcity of interior data. In this work, we present a\nproof of concept for deep ocean flow reconstruction using a Physics-Informed\nNeural Network (PINN), a machine learning approach that offers an alternative\nto traditional data assimilation methods. We introduce an efficient algorithm\ncalled StrAssPINN (for Stratified Assimilation PINNs), which assigns a separate\nnetwork to each layer of the ocean model while allowing them to interact during\ntraining. The neural network takes spatiotemporal coordinates as input and\npredicts the velocity field at those points. Using a SIREN architecture (a\nmultilayer perceptron with sine activation functions), which has proven\neffective in various contexts, the network is trained using both available\nobservational data and dynamical priors enforced at several collocation points.\nWe apply this method to pseudo-observed ocean data generated from a 3-layer\nquasi-geostrophic model, where the pseudo-observations include surface-level\ndata akin to SWOT observations of sea surface height, interior data similar to\nARGO floats, and a limited number of deep ARGO-like measurements in the lower\nlayers. Our approach successfully reconstructs ocean flows in both the interior\nand surface layers, demonstrating a strong ability to resolve key ocean\nmesoscale features, including vortex rings, eastward jets associated with\npotential vorticity fronts, and smoother Rossby waves. This work serves as a\nprelude to applying StrAssPINN to real-world observational data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Towards Location-Specific Precipitation Projections Using Deep Neural Networks",
    "url": "http://arxiv.org/abs/2503.14095v1",
    "authors": [
      "Bipin Kumar",
      "Bhvisy Kumar Yadav",
      "Soumypdeep Mukhopadhyay",
      "Rakshit Rohan",
      "Bhupendra Bahadur Singh",
      "Rajib Chattopadhyay",
      "Nagraju Chilukoti",
      "Atul Kumar Sahai"
    ],
    "published": "2025-03-18",
    "abstract": "Accurate precipitation estimates at individual locations are crucial for\nweather forecasting and spatial analysis. This study presents a paradigm shift\nby leveraging Deep Neural Networks (DNNs) to surpass traditional methods like\nKriging for station-specific precipitation approximation. We propose two\ninnovative NN architectures: one utilizing precipitation, elevation, and\nlocation, and another incorporating additional meteorological parameters like\nhumidity, temperature, and wind speed. Trained on a vast dataset (1980-2019),\nthese models outperform Kriging across various evaluation metrics (correlation\ncoefficient, root mean square error, bias, and skill score) on a five-year\nvalidation set. This compelling evidence demonstrates the transformative power\nof deep learning for spatial prediction, offering a robust and precise\nalternative for station-specific precipitation estimation.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "COSMOS: Continuous Simplicial Neural Networks",
    "url": "http://arxiv.org/abs/2503.12919v1",
    "authors": [
      "Aref Einizade",
      "Dorina Thanou",
      "Fragkiskos D. Malliaros",
      "Jhony H. Giraldo"
    ],
    "published": "2025-03-17",
    "abstract": "Simplicial complexes provide a powerful framework for modeling high-order\ninteractions in structured data, making them particularly suitable for\napplications such as trajectory prediction and mesh processing. However,\nexisting simplicial neural networks (SNNs), whether convolutional or\nattention-based, rely primarily on discrete filtering techniques, which can be\nrestrictive. In contrast, partial differential equations (PDEs) on simplicial\ncomplexes offer a principled approach to capture continuous dynamics in such\nstructures. In this work, we introduce COntinuous SiMplicial neural netwOrkS\n(COSMOS), a novel SNN architecture derived from PDEs on simplicial complexes.\nWe provide theoretical and experimental justifications of COSMOS's stability\nunder simplicial perturbations. Furthermore, we investigate the over-smoothing\nphenomenon, a common issue in geometric deep learning, demonstrating that\nCOSMOS offers better control over this effect than discrete SNNs. Our\nexperiments on real-world datasets of ocean trajectory prediction and\nregression on partial deformable shapes demonstrate that COSMOS achieves\ncompetitive performance compared to state-of-the-art SNNs in complex and noisy\nenvironments.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Open-Set Plankton Recognition",
    "url": "http://arxiv.org/abs/2503.11318v1",
    "authors": [
      "Joona Kareinen",
      "Annaliina Skytt\u00e4",
      "Tuomas Eerola",
      "Kaisa Kraft",
      "Lasse Lensu",
      "Sanna Suikkanen",
      "Maiju Lehtiniemi",
      "Heikki K\u00e4lvi\u00e4inen"
    ],
    "published": "2025-03-14",
    "abstract": "This paper considers open-set recognition (OSR) of plankton images. Plankton\ninclude a diverse range of microscopic aquatic organisms that have an important\nrole in marine ecosystems as primary producers and as a base of food webs.\nGiven their sensitivity to environmental changes, fluctuations in plankton\npopulations offer valuable information about oceans' health and climate change\nmotivating their monitoring. Modern automatic plankton imaging devices enable\nthe collection of large-scale plankton image datasets, facilitating\nspecies-level analysis. Plankton species recognition can be seen as an image\nclassification task and is typically solved using deep learning-based image\nrecognition models. However, data collection in real aquatic environments\nresults in imaging devices capturing a variety of non-plankton particles and\nplankton species not present in the training set. This creates a challenging\nfine-grained OSR problem, characterized by subtle differences between\ntaxonomically close plankton species. We address this challenge by conducting\nextensive experiments on three OSR approaches using both phyto- and zooplankton\nimages analyzing also on the effect of the rejection thresholds for OSR. The\nresults demonstrate that high OSR accuracy can be obtained promoting the use of\nthese methods in operational plankton research. We have made the data publicly\navailable to the research community.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Physics-Informed Residual Neural Ordinary Differential Equations for Enhanced Tropical Cyclone Intensity Forecasting",
    "url": "http://arxiv.org/abs/2503.06436v1",
    "authors": [
      "Fan Meng"
    ],
    "published": "2025-03-09",
    "abstract": "Accurate tropical cyclone (TC) intensity prediction is crucial for mitigating\nstorm hazards, yet its complex dynamics pose challenges to traditional methods.\nHere, we introduce a Physics-Informed Residual Neural Ordinary Differential\nEquation (PIR-NODE) model to precisely forecast TC intensity evolution. This\nmodel leverages the powerful non-linear fitting capabilities of deep learning,\nintegrates residual connections to enhance model depth and training stability,\nand explicitly models the continuous temporal evolution of TC intensity using\nNeural ODEs. Experimental results in the SHIPS dataset demonstrate that the\nPIR-NODE model achieves a significant improvement in 24-hour intensity\nprediction accuracy compared to traditional statistical models and benchmark\ndeep learning methods, with a 25. 2\\% reduction in the root mean square error\n(RMSE) and a 19.5\\% increase in R-square (R2) relative to a baseline of neural\nnetwork. Crucially, the residual structure effectively preserves initial state\ninformation, and the model exhibits robust generalization capabilities. This\nstudy details the PIR-NODE model architecture, physics-informed integration\nstrategies, and comprehensive experimental validation, revealing the\nsubstantial potential of deep learning techniques in predicting complex\ngeophysical systems and laying the foundation for future refined TC forecasting\nresearch.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Correlation to Causation: A Causal Deep Learning Framework for Arctic Sea Ice Prediction",
    "url": "http://arxiv.org/abs/2503.02093v1",
    "authors": [
      "Emam Hossain",
      "Muhammad Hasan Ferdous",
      "Jianwu Wang",
      "Aneesh Subramanian",
      "Md Osman Gani"
    ],
    "published": "2025-03-03",
    "abstract": "Traditional machine learning and deep learning techniques rely on\ncorrelation-based learning, often failing to distinguish spurious associations\nfrom true causal relationships, which limits robustness, interpretability, and\ngeneralizability. To address these challenges, we propose a causality-driven\ndeep learning framework that integrates Multivariate Granger Causality (MVGC)\nand PCMCI+ causal discovery algorithms with a hybrid deep learning\narchitecture. Using 43 years (1979-2021) of daily and monthly Arctic Sea Ice\nExtent (SIE) and ocean-atmospheric datasets, our approach identifies causally\nsignificant factors, prioritizes features with direct influence, reduces\nfeature overhead, and improves computational efficiency. Experiments\ndemonstrate that integrating causal features enhances the deep learning model's\npredictive accuracy and interpretability across multiple lead times. Beyond SIE\nprediction, the proposed framework offers a scalable solution for dynamic,\nhigh-dimensional systems, advancing both theoretical understanding and\npractical applications in predictive modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "The Role, Trends, and Applications of Machine Learning in Undersea Communication: A Bangladesh Perspective",
    "url": "http://arxiv.org/abs/2503.00669v1",
    "authors": [
      "Yousuf Islam",
      "Sumon Chandra Das",
      "Md. Jalal Uddin Chowdhury"
    ],
    "published": "2025-03-01",
    "abstract": "The rapid evolution of machine learning (ML) has brought about groundbreaking\ndevelopments in numerous industries, not the least of which is in the area of\nundersea communication. This domain is critical for applications like ocean\nexploration, environmental monitoring, resource management, and national\nsecurity. Bangladesh, a maritime nation with abundant resources in the Bay of\nBengal, can harness the immense potential of ML to tackle the unprecedented\nchallenges associated with underwater communication. Beyond that, environmental\nconditions are unique to the region: in addition to signal attenuation,\nmultipath propagation, noise interference, and limited bandwidth. In this\nstudy, we address the necessity to bring ML into communication via undersea; it\ninvestigates the latest technologies under the domain of ML in that respect,\nsuch as deep learning and reinforcement learning, especially concentrating on\nBangladesh scenarios in the sense of implementation. This paper offers a\ncontextualized regional perspective by incorporating region-specific needs,\ncase studies, and recent research to propose a roadmap for deploying ML-driven\nsolutions to improve safety at sea, promote sustainable resource use, and\nenhance disaster response systems. This research ultimately highlights the\npromise of ML-powered solutions for transforming undersea communication,\nleading to more efficient and cost-effective technologies that subsequently\ncontribute to both economic growth and environmental sustainability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Integrating Weather Station Data and Radar for Precipitation Nowcasting: SmaAt-fUsion and SmaAt-Krige-GNet",
    "url": "http://arxiv.org/abs/2502.16116v1",
    "authors": [
      "Aleksej Cornelissen",
      "Jie Shi",
      "Siamak Mehrkanoon"
    ],
    "published": "2025-02-22",
    "abstract": "In recent years, data-driven, deep learning-based approaches for\nprecipitation nowcasting have attracted significant attention, showing\npromising results. However, many existing models fail to fully exploit the\nextensive atmospheric information available, relying primarily on precipitation\ndata alone. This study introduces two novel deep learning architectures,\nSmaAt-fUsion and SmaAt-Krige-GNet, specifically designed to enhance\nprecipitation nowcasting by integrating multi-variable weather station data\nwith radar datasets. By leveraging additional meteorological information, these\nmodels improve representation learning in the latent space, resulting in\nenhanced nowcasting performance. The SmaAt-fUsion model extends the SmaAt-UNet\nframework by incorporating weather station data through a convolutional layer,\nintegrating it into the bottleneck of the network. Conversely, the\nSmaAt-Krige-GNet model combines precipitation maps with weather station data\nprocessed using Kriging, a geo-statistical interpolation method, to generate\nvariable-specific maps. These maps are then utilized in a dual-encoder\narchitecture based on SmaAt-GNet, allowing multi-level data integration.\nExperimental evaluations were conducted using four years (2016--2019) of\nweather station and precipitation radar data from the Netherlands. Results\ndemonstrate that SmaAt-Krige-GNet outperforms the standard SmaAt-UNet, which\nrelies solely on precipitation radar data, in low precipitation scenarios,\nwhile SmaAt-fUsion surpasses SmaAt-UNet in both low and high precipitation\nscenarios. This highlights the potential of incorporating discrete weather\nstation data to enhance the performance of deep learning-based weather\nnowcasting models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "CondensNet: Enabling stable long-term climate simulations via hybrid deep learning models with adaptive physical constraints",
    "url": "http://arxiv.org/abs/2502.13185v1",
    "authors": [
      "Xin Wang",
      "Juntao Yang",
      "Jeff Adie",
      "Simon See",
      "Kalli Furtado",
      "Chen Chen",
      "Troy Arcomano",
      "Romit Maulik",
      "Gianmarco Mengaldo"
    ],
    "published": "2025-02-18",
    "abstract": "Accurate and efficient climate simulations are crucial for understanding\nEarth's evolving climate. However, current general circulation models (GCMs)\nface challenges in capturing unresolved physical processes, such as cloud and\nconvection. A common solution is to adopt cloud resolving models, that provide\nmore accurate results than the standard subgrid parametrisation schemes\ntypically used in GCMs. However, cloud resolving models, also referred to as\nsuper paramtetrizations, remain computationally prohibitive. Hybrid modeling,\nwhich integrates deep learning with equation-based GCMs, offers a promising\nalternative but often struggles with long-term stability and accuracy issues.\nIn this work, we find that water vapor oversaturation during condensation is a\nkey factor compromising the stability of hybrid models. To address this, we\nintroduce CondensNet, a novel neural network architecture that embeds a\nself-adaptive physical constraint to correct unphysical condensation processes.\nCondensNet effectively mitigates water vapor oversaturation, enhancing\nsimulation stability while maintaining accuracy and improving computational\nefficiency compared to super parameterization schemes.\n  We integrate CondensNet into a GCM to form PCNN-GCM (Physics-Constrained\nNeural Network GCM), a hybrid deep learning framework designed for long-term\nstable climate simulations in real-world conditions, including ocean and land.\nPCNN-GCM represents a significant milestone in hybrid climate modeling, as it\nshows a novel way to incorporate physical constraints adaptively, paving the\nway for accurate, lightweight, and stable long-term climate simulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Skillful Nowcasting of Convective Clouds With a Cascade Diffusion Model",
    "url": "http://arxiv.org/abs/2502.10957v1",
    "authors": [
      "Haoming Chen",
      "Xiaohui Zhong",
      "Qiang Zhai",
      "Xiaomeng Li",
      "Ying Wa Chan",
      "Pak Wai Chan",
      "Yuanyuan Huang",
      "Hao Li",
      "Xiaoming Shi"
    ],
    "published": "2025-02-16",
    "abstract": "Accurate nowcasting of convective clouds from satellite imagery is essential\nfor mitigating the impacts of meteorological disasters, especially in\ndeveloping countries and remote regions with limited ground-based observations.\nRecent advances in deep learning have shown promise in video prediction;\nhowever, existing models frequently produce blurry results and exhibit reduced\naccuracy when forecasting physical fields. Here, we introduce SATcast, a\ndiffusion model that leverages a cascade architecture and multimodal inputs for\nnowcasting cloud fields in satellite imagery. SATcast incorporates physical\nfields predicted by FuXi, a deep-learning weather model, alongside past\nsatellite observations as conditional inputs to generate high-quality future\ncloud fields. Through comprehensive evaluation, SATcast outperforms\nconventional methods on multiple metrics, demonstrating its superior accuracy\nand robustness. Ablation studies underscore the importance of its multimodal\ndesign and the cascade architecture in achieving reliable predictions. Notably,\nSATcast maintains predictive skill for up to 24 hours, underscoring its\npotential for operational nowcasting applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Learning to generate physical ocean states: Towards hybrid climate modeling",
    "url": "http://arxiv.org/abs/2502.02499v1",
    "authors": [
      "Etienne Meunier",
      "David Kamm",
      "Guillaume Gachon",
      "Redouane Lguensat",
      "Julie Deshayes"
    ],
    "published": "2025-02-04",
    "abstract": "Ocean General Circulation Models require extensive computational resources to\nreach equilibrium states, while deep learning emulators, despite offering fast\npredictions, lack the physical interpretability and long-term stability\nnecessary for climate scientists to understand climate sensitivity (to\ngreenhouse gas emissions) and mechanisms of abrupt % variability such as\ntipping points. We propose to take the best from both worlds by leveraging deep\ngenerative models to produce physically consistent oceanic states that can\nserve as initial conditions for climate projections. We assess the viability of\nthis hybrid approach through both physical metrics and numerical experiments,\nand highlight the benefits of enforcing physical constraints during generation.\nAlthough we train here on ocean variables from idealized numerical simulations,\nwe claim that this hybrid approach, combining the computational efficiency of\ndeep learning with the physical accuracy of numerical models, can effectively\nreduce the computational burden of running climate models to equilibrium, and\nreduce uncertainties in climate projections by minimizing drifts in baseline\nsimulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FireCastNet: Earth-as-a-Graph for Seasonal Fire Prediction",
    "url": "http://arxiv.org/abs/2502.01550v1",
    "authors": [
      "Dimitrios Michail",
      "Charalampos Davalas",
      "Lefki-Ioanna Panagiotou",
      "Ioannis Prapas",
      "Spyros Kondylatos",
      "Nikolaos Ioannis Bountos",
      "Ioannis Papoutsis"
    ],
    "published": "2025-02-03",
    "abstract": "With climate change expected to exacerbate fire weather conditions, the\naccurate and timely anticipation of wildfires becomes increasingly crucial for\ndisaster mitigation. In this study, we utilize SeasFire, a comprehensive global\nwildfire dataset with climate, vegetation, oceanic indices, and human-related\nvariables, to enable seasonal wildfire forecasting with machine learning. For\nthe predictive analysis, we present FireCastNet, a novel architecture which\ncombines a 3D convolutional encoder with GraphCast, originally developed for\nglobal short-term weather forecasting using graph neural networks. FireCastNet\nis trained to capture the context leading to wildfires, at different spatial\nand temporal scales. Our investigation focuses on assessing the effectiveness\nof our model in predicting the presence of burned areas at varying forecasting\ntime horizons globally, extending up to six months into the future, and on how\ndifferent spatial or/and temporal context affects the performance. Our findings\ndemonstrate the potential of deep learning models in seasonal fire forecasting;\nlonger input time-series leads to more robust predictions, while integrating\nspatial information to capture wildfire spatio-temporal dynamics boosts\nperformance. Finally, our results hint that in order to enhance performance at\nlonger forecasting horizons, a larger receptive field spatially needs to be\nconsidered.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "OneForecast: A Universal Framework for Global and Regional Weather Forecasting",
    "url": "http://arxiv.org/abs/2502.00338v1",
    "authors": [
      "Yuan Gao",
      "Hao Wu",
      "Ruiqi Shu",
      "Huanshuo Dong",
      "Fan Xu",
      "Rui Chen",
      "Yibo Yan",
      "Qingsong Wen",
      "Xuming Hu",
      "Kun Wang",
      "Jiahao Wu",
      "Qing Li",
      "Hui Xiong",
      "Xiaomeng Huang"
    ],
    "published": "2025-02-01",
    "abstract": "Accurate weather forecasts are important for disaster prevention,\nagricultural planning, and water resource management. Traditional numerical\nweather prediction (NWP) methods offer physically interpretable high-accuracy\npredictions but are computationally expensive and fail to fully leverage\nrapidly growing historical data. In recent years, deep learning methods have\nmade significant progress in weather forecasting, but challenges remain, such\nas balancing global and regional high-resolution forecasts, excessive smoothing\nin extreme event predictions, and insufficient dynamic system modeling. To\naddress these issues, this paper proposes a global-regional nested weather\nforecasting framework based on graph neural networks (GNNs). By combining a\ndynamic system perspective with multi-grid theory, we construct a multi-scale\ngraph structure and densify the target region to capture local high-frequency\nfeatures. We introduce an adaptive information propagation mechanism, using\ndynamic gating units to deeply integrate node and edge features for more\naccurate extreme event forecasting. For high-resolution regional forecasts, we\npropose a neural nested grid method to mitigate boundary information loss.\nExperimental results show that the proposed method performs excellently across\nglobal to regional scales and short-term to long-term forecasts, especially in\nextreme event predictions (e.g., typhoons), significantly improving forecast\naccuracy. Our codes are available at https://github.com/YuanGao-YG/OneForecast.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Improving Tropical Cyclone Forecasting With Video Diffusion Models",
    "url": "http://arxiv.org/abs/2501.16003v4",
    "authors": [
      "Zhibo Ren",
      "Pritthijit Nath",
      "Pancham Shukla"
    ],
    "published": "2025-01-27",
    "abstract": "Tropical cyclone (TC) forecasting is crucial for disaster preparedness and\nmitigation. While recent deep learning approaches have shown promise, existing\nmethods often treat TC evolution as a series of independent frame-to-frame\npredictions, limiting their ability to capture long-term dynamics. We present a\nnovel application of video diffusion models for TC forecasting that explicitly\nmodels temporal dependencies through additional temporal layers. Our approach\nenables the model to generate multiple frames simultaneously, better capturing\ncyclone evolution patterns. We introduce a two-stage training strategy that\nsignificantly improves individual-frame quality and performance in low-data\nregimes. Experimental results show our method outperforms the previous approach\nof Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably,\nwe extend the reliable forecasting horizon from 36 to 50 hours. Through\ncomprehensive evaluation using both traditional metrics and Fr\\'echet Video\nDistance (FVD), we demonstrate that our approach produces more temporally\ncoherent forecasts while maintaining competitive single-frame quality. Code\naccessible at https://github.com/Ren-creater/forecast-video-diffmodels.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Deep State Space Model for Rainfall-Runoff Simulations",
    "url": "http://arxiv.org/abs/2501.14980v1",
    "authors": [
      "Yihan Wang",
      "Lujun Zhang",
      "Annan Yu",
      "N. Benjamin Erichson",
      "Tiantian Yang"
    ],
    "published": "2025-01-24",
    "abstract": "The classical way of studying the rainfall-runoff processes in the water\ncycle relies on conceptual or physically-based hydrologic models. Deep learning\n(DL) has recently emerged as an alternative and blossomed in hydrology\ncommunity for rainfall-runoff simulations. However, the decades-old Long\nShort-Term Memory (LSTM) network remains the benchmark for this task,\noutperforming newer architectures like Transformers. In this work, we propose a\nState Space Model (SSM), specifically the Frequency Tuned Diagonal State Space\nSequence (S4D-FT) model, for rainfall-runoff simulations. The proposed S4D-FT\nis benchmarked against the established LSTM and a physically-based Sacramento\nSoil Moisture Accounting model across 531 watersheds in the contiguous United\nStates (CONUS). Results show that S4D-FT is able to outperform the LSTM model\nacross diverse regions. Our pioneering introduction of the S4D-FT for\nrainfall-runoff simulations challenges the dominance of LSTM in the hydrology\ncommunity and expands the arsenal of DL tools available for hydrological\nmodeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "Simultaneous emulation and downscaling with physically-consistent deep learning-based regional ocean emulators",
    "url": "http://arxiv.org/abs/2501.05058v1",
    "authors": [
      "Leonard Lupin-Jimenez",
      "Moein Darman",
      "Subhashis Hazarika",
      "Tianning Wu",
      "Michael Gray",
      "Ruyoing He",
      "Anthony Wong",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-01-09",
    "abstract": "Building on top of the success in AI-based atmospheric emulation, we propose\nan AI-based ocean emulation and downscaling framework focusing on the\nhigh-resolution regional ocean over Gulf of Mexico. Regional ocean emulation\npresents unique challenges owing to the complex bathymetry and lateral boundary\nconditions as well as from fundamental biases in deep learning-based\nframeworks, such as instability and hallucinations. In this paper, we develop a\ndeep learning-based framework to autoregressively integrate ocean-surface\nvariables over the Gulf of Mexico at $8$ Km spatial resolution without\nunphysical drifts over decadal time scales and simulataneously downscale and\nbias-correct it to $4$ Km resolution using a physics-constrained generative\nmodel. The framework shows both short-term skills as well as accurate long-term\nstatistics in terms of mean and variability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Analogue Forecast System for Daily Precipitation Prediction Using Autoencoder Feature Extraction: Application in Hong Kong",
    "url": "http://arxiv.org/abs/2501.02814v1",
    "authors": [
      "Yee Chun Tsoi",
      "Yu Ting Kwok",
      "Ming Chun Lam",
      "Wai Kin Wong"
    ],
    "published": "2025-01-06",
    "abstract": "In the Hong Kong Observatory, the Analogue Forecast System (AFS) for\nprecipitation has been providing useful reference in predicting possible daily\nrainfall scenarios for the next 9 days, by identifying historical cases with\nsimilar weather patterns to the latest output from the deterministic model of\nthe European Centre for Medium-Range Weather Forecasts (ECMWF). Recent advances\nin machine learning allow more sophisticated models to be trained using\nhistorical data and the patterns of high-impact weather events to be\nrepresented more effectively. As such, an enhanced AFS has been developed using\nthe deep learning technique autoencoder. The datasets of the fifth generation\nof the ECMWF Reanalysis (ERA5) are utilised where more meteorological elements\nin higher horizontal, vertical and temporal resolutions are available as\ncompared to the previous ECMWF reanalysis products used in the existing AFS.\nThe enhanced AFS features four major steps in generating the daily rain class\nforecasts: (1) preprocessing of gridded ERA5 and ECMWF model forecast, (2)\nfeature extraction by the pretrained autoencoder, (3) application of optimised\nfeature weightings based on historical cases, and (4) calculation of the final\nrain class from a weighted ensemble of top analogues. The enhanced AFS\ndemonstrates a consistent and superior performance over the existing AFS,\nespecially in capturing heavy rain cases, during the verification period from\n2019 to 2022. This paper presents the detailed formulation of the enhanced AFS\nand discusses its advantages and limitations in supporting precipitation\nforecasting in Hong Kong.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "LWFNet: Coherent Doppler Wind Lidar-Based Network for Wind Field Retrieval",
    "url": "http://arxiv.org/abs/2501.02613v1",
    "authors": [
      "Ran Tao",
      "Chong Wang",
      "Hao Chen",
      "Mingjiao Jia",
      "Xiang Shang",
      "Luoyuan Qu",
      "Guoliang Shentu",
      "Yanyu Lu",
      "Yanfeng Huo",
      "Lei Bai",
      "Xianghui Xue",
      "Xiankang Dou"
    ],
    "published": "2025-01-05",
    "abstract": "Accurate detection of wind fields within the troposphere is essential for\natmospheric dynamics research and plays a crucial role in extreme weather\nforecasting. Coherent Doppler wind lidar (CDWL) is widely regarded as the most\nsuitable technique for high spatial and temporal resolution wind field\ndetection. However, since coherent detection relies heavily on the\nconcentration of aerosol particles, which cause Mie scattering, the received\nbackscattering lidar signal exhibits significantly low intensity at high\naltitudes. As a result, conventional methods, such as spectral centroid\nestimation, often fail to produce credible and accurate wind retrieval results\nin these regions. To address this issue, we propose LWFNet, the first\nLidar-based Wind Field (WF) retrieval neural Network, built upon Transformer\nand the Kolmogorov-Arnold network. Our model is trained solely on targets\nderived from the traditional wind retrieval algorithm and utilizes radiosonde\nmeasurements as the ground truth for test results evaluation. Experimental\nresults demonstrate that LWFNet not only extends the maximum wind field\ndetection range but also produces more accurate results, exhibiting a level of\nprecision that surpasses the labeled targets. This phenomenon, which we refer\nto as super-accuracy, is explored by investigating the potential underlying\nfactors that contribute to this intriguing occurrence. In addition, we compare\nthe performance of LWFNet with other state-of-the-art (SOTA) models,\nhighlighting its superior effectiveness and capability in high-resolution wind\nretrieval. LWFNet demonstrates remarkable performance in lidar-based wind field\nretrieval, setting a benchmark for future research and advancing the\ndevelopment of deep learning models in this domain.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Paraformer: Parameterization of Sub-grid Scale Processes Using Transformers",
    "url": "http://arxiv.org/abs/2412.16763v1",
    "authors": [
      "Shuochen Wang",
      "Nishant Yadav",
      "Auroop R. Ganguly"
    ],
    "published": "2024-12-21",
    "abstract": "One of the major sources of uncertainty in the current generation of Global\nClimate Models (GCMs) is the representation of sub-grid scale physical\nprocesses. Over the years, a series of deep-learning-based parameterization\nschemes have been developed and tested on both idealized and real-geography\nGCMs. However, datasets on which previous deep-learning models were trained\neither contain limited variables or have low spatial-temporal coverage, which\ncan not fully simulate the parameterization process. Additionally, these\nschemes rely on classical architectures while the latest attention mechanism\nused in Transformer models remains unexplored in this field. In this paper, we\npropose Paraformer, a \"memory-aware\" Transformer-based model on ClimSim, the\nlargest dataset ever created for climate parameterization. Our results\ndemonstrate that the proposed model successfully captures the complex\nnon-linear dependencies in the sub-grid scale variables and outperforms\nclassical deep-learning architectures. This work highlights the applicability\nof the attenuation mechanism in this field and provides valuable insights for\ndeveloping future deep-learning-based climate parameterization schemes.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Improved Forecasts of Global Extreme Marine Heatwaves Through a Physics-guided Data-driven Approach",
    "url": "http://arxiv.org/abs/2412.15532v1",
    "authors": [
      "Ruiqi Shu",
      "Hao Wu",
      "Yuan Gao",
      "Fanghua Xu",
      "Ruijian Gou",
      "Xiaomeng Huang"
    ],
    "published": "2024-12-20",
    "abstract": "The unusually warm sea surface temperature events known as marine heatwaves\n(MHWs) have a profound impact on marine ecosystems. Accurate prediction of\nextreme MHWs has significant scientific and financial worth. However, existing\nmethods still have certain limitations, especially in the most extreme MHWs. In\nthis study, to address these issues, based on the physical nature of MHWs, we\ncreated a novel deep learning neural network that is capable of accurate 10-day\nMHW forecasting. Our framework significantly improves the forecast ability of\nextreme MHWs through two specially designed modules inspired by numerical\nmodels: a coupler and a probabilistic data argumentation. The coupler simulates\nthe driving effect of atmosphere on MHWs while the probabilistic data\nargumentation approaches significantly boost the forecast ability of extreme\nMHWs based on the idea of ensemble forecast. Compared with traditional\nnumerical prediction, our framework has significantly higher accuracy and\nrequires fewer computational resources. What's more, explainable AI methods\nshow that wind forcing is the primary driver of MHW evolution and reveal its\nrelation with air-sea heat exchange. Overall, our model provides a framework\nfor understanding MHWs' driving processes and operational forecasts in the\nfuture.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Downscaling Precipitation with Bias-informed Conditional Diffusion Model",
    "url": "http://arxiv.org/abs/2412.14539v1",
    "authors": [
      "Ran Lyu",
      "Linhan Wang",
      "Yanshen Sun",
      "Hedanqiu Bai",
      "Chang-Tien Lu"
    ],
    "published": "2024-12-19",
    "abstract": "Climate change is intensifying rainfall extremes, making high-resolution\nprecipitation projections crucial for society to better prepare for impacts\nsuch as flooding. However, current Global Climate Models (GCMs) operate at\nspatial resolutions too coarse for localized analyses. To address this\nlimitation, deep learning-based statistical downscaling methods offer promising\nsolutions, providing high-resolution precipitation projections with a moderate\ncomputational cost. In this work, we introduce a bias-informed conditional\ndiffusion model for statistical downscaling of precipitation. Specifically, our\nmodel leverages a conditional diffusion approach to learn distribution priors\nfrom large-scale, high-resolution precipitation datasets. The long-tail\ndistribution of precipitation poses a unique challenge for training diffusion\nmodels; to address this, we apply gamma correction during preprocessing.\nAdditionally, to correct biases in the downscaled results, we employ a\nguided-sampling strategy to enhance bias correction. Our experiments\ndemonstrate that the proposed model achieves highly accurate results in an 8\ntimes downscaling setting, outperforming previous deterministic methods. The\ncode and dataset are available at\nhttps://github.com/RoseLV/research_super-resolution",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Generating Unseen Nonlinear Evolution in Sea Surface Temperature Using a Deep Learning-Based Latent Space Data Assimilation Framework",
    "url": "http://arxiv.org/abs/2412.13477v1",
    "authors": [
      "Qingyu Zheng",
      "Guijun Han",
      "Wei Li",
      "Lige Cao",
      "Gongfu Zhou",
      "Haowen Wu",
      "Qi Shao",
      "Ru Wang",
      "Xiaobo Wu",
      "Xudong Cui",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2024-12-18",
    "abstract": "Advances in data assimilation (DA) methods have greatly improved the accuracy\nof Earth system predictions. To fuse multi-source data and reconstruct the\nnonlinear evolution missing from observations, geoscientists are developing\nfuture-oriented DA methods. In this paper, we redesign a purely data-driven\nlatent space DA framework (DeepDA) that employs a generative artificial\nintelligence model to capture the nonlinear evolution in sea surface\ntemperature. Under variational constraints, DeepDA embedded with nonlinear\nfeatures can effectively fuse heterogeneous data. The results show that DeepDA\nremains highly stable in capturing and generating nonlinear evolutions even\nwhen a large amount of observational information is missing. It can be found\nthat when only 10% of the observation information is available, the error\nincrease of DeepDA does not exceed 40%. Furthermore, DeepDA has been shown to\nbe robust in the fusion of real observations and ensemble simulations. In\nparticular, this paper provides a mechanism analysis of the nonlinear evolution\ngenerated by DeepDA from the perspective of physical patterns, which reveals\nthe inherent explainability of our DL model in capturing multi-scale ocean\nsignals.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Staged Deep Learning Approach to Spatial Refinement in 3D Temporal Atmospheric Transport",
    "url": "http://arxiv.org/abs/2412.10945v2",
    "authors": [
      "M. Giselle Fern\u00e1ndez-Godino",
      "Wai Tong Chung",
      "Akshay A. Gowardhan",
      "Matthias Ihme",
      "Qingkai Kong",
      "Donald D. Lucas",
      "Stephen C. Myers"
    ],
    "published": "2024-12-14",
    "abstract": "High-resolution spatiotemporal simulations effectively capture the\ncomplexities of atmospheric plume dispersion in complex terrain. However, their\nhigh computational cost makes them impractical for applications requiring rapid\nresponses or iterative processes, such as optimization, uncertainty\nquantification, or inverse modeling. To address this challenge, this work\nintroduces the Dual-Stage Temporal Three-dimensional UNet Super-resolution\n(DST3D-UNet-SR) model, a highly efficient deep learning model for plume\ndispersion prediction. DST3D-UNet-SR is composed of two sequential modules: the\ntemporal module (TM), which predicts the transient evolution of a plume in\ncomplex terrain from low-resolution temporal data, and the spatial refinement\nmodule (SRM), which subsequently enhances the spatial resolution of the TM\npredictions. We train DST3DUNet- SR using a comprehensive dataset derived from\nhigh-resolution large eddy simulations (LES) of plume transport. We propose the\nDST3D-UNet-SR model to significantly accelerate LES simulations of\nthree-dimensional plume dispersion by three orders of magnitude. Additionally,\nthe model demonstrates the ability to dynamically adapt to evolving conditions\nthrough the incorporation of new observational data, substantially improving\nprediction accuracy in high-concentration regions near the source.\n  Keywords: Atmospheric sciences, Geosciences, Plume transport,3D temporal\nsequences, Artificial intelligence, CNN, LSTM, Autoencoder, Autoregressive\nmodel, U-Net, Super-resolution, Spatial Refinement.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "LSTM",
      "Autoencoder"
    ],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "A Hybrid Deep-Learning Model for El Ni\u00f1o Southern Oscillation in the Low-Data Regime",
    "url": "http://arxiv.org/abs/2412.03743v2",
    "authors": [
      "Jakob Schloer",
      "Matthew Newman",
      "Jannik Thuemmel",
      "Antonietta Capotondi",
      "Bedartha Goswami"
    ],
    "published": "2024-12-04",
    "abstract": "While deep-learning models have demonstrated skillful El Ni\\~no Southern\nOscillation (ENSO) forecasts up to one year in advance, they are predominantly\ntrained on climate model simulations that provide thousands of years of\ntraining data at the expense of introducing climate model biases. Simpler\nLinear Inverse Models (LIMs) trained on the much shorter observational record\nalso make skillful ENSO predictions but do not capture predictable nonlinear\nprocesses. This motivates a hybrid approach, combining the LIMs modest data\nneeds with a deep-learning non-Markovian correction of the LIM. For O(100 yr)\ndatasets, our resulting Hybrid model is more skillful than the LIM while also\nexceeding the skill of a full deep-learning model. Additionally, while the most\npredictable ENSO events are still identified in advance by the LIM, they are\nbetter predicted by the Hybrid model, especially in the western tropical\nPacific for leads beyond about 9 months, by capturing the subsequent asymmetric\n(warm versus cold phases) evolution of ENSO.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Learning Surrogate Rainfall-driven Inundation Models with Few Data",
    "url": "http://arxiv.org/abs/2411.19323v1",
    "authors": [
      "Marzieh Alireza Mirhoseini"
    ],
    "published": "2024-11-28",
    "abstract": "Flood hazard assessment demands fast and accurate predictions. Hydrodynamic\nmodels are detailed but computationally intensive, making them impractical for\nquantifying uncertainty or identifying extremes. In contrast, machine learning\nsurrogates can be rapid, but training on scarce simulated or observed extreme\ndata can also be ineffective. This work demonstrates the development of an\neffective surrogate model for flood hazard prediction by initializing deep\nlearning (ResNet-18) with ensemble-approximated Conditional Gaussian Processes\n(EnsCGP) and finalizing it with a bias correction. The proposed methodology\ncouples EnsCGP with a ResNet-18 architecture to estimate flood depth and uses\nensemble optimal estimation for bias correction. The surrogate model was\ntrained and evaluated using rainfall data from Daymet and hydrodynamic\nsimulations from LISFLOOD-FP, spanning the period from 1981 to 2019. The\ntraining involved using data up to a certain year and testing on the subsequent\nyear, iteratively progressing through the dataset. This process required\napproximately 100 training iterations and extensive data. Inundation depths are\nestimated rapidly at runtime (approximately 0.006 seconds per event). Results\nover multiple years in the current climate over Chicago demonstrate an average\nR-squared greater than 0.96, with median relative errors in flood depth\nestimates of about 1 percent.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Maximizing the Impact of Deep Learning on Subseasonal-to-Seasonal Climate Forecasting: The Essential Role of Optimization",
    "url": "http://arxiv.org/abs/2411.16728v1",
    "authors": [
      "Yizhen Guo",
      "Tian Zhou",
      "Wanyi Jiang",
      "Bo Wu",
      "Liang Sun",
      "Rong Jin"
    ],
    "published": "2024-11-23",
    "abstract": "Weather and climate forecasting is vital for sectors such as agriculture and\ndisaster management. Although numerical weather prediction (NWP) systems have\nadvanced, forecasting at the subseasonal-to-seasonal (S2S) scale, spanning 2 to\n6 weeks, remains challenging due to the chaotic and sparse atmospheric signals\nat this interval. Even state-of-the-art deep learning models struggle to\noutperform simple climatology models in this domain. This paper identifies that\noptimization, instead of network structure, could be the root cause of this\nperformance gap, and then we develop a novel multi-stage optimization strategy\nto close the gap. Extensive empirical studies demonstrate that our multi-stage\noptimization approach significantly improves key skill metrics, PCC and TCC,\nwhile utilizing the same backbone structure, surpassing the state-of-the-art\nNWP systems (ECMWF-S2S) by over \\textbf{19-91\\%}. Our research contests the\nrecent study that direct forecasting outperforms rolling forecasting for S2S\ntasks. Through theoretical analysis, we propose that the underperformance of\nrolling forecasting may arise from the accumulation of Jacobian matrix products\nduring training. Our multi-stage framework can be viewed as a form of teacher\nforcing to address this issue. Code is available at\n\\url{https://anonymous.4open.science/r/Baguan-S2S-23E7/}",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Global spatio-temporal downscaling of ERA5 precipitation through generative AI",
    "url": "http://arxiv.org/abs/2411.16098v1",
    "authors": [
      "Luca Glawion",
      "Julius Polz",
      "Harald Kunstmann",
      "Benjamin Fersch",
      "Christian Chwala"
    ],
    "published": "2024-11-22",
    "abstract": "The spatial and temporal distribution of precipitation has a significant\nimpact on human lives by determining freshwater resources and agricultural\nyield, but also rainfall-driven hazards like flooding or landslides. While the\nERA5 reanalysis dataset provides consistent long-term global precipitation\ninformation that allows investigations of these impacts, it lacks the\nresolution to capture the high spatio-temporal variability of precipitation.\nERA5 misses intense local rainfall events that are crucial drivers of\ndevastating flooding - a critical limitation since extreme weather events\nbecome increasingly frequent. Here, we introduce spateGAN-ERA5, the first deep\nlearning based spatio-temporal downscaling of precipitation data on a global\nscale. SpateGAN-ERA5 uses a conditional generative adversarial neural network\n(cGAN) that enhances the resolution of ERA5 precipitation data from 24 km and 1\nhour to 2 km and 10 minutes, delivering high-resolution rainfall fields with\nrealistic spatio-temporal patterns and accurate rain rate distribution\nincluding extremes. Its computational efficiency enables the generation of a\nlarge ensemble of solutions, addressing uncertainties inherent to the\nchallenges of downscaling. Trained solely on data from Germany and validated in\nthe US and Australia considering diverse climate zones, spateGAN-ERA5\ndemonstrates strong generalization indicating a robust global applicability.\nSpateGAN-ERA5 fulfils a critical need for high-resolution precipitation data in\nhydrological and meteorological research, offering new capabilities for flood\nrisk assessment, AI-enhanced weather forecasting, and impact modelling to\naddress climate-driven challenges worldwide.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Leadsee-Precip: A Deep Learning Diagnostic Model for Precipitation",
    "url": "http://arxiv.org/abs/2411.12640v1",
    "authors": [
      "Weiwen Ji",
      "Jin Feng",
      "Yueqi Liu",
      "Yulu Qiu",
      "Hua Gao"
    ],
    "published": "2024-11-19",
    "abstract": "Recently, deep-learning weather forecasting models have surpassed traditional\nnumerical models in terms of the accuracy of meteorological variables. However,\nthere is considerable potential for improvements in precipitation forecasts,\nespecially for heavy precipitation events. To address this deficiency, we\npropose Leadsee-Precip, a global deep learning model to generate precipitation\nfrom meteorological circulation fields. The model utilizes an information\nbalance scheme to tackle the challenges of predicting heavy precipitation\ncaused by the long-tail distribution of precipitation data. Additionally, more\naccurate satellite and radar-based precipitation retrievals are used as\ntraining targets. Compared to artificial intelligence global weather models,\nthe heavy precipitation from Leadsee-Precip is more consistent with\nobservations and shows competitive performance against global numerical weather\nprediction models. Leadsee-Precip can be integrated with any global circulation\nmodel to generate precipitation forecasts. But the deviations between the\npredicted and the ground-truth circulation fields may lead to a weakened\nprecipitation forecast, which could potentially be mitigated by further\nfine-tuning based on the predicted circulation fields.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Advancing Marine Heatwave Forecasts: An Integrated Deep Learning Approach",
    "url": "http://arxiv.org/abs/2412.04475v1",
    "authors": [
      "Ding Ning",
      "Varvara Vetrova",
      "Yun Sing Koh",
      "Karin R. Bryan"
    ],
    "published": "2024-11-19",
    "abstract": "Marine heatwaves (MHWs), an extreme climate phenomenon, pose significant\nchallenges to marine ecosystems and industries, with their frequency and\nintensity increasing due to climate change. This study introduces an integrated\ndeep learning approach to forecast short-to-long-term MHWs on a global scale.\nThe approach combines graph representation for modeling spatial properties in\nclimate data, imbalanced regression to handle skewed data distributions, and\ntemporal diffusion to enhance forecast accuracy across various lead times. To\nthe best of our knowledge, this is the first study that synthesizes three\nspatiotemporal anomaly methodologies to predict MHWs. Additionally, we\nintroduce a method for constructing graphs that avoids isolated nodes and\nprovide a new publicly available sea surface temperature anomaly graph dataset.\nWe examine the trade-offs in the selection of loss functions and evaluation\nmetrics for MHWs. We analyze spatial patterns in global MHW predictability by\nfocusing on historical hotspots, and our approach demonstrates better\nperformance compared to traditional numerical models in regions such as the\nmiddle south Pacific, equatorial Atlantic near Africa, south Atlantic, and\nhigh-latitude Indian Ocean. We highlight the potential of temporal diffusion to\nreplace the conventional sliding window approach for long-term forecasts,\nachieving improved prediction up to six months in advance. These insights not\nonly establish benchmarks for machine learning applications in MHW forecasting\nbut also enhance understanding of general climate forecasting methodologies.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "FengWu-W2S: A deep learning model for seamless weather-to-subseasonal forecast of global atmosphere",
    "url": "http://arxiv.org/abs/2411.10191v2",
    "authors": [
      "Fenghua Ling",
      "Kang Chen",
      "Jiye Wu",
      "Tao Han",
      "Jing-Jia Luo",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "published": "2024-11-15",
    "abstract": "Seamless forecasting that produces warning information at continuum\ntimescales based on only one system is a long-standing pursuit for\nweather-climate service. While the rapid advancement of deep learning has\ninduced revolutionary changes in classical forecasting field, current efforts\nare still focused on building separate AI models for weather and climate\nforecasts. To explore the seamless forecasting ability based on one AI model,\nwe propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the\nFengWu global weather forecast model and incorporates an ocean-atmosphere-land\ncoupling structure along with a diverse perturbation strategy. FengWu-W2S can\ngenerate 6-hourly atmosphere forecasts extending up to 42 days through an\nautoregressive and seamless manner. Our hindcast results demonstrate that\nFengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead,\nenhancing predictive capabilities for global surface air temperature,\nprecipitation, geopotential height and intraseasonal signals such as the\nMadden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover,\nour ablation experiments on forecast error growth from daily to seasonal\ntimescales reveal potential pathways for developing AI-based integrated system\nfor seamless weather-climate forecasting in the future.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "An Analysis of Deep Learning Parameterizations for Ocean Subgrid Eddy Forcing",
    "url": "http://arxiv.org/abs/2411.06604v1",
    "authors": [
      "Cem Gultekin",
      "Adam Subel",
      "Cheng Zhang",
      "Matan Leibovich",
      "Pavel Perezhogin",
      "Alistair Adcroft",
      "Carlos Fernandez-Granda",
      "Laure Zanna"
    ],
    "published": "2024-11-10",
    "abstract": "Due to computational constraints, climate simulations cannot resolve a range\nof small-scale physical processes, which have a significant impact on the\nlarge-scale evolution of the climate system. Parameterization is an approach to\ncapture the effect of these processes, without resolving them explicitly. In\nrecent years, data-driven parameterizations based on convolutional neural\nnetworks have obtained promising results. In this work, we provide an in-depth\nanalysis of these parameterizations developed using data from ocean\nsimulations. The parametrizations account for the effect of mesoscale eddies\ntoward improving simulations of momentum, heat, and mass exchange in the ocean.\nOur results provide several insights into the properties of data-driven\nparameterizations based on neural networks. First, their performance can be\nsubstantially improved by increasing the geographic extent of the training\ndata. Second, they learn nonlinear structure, since they are able to outperform\na linear baseline. Third, they generalize robustly across different CO2\nforcings, but not necessarily across different ocean depths. Fourth, they\nexploit a relatively small region of their input to generate their output. Our\nresults will guide the further development of ocean mesoscale eddy\nparameterizations, and multiscale modeling more generally.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Are Deep Learning Methods Suitable for Downscaling Global Climate Projections? Review and Intercomparison of Existing Models",
    "url": "http://arxiv.org/abs/2411.05850v1",
    "authors": [
      "Jose Gonz\u00e1lez-Abad",
      "Jos\u00e9 Manuel Guti\u00e9rrez"
    ],
    "published": "2024-11-06",
    "abstract": "Deep Learning (DL) has shown promise for downscaling global climate change\nprojections under different approaches, including Perfect Prognosis (PP) and\nRegional Climate Model (RCM) emulation. Unlike emulators, PP downscaling models\nare trained on observational data, so it remains an open question whether they\ncan plausibly extrapolate unseen conditions and changes in future emissions\nscenarios. Here we focus on this problem as the main drawback for the\noperationalization of these methods and present the results of 1) a literature\nreview to identify state-of-the-art DL models for PP downscaling and 2) an\nintercomparison experiment to evaluate the performance of these models and to\nassess their extrapolation capability using a common experimental framework,\ntaking into account the sensitivity of results to different training replicas.\nWe focus on minimum and maximum temperatures and precipitation over Spain, a\nregion with a range of climatic conditions with different influential regional\nprocesses. We conclude with a discussion of the findings, limitations of\nexisting methods, and prospects for future development.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "PACER: Physics Informed Uncertainty Aware Climate Emulator",
    "url": "http://arxiv.org/abs/2410.21657v2",
    "authors": [
      "Hira Saleem",
      "Flora Salim",
      "Cormac Purcell"
    ],
    "published": "2024-10-29",
    "abstract": "Climate models serve as critical tools for evaluating the effects of climate\nchange and projecting future climate scenarios. However, the reliance on\nnumerical simulations of physical equations renders them computationally\nintensive and inefficient. While deep learning methodologies have made\nsignificant progress in weather forecasting, they are still unstable for\nclimate emulation tasks. Here, we propose PACER, a lightweight 684K parameter\nPhysics Informed Uncertainty Aware Climate Emulator. PACER emulates temperature\nand precipitation stably for 86 years while only being trained on greenhouse\ngas emissions data. We incorporate a fundamental physical law of\nadvection-diffusion in PACER accounting for boundary conditions and empirically\nestimating the diffusion co-efficient and flow velocities from emissions data.\nPACER has been trained on 15 climate models provided by ClimateSet\noutperforming baselines across most of the climate models and advancing a new\nstate of the art in a climate diagnostic task.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Modulated Adaptive Fourier Neural Operators for Temporal Interpolation of Weather Forecasts",
    "url": "http://arxiv.org/abs/2410.18904v1",
    "authors": [
      "Jussi Leinonen",
      "Boris Bonev",
      "Thorsten Kurth",
      "Yair Cohen"
    ],
    "published": "2024-10-24",
    "abstract": "Weather and climate data are often available at limited temporal resolution,\neither due to storage limitations, or in the case of weather forecast models\nbased on deep learning, their inherently long time steps. The coarse temporal\nresolution makes it difficult to capture rapidly evolving weather events. To\naddress this limitation, we introduce an interpolation model that reconstructs\nthe atmospheric state between two points in time for which the state is known.\nThe model makes use of a novel network layer that modifies the adaptive Fourier\nneural operator (AFNO), which has been previously used in weather prediction\nand other applications of machine learning to physics problems. The modulated\nAFNO (ModAFNO) layer takes an embedding, here computed from the interpolation\ntarget time, as an additional input and applies a learned shift-scale operation\ninside the AFNO layers to adapt them to the target time. Thus, one model can be\nused to produce all intermediate time steps. Trained to interpolate between two\ntime steps 6 h apart, the ModAFNO-based interpolation model produces 1 h\nresolution intermediate time steps that are visually nearly indistinguishable\nfrom the actual corresponding 1 h resolution data. The model reduces the RMSE\nloss of reconstructing the intermediate steps by approximately 50% compared to\nlinear interpolation. We also demonstrate its ability to reproduce the\nstatistics of extreme weather events such as hurricanes and heat waves better\nthan 6 h resolution data. The ModAFNO layer is generic and is expected to be\napplicable to other problems, including weather forecasting with tunable lead\ntime.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Evaluating Deep Learning Approaches for Predictions in Unmonitored Basins with Continental-scale Stream Temperature Models",
    "url": "http://arxiv.org/abs/2410.19865v1",
    "authors": [
      "Jared D. Willard",
      "Fabio Ciulla",
      "Helen Weierbach",
      "Vipin Kumar",
      "Charuleka Varadharajan"
    ],
    "published": "2024-10-23",
    "abstract": "The prediction of streamflows and other environmental variables in\nunmonitored basins is a grand challenge in hydrology. Recent machine learning\n(ML) models can harness vast datasets for accurate predictions at large spatial\nscales. However, there are open questions regarding model design and data\nneeded for inputs and training to improve performance. This study explores\nthese questions while demonstrating the ability of deep learning models to make\naccurate stream temperature predictions in unmonitored basins across the\nconterminous United States. First, we compare top-down models that utilize data\nfrom a large number of basins with bottom-up methods that transfer ML models\nbuilt on local sites, reflecting traditional regionalization techniques. We\nalso evaluate an intermediary grouped modeling approach that categorizes sites\nbased on regional co-location or similarity of catchment characteristics.\nSecond, we evaluate trade-offs between model complexity, prediction accuracy,\nand applicability for more target locations by systematically removing inputs.\nWe then examine model performance when additional training data becomes\navailable due to reductions in input requirements. Our results suggest that\ntop-down models significantly outperform bottom-up and grouped models.\nMoreover, it is possible to get acceptable accuracy by reducing both dynamic\nand static inputs enabling predictions for more sites with lower model\ncomplexity and computational needs. From detailed error analysis, we determined\nthat the models are more accurate for sites primarily controlled by air\ntemperatures compared to locations impacted by groundwater and dams. By\naddressing these questions, this research offers a comprehensive perspective on\noptimizing ML model design for accurate predictions in unmonitored regions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark",
    "url": "http://arxiv.org/abs/2410.16695v2",
    "authors": [
      "Yang Yu",
      "Yuezun Li",
      "Xin Sun",
      "Junyu Dong"
    ],
    "published": "2024-10-22",
    "abstract": "Phytoplankton are a crucial component of aquatic ecosystems, and effective\nmonitoring of them can provide valuable insights into ocean environments and\necosystem changes. Traditional phytoplankton monitoring methods are often\ncomplex and lack timely analysis. Therefore, deep learning algorithms offer a\npromising approach for automated phytoplankton monitoring. However, the lack of\nlarge-scale, high-quality training samples has become a major bottleneck in\nadvancing phytoplankton tracking. In this paper, we propose a challenging\nbenchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse\nbackground information and variations in motion during observation. The dataset\nincludes 27 species of phytoplankton and zooplankton, 14 different backgrounds\nto simulate diverse and complex underwater environments, and a total of 140\nvideos. To enable accurate real-time observation of phytoplankton, we introduce\na multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion\nTracker(DSFT), which addresses issues such as focus shifts during tracking and\nthe loss of small target information when computing frame-to-frame similarity.\nSpecifically, we introduce an additional feature extractor to predict the\nresiduals of the standard feature extractor's output, and compute multi-scale\nframe-to-frame similarity based on features from different layers of the\nextractor. Extensive experiments on the MPT have demonstrated the validity of\nthe dataset and the superiority of DSFT in tracking phytoplankton, providing an\neffective solution for phytoplankton monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Deep Learning for Weather Forecasting: A CNN-LSTM Hybrid Model for Predicting Historical Temperature Data",
    "url": "http://arxiv.org/abs/2410.14963v1",
    "authors": [
      "Yuhao Gong",
      "Yuchen Zhang",
      "Fei Wang",
      "Chi-Han Lee"
    ],
    "published": "2024-10-19",
    "abstract": "As global climate change intensifies, accurate weather forecasting has become\nincreasingly important, affecting agriculture, energy management, environmental\nprotection, and daily life. This study introduces a hybrid model combining\nConvolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks\nto predict historical temperature data. CNNs are utilized for spatial feature\nextraction, while LSTMs handle temporal dependencies, resulting in\nsignificantly improved prediction accuracy and stability. By using Mean\nAbsolute Error (MAE) as the loss function, the model demonstrates excellent\nperformance in processing complex meteorological data, addressing challenges\nsuch as missing data and high-dimensionality. The results show a strong\nalignment between the prediction curve and test data, validating the model's\npotential in climate prediction. This study offers valuable insights for fields\nsuch as agriculture, energy management, and urban planning, and lays the\ngroundwork for future applications in weather forecasting under the context of\nglobal climate change.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Accelerate Coastal Ocean Circulation Model with AI Surrogate",
    "url": "http://arxiv.org/abs/2410.14952v2",
    "authors": [
      "Zelin Xu",
      "Jie Ren",
      "Yupu Zhang",
      "Jose Maria Gonzalez Ondina",
      "Maitane Olabarrieta",
      "Tingsong Xiao",
      "Wenchong He",
      "Zibo Liu",
      "Shigang Chen",
      "Kaleb Smith",
      "Zhe Jiang"
    ],
    "published": "2024-10-19",
    "abstract": "Nearly 900 million people live in low-lying coastal zones around the world\nand bear the brunt of impacts from more frequent and severe hurricanes and\nstorm surges. Oceanographers simulate ocean current circulation along the\ncoasts to develop early warning systems that save lives and prevent loss and\ndamage to property from coastal hazards. Traditionally, such simulations are\nconducted using coastal ocean circulation models such as the Regional Ocean\nModeling System (ROMS), which usually runs on an HPC cluster with multiple CPU\ncores. However, the process is time-consuming and energy expensive. While\ncoarse-grained ROMS simulations offer faster alternatives, they sacrifice\ndetail and accuracy, particularly in complex coastal environments. Recent\nadvances in deep learning and GPU architecture have enabled the development of\nfaster AI (neural network) surrogates. This paper introduces an AI surrogate\nbased on a 4D Swin Transformer to simulate coastal tidal wave propagation in an\nestuary for both hindcast and forecast (up to 12 days). Our approach not only\naccelerates simulations but also incorporates a physics-based constraint to\ndetect and correct inaccurate results, ensuring reliability while minimizing\nmanual intervention. We develop a fully GPU-accelerated workflow, optimizing\nthe model training and inference pipeline on NVIDIA DGX-2 A100 GPUs. Our\nexperiments demonstrate that our AI surrogate reduces the time cost of 12-day\nforecasting of traditional ROMS simulations from 9,908 seconds (on 512 CPU\ncores) to 22 seconds (on one A100 GPU), achieving over 450$\\times$ speedup\nwhile maintaining high-quality simulation results. This work contributes to\noceanographic modeling by offering a fast, accurate, and physically consistent\nalternative to traditional simulation models, particularly for real-time\nforecasting in rapid disaster response.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "TCP-Diffusion: A Multi-modal Diffusion Model for Global Tropical Cyclone Precipitation Forecasting with Change Awareness",
    "url": "http://arxiv.org/abs/2410.13175v1",
    "authors": [
      "Cheng Huang",
      "Pan Mu",
      "Cong Bai",
      "Peter AG Watson"
    ],
    "published": "2024-10-17",
    "abstract": "Precipitation from tropical cyclones (TCs) can cause disasters such as\nflooding, mudslides, and landslides. Predicting such precipitation in advance\nis crucial, giving people time to prepare and defend against these\nprecipitation-induced disasters. Developing deep learning (DL) rainfall\nprediction methods offers a new way to predict potential disasters. However,\none problem is that most existing methods suffer from cumulative errors and\nlack physical consistency. Second, these methods overlook the importance of\nmeteorological factors in TC rainfall and their integration with the numerical\nweather prediction (NWP) model. Therefore, we propose Tropical Cyclone\nPrecipitation Diffusion (TCP-Diffusion), a multi-modal model for global\ntropical cyclone precipitation forecasting. It forecasts TC rainfall around the\nTC center for the next 12 hours at 3 hourly resolution based on past rainfall\nobservations and multi-modal environmental variables. Adjacent residual\nprediction (ARP) changes the training target from the absolute rainfall value\nto the rainfall trend and gives our model the ability of rainfall change\nawareness, reducing cumulative errors and ensuring physical consistency.\nConsidering the influence of TC-related meteorological factors and the useful\ninformation from NWP model forecasts, we propose a multi-model framework with\nspecialized encoders to extract richer information from environmental variables\nand results provided by NWP models. The results of extensive experiments show\nthat our method outperforms other DL methods and the NWP method from the\nEuropean Centre for Medium-Range Weather Forecasts (ECMWF).",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SIFM: A Foundation Model for Multi-granularity Arctic Sea Ice Forecasting",
    "url": "http://arxiv.org/abs/2410.14732v1",
    "authors": [
      "Jingyi Xu",
      "Yeqi Luo",
      "Weidong Yang",
      "Keyi Liu",
      "Shengnan Wang",
      "Ben Fei",
      "Lei Bai"
    ],
    "published": "2024-10-16",
    "abstract": "Arctic sea ice performs a vital role in global climate and has paramount\nimpacts on both polar ecosystems and coastal communities. In the last few\nyears, multiple deep learning based pan-Arctic sea ice concentration (SIC)\nforecasting methods have emerged and showcased superior performance over\nphysics-based dynamical models. However, previous methods forecast SIC at a\nfixed temporal granularity, e.g. sub-seasonal or seasonal, thus only leveraging\ninter-granularity information and overlooking the plentiful inter-granularity\ncorrelations. SIC at various temporal granularities exhibits cumulative effects\nand are naturally consistent, with short-term fluctuations potentially\nimpacting long-term trends and long-term trends provides effective hints for\nfacilitating short-term forecasts in Arctic sea ice. Therefore, in this study,\nwe propose to cultivate temporal multi-granularity that naturally derived from\nArctic sea ice reanalysis data and provide a unified perspective for modeling\nSIC via our Sea Ice Foundation Model. SIFM is delicately designed to leverage\nboth intra-granularity and inter-granularity information for capturing\ngranularity-consistent representations that promote forecasting skills. Our\nextensive experiments show that SIFM outperforms off-the-shelf deep learning\nmodels for their specific temporal granularity.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MAX: Masked Autoencoder for X-ray Fluorescence in Geological Investigation",
    "url": "http://arxiv.org/abs/2410.12330v1",
    "authors": [
      "An-Sheng Lee",
      "Yu-Wen Pao",
      "Hsuan-Tien Lin",
      "Sofia Ya Hsuan Liou"
    ],
    "published": "2024-10-16",
    "abstract": "Pre-training foundation models has become the de-facto procedure for deep\nlearning approaches, yet its application remains limited in the geological\nstudies, where in needs of the model transferability to break the shackle of\ndata scarcity. Here we target on the X-ray fluorescence (XRF) scanning data, a\nstandard high-resolution measurement in extensive scientific drilling projects.\nWe propose a scalable self-supervised learner, masked autoencoders on XRF\nspectra (MAX), to pre-train a foundation model covering geological records from\nmultiple regions of the Pacific and Southern Ocean. In pre-training, we find\nthat masking a high proportion of the input spectrum (50\\%) yields a nontrivial\nand meaningful self-supervisory task. For downstream tasks, we select the\nquantification of XRF spectra into two costly geochemical measurements,\nCaCO$_3$ and total organic carbon, due to their importance in understanding the\npaleo-oceanic carbon system. Our results show that MAX, requiring only\none-third of the data, outperforms models without pre-training in terms of\nquantification accuracy. Additionally, the model's generalizability improves by\nmore than 60\\% in zero-shot tests on new materials, with explainability further\nensuring its robustness. Thus, our approach offers a promising pathway to\novercome data scarcity in geological discovery by leveraging the\nself-supervised foundation model and fast-acquired XRF scanning data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "IceDiff: High Resolution and High-Quality Sea Ice Forecasting with Generative Diffusion Prior",
    "url": "http://arxiv.org/abs/2410.09111v1",
    "authors": [
      "Jingyi Xu",
      "Siwei Tu",
      "Weidong Yang",
      "Shuhao Li",
      "Keyi Liu",
      "Yeqi Luo",
      "Lipeng Ma",
      "Ben Fei",
      "Lei Bai"
    ],
    "published": "2024-10-10",
    "abstract": "Variation of Arctic sea ice has significant impacts on polar ecosystems,\ntransporting routes, coastal communities, and global climate. Tracing the\nchange of sea ice at a finer scale is paramount for both operational\napplications and scientific studies. Recent pan-Arctic sea ice forecasting\nmethods that leverage advances in artificial intelligence has made promising\nprogress over numerical models. However, forecasting sea ice at higher\nresolutions is still under-explored. To bridge the gap, we propose a two-staged\ndeep learning framework, IceDiff, to forecast sea ice concentration at finer\nscales. IceDiff first leverages an independently trained vision transformer to\ngenerate coarse yet superior forecasting over previous methods at a regular\n25km x 25km grid. This high-quality sea ice forecasting can be utilized as\nreliable guidance for the next stage. Subsequently, an unconditional diffusion\nmodel pre-trained on sea ice concentration maps is utilized for sampling\ndown-scaled sea ice forecasting via a zero-shot guided sampling strategy and a\npatch-based method. For the first time, IceDiff demonstrates sea ice\nforecasting with the 6.25km x 6.25km resolution. IceDiff extends the boundary\nof existing sea ice forecasting models and more importantly, its capability to\ngenerate high-resolution sea ice concentration data is vital for pragmatic\nusages and research.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Improved deep learning of chaotic dynamical systems with multistep penalty losses",
    "url": "http://arxiv.org/abs/2410.05572v1",
    "authors": [
      "Dibyajyoti Chakraborty",
      "Seung Whan Chung",
      "Ashesh Chattopadhyay",
      "Romit Maulik"
    ],
    "published": "2024-10-08",
    "abstract": "Predicting the long-term behavior of chaotic systems remains a formidable\nchallenge due to their extreme sensitivity to initial conditions and the\ninherent limitations of traditional data-driven modeling approaches. This paper\nintroduces a novel framework that addresses these challenges by leveraging the\nrecently proposed multi-step penalty (MP) optimization technique. Our approach\nextends the applicability of MP optimization to a wide range of deep learning\narchitectures, including Fourier Neural Operators and UNETs. By introducing\npenalized local discontinuities in the forecast trajectory, we effectively\nhandle the non-convexity of loss landscapes commonly encountered in training\nneural networks for chaotic systems. We demonstrate the effectiveness of our\nmethod through its application to two challenging use-cases: the prediction of\nflow velocity evolution in two-dimensional turbulence and ocean dynamics using\nreanalysis data. Our results highlight the potential of this approach for\naccurate and stable long-term prediction of chaotic dynamics, paving the way\nfor new advancements in data-driven modeling of complex natural phenomena.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Predictability of Global AI Weather Models",
    "url": "http://arxiv.org/abs/2410.03266v1",
    "authors": [
      "Chanh Kieu"
    ],
    "published": "2024-10-04",
    "abstract": "This study examines the predictability of artificial intelligence (AI) models\nfor weather prediction. Using a simple deep-learning architecture based on\nconvolutional long short-term memory and the ERA5 data for training, we show\nthat different time-stepping techniques can have a strong influence on the\nmodel performance and weather predictability. Specifically, a small-step\napproach for which the future state is predicted by recursively iterating an AI\nmodel over a small time increment displays strong sensitivity to the type of\ninput channels, the number of data frames, or forecast lead times. In contrast,\na big-step approach for which a current state is directly projected to a future\nstate at each corresponding lead time provides much better forecast skill and a\nlonger predictability range. In particular, the big-step approach is very\nresilient to different input channels, or data frames. In this regard, our\nresults present a different method for implementing global AI models for\nweather prediction, which can optimize the model performance even with minimum\ninput channels or data frames.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Anti-biofouling Lensless Camera System with Deep Learning based Image Reconstruction",
    "url": "http://arxiv.org/abs/2410.01365v1",
    "authors": [
      "Naoki Ide",
      "Tomohiro Kawahara",
      "Hiroshi Ueno",
      "Daiki Yanagidaira",
      "Susumu Takatsuka"
    ],
    "published": "2024-10-02",
    "abstract": "In recent years, there has been an increasing demand for underwater cameras\nthat monitor the condition of offshore structures and check the number of\nindividuals in aqua culture environments with long-period observation. One of\nthe significant issues with this observation is that biofouling sticks to the\naperture and lens densely and prevents cameras from capturing clear images.\nThis study examines an underwater camera that applies material technologies\nwith high inherent resistance to biofouling and computer vision technologies\nbased on image reconstruction by deep learning to lens-less cameras. For this\npurpose, our prototype camera uses a coded aperture with 1k rectangular shape\npinholes in a thin metal plate, such as copper, which hinder the growth of\nbiofouling and keep the surface clean. Although images taken by lens-less\ncameras are usually not well formed due to lack of the traditional glass-based\nlens, a deep learning approach using ViT (Vision Transformer) has recently\ndemonstrated reconstructing original photo images well and our study shows that\nusing gated MLP (Multilayer Perceptron) also yields good results. On the other\nhand, a certain degree of thickness for bio-repellence materials is required to\nexhibit their effect the thickness of aperture is necessary to use apertures\nsufficiently thinner than the size of the pinholes to avoid unintentional\nreflection and absorption on the sidewalls. Therefore, we prepared a\nsufficiently thin plate for image reconstruction and now currently we conduct\ntests of the lens-less camera of the bio-repellence aperture with actual\nseawater environments to determine whether it can sufficiently demonstrate the\nbiofouling effect compared with usual camera with only waterproof.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Pose Estimation of Buried Deep-Sea Objects using 3D Vision Deep Learning Models",
    "url": "http://arxiv.org/abs/2410.01061v2",
    "authors": [
      "Jerry Yan",
      "Chinmay Talegaonkar",
      "Nicholas Antipa",
      "Eric Terrill",
      "Sophia Merrifield"
    ],
    "published": "2024-10-01",
    "abstract": "We present an approach for pose and burial fraction estimation of debris\nfield barrels found on the seabed in the Southern California San Pedro Basin.\nOur computational workflow leverages recent advances in foundation models for\nsegmentation and a vision transformer-based approach to estimate the point\ncloud which defines the geometry of the barrel. We propose BarrelNet for\nestimating the 6-DOF pose and radius of buried barrels from the barrel point\nclouds as input. We train BarrelNet using synthetically generated barrel point\nclouds, and qualitatively demonstrate the potential of our approach using\nremotely operated vehicle (ROV) video footage of barrels found at a historic\ndump site. We compare our method to a traditional least squares fitting\napproach and show significant improvement according to our defined benchmarks.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Advancing Spatio-temporal Storm Surge Prediction with Hierarchical Deep Neural Networks",
    "url": "http://arxiv.org/abs/2410.12823v1",
    "authors": [
      "Saeed Saviz Naeini",
      "Reda Snaiki",
      "Teng Wu"
    ],
    "published": "2024-10-01",
    "abstract": "Coastal regions in North America face major threats from storm surges caused\nby hurricanes and nor'easters. Traditional numerical models, while accurate,\nare computationally expensive, limiting their practicality for real-time\npredictions. Recently, deep learning techniques have been developed for\nefficient simulation of time-dependent storm surge. To resolve the small scales\nof storm surge in both time and space over a long duration and a large area,\nthese simulations typically need to employ oversized neural networks that\nstruggle with the accumulation of prediction errors over successive time steps.\nTo address these challenges, this study introduces a hierarchical deep neural\nnetwork (HDNN) combined with a convolutional autoencoder (CAE) to accurately\nand efficiently predict storm surge time series. The CAE reduces the\ndimensionality of storm surge data, streamlining the learning process. HDNNs\nthen map storm parameters to the low-dimensional representation of storm surge,\nallowing for sequential predictions across different time scales. Specifically,\nthe current-level neural network is utilized to predict future states with a\nrelatively large time step, which are passed as inputs to the next-level neural\nnetwork for smaller time-step predictions. This process continues sequentially\nfor all time steps. The results from different-level neural networks across\nvarious time steps are then stacked to acquire the entire time series of storm\nsurge. The simulated low-dimensional representations are finally decoded back\ninto storm surge time series. The proposed model was trained and tested using\nsynthetic data from the North Atlantic Comprehensive Coastal Study. Results\ndemonstrate its excellent performance to effectively handle high-dimensional\nsurge data while mitigating the accumulation of prediction errors over time,\nmaking it a promising tool for advancing storm surge prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Inferring Thunderstorm Occurrence from Vertical Profiles of Convection-Permitting Simulations: Physical Insights from a Physical Deep Learning Model",
    "url": "http://arxiv.org/abs/2409.20087v3",
    "authors": [
      "Kianusch Vahid Yousefnia",
      "Christoph Metzl",
      "Tobias B\u00f6lle"
    ],
    "published": "2024-09-30",
    "abstract": "Thunderstorms have significant social and economic impacts due to heavy\nprecipitation, hail, lightning, and strong winds, necessitating reliable\nforecasts. Thunderstorm forecasts based on numerical weather prediction (NWP)\noften rely on single-level surrogate predictors, like convective available\npotential energy and convective inhibition, derived from vertical profiles of\nthree-dimensional atmospheric variables. In this study, we develop SALAMA 1D, a\ndeep neural network which directly infers the probability of thunderstorm\noccurrence from vertical profiles of ten atmospheric variables, bypassing\nsingle-level predictors. By training the model on convection-permitting NWP\nforecasts, we allow SALAMA 1D to flexibly identify convective patterns, with\nthe goal of enhancing forecast accuracy. The model's architecture is physically\nmotivated: sparse connections encourage interactions at similar height levels\nwhile keeping model size and inference times computationally efficient, whereas\na shuffling mechanism prevents the model from learning non-physical patterns\ntied to the vertical grid. SALAMA 1D is trained over Central Europe with\nlightning observations as the ground truth. Comparative analysis against a\nbaseline machine learning model that uses single-level predictors shows SALAMA\n1D's superior skill across various metrics and lead times of up to at least 11\nhours. Moreover, expanding the archive of forecasts from which training\nexamples are sampled improves skill, even when training set size remains\nconstant. Finally, a sensitivity analysis using saliency maps indicates that\nour model relies on physically interpretable patterns consistent with\nestablished theoretical understanding, such as ice particle content near the\ntropopause, cloud cover, conditional instability, and low-level moisture.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Center-fixing of tropical cyclones using uncertainty-aware deep learning applied to high-temporal-resolution geostationary satellite imagery",
    "url": "http://arxiv.org/abs/2409.16507v2",
    "authors": [
      "Ryan Lagerquist",
      "Galina Chirokova",
      "Robert DeMaria",
      "Mark DeMaria",
      "Imme Ebert-Uphoff"
    ],
    "published": "2024-09-24",
    "abstract": "Determining the location of a tropical cyclone's (TC) surface circulation\ncenter -- \"center-fixing\" -- is a critical first step in the TC-forecasting\nprocess, affecting current and future estimates of track, intensity, and\nstructure. Despite a recent increase in automated center-fixing methods, only\none such method (ARCHER-2) is operational, and its best performance is achieved\nwhen using microwave or scatterometer data, which are not available at every\nforecast cycle. We develop a deep-learning algorithm called GeoCenter; besides\na few scalars in the operational ATCF, it relies only on geostationary IR\nsatellite imagery, which is available for all TC basins at high frequency (10\nmin) and low latency (< 10 min) during both day and night. GeoCenter ingests an\nanimation (time series) of IR images, including 9 channels at lag times up to 4\nhours. The animation is centered at a \"first guess\" location, offset from the\ntrue TC-center location by 48 km on average and sometimes > 100 km; GeoCenter\nis tasked with correcting this offset. On an independent testing dataset,\nGeoCenter achieves a mean/median/RMS (root mean square) error of 26.6/22.2/32.4\nkm for all systems, 24.7/20.8/30.0 km for tropical systems, and 14.6/12.5/17.3\nkm for category-2--5 hurricanes. These values are similar to ARCHER-2 errors\nwith microwave or scatterometer data, and better than ARCHER-2 errors when only\nIR data are available. GeoCenter also performs skillful uncertainty\nquantification, producing a well calibrated ensemble of 150 TC-center\nlocations. Furthermore, all predictors used by GeoCenter are available in real\ntime, which would make GeoCenter easy to implement operationally every 10 min.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Deep Learning Earth System Model for Efficient Simulation of the Observed Climate",
    "url": "http://arxiv.org/abs/2409.16247v2",
    "authors": [
      "Nathaniel Cresswell-Clay",
      "Bowen Liu",
      "Dale Durran",
      "Zihui Liu",
      "Zachary I. Espinosa",
      "Raul Moreno",
      "Matthias Karlbauer"
    ],
    "published": "2024-09-24",
    "abstract": "A key challenge for computationally intensive state-of-the-art Earth System\nmodels is to distinguish global warming signals from interannual variability.\nHere we introduce DLESyM, a parsimonious deep learning model that accurately\nsimulates the Earth's current climate over 1000-year periods with no smoothing\nor drift. DLESyM simulations equal or exceed key metrics of seasonal and\ninterannual variability--such as tropical cyclogenesis over the range of\nobserved intensities, the cycle of the Indian Summer monsoon, and the\nclimatology of mid-latitude blocking events--when compared to historical\nsimulations from four leading models from the 6th Climate Model Intercomparison\nProject. DLESyM, trained on both historical reanalysis data and satellite\nobservations, is an accurate, highly efficient model of the coupled Earth\nsystem, empowering long-range sub-seasonal and seasonal forecasts while using a\nfraction of the energy and computational time required by traditional models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A competitive baseline for deep learning enhanced data assimilation using conditional Gaussian ensemble Kalman filtering",
    "url": "http://arxiv.org/abs/2409.14300v1",
    "authors": [
      "Zachariah Malik",
      "Romit Maulik"
    ],
    "published": "2024-09-22",
    "abstract": "Ensemble Kalman Filtering (EnKF) is a popular technique for data\nassimilation, with far ranging applications. However, the vanilla EnKF\nframework is not well-defined when perturbations are nonlinear. We study two\nnon-linear extensions of the vanilla EnKF - dubbed the conditional-Gaussian\nEnKF (CG-EnKF) and the normal score EnKF (NS-EnKF) - which sidestep assumptions\nof linearity by constructing the Kalman gain matrix with the `conditional\nGaussian' update formula in place of the traditional one. We then compare these\nmodels against a state-of-the-art deep learning based particle filter called\nthe score filter (SF). This model uses an expensive score diffusion model for\nestimating densities and also requires a strong assumption on the perturbation\noperator for validity. In our comparison, we find that CG-EnKF and NS-EnKF\ndramatically outperform SF for a canonical problem in high-dimensional\nmultiscale data assimilation given by the Lorenz-96 system. Our analysis also\ndemonstrates that the CG-EnKF and NS-EnKF can handle highly non-Gaussian\nadditive noise perturbations, with the latter typically outperforming the\nformer.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "WeatherFormer: Empowering Global Numerical Weather Forecasting with Space-Time Transformer",
    "url": "http://arxiv.org/abs/2409.16321v1",
    "authors": [
      "Junchao Gong",
      "Tao Han",
      "Kang Chen",
      "Lei Bai"
    ],
    "published": "2024-09-21",
    "abstract": "Numerical Weather Prediction (NWP) system is an infrastructure that exerts\nconsiderable impacts on modern society.Traditional NWP system, however,\nresolves it by solving complex partial differential equations with a huge\ncomputing cluster, resulting in tons of carbon emission. Exploring efficient\nand eco-friendly solutions for NWP attracts interest from Artificial\nIntelligence (AI) and earth science communities. To narrow the performance gap\nbetween the AI-based methods and physic predictor, this work proposes a new\ntransformer-based NWP framework, termed as WeatherFormer, to model the complex\nspatio-temporal atmosphere dynamics and empowering the capability of\ndata-driven NWP. WeatherFormer innovatively introduces the space-time\nfactorized transformer blocks to decrease the parameters and memory\nconsumption, in which Position-aware Adaptive Fourier Neural Operator (PAFNO)\nis proposed for location sensible token mixing. Besides, two data augmentation\nstrategies are utilized to boost the performance and decrease training\nconsumption. Extensive experiments on WeatherBench dataset show WeatherFormer\nachieves superior performance over existing deep learning methods and further\napproaches the most advanced physical model.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Developing a Thailand solar irradiance map using Himawari-8 satellite imageries and deep learning models",
    "url": "http://arxiv.org/abs/2409.16320v3",
    "authors": [
      "Suwichaya Suwanwimolkul",
      "Natanon Tongamrak",
      "Nuttamon Thungka",
      "Naebboon Hoonchareon",
      "Jitkomut Songsiri"
    ],
    "published": "2024-09-21",
    "abstract": "This paper presents an online platform showing Thailand solar irradiance map\nevery 30 minutes, available at https://www.cusolarforecast.com. The methodology\nfor estimating global horizontal irradiance (GHI) across Thailand relies on\ncloud index extracted from Himawari-8 satellite imagery, Ineichen clear-sky\nmodel with locally-tuned Linke turbidity, and machine learning models. The\nmethods take clear-sky irradiance, cloud index, re-analyzed GHI and temperature\ndata from the MERRA-2 database, and date-time as inputs for GHI estimation\nmodels, including LightGBM, LSTM, Informer, and Transformer. These are\nbenchmarked with the estimate from a commercial service X by evaluation of\n15-minute ground GHI data from 53 ground stations over 1.5 years during\n2022-2023. The results show that the four models exhibit comparable overall MAE\nperformance to the service X. The best model is LightGBM with an overall MAE of\n78.58 W/sqm and RMSE of 118.97 W/sqm, while the service X achieves the lowest\nMAE, RMSE, and MBE in cloudy condition. Obtaining re-analyzed MERRA-2 data for\nthe whole Thailand region is not economically feasible for deployment. When\nremoving these features, the Informer model has a winning performance in MAE of\n78.67 W/sqm. The obtained performance aligns with existing literature by taking\nthe climate zone and time granularity of data into consideration. As the map\nshows an estimate of GHI over 93,000 grids with a frequent update, the paper\nalso describes a computational framework for displaying the entire map. It\ntests the runtime performance of deep learning models in the GHI estimation\nprocess.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "On the Extrapolation of Generative Adversarial Networks for downscaling precipitation extremes in warmer climates",
    "url": "http://arxiv.org/abs/2409.13934v1",
    "authors": [
      "Neelesh Rampal",
      "Peter B. Gibson",
      "Steven Sherwood",
      "Gab Abramowitz"
    ],
    "published": "2024-09-20",
    "abstract": "While deep-learning downscaling algorithms can generate fine-scale climate\nprojections cost-effectively, it is still unclear how well they will\nextrapolate to unobserved climates. We assess the extrapolation capabilities of\na deterministic Convolutional Neural Network baseline and a Generative\nAdversarial Network (GAN) built with this baseline, trained to predict daily\nprecipitation simulated by a Regional Climate Model (RCM). Both approaches\nemulate future changes in annual mean precipitation well, even when trained on\nhistorical data, though training on a future climate improves performance. For\nextreme precipitation (99.5th percentile), RCM simulations predict a robust\nend-of-century increase with future warming (~5.8%/{\\deg}C on average from five\nsimulations). When trained on a future climate, GANs capture 97% of the\nwarming-driven increase in extreme precipitation compared to 65% in a\ndeterministic baseline. Even GANs trained historically capture 77% of this\nincrease. Overall, GANs offer better generalization for downscaling extremes,\nwhich is important in applications relying on historical data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "GAN"
    ],
    "applications": []
  },
  {
    "title": "Investigation of Time-Frequency Feature Combinations with Histogram Layer Time Delay Neural Networks",
    "url": "http://arxiv.org/abs/2409.13881v2",
    "authors": [
      "Amirmohammad Mohammadi",
      "Iren'e Masabarakiza",
      "Ethan Barnes",
      "Davelle Carreiro",
      "Alexandra Van Dine",
      "Joshua Peeples"
    ],
    "published": "2024-09-20",
    "abstract": "While deep learning has reduced the prevalence of manual feature extraction,\ntransformation of data via feature engineering remains essential for improving\nmodel performance, particularly for underwater acoustic signals. The methods by\nwhich audio signals are converted into time-frequency representations and the\nsubsequent handling of these spectrograms can significantly impact performance.\nThis work demonstrates the performance impact of using different combinations\nof time-frequency features in a histogram layer time delay neural network. An\noptimal set of features is identified with results indicating that specific\nfeature combinations outperform single data features.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-Sensor Deep Learning for Glacier Mapping",
    "url": "http://arxiv.org/abs/2409.12034v2",
    "authors": [
      "Codru\u0163-Andrei Diaconu",
      "Konrad Heidler",
      "Jonathan L. Bamber",
      "Harry Zekollari"
    ],
    "published": "2024-09-18",
    "abstract": "The more than 200,000 glaciers outside the ice sheets play a crucial role in\nour society by influencing sea-level rise, water resource management, natural\nhazards, biodiversity, and tourism. However, only a fraction of these glaciers\nbenefit from consistent and detailed in-situ observations that allow for\nassessing their status and changes over time. This limitation can, in part, be\novercome by relying on satellite-based Earth Observation techniques.\nSatellite-based glacier mapping applications have historically mainly relied on\nmanual and semi-automatic detection methods, while recently, a fast and notable\ntransition to deep learning techniques has started.\n  This chapter reviews how combining multi-sensor remote sensing data and deep\nlearning allows us to better delineate (i.e. map) glaciers and detect their\ntemporal changes. We explain how relying on deep learning multi-sensor\nframeworks to map glaciers benefits from the extensive availability of regional\nand global glacier inventories. We also analyse the rationale behind glacier\nmapping, the benefits of deep learning methodologies, and the inherent\nchallenges in integrating multi-sensor earth observation data with deep\nlearning algorithms.\n  While our review aims to provide a broad overview of glacier mapping efforts,\nwe highlight a few setups where deep learning multi-sensor remote sensing\napplications have a considerable potential added value. This includes\napplications for debris-covered and rock glaciers that are visually difficult\nto distinguish from surroundings and for calving glaciers that are in contact\nwith the ocean. These specific cases are illustrated through a series of visual\nimageries, highlighting some significant advantages and challenges when\ndetecting glacier changes, including dealing with seasonal snow cover, changing\ndebris coverage, and distinguishing glacier fronts from the surrounding sea\nice.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "DiffESM: Conditional Emulation of Temperature and Precipitation in Earth System Models with 3D Diffusion Models",
    "url": "http://arxiv.org/abs/2409.11601v1",
    "authors": [
      "Seth Bassetti",
      "Brian Hutchinson",
      "Claudia Tebaldi",
      "Ben Kravitz"
    ],
    "published": "2024-09-17",
    "abstract": "Earth System Models (ESMs) are essential for understanding the interaction\nbetween human activities and the Earth's climate. However, the computational\ndemands of ESMs often limit the number of simulations that can be run,\nhindering the robust analysis of risks associated with extreme weather events.\nWhile low-cost climate emulators have emerged as an alternative to emulate ESMs\nand enable rapid analysis of future climate, many of these emulators only\nprovide output on at most a monthly frequency. This temporal resolution is\ninsufficient for analyzing events that require daily characterization, such as\nheat waves or heavy precipitation. We propose using diffusion models, a class\nof generative deep learning models, to effectively downscale ESM output from a\nmonthly to a daily frequency. Trained on a handful of ESM realizations,\nreflecting a wide range of radiative forcings, our DiffESM model takes monthly\nmean precipitation or temperature as input, and is capable of producing daily\nvalues with statistical characteristics close to ESM output. Combined with a\nlow-cost emulator providing monthly means, this approach requires only a small\nfraction of the computational resources needed to run a large ensemble. We\nevaluate model behavior using a number of extreme metrics, showing that DiffESM\nclosely matches the spatio-temporal behavior of the ESM output it emulates in\nterms of the frequency and spatial characteristics of phenomena such as heat\nwaves, dry spells, or rainfall intensity.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": []
  },
  {
    "title": "Super Resolution On Global Weather Forecasts",
    "url": "http://arxiv.org/abs/2409.11502v2",
    "authors": [
      "Lawrence Zhang",
      "Adam Yang",
      "Rodz Andrie Amor",
      "Bryan Zhang",
      "Dhruv Rao"
    ],
    "published": "2024-09-17",
    "abstract": "Weather forecasting is a vitally important tool for tasks ranging from\nplanning day to day activities to disaster response planning. However, modeling\nweather has proven to be challenging task due to its chaotic and unpredictable\nnature. Each variable, from temperature to precipitation to wind, all influence\nthe path the environment will take. As a result, all models tend to rapidly\nlose accuracy as the temporal range of their forecasts increase. Classical\nforecasting methods use a myriad of physics-based, numerical, and stochastic\ntechniques to predict the change in weather variables over time. However, such\nforecasts often require a very large amount of data and are extremely\ncomputationally expensive. Furthermore, as climate and global weather patterns\nchange, classical models are substantially more difficult and time-consuming to\nupdate for changing environments. Fortunately, with recent advances in deep\nlearning and publicly available high quality weather datasets, deploying\nlearning methods for estimating these complex systems has become feasible. The\ncurrent state-of-the-art deep learning models have comparable accuracy to the\nindustry standard numerical models and are becoming more ubiquitous in practice\ndue to their adaptability. Our group seeks to improve upon existing deep\nlearning based forecasting methods by increasing spatial resolutions of global\nweather predictions. Specifically, we are interested in performing super\nresolution (SR) on GraphCast temperature predictions by increasing the global\nprecision from 1 degree of accuracy to 0.5 degrees, which is approximately\n111km and 55km respectively.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Surface solar radiation: AI satellite retrieval can outperform Heliosat and generalizes well to other climate zones",
    "url": "http://arxiv.org/abs/2409.16316v1",
    "authors": [
      "K. R. Schuurman",
      "A. Meyer"
    ],
    "published": "2024-09-16",
    "abstract": "Accurate estimates of surface solar irradiance (SSI) are essential for solar\nresource assessments and solar energy forecasts in grid integration and\nbuilding control applications. SSI estimates for spatially extended regions can\nbe retrieved from geostationary satellites such as Meteosat. Traditional SSI\nsatellite retrievals like Heliosat rely on physical radiative transfer\nmodelling. We introduce the first machine-learning-based satellite retrieval\nfor instantaneous SSI and demonstrate its capability to provide accurate and\ngeneralizable SSI estimates across Europe. Our deep learning retrieval provides\nnear real-time SSI estimates based on data-driven emulation of Heliosat and\nfine-tuning on pyranometer networks. By including SSI from ground stations, our\nSSI retrieval model can outperform Heliosat accuracy and generalize well to\nregions with other climates and surface albedos in cloudy conditions (clear-sky\nindex < 0.8). We also show that the SSI retrieved from Heliosat exhibits large\nbiases in mountain regions, and that training and fine-tuning our retrieval\nmodels on SSI data from ground stations strongly reduces these biases,\noutperforming Heliosat. Furthermore, we quantify the relative importance of the\nMeteosat channels and other predictor variables like solar zenith angle for the\naccuracy of our deep learning SSI retrieval model in different cloud\nconditions. We find that in cloudy conditions multiple near-infrared and\ninfrared channels enhance the performance. Our results can facilitate the\ndevelopment of more accurate satellite retrieval models of surface solar\nirradiance.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling",
    "url": "http://arxiv.org/abs/2409.16313v2",
    "authors": [
      "Teerapong Panboonyuen"
    ],
    "published": "2024-09-14",
    "abstract": "Forecasting sea surface currents is essential for applications such as\nmaritime navigation, environmental monitoring, and climate analysis,\nparticularly in regions like the Gulf of Thailand and the Andaman Sea. This\npaper introduces SEA-ViT, an advanced deep learning model that integrates\nVision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to\ncapture spatio-temporal covariance for predicting sea surface currents (U, V)\nusing high-frequency radar (HF) data. The name SEA-ViT is derived from ``Sea\nSurface Currents Forecasting using Vision Transformer,'' highlighting the\nmodel's emphasis on ocean dynamics and its use of the ViT architecture to\nenhance forecasting capabilities. SEA-ViT is designed to unravel complex\ndependencies by leveraging a rich dataset spanning over 30 years and\nincorporating ENSO indices (El Ni\\~no, La Ni\\~na, and neutral phases) to\naddress the intricate relationship between geographic coordinates and climatic\nvariations. This development enhances the predictive capabilities for sea\nsurface currents, supporting the efforts of the Geo-Informatics and Space\nTechnology Development Agency (GISTDA) in Thailand's maritime regions. The code\nand pretrained models are available at\n\\url{https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents}.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multi-scale decomposition of sea surface height snapshots using machine learning",
    "url": "http://arxiv.org/abs/2409.17354v1",
    "authors": [
      "Jingwen Lyu",
      "Yue Wang",
      "Christian Pedersen",
      "Spencer Jones",
      "Dhruv Balwada"
    ],
    "published": "2024-09-11",
    "abstract": "Knowledge of ocean circulation is important for understanding and predicting\nweather and climate, and managing the blue economy. This circulation can be\nestimated through Sea Surface Height (SSH) observations, but requires\ndecomposing the SSH into contributions from balanced and unbalanced motions\n(BMs and UBMs). This decomposition is particularly pertinent for the novel SWOT\nsatellite, which measures SSH at an unprecedented spatial resolution.\nSpecifically, the requirement, and the goal of this work, is to decompose\ninstantaneous SSH into BMs and UBMs. While a few studies using deep learning\n(DL) approaches have shown promise in framing this decomposition as an\nimage-to-image translation task, these models struggle to work well across a\nwide range of spatial scales and require extensive training data, which is\nscarce in this domain. These challenges are not unique to our task, and pervade\nmany problems requiring multi-scale fidelity. We show that these challenges can\nbe addressed by using zero-phase component analysis (ZCA) whitening and data\naugmentation; making this a viable option for SSH decomposition across scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Deep Learning for predicting rate-induced tipping",
    "url": "http://arxiv.org/abs/2409.07590v1",
    "authors": [
      "Yu Huang",
      "Sebastian Bathiany",
      "Peter Ashwin",
      "Niklas Boers"
    ],
    "published": "2024-09-11",
    "abstract": "Nonlinear dynamical systems exposed to changing forcing can exhibit\ncatastrophic transitions between alternative and often markedly different\nstates. The phenomenon of critical slowing down (CSD) can be used to anticipate\nsuch transitions if caused by a bifurcation and if the change in forcing is\nslow compared to the internal time scale of the system. However, in many\nreal-world situations, these assumptions are not met and transitions can be\ntriggered because the forcing exceeds a critical rate. For example, given the\npace of anthropogenic climate change in comparison to the internal time scales\nof key Earth system components, such as the polar ice sheets or the Atlantic\nMeridional Overturning Circulation, such rate-induced tipping poses a severe\nrisk. Moreover, depending on the realisation of random perturbations, some\ntrajectories may transition across an unstable boundary, while others do not,\neven under the same forcing. CSD-based indicators generally cannot distinguish\nthese cases of noise-induced tipping versus no tipping. This severely limits\nour ability to assess the risks of tipping, and to predict individual\ntrajectories. To address this, we make a first attempt to develop a deep\nlearning framework to predict transition probabilities of dynamical systems\nahead of rate-induced transitions. Our method issues early warnings, as\ndemonstrated on three prototypical systems for rate-induced tipping, subjected\nto time-varying equilibrium drift and noise perturbations. Exploiting\nexplainable artificial intelligence methods, our framework captures the\nfingerprints necessary for early detection of rate-induced tipping, even in\ncases of long lead times. Our findings demonstrate the predictability of\nrate-induced and noise-induced tipping, advancing our ability to determine safe\noperating spaces for a broader class of dynamical systems than possible so far.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Segmenting sea ice floes in close-range optical imagery with active contour and foundation models",
    "url": "http://arxiv.org/abs/2409.06641v2",
    "authors": [
      "Giulio Passerotti",
      "Alberto Alberello",
      "Marcello Vichi",
      "Luke G. Bennetts",
      "James Bailey",
      "Alessandro Toffoli"
    ],
    "published": "2024-09-10",
    "abstract": "The size and shape of sea ice floes play a crucial role in influencing\nocean-atmosphere energy exchanges, sea ice concentrations, albedo, and wave\npropagation through ice-covered waters. Despite the availability of diverse\nimage segmentation techniques for analyzing sea ice imagery, accurately\ndetecting and measuring floes remains a considerable challenge. This study\npresents a precise methodology for in-situ sea ice imagery acquisition,\nincluding automated orthorectification to correct perspective distortions. The\nimage dataset, collected during an Antarctic winter expedition, was used to\nevaluate various automated image segmentation approaches: the traditional GVF\nSnake algorithm and the advanced deep learning model, Segment Anything Model\n(SAM). To address the limitations of each method, a hybrid algorithm combining\ntraditional and AI-based techniques is proposed. The effectiveness of these\napproaches was validated through a detailed analysis of ice floe detection\naccuracy, floe size, and ice concentration statistics, with the outcomes\nnormalized against a manually segmented benchmark.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Deep Learning for Koopman Operator Estimation in Idealized Atmospheric Dynamics",
    "url": "http://arxiv.org/abs/2409.06522v1",
    "authors": [
      "David Millard",
      "Arielle Carr",
      "St\u00e9phane Gaudreault"
    ],
    "published": "2024-09-10",
    "abstract": "Deep learning is revolutionizing weather forecasting, with new data-driven\nmodels achieving accuracy on par with operational physical models for\nmedium-term predictions. However, these models often lack interpretability,\nmaking their underlying dynamics difficult to understand and explain. This\npaper proposes methodologies to estimate the Koopman operator, providing a\nlinear representation of complex nonlinear dynamics to enhance the transparency\nof data-driven models. Despite its potential, applying the Koopman operator to\nlarge-scale problems, such as atmospheric modeling, remains challenging. This\nstudy aims to identify the limitations of existing methods, refine these models\nto overcome various bottlenecks, and introduce novel convolutional neural\nnetwork architectures that capture simplified dynamics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "CAS-Canglong: A skillful 3D Transformer model for sub-seasonal to seasonal global sea surface temperature prediction",
    "url": "http://arxiv.org/abs/2409.05369v1",
    "authors": [
      "Longhao Wang",
      "Xuanze Zhang",
      "L. Ruby Leung",
      "Francis H. S. Chiew",
      "Amir AghaKouchak",
      "Kairan Ying",
      "Yongqiang Zhang"
    ],
    "published": "2024-09-09",
    "abstract": "Accurate prediction of global sea surface temperature at sub-seasonal to\nseasonal (S2S) timescale is critical for drought and flood forecasting, as well\nas for improving disaster preparedness in human society. Government departments\nor academic studies normally use physics-based numerical models to predict S2S\nsea surface temperature and corresponding climate indices, such as El\nNi\\~no-Southern Oscillation. However, these models are hampered by\ncomputational inefficiencies, limited retention of ocean-atmosphere initial\nconditions, and significant uncertainty and biases. Here, we introduce a novel\nthree-dimensional deep learning neural network to model the nonlinear and\ncomplex coupled atmosphere-ocean weather systems. This model incorporates\nclimatic and temporal features and employs a self-attention mechanism to\nenhance the prediction of global S2S sea surface temperature pattern. Compared\nto the physics-based models, it shows significant computational efficiency and\npredictive capability, improving one to three months sea surface temperature\npredictive skill by 13.7% to 77.1% in seven ocean regions with dominant\ninfluence on S2S variability over land. This achievement underscores the\nsignificant potential of deep learning for largely improving forecasting skills\nat the S2S scale over land.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning Framework for High-Resolution Air Temperature Downscaling Using LiDAR-Derived Urban Morphological Features",
    "url": "http://arxiv.org/abs/2409.02120v1",
    "authors": [
      "Fatemeh Chajaei",
      "Hossein Bagheri"
    ],
    "published": "2024-08-31",
    "abstract": "Climate models lack the necessary resolution for urban climate studies,\nrequiring computationally intensive processes to estimate high resolution air\ntemperatures. In contrast, Data-driven approaches offer faster and more\naccurate air temperature downscaling. This study presents a data-driven\nframework for downscaling air temperature using publicly available outputs from\nurban climate models, specifically datasets generated by UrbClim. The proposed\nframework utilized morphological features extracted from LiDAR data. To extract\nurban morphological features, first a three-dimensional building model was\ncreated using LiDAR data and deep learning models. Then, these features were\nintegrated with meteorological parameters such as wind, humidity, etc., to\ndownscale air temperature using machine learning algorithms. The results\ndemonstrated that the developed framework effectively extracted urban\nmorphological features from LiDAR data. Deep learning algorithms played a\ncrucial role in generating three-dimensional models for extracting the\naforementioned features. Also, the evaluation of air temperature downscaling\nresults using various machine learning models indicated that the LightGBM model\nhad the best performance with an RMSE of 0.352{\\deg}K and MAE of 0.215{\\deg}K.\nFurthermore, the examination of final air temperature maps derived from\ndownscaling showed that the developed framework successfully estimated air\ntemperatures at higher resolutions, enabling the identification of local air\ntemperature patterns at street level. The corresponding source codes are\navailable on GitHub:\nhttps://github.com/FatemehCh97/Air-Temperature-Downscaling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "AI-driven weather forecasts enable anticipated attribution of extreme events to human-made climate change",
    "url": "http://arxiv.org/abs/2408.16433v1",
    "authors": [
      "Bernat Jim\u00e9nez-Esteve",
      "David Barriopedro",
      "Juan Emmanuel Johnson",
      "Ricardo Garcia-Herrera"
    ],
    "published": "2024-08-29",
    "abstract": "Anthropogenic climate change (ACC) is altering the frequency and intensity of\nextreme weather events. Attributing individual extreme events (EEs) to ACC is\nbecoming crucial to assess the risks of climate change. Traditional attribution\nmethods often suffer from a selection bias, are computationally demanding, and\nprovide answers after the EE occurs. This study presents a ground-breaking\nhybrid attribution method by combining physics-based ACC estimates from global\nclimate models with deep-learning weather forecasts. This hybrid approach\ncircumvents the framing choices and accelerates the attribution process, paving\nthe way for operational anticipated global forecast-based attribution. We apply\nthis methodology to three distinct high-impact weather EEs. Despite some\nlimitations in predictability, the method uncovers ACC fingerprints in the\nforecasted fields of EEs. Specifically, forecasts successfully anticipate that\nACC exacerbated the 2018 Iberian heatwave, deepened hurricane Florence, and\nintensified the wind and precipitable water of the explosive cyclone Ciar\\'an.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution",
    "url": "http://arxiv.org/abs/2408.15993v2",
    "authors": [
      "Sungduk Yu",
      "Brian L. White",
      "Anahita Bhiwandiwalla",
      "Musashi Hinck",
      "Matthew Lyle Olson",
      "Yaniv Gurwicz",
      "Raanan Y. Rohekar",
      "Tung Nguyen",
      "Vasudev Lal"
    ],
    "published": "2024-08-28",
    "abstract": "Detecting and attributing temperature increases driven by climate change is\ncrucial for understanding global warming and informing adaptation strategies.\nHowever, distinguishing human-induced climate signals from natural variability\nremains challenging for traditional detection and attribution (D&A) methods,\nwhich rely on identifying specific \"fingerprints\" -- spatial patterns expected\nto emerge from external forcings such as greenhouse gas emissions. Deep\nlearning offers promise in discerning these complex patterns within expansive\nspatial datasets, yet the lack of standardized protocols has hindered\nconsistent comparisons across studies.\n  To address this gap, we introduce ClimDetect, a standardized dataset\ncomprising 1.17M daily climate snapshots paired with target climate change\nindicator variables. The dataset is curated from both CMIP6 climate model\nsimulations and real-world observation-assimilated reanalysis datasets (ERA5,\nJRA-3Q, and MERRA-2), and is designed to enhance model accuracy in detecting\nclimate change signals. ClimDetect integrates various input and target\nvariables used in previous research, ensuring comparability and consistency\nacross studies. We also explore the application of vision transformers (ViT) to\nclimate data -- a novel approach that, to our knowledge, has not been attempted\nbefore for climate change detection tasks. Our open-access data serve as a\nbenchmark for advancing climate science by enabling end-to-end model\ndevelopment and evaluation. ClimDetect is publicly accessible via Hugging Face\ndataset repository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Uncertainty-aware segmentation for rainfall prediction post processing",
    "url": "http://arxiv.org/abs/2408.16792v1",
    "authors": [
      "Simone Monaco",
      "Luca Monaco",
      "Daniele Apiletti"
    ],
    "published": "2024-08-28",
    "abstract": "Accurate precipitation forecasts are crucial for applications such as flood\nmanagement, agricultural planning, water resource allocation, and weather\nwarnings. Despite advances in numerical weather prediction (NWP) models, they\nstill exhibit significant biases and uncertainties, especially at high spatial\nand temporal resolutions. To address these limitations, we explore\nuncertainty-aware deep learning models for post-processing daily cumulative\nquantitative precipitation forecasts to obtain forecast uncertainties that lead\nto a better trade-off between accuracy and reliability. Our study compares\ndifferent state-of-the-art models, and we propose a variant of the well-known\nSDE-Net, called SDE U-Net, tailored to segmentation problems like ours. We\nevaluate its performance for both typical and intense precipitation events.\n  Our results show that all deep learning models significantly outperform the\naverage baseline NWP solution, with our implementation of the SDE U-Net showing\nthe best trade-off between accuracy and reliability. Integrating these models,\nwhich account for uncertainty, into operational forecasting systems can improve\ndecision-making and preparedness for weather-related events.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Generative Diffusion Model-based Downscaling of Observed Sea Surface Height over Kuroshio Extension since 2000",
    "url": "http://arxiv.org/abs/2408.12632v1",
    "authors": [
      "Qiuchang Han",
      "Xingliang Jiang",
      "Yang Zhao",
      "Xudong Wang",
      "Zhijin Li",
      "Renhe Zhang"
    ],
    "published": "2024-08-22",
    "abstract": "Satellite altimetry has been widely utilized to monitor global sea surface\ndynamics, enabling investigation of upper ocean variability from basin-scale to\nlocalized eddy ranges. However, the sparse spatial resolution of observational\naltimetry limits our understanding of oceanic submesoscale variability,\nprevalent at horizontal scales below 0.25o resolution. Here, we introduce a\nstate-of-the-art generative diffusion model to train high-resolution sea\nsurface height (SSH) reanalysis data and demonstrate its advantage in\nobservational SSH downscaling over the eddy-rich Kuroshio Extension region. The\ndiffusion-based model effectively downscales raw satellite-interpolated data\nfrom 0.25o resolution to 1/16o, corresponding to approximately 12-km\nwavelength. This model outperforms other high-resolution reanalysis datasets\nand neural network-based methods. Also, it successfully reproduces the spatial\npatterns and power spectra of satellite along-track observations. Our\ndiffusion-based results indicate that eddy kinetic energy at horizontal scales\nless than 250 km has intensified significantly since 2004 in the Kuroshio\nExtension region. These findings underscore the great potential of deep\nlearning in reconstructing satellite altimetry and enhancing our understanding\nof ocean dynamics at eddy scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Long-Range Vision-Based UAV-assisted Localization for Unmanned Surface Vehicles",
    "url": "http://arxiv.org/abs/2408.11429v1",
    "authors": [
      "Waseem Akram",
      "Siyuan Yang",
      "Hailiang Kuang",
      "Xiaoyu He",
      "Muhayy Ud Din",
      "Yihao Dong",
      "Defu Lin",
      "Lakmal Seneviratne",
      "Shaoming He",
      "Irfan Hussain"
    ],
    "published": "2024-08-21",
    "abstract": "The global positioning system (GPS) has become an indispensable navigation\nmethod for field operations with unmanned surface vehicles (USVs) in marine\nenvironments. However, GPS may not always be available outdoors because it is\nvulnerable to natural interference and malicious jamming attacks. Thus, an\nalternative navigation system is required when the use of GPS is restricted or\nprohibited. To this end, we present a novel method that utilizes an Unmanned\nAerial Vehicle (UAV) to assist in localizing USVs in GNSS-restricted marine\nenvironments. In our approach, the UAV flies along the shoreline at a\nconsistent altitude, continuously tracking and detecting the USV using a deep\nlearning-based approach on camera images. Subsequently, triangulation\ntechniques are applied to estimate the USV's position relative to the UAV,\nutilizing geometric information and datalink range from the UAV. We propose\nadjusting the UAV's camera angle based on the pixel error between the USV and\nthe image center throughout the localization process to enhance accuracy.\nAdditionally, visual measurements are integrated into an Extended Kalman Filter\n(EKF) for robust state estimation. To validate our proposed method, we utilize\na USV equipped with onboard sensors and a UAV equipped with a camera. A\nheterogeneous robotic interface is established to facilitate communication\nbetween the USV and UAV. We demonstrate the efficacy of our approach through a\nseries of experiments conducted during the ``Muhammad Bin Zayed International\nRobotic Challenge (MBZIRC-2024)'' in real marine environments, incorporating\nnoisy measurements and ocean disturbances. The successful outcomes indicate the\npotential of our method to complement GPS for USV navigation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Kilometer-Scale Convection Allowing Model Emulation using Generative Diffusion Modeling",
    "url": "http://arxiv.org/abs/2408.10958v1",
    "authors": [
      "Jaideep Pathak",
      "Yair Cohen",
      "Piyush Garg",
      "Peter Harrington",
      "Noah Brenowitz",
      "Dale Durran",
      "Morteza Mardani",
      "Arash Vahdat",
      "Shaoming Xu",
      "Karthik Kashinath",
      "Michael Pritchard"
    ],
    "published": "2024-08-20",
    "abstract": "Storm-scale convection-allowing models (CAMs) are an important tool for\npredicting the evolution of thunderstorms and mesoscale convective systems that\nresult in damaging extreme weather. By explicitly resolving convective dynamics\nwithin the atmosphere they afford meteorologists the nuance needed to provide\noutlook on hazard. Deep learning models have thus far not proven skilful at\nkm-scale atmospheric simulation, despite being competitive at coarser\nresolution with state-of-the-art global, medium-range weather forecasting. We\npresent a generative diffusion model called StormCast, which emulates the\nhigh-resolution rapid refresh (HRRR) model-NOAA's state-of-the-art 3km\noperational CAM. StormCast autoregressively predicts 99 state variables at km\nscale using a 1-hour time step, with dense vertical resolution in the\natmospheric boundary layer, conditioned on 26 synoptic variables. We present\nevidence of successfully learnt km-scale dynamics including competitive 1-6\nhour forecast skill for composite radar reflectivity alongside physically\nrealistic convective cluster evolution, moist updrafts, and cold pool\nmorphology. StormCast predictions maintain realistic power spectra for multiple\npredicted variables across multi-hour forecasts. Together, these results\nestablish the potential for autoregressive ML to emulate CAMs -- opening up new\nkm-scale frontiers for regional ML weather prediction and future climate hazard\ndynamical downscaling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Forecasting seasonal rainfall in SE Australia using Empirical Orthogonal Functions and Neural Networks",
    "url": "http://arxiv.org/abs/2408.10550v1",
    "authors": [
      "Stjepan Marcelja"
    ],
    "published": "2024-08-20",
    "abstract": "Quantitative forecasting of average rainfall into the next season remains\nhighly challenging, but in some favourable isolated cases may be possible with\na series of relatively simple steps. We chose to explore predictions of austral\nspringtime rainfall in SE Australia regions based on the surrounding ocean\nsurface temperatures during the winter. In the first stage, we search for\ncorrelations between the target rainfall and both the standard ocean climate\nindicators as well as the time series of surface temperature data expanded in\nterms of Empirical Orthogonal Functions (EOFs). In the case of the Indian\nOcean, during the winter the dominant EOF shows stronger correlation with the\nfuture rainfall than the commonly used Indian Ocean Dipole. Information sources\nwith the strongest correlation to the historical rainfall data are then used as\ninputs into deep learning artificial neural networks. The resulting hindcasts\nappear accurate for September and October and less reliable for November. We\nalso attempt to forecast the rainfall in several regions for the coming austral\nspring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "DUNE: A Machine Learning Deep UNet++ based Ensemble Approach to Monthly, Seasonal and Annual Climate Forecasting",
    "url": "http://arxiv.org/abs/2408.06262v1",
    "authors": [
      "Pratik Shukla",
      "Milton Halem"
    ],
    "published": "2024-08-12",
    "abstract": "Capitalizing on the recent availability of ERA5 monthly averaged long-term\ndata records of mean atmospheric and climate fields based on high-resolution\nreanalysis, deep-learning architectures offer an alternative to physics-based\ndaily numerical weather predictions for subseasonal to seasonal (S2S) and\nannual means. A novel Deep UNet++-based Ensemble (DUNE) neural architecture is\nintroduced, employing multi-encoder-decoder structures with residual blocks.\nWhen initialized from a prior month or year, this architecture produced the\nfirst AI-based global monthly, seasonal, or annual mean forecast of 2-meter\ntemperatures (T2m) and sea surface temperatures (SST). ERA5 monthly mean data\nis used as input for T2m over land, SST over oceans, and solar radiation at the\ntop of the atmosphere for each month of 40 years to train the model. Validation\nforecasts are performed for an additional two years, followed by five years of\nforecast evaluations to account for natural annual variability. AI-trained\ninference forecast weights generate forecasts in seconds, enabling ensemble\nseasonal forecasts. Root Mean Squared Error (RMSE), Anomaly Correlation\nCoefficient (ACC), and Heidke Skill Score (HSS) statistics are presented\nglobally and over specific regions. These forecasts outperform persistence,\nclimatology, and multiple linear regression for all domains. DUNE forecasts\ndemonstrate comparable statistical accuracy to NOAA's operational monthly and\nseasonal probabilistic outlook forecasts over the US but at significantly\nhigher resolutions. RMSE and ACC error statistics for other recent AI-based\ndaily forecasts also show superior performance for DUNE-based forecasts. The\nDUNE model's application to an ensemble data assimilation cycle shows\ncomparable forecast accuracy with a single high-resolution model, potentially\neliminating the need for retraining on extrapolated datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "MetMamba: Regional Weather Forecasting with Spatial-Temporal Mamba Model",
    "url": "http://arxiv.org/abs/2408.06400v2",
    "authors": [
      "Haoyu Qin",
      "Yungang Chen",
      "Qianchuan Jiang",
      "Pengchao Sun",
      "Xiancai Ye",
      "Chao Lin"
    ],
    "published": "2024-08-12",
    "abstract": "Deep Learning based Weather Prediction (DLWP) models have been improving\nrapidly over the last few years, surpassing state of the art numerical weather\nforecasts by significant margins. While much of the optimization effort is\nfocused on training curriculum to extend forecast range in the global context,\ntwo aspects remains less explored: limited area modeling and better backbones\nfor weather forecasting. We show in this paper that MetMamba, a DLWP model\nbuilt on a state-of-the-art state-space model, Mamba, offers notable\nperformance gains and unique advantages over other popular backbones using\ntraditional attention mechanisms and neural operators. We also demonstrate the\nfeasibility of deep learning based limited area modeling via coupled training\nwith a global host model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Comparative Study of Convolutional and Recurrent Neural Networks for Storm Surge Prediction in Tampa Bay",
    "url": "http://arxiv.org/abs/2408.05797v1",
    "authors": [
      "Mandana Farhang Ghahfarokhi",
      "Seyed Hossein Sonbolestan",
      "Mahta Zamanizadeh"
    ],
    "published": "2024-08-11",
    "abstract": "In this paper, we compare the performance of three common deep learning\narchitectures, CNN-LSTM, LSTM, and 3D-CNN, in the context of surrogate storm\nsurge modeling. The study site for this paper is the Tampa Bay area in Florida.\nUsing high-resolution atmospheric data from the reanalysis models and\nhistorical water level data from NOAA tide stations, we trained and tested\nthese models to evaluate their performance. Our findings indicate that the\nCNN-LSTM model outperforms the other architectures, achieving a test loss of\n0.010 and an R-squared (R2) score of 0.84. The LSTM model, although it achieved\nthe lowest training loss of 0.007 and the highest training R2 of 0.88,\nexhibited poorer generalization with a test loss of 0.014 and an R2 of 0.77.\nThe 3D-CNN model showed reasonable performance with a test loss of 0.011 and an\nR2 of 0.82 but displayed instability under extreme conditions. A case study on\nHurricane Ian, which caused a significant negative surge of -1.5 meters in\nTampa Bay indicates the CNN-LSTM model's robustness and accuracy in extreme\nscenarios.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "Personalized Federated Learning for improving radar based precipitation nowcasting on heterogeneous areas",
    "url": "http://arxiv.org/abs/2408.05761v1",
    "authors": [
      "Judith S\u00e1inz-Pardo D\u00edaz",
      "Mar\u00eda Castrillo",
      "Juraj Bartok",
      "Ignacio Heredia Cach\u00e1",
      "Irina Malkin Ond\u00edk",
      "Ivan Martynovskyi",
      "Khadijeh Alibabaei",
      "Lisana Berberi",
      "Valentin Kozlov",
      "\u00c1lvaro L\u00f3pez Garc\u00eda"
    ],
    "published": "2024-08-11",
    "abstract": "The increasing generation of data in different areas of life, such as the\nenvironment, highlights the need to explore new techniques for processing and\nexploiting data for useful purposes. In this context, artificial intelligence\ntechniques, especially through deep learning models, are key tools to be used\non the large amount of data that can be obtained, for example, from weather\nradars. In many cases, the information collected by these radars is not open,\nor belongs to different institutions, thus needing to deal with the distributed\nnature of this data. In this work, the applicability of a personalized\nfederated learning architecture, which has been called adapFL, on distributed\nweather radar images is addressed. To this end, given a single available radar\ncovering 400 km in diameter, the captured images are divided in such a way that\nthey are disjointly distributed into four different federated clients. The\nresults obtained with adapFL are analyzed in each zone, as well as in a central\narea covering part of the surface of each of the previously distributed areas.\nThe ultimate goal of this work is to study the generalization capability of\nthis type of learning technique for its extrapolation to use cases in which a\nrepresentative number of radars is available, whose data can not be centralized\ndue to technical, legal or administrative concerns. The results of this\npreliminary study indicate that the performance obtained in each zone with the\nadapFL approach allows improving the results of the federated learning\napproach, the individual deep learning models and the classical Continuity\nTracking Radar Echoes by Correlation approach.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Accurate deep learning-based filtering for chaotic dynamics by identifying instabilities without an ensemble",
    "url": "http://arxiv.org/abs/2408.04739v2",
    "authors": [
      "Marc Bocquet",
      "Alban Farchi",
      "Tobias S. Finn",
      "Charlotte Durand",
      "Sibo Cheng",
      "Yumeng Chen",
      "Ivo Pasmans",
      "Alberto Carrassi"
    ],
    "published": "2024-08-08",
    "abstract": "We investigate the ability to discover data assimilation (DA) schemes meant\nfor chaotic dynamics with deep learning. The focus is on learning the analysis\nstep of sequential DA, from state trajectories and their observations, using a\nsimple residual convolutional neural network, while assuming the dynamics to be\nknown. Experiments are performed with the Lorenz 96 dynamics, which display\nspatiotemporal chaos and for which solid benchmarks for DA performance exist.\nThe accuracy of the states obtained from the learned analysis approaches that\nof the best possibly tuned ensemble Kalman filter, and is far better than that\nof variational DA alternatives. Critically, this can be achieved while\npropagating even just a single state in the forecast step. We investigate the\nreason for achieving ensemble filtering accuracy without an ensemble. We\ndiagnose that the analysis scheme actually identifies key dynamical\nperturbations, mildly aligned with the unstable subspace, from the forecast\nstate alone, without any ensemble-based covariances representation. This\nreveals that the analysis scheme has learned some multiplicative ergodic\ntheorem associated to the DA process seen as a non-autonomous random dynamical\nsystem.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Underwater litter monitoring using consumer-grade aerial-aquatic speedy scanner (AASS) and deep learning based super-resolution reconstruction and detection network",
    "url": "http://arxiv.org/abs/2408.03564v2",
    "authors": [
      "Fan Zhao",
      "Yongying Liu",
      "Jiaqi Wang",
      "Yijia Chen",
      "Dianhan Xi",
      "Xinlei Shao",
      "Shigeru Tabeta",
      "Katsunori Mizuno"
    ],
    "published": "2024-08-07",
    "abstract": "Underwater litter is widely spread across aquatic environments such as lakes,\nrivers, and oceans, significantly impacting natural ecosystems. Current\nmonitoring technologies for detecting underwater litter face limitations in\nsurvey efficiency, cost, and environmental conditions, highlighting the need\nfor efficient, consumer-grade technologies for automatic detection. This\nresearch introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with\nSuper-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network.\nAASS enhances data acquisition efficiency over traditional methods, capturing\nhigh-quality images that accurately identify underwater waste. SRR improves\nimage-resolution by mitigating motion blur and insufficient resolution, thereby\nenhancing detection tasks. Specifically, the RCAN model achieved the highest\nmean average precision (mAP) of 78.6% for detection accuracy on reconstructed\nimages among the tested SRR models. With a magnification factor of 4, the SRR\ntest set shows an improved mAP compared to the conventional bicubic set. These\nresults demonstrate the effectiveness of the proposed method in detecting\nunderwater litter.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Super-Resolution"
    ]
  },
  {
    "title": "ORCAst: Operational High-Resolution Current Forecasts",
    "url": "http://arxiv.org/abs/2501.12054v1",
    "authors": [
      "Pierre Garcia",
      "In\u00e8s Larroche",
      "Am\u00e9lie Pesnec",
      "Hannah Bull",
      "Th\u00e9o Archambault",
      "Evangelos Moschos",
      "Alexandre Stegner",
      "Anastase Charantonis",
      "Dominique B\u00e9r\u00e9ziat"
    ],
    "published": "2025-01-21",
    "abstract": "We present ORCAst, a multi-stage, multi-arm network for Operational\nhigh-Resolution Current forecAsts over one week. Producing real-time nowcasts\nand forecasts of ocean surface currents is a challenging problem due to\nindirect or incomplete information from satellite remote sensing data. Entirely\ntrained on real satellite data and in situ measurements from drifters, our\nmodel learns to forecast global ocean surface currents using various sources of\nground truth observations in a multi-stage learning procedure. Our multi-arm\nencoder-decoder model architecture allows us to first predict sea surface\nheight and geostrophic currents from larger quantities of nadir and SWOT\naltimetry data, before learning to predict ocean surface currents from much\nmore sparse in situ measurements from drifters. Training our model on specific\nregions improves performance. Our model achieves stronger nowcast and forecast\nperformance in predicting ocean surface currents than various state-of-the-art\nmethods.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Nowcast"
    ]
  },
  {
    "title": "An analysis on OpenMetBuoy-v2021 drifter in-situ data and Lagrangian trajectory simulations in the Agulhas Current System",
    "url": "http://arxiv.org/abs/2409.20096v1",
    "authors": [
      "Bente Moerman",
      "\u00d8yvind Breivik",
      "Lars R. Hole",
      "Gaute Hope",
      "Johnny A. Johannessen",
      "Jean Rabault"
    ],
    "published": "2024-09-30",
    "abstract": "In order to perform a sensitivity analysis of Lagrangian trajectory models,\nLagrangian trajectory simulations have been compared to six OpenMetBuoy-v2021\ndrifter trajectories in the Agulhas Current System (Jan-Mar 2023). Three\ndifferent Lagrangian trajectory simulations have been assessed: (1) two offline\nLagrangian tracking tools, OpenDrift and Parcels, (2) three Eulerian ocean\nsurface current products, HYCOM, Mercator and Globcurrent, and (3) the addition\nof wind and/or wave forcing parameterizations. The latter has also been\nevaluated by strong ocean current, high wind speed and Stokes drift regimes.\n  Firstly, using the same time stepping scheme and linear interpolation\nmethods, the different Lagrangian simulators OpenDrift and Parcels, performed\nidentically. Secondly, the Globcurrent product showed the highest mean skill of\nthe three ocean current products, although it underestimated the speed for\nstrong ocean currents due to its spatial resolution. The HYCOM and Mercator\nmodel simulations showed, respectively, 40\\% and 15\\% lower skill than the\nGlobcurrent simulations. Finally, the addition of the Stokes drift and a wind\ndrift factor (WDF), improved the Lagrangian simulation performance in skill and\nspeed, especially in high wind (>10 m/s) and/or Stokes drift regimes (>0.15\nm/s). The optimal WDF for the OpenMetBuoy-v2021 is found to be ~1.8\\% and\n~2.3\\% for simulations including and excluding Stokes drift forcing\nrespectively. To further improve the incorporation of Stokes drift and direct\nwind drag on the trajectory simulations, a more physically based solution is\nadvised as there are still numerous wind and wave related processes that remain\nunresolved, like wave-current interactions and vertical shear.\n  To statistically strengthen the conclusions from this research, incorporating\nadditional observed drifter trajectories would be highly favourable.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "High-Frequency Radar observation of strong and contrasted currents: the Alderney race paradigm",
    "url": "http://arxiv.org/abs/2407.03827v1",
    "authors": [
      "Dylan Dumas",
      "Anne-Claire Bennis",
      "Charles-Antoine Gu\u00e9rin",
      "Guiomar Lopez",
      "Laurent Benoit"
    ],
    "published": "2024-07-04",
    "abstract": "The Alderney Race has been identified as a future site for the development of\ntidal energy, due to its bidirectional strong current reaching 5 m/s during\nspring tides. This hydrodynamics is very difficult to measure by in situ or\nremote sensing means. High-frequency coastal radars can provide a synoptic and\nnear-real-time view of such a complex circulation, but the classical processing\nalgorithms are not adapted to the extreme situation of strongly sheared\ncurrents. We propose an improved high-resolution direction-finding technique\nfor the azimuthal processing of such radar data. It uses phased-array systems\nand combines the advantages of the usual beam-forming technique to eliminate\nmany problems related to the distortion of Doppler spectra by extreme currents.\nThe method is evaluated with a unique data set of radar measurements at two\nradar frequencies (13 and 24.5 MHz) and three spatial resolutions (200, 750,\nand 1500 m). The radar-based surface currents are analyzed in the light of a\nhigh-resolution numerical model and also compared with in situ measurements.\nWhile high azimuthal resolution can be achieved in this way, it is shown that\nthe typical range resolutions of 750 and 1500 m are insufficient to account for\nthe strong spatial variations of the surface current at some specific times and\nlocations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "High Frequency Radar Observing System Simulation Experiment in the Western Mediterranean Sea: a Lagrangian assessment approach",
    "url": "http://arxiv.org/abs/2406.03579v2",
    "authors": [
      "Jaime Hernandez Lasheras",
      "Alejandro Orfila",
      "Alex Santana",
      "Ismael Hernandez Carrasco",
      "Baptiste Mourre"
    ],
    "published": "2024-06-05",
    "abstract": "The impact of the expansion of a high-frequency radar (HFR) system in a\ndynamic coastal area (the Ibiza Channel in the Western Mediterranean Sea) is\nevaluated through an Observing System Simulation Experiment (OSSE). The\ninstallation of two new antennas in the Iberian Peninsula would complement the\nexisting ones in the islands of Ibiza and Formentera, providing surface\ncurrents observations of the full channel. Two different configurations of the\nsame model, validated to give realistic simulations, are used: i) a Nature Run\n(NR) which is considered as the real ocean state and that is used to generate\npseudo-observations, and ii) a Control Run (CR) in which the\npseudo-observations are assimilated. The OSSE is first validated by comparison\nagainst a previous Observing System Experiment (OSE). The impact of the new\nantennas for forecasting surface currents is evaluated in two different periods\nwith different levels of agreement between NR and CR. The HFR expansion is\nfound to contribute to significantly correct the circulation patterns in the\nChannel, leading to surface merdional velocity error reductions up to 19%. The\neffects on the transport in the area are also analyzed from a Lagrangian\nperspective, showing that DA can help to better represent the Lagrangian\nCoherent Structures present in the NR and constrain the ocean dynamics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Synthetic RAW data generator for ESA HARMONY mission",
    "url": "http://arxiv.org/abs/2405.09938v1",
    "authors": [
      "Goulven Monnier",
      "Benjamin Camus",
      "Yann-Herv\u00e9 Hellouvry",
      "Pierre Dubois",
      "Erik de Witte"
    ],
    "published": "2024-05-16",
    "abstract": "In this paper, we introduce HEEPS/MARE, the end-to-end simulator developed\nfor the SAR oceanographic products of ESA Earth Explorer 10 mission, Harmony,\nexpected to launch in Decembre 2029. Harmony is primarily dedicated to the\nobservation of small-scale motion and deformation fields of the Earth surface\n(oceans, glaciers and ice sheets, solid Earth), thanks to passive SAR/ATI\nreceivers carried by two companion satellites for Sentinel-1. The paper focuses\non the raw data generator designed to efficiently simulate large,\nheterogeneous, moving oceanic areas and produce the acquired SAR/ATI bistatic\nIQ signals. The heterogeneous sea-surface model, bistatic scattering model,\nmulti-GPU implementation and achieved performance are emphasized. Finally,\nsample results are presented, to illustrate the ability of Harmony to map wind\nand surface current vectors at kilometric scale.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Scattering of surface waves by ocean currents: the U2H map",
    "url": "http://arxiv.org/abs/2402.05652v3",
    "authors": [
      "Han Wang",
      "Ana B. Villas B\u00f4as",
      "Jacques Vanneste",
      "William R. Young"
    ],
    "published": "2024-02-08",
    "abstract": "Ocean turbulence at meso- and submesocales affects the propagation of surface\nwaves through refraction and scattering, inducing spatial modulations in\nsignificant wave height (SWH). We develop a theoretical framework that relates\nthese modulations to the current that induces them. We exploit the asymptotic\nsmallness of the ratio of typical current speed to wave group speed to derive a\nlinear map -- the U2H map -- between surface current velocity and SWH anomaly.\nThe U2H map is a convolution, non-local in space, expressible as a product in\nFourier space by a factor independent of the magnitude of the wavenumber\nvector. Analytic expressions of the U2H map show how the SWH responds\ndifferently to the vortical and divergent parts of the current, and how the\nanisotropy of the wave spectrum is key to large current-induced SWH anomalies.\nWe implement the U2H map numerically and test its predictions against WAVEWATCH\nIII numerical simulations for both idealised and realistic current\nconfigurations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
    "url": "http://arxiv.org/abs/2406.02770v1",
    "authors": [
      "Kathrin Donandt",
      "Karim B\u00f6ttger",
      "Dirk S\u00f6ffker"
    ],
    "published": "2024-06-04",
    "abstract": "Accurate vessel trajectory prediction is necessary for save and efficient\nnavigation. Deep learning-based prediction models, esp. encoder-decoders, are\nrarely applied to inland navigation specifically. Approaches from the maritime\ndomain cannot directly be transferred to river navigation due to specific\ndriving behavior influencing factors. Different encoder-decoder architectures,\nincluding a transformer encoder-decoder, are compared herein for predicting the\nnext positions of inland vessels, given not only spatio-temporal information\nfrom AIS, but also river specific features. The results show that the\nreformulation of the regression task as classification problem and the\ninclusion of river specific features yield the lowest displacement errors. The\nstandard LSTM encoder-decoder outperforms the transformer encoder-decoder for\nthe data considered, but is computationally more expensive. In this study for\nthe first time a transformer-based encoder-decoder model is applied to the\nproblem of predicting the ship trajectory. Here, a feature vector using the\nriver-specific context of navigation input parameters is established. Future\nstudies can built on the proposed models, investigate the improvement of the\ncomputationally more efficient transformer, e.g. through further\nhyper-parameter optimization, and use additional river-specific information in\nthe context representation to further increase prediction accuracy.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction",
    "url": "http://arxiv.org/abs/2503.21834v1",
    "authors": [
      "Haomin Yu",
      "Tianyi Li",
      "Kristian Torp",
      "Christian S. Jensen"
    ],
    "published": "2025-03-27",
    "abstract": "Accurate vessel trajectory prediction facilitates improved navigational\nsafety, routing, and environmental protection. However, existing prediction\nmethods are challenged by the irregular sampling time intervals of the vessel\ntracking data from the global AIS system and the complexity of vessel movement.\nThese aspects render model learning and generalization difficult. To address\nthese challenges and improve vessel trajectory prediction, we propose the\nmulti-modal knowledge-enhanced framework (MAKER) for vessel trajectory\nprediction. To contend better with the irregular sampling time intervals, MAKER\nfeatures a Large language model-guided Knowledge Transfer (LKT) module that\nleverages pre-trained language models to transfer trajectory-specific\ncontextual knowledge effectively. To enhance the ability to learn complex\ntrajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning\n(KSL) module. This module employs kinematic knowledge to progressively\nintegrate complex patterns during training, allowing for adaptive learning and\nenhanced generalization. Experimental results on two vessel trajectory datasets\nshow that MAKER can improve the prediction accuracy of state-of-the-art methods\nby 12.08%-17.86%.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "Predicting Barge Presence and Quantity on Inland Waterways using Vessel Tracking Data: A Machine Learning Approach",
    "url": "http://arxiv.org/abs/2501.00615v1",
    "authors": [
      "Geoffery Agorkua",
      "Sarah Hernandez",
      "Maria Falquez",
      "Subhadipto Poddar",
      "Shihao Pang"
    ],
    "published": "2024-12-31",
    "abstract": "This study presents a machine learning approach to predict the number of\nbarges transported by vessels on inland waterways using tracking data from the\nAutomatic Identification System (AIS). While AIS tracks the location of tug and\ntow vessels, it does not monitor the presence or number of barges transported\nby those vessels. Understanding the number and types of barges conveyed along\nriver segments, between ports, and at ports is crucial for estimating the\nquantities of freight transported on the nation's waterways. This insight is\nalso valuable for waterway management and infrastructure operations impacting\nareas such as targeted dredging operations, and data-driven resource\nallocation. Labeled sample data was generated using observations from traffic\ncameras located along key river segments and matched to AIS data records. A\nsample of 164 vessels representing up to 42 barge convoys per vessel was used\nfor model development. The methodology involved first predicting barge presence\nand then predicting barge quantity. Features derived from the AIS data included\nspeed measures, vessel characteristics, turning measures, and interaction\nterms. For predicting barge presence, the AdaBoost model achieved an F1 score\nof 0.932. For predicting barge quantity, the Random Forest combined with an\nAdaBoost ensemble model achieved an F1 score of 0.886. Bayesian optimization\nwas used for hyperparameter tuning. By advancing predictive modeling for inland\nwaterways, this study offers valuable insights for transportation planners and\norganizations, which require detailed knowledge of traffic volumes, including\nthe flow of commodities, their destinations, and the tonnage moving in and out\nof ports.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "DisBeaNet: A Deep Neural Network to augment Unmanned Surface Vessels for maritime situational awareness",
    "url": "http://arxiv.org/abs/2405.06149v2",
    "authors": [
      "Srikanth Vemula",
      "Eulises Franco",
      "Michael Frye"
    ],
    "published": "2024-05-10",
    "abstract": "Intelligent detection and tracking of the vessels on the sea play a\nsignificant role in conducting traffic avoidance in unmanned surface\nvessels(USV). Current traffic avoidance software relies mainly on Automated\nIdentification System (AIS) and radar to track other vessels to avoid\ncollisions and acts as a typical perception system to detect targets. However,\nin a contested environment, emitting radar energy also presents the\nvulnerability to detection by adversaries. Deactivating these Radiofrequency\ntransmitting sources will increase the threat of detection and degrade the\nUSV's ability to monitor shipping traffic in the vicinity. Therefore, an\nintelligent visual perception system based on an onboard camera with passive\nsensing capabilities that aims to assist USV in addressing this problem is\npresented in this paper. This paper will present a novel low-cost vision\nperception system for detecting and tracking vessels in the maritime\nenvironment. This novel low-cost vision perception system is introduced using\nthe deep learning framework. A neural network, DisBeaNet, can detect vessels,\ntrack, and estimate the vessel's distance and bearing from the monocular\ncamera. The outputs obtained from this neural network are used to determine the\nlatitude and longitude of the identified vessel.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Spatio-temporal characterisation of underwater noise through semantic trajectories",
    "url": "http://arxiv.org/abs/2501.11131v1",
    "authors": [
      "Giulia Rovinelli",
      "Davide Rocchesso",
      "Marta Simeoni",
      "Esteban Zim\u00e1nyi",
      "Alessandra Raffaet\u00e0"
    ],
    "published": "2025-01-19",
    "abstract": "Underwater noise pollution from human activities, particularly shipping, has\nbeen recognised as a serious threat to marine life. The sound generated by\nvessels can have various adverse effects on fish and aquatic ecosystems in\ngeneral. In this setting, the estimation and analysis of the underwater noise\nproduced by vessels is an important challenge for the preservation of the\nmarine environment. In this paper we propose a model for the spatio-temporal\ncharacterisation of the underwater noise generated by vessels. The approach is\nbased on the reconstruction of the vessels' trajectories from Automatic\nIdentification System (AIS) data and on their deployment in a spatio-temporal\ndatabase. Trajectories are enriched with semantic information like the acoustic\ncharacteristics of the vessels' engines or the activity performed by the\nvessels. We define a model for underwater noise propagation and use the\ntrajectories' information to infer how noise propagates in the area of\ninterest. We develop our approach for the case study of the fishery activities\nin the Northern Adriatic sea, an area of the Mediterranean sea which is well\nknown to be highly exploited. We implement our approach using MobilityDB, an\nopen source geospatial trajectory data management and analysis platform, which\noffers spatio-temporal operators and indexes improving the efficiency of our\nsystem. We use this platform to conduct various analyses of the underwater\nnoise generated in the Northern Adriatic Sea, aiming at estimating the impact\nof fishing activities on underwater noise pollution and at demonstrating the\nflexibility and expressiveness of our approach.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Architecture for Trajectory-Based Fishing Ship Classification with AIS Data",
    "url": "http://arxiv.org/abs/2501.02038v1",
    "authors": [
      "David S\u00e1nchez Pedroche",
      "Daniel Amigo",
      "Jes\u00fas Garc\u00eda",
      "Jose M. Molina"
    ],
    "published": "2025-01-03",
    "abstract": "This paper proposes a data preparation process for managing real-world\nkinematic data and detecting fishing vessels. The solution is a binary\nclassification that classifies ship trajectories into either fishing or\nnon-fishing ships. The data used are characterized by the typical problems\nfound in classic data mining applications using real-world data, such as noise\nand inconsistencies. The two classes are also clearly unbalanced in the data, a\nproblem which is addressed using algorithms that resample the instances. For\nclassification, a series of features are extracted from spatiotemporal data\nthat represent the trajectories of the ships, available from sequences of\nAutomatic Identification System (AIS) reports. These features are proposed for\nthe modelling of ship behavior but, because they do not contain context-related\ninformation, the classification can be applied in other scenarios.\nExperimentation shows that the proposed data preparation process is useful for\nthe presented classification problem. In addition, positive results are\nobtained using minimal information.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Composing Open-domain Vision with RAG for Ocean Monitoring and Conservation",
    "url": "http://arxiv.org/abs/2412.02262v1",
    "authors": [
      "Sepand Dyanatkar",
      "Angran Li",
      "Alexander Dungate"
    ],
    "published": "2024-12-03",
    "abstract": "Climate change's destruction of marine biodiversity is threatening\ncommunities and economies around the world which rely on healthy oceans for\ntheir livelihoods. The challenge of applying computer vision to niche,\nreal-world domains such as ocean conservation lies in the dynamic and diverse\nenvironments where traditional top-down learning struggle with long-tailed\ndistributions, generalization, and domain transfer. Scalable species\nidentification for ocean monitoring is particularly difficult due to the need\nto adapt models to new environments and identify rare or unseen species. To\novercome these limitations, we propose leveraging bottom-up, open-domain\nlearning frameworks as a resilient, scalable solution for image and video\nanalysis in marine applications. Our preliminary demonstration uses pretrained\nvision-language models (VLMs) combined with retrieval-augmented generation\n(RAG) as grounding, leaving the door open for numerous architectural, training\nand engineering optimizations. We validate this approach through a preliminary\napplication in classifying fish from video onboard fishing vessels,\ndemonstrating impressive emergent retrieval and prediction capabilities without\ndomain-specific training or knowledge of the task itself.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Identifying companies and financial actors exposed to marine tipping points",
    "url": "http://arxiv.org/abs/2411.10307v1",
    "authors": [
      "Juan C. Rocha",
      "Jean-Baptiste Jouffray",
      "Frida Bengtsson",
      "Bianca-Ioana Voicu",
      "Paula A. S\u00e1nchez",
      "Victor Galaz"
    ],
    "published": "2024-11-15",
    "abstract": "Climate change and other anthropogenic pressures are likely to induce tipping\npoints in marine ecosystems, potentially leading to declines in primary\nproductivity and fisheries. Despite increasing attention to nature-related\nfinancial risks and opportunities within the ocean economy, the extent to which\nthese tipping points could affect investors has remained largely unexplored.\nHere we used satellite data to track fishing vessels operating in areas prone\nto marine regime shifts, as identified by their loss of resilience and\nvulnerability to marine heatwaves, and uncovered their corporate beneficial\nowners and shareholders. Despite some data gaps, we identified key countries,\ncompanies, and shareholders exposed to tipping risk. We also outline the\npotential challenges and opportunities that these actors may face if marine\necosystems shift to less productive states.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Vessel Re-identification and Activity Detection in Thermal Domain for Maritime Surveillance",
    "url": "http://arxiv.org/abs/2406.08294v1",
    "authors": [
      "Yasod Ginige",
      "Ransika Gunasekara",
      "Darsha Hewavitharana",
      "Manjula Ariyarathne",
      "Ranga Rodrigo",
      "Peshala Jayasekara"
    ],
    "published": "2024-06-12",
    "abstract": "Maritime surveillance is vital to mitigate illegal activities such as drug\nsmuggling, illegal fishing, and human trafficking. Vision-based maritime\nsurveillance is challenging mainly due to visibility issues at night, which\nresults in failures in re-identifying vessels and detecting suspicious\nactivities. In this paper, we introduce a thermal, vision-based approach for\nmaritime surveillance with object tracking, vessel re-identification, and\nsuspicious activity detection capabilities. For vessel re-identification, we\npropose a novel viewpoint-independent algorithm which compares features of the\nsides of the vessel separately (separate side-spaces) leveraging shape\ninformation in the absence of color features. We propose techniques to adapt\ntracking and activity detection algorithms for the thermal domain and train\nthem using a thermal dataset we created. This dataset will be the first\npublicly available benchmark dataset for thermal maritime surveillance. Our\nsystem is capable of re-identifying vessels with an 81.8% Top1 score and\nidentifying suspicious activities with a 72.4\\% frame mAP score; a new\nbenchmark for each task in the thermal domain.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General Optimization Problems",
    "url": "http://arxiv.org/abs/2405.02850v7",
    "authors": [
      "Jiawen Li",
      "Anwar PP Abdul Majeed",
      "Pascal Lefevre"
    ],
    "published": "2024-05-05",
    "abstract": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems. The HEO mimics the effects between quantum such as tunneling,\nentanglement. After the introduction to the HEO mechansims, the study presents\na comprehensive evaluation of HEO's performance against extensively-used\noptimization algorithms, including Particle Swarm Optimization (PSO), Genetic\nAlgorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer\n(GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary\nanalysis encompasses 14 benchmark functions with dimension 30, demonstrating\nHEO's effectiveness and adaptability in navigating general optimization\nproblems. The test of HEO in Pressure Vessel Design and Tubular Column Design\nalso infers its feasibility and potential in real-time applications. Further\nvalidation of HEO in Osmancik-97 and Cammeo Rice Classification achieves a\nhigher accuracy record.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "FAD-SAR: A Novel Fishing Activity Detection System via Synthetic Aperture Radar Images Based on Deep Learning Method",
    "url": "http://arxiv.org/abs/2404.18245v2",
    "authors": [
      "Yanbing Bai",
      "Siao Li",
      "Rui-Yang Ju",
      "Zihao Yang",
      "Jinze Yu",
      "Jen-Shiun Chiang"
    ],
    "published": "2024-04-28",
    "abstract": "Illegal, unreported, and unregulated (IUU) fishing activities seriously\naffect various aspects of human life. However, traditional methods for\ndetecting and monitoring IUU fishing activities at sea have limitations.\nAlthough synthetic aperture radar (SAR) can complement existing vessel\ndetection systems, extracting useful information from SAR images using\ntraditional methods remains a challenge, especially in IUU fishing. This paper\nproposes a deep learning based fishing activity detection system, which is\nimplemented on the xView3 dataset using six classical object detection models:\nSSD, RetinaNet, FSAF, FCOS, Faster R-CNN, and Cascade R-CNN. In addition, this\nwork employs different enhancement techniques to improve the performance of the\nFaster R-CNN model. The experimental results demonstrate that training the\nFaster R-CNN model using the Online Hard Example Mining (OHEM) strategy\nincreases the Avg-F1 value from 0.212 to 0.216.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Shipping traffic through the Arctic Ocean: spatial distribution, temporal evolution and its dependence on the sea ice extent",
    "url": "http://arxiv.org/abs/2403.01856v1",
    "authors": [
      "Jorge P. Rodr\u00edguez",
      "Konstantin Klemm",
      "Carlos M. Duarte",
      "V\u00edctor M. Egu\u00edluz"
    ],
    "published": "2024-03-04",
    "abstract": "The reduction in sea ice cover with Arctic warming facilitates the transit of\nships through routes that are remarkably shorter than the traditional shipping\nroutes. Automatic Identification System (AIS), ideally designed to avoid vessel\ncollisions, transmits on vessel navigation information (currently 27 types of\nmessages) such as name, position or speed, is a powerful data source to monitor\nthe progress of Arctic shipping as the ice cover decreases. Based on the\nanalysis of an online platform collecting shipping AIS data, we quantified the\nspatial distribution of shipping through the Arctic Ocean, its intensity and\nthe temporal evolution, in relation to the area released by the sea ice area.\nShipping through the Arctic Ocean is distributed spatially following a\nheavy-tailed distribution, implying heavy traffic through a limited Arctic\narea, with an exponent that depends on the vessel category. Fishing is the\ncategory with the largest spatial spread, with the width of shipping routes\ncorrelated with the proximal sea ice area. The time evolution of these routes\nis characterized by increasing extended periods of shipping activity through\nthe year. AIS data offers valuable information on the activity of the\ninternational fleet worldwide. In the context of the new international\nagreements, it is a valuable source to monitor shipping, fishing and the\npotential impact in marine life among other aspects. Here we have focused on\nthe Arctic shipping in recent years, which is rapidly growing, particularly\naround the Northeastern and Northwest Passage coastal routes, providing an\nopportunity for the design of shorter shipping routes and reduced greenhouse\ngas emissions from transport of goods, but at a risk of impacts on the Arctic\necosystem.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Prediction of the Economic Behavior of Fishery Biotechnology Companies Based on Machine Learning-Based Deep Metacellular Automata",
    "url": "http://arxiv.org/abs/2402.13509v2",
    "authors": [
      "Liguo Chen",
      "Hongyang Hua",
      "Xinyue Luo",
      "Guoli Xu",
      "Xu Yan"
    ],
    "published": "2024-02-21",
    "abstract": "Ocean warming significantly affects the fishing industry, with species like\nScottish herring and mackerel migrating northwards. Our research, a fusion of\nartificial intelligence, data science, and operations research, addresses this\ncrisis. Using Long Short Term Memory networks, we forecast sea surface\ntemperatures (SST) and model fish migratory patterns with Enhanced Cellular\nAutomata. A corrective factor within our model adjusts for human impact on SST,\nguiding diverse mitigation scenarios. We apply operational research to\nstrategize responses, including the modernization of fishing vessels as a less\ncostly alternative to relocation. Our data-driven approach, suggesting fleet\nmodernization, strategic relocation, and product diversification, offers an\neffective approach to mitigating the threats to the ocean warming phenomenon.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Experimental investigations of underwater and airborne noises produced by a large hovercraft in Ural River estuary",
    "url": "http://arxiv.org/abs/2401.14204v1",
    "authors": [
      "A. I. Vedenev",
      "O. Yu. Kochetov",
      "A. A. Lunkov",
      "A. S. Shurup",
      "S. S. Kassymbekova"
    ],
    "published": "2024-01-25",
    "abstract": "Simultaneous measurements of underwater and airborne noises produced by\nGriffon Hoverwork BHT130 hovercraft were carried out in environmentally\nsensitive area - wildlife preserve in the area of the Ural River estuary near\nthe Caspian Sea shelf. Measurements were organized to assess the possible\nnegative impact of noise from hovercraft on fish and birds in wildlife\npreserve. The particle velocity of underwater noise was estimated by using a\ngradient-type vector receiver. That was a distinctive aspect of the underwater\nnoise studies since the majority of fish perceives the sound in terms of\nvibration of particles, and only a few as the pressure. Using synchronous\nrecording of underwater and airborne noises, the mutual correlation of these\ndata was investigated. The obtained correlation levels between underwater and\nairborne noises produced by hovercraft can be used for simplified estimation of\nthe upper boundary of underwater noise level by measuring levels of airborne\nnoise. The measured and estimated maximal levels of underwater noises of\nhovercraft are considerably lower than noises from conventional vessels with\nunderwater engines, that makes hovercraft attractive alternative for use in\nlocations with high underwater noise requirements, such as Ural River estuary\nand Caspian Sea shelf.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case",
    "url": "http://arxiv.org/abs/2401.02278v1",
    "authors": [
      "Febrian Kurniawan",
      "Gandeva Bayu Satrya",
      "Firuz Kamalov"
    ],
    "published": "2024-01-04",
    "abstract": "The enormous demand for seafood products has led to exploitation of marine\nresources and near-extinction of some species. In particular, overfishing is\none the main issues in sustainable marine development. In alignment with the\nprotection of marine resources and sustainable fishing, this study proposes to\nadvance fish classification techniques that support identifying protected fish\nspecies using state-of-the-art machine learning. We use a custom modification\nof the MobileNet model to design a lightweight classifier called M-MobileNet\nthat is capable of running on limited hardware. As part of the study, we\ncompiled a labeled dataset of 37,462 images of fish found in the waters of the\nIndonesian archipelago. The proposed model is trained on the dataset to\nclassify images of the captured fish into their species and give\nrecommendations on whether they are consumable or not. Our modified MobileNet\nmodel uses only 50\\% of the top layer parameters with about 42% GTX 860M\nutility and achieves up to 97% accuracy in fish classification and determining\nits consumability. Given the limited computing capacity available on many\nfishing vessels, the proposed model provides a practical solution to on-site\nfish classification. In addition, synchronized implementation of the proposed\nmodel on multiple vessels can supply valuable information about the movement\nand location of different species of fish.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions",
    "url": "http://arxiv.org/abs/2504.13476v1",
    "authors": [
      "Jiadong Lou",
      "Bingqing Liu",
      "Yuanheng Xiong",
      "Xiaodong Zhang",
      "Xu Yuan"
    ],
    "published": "2025-04-18",
    "abstract": "Phytoplankton absorb and scatter light in unique ways, subtly altering the\ncolor of water, changes that are often minor for human eyes to detect but can\nbe captured by sensitive ocean color instruments onboard satellites from space.\nHyperspectral sensors, paired with advanced algorithms, are expected to\nsignificantly enhance the characterization of phytoplankton community\ncomposition, especially in coastal waters where ocean color remote sensing\napplications have historically encountered significant challenges. This study\npresents novel machine learning-based solutions for NASA's hyperspectral\nmissions, including EMIT and PACE, tackling high-fidelity retrievals of\nphytoplankton absorption coefficient and chlorophyll a from their hyperspectral\nremote sensing reflectance. Given that a single Rrs spectrum may correspond to\nvaried combinations of inherent optical properties and associated\nconcentrations, the Variational Autoencoder (VAE) is used as a backbone in this\nstudy to handle such multi-distribution prediction problems. We first time\ntailor the VAE model with innovative designs to achieve hyperspectral\nretrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex\nestuarine-coastal waters. Validation with extensive experimental observation\ndemonstrates superior performance of the VAE models with high precision and low\nbias. The in-depth analysis of VAE's advanced model structures and learning\ndesigns highlights the improvement and advantages of VAE-based solutions over\nthe mixture density network (MDN) approach, particularly on high-dimensional\ndata, such as PACE. Our study provides strong evidence that current EMIT and\nPACE hyperspectral data as well as the upcoming Surface Biology Geology mission\nwill open new pathways toward a better understanding of phytoplankton community\ndynamics in aquatic ecosystems when integrated with AI technologies.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Bayesian hierarchical framework for fusion of remote sensing data: An example with solar-induced fluorescence",
    "url": "http://arxiv.org/abs/2503.03901v1",
    "authors": [
      "Manju Johny",
      "Jonathan Hobbs",
      "Vineet Yadav",
      "Margaret Johnson",
      "Nicholas Parazoo",
      "Hai Nguyen",
      "Amy Braverman"
    ],
    "published": "2025-03-05",
    "abstract": "Solar-induced chlorophyll fluorescence (SIF) has emerged as an effective\nindicator of vegetation productivity and plant health. The global\nquantification of SIF and its associated uncertainties yields many important\ncapabilities, including improving carbon flux estimation, improving the\nidentification of carbon sources and sinks, monitoring a variety of ecosystems,\nand evaluating carbon sequestration efforts. Long-term, regional-to-global\nscale monitoring is now feasible with the availability of SIF estimates from\nmultiple Earth-observing satellites. These efforts can be aided by a rigorous\naccounting of the sources of uncertainty present in satellite SIF data\nproducts. In this paper, we introduce a Bayesian Hierarchical Model (BHM) for\nthe estimation of SIF and associated uncertainties from Orbiting Carbon\nObservatory-2 (OCO-2) satellite observations at one-degree resolution with\nglobal coverage. The hierarchical structure of our modeling framework allows\nfor convenient model specification, quantification of various sources of\nvariation, and the incorporation of seasonal SIF information through Fourier\nterms in the regression model. The modeling framework leverages the predictable\nseasonality of SIF in most temperate land areas. The resulting data product\ncomplements existing atmospheric carbon dioxide estimates at the same\nspatio-temporal resolution.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Fine scale depth regulation of invertebrate larvae around coastal fronts",
    "url": "http://arxiv.org/abs/2401.10303v1",
    "authors": [
      "Nicolas Weidberg",
      "Wayne Goschen",
      "Jennifer M. Jackson",
      "Paula Pattrick",
      "Christopher D. McQuaid",
      "Francesca Porri"
    ],
    "published": "2024-01-18",
    "abstract": "Vertical migrations of zooplankters have been widely described, but their\nactive movements through shallow, highly dynamic water columns within the inner\nshelf may be more complex and difficult to characterize. In this study,\ninvertebrate larvae, currents, and hydrographic variables were sampled at\ndifferent depths during and after the presence of fronts on three different\ncruises off the southern coast of South Africa. Internal wave dynamics were\nobserved in the hydrographic data set but also through satellite imagery,\nalthough strong surface convergent currents were absent and thermal\nstratification was weak. During the first two cruises, fronts were more\nconspicuous and they preceded strong onshore currents at depth which developed\nwith the rising tide. Vertical distributions of larvae changed accordingly,\nwith higher abundances at these deep layers once the front disappeared. The\nthird cruise was carried out during slack tides, the front was not conspicuous,\ndeep strong onshore currents did not occur afterward and larval distributions\ndid not change consistently through time. Overall, the vertical distributions\nof many larval taxa matched the vertical profiles of shoreward currents and\nmultivariate analyses revealed that these flows structured the larval\ncommunity, which was neither influenced by temperature nor chlorophyll. Thus,\nthe ability to regulate active vertical positioning may enhance shoreward\nadvection and determine nearshore larval distributions.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Mapping biodiversity at very-high resolution in Europe",
    "url": "http://arxiv.org/abs/2504.05231v1",
    "authors": [
      "C\u00e9sar Leblanc",
      "Lukas Picek",
      "Benjamin Deneu",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "R\u00e9mi Palard",
      "Alexis Joly"
    ],
    "published": "2025-04-07",
    "abstract": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "LLM",
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Climplicit: Climatic Implicit Embeddings for Global Ecological Tasks",
    "url": "http://arxiv.org/abs/2504.05089v2",
    "authors": [
      "Johannes Dollinger",
      "Damien Robert",
      "Elena Plekhanova",
      "Lukas Drees",
      "Jan Dirk Wegner"
    ],
    "published": "2025-04-07",
    "abstract": "Deep learning on climatic data holds potential for macroecological\napplications. However, its adoption remains limited among scientists outside\nthe deep learning community due to storage, compute, and technical expertise\nbarriers. To address this, we introduce Climplicit, a spatio-temporal\ngeolocation encoder pretrained to generate implicit climatic representations\nanywhere on Earth. By bypassing the need to download raw climatic rasters and\ntrain feature extractors, our model uses x3500 less disk space and\nsignificantly reduces computational needs for downstream tasks. We evaluate our\nClimplicit embeddings on biomes classification, species distribution modeling,\nand plant trait regression. We find that single-layer probing our Climplicit\nembeddings consistently performs better or on par with training a model from\nscratch on downstream tasks and overall better than alternative geolocation\nencoding models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Bayesian Deep Latent Class Regression",
    "url": "http://arxiv.org/abs/2503.17531v2",
    "authors": [
      "Yuren Zhou",
      "Yuqi Gu",
      "David B. Dunson"
    ],
    "published": "2025-03-21",
    "abstract": "High-dimensional categorical data arise in diverse scientific domains and are\noften accompanied by covariates. Latent class regression models are routinely\nused in such settings, reducing dimensionality by assuming conditional\nindependence of the categorical variables given a single latent class that\ndepends on covariates through a logistic regression model. However, such\nmethods become unreliable as the dimensionality increases. To address this, we\npropose a flexible family of deep latent class models. Our model satisfies key\ntheoretical properties, including identifiability and posterior consistency,\nand we establish a Bayes oracle clustering property that ensures robustness\nagainst the curse of dimensionality. We develop efficient posterior computation\nmethods, validate them through simulation studies, and apply our model to joint\nspecies distribution modeling in ecology.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling",
    "url": "http://arxiv.org/abs/2503.13057v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Gencer Sumbul",
      "Chiara Vanalli",
      "Benjamin Kellenberger",
      "Devis Tuia"
    ],
    "published": "2025-03-17",
    "abstract": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Heterogenous graph neural networks for species distribution modeling",
    "url": "http://arxiv.org/abs/2503.11900v2",
    "authors": [
      "Lauren Harrell",
      "Christine Kaeser-Chen",
      "Burcu Karagol Ayan",
      "Keith Anderson",
      "Michelangelo Conserva",
      "Elise Kleeman",
      "Maxim Neumann",
      "Matt Overlan",
      "Melissa Chapman",
      "Drew Purves"
    ],
    "published": "2025-03-14",
    "abstract": "Species distribution models (SDMs) are necessary for measuring and predicting\noccurrences and habitat suitability of species and their relationship with\nenvironmental factors. We introduce a novel presence-only SDM with graph neural\nnetworks (GNN). In our model, species and locations are treated as two distinct\nnode sets, and the learning task is predicting detection records as the edges\nthat connect locations to species. Using GNN for SDM allows us to model\nfine-grained interactions between species and the environment. We evaluate the\npotential of this methodology on the six-region dataset compiled by National\nCenter for Ecological Analysis and Synthesis (NCEAS) for benchmarking SDMs. For\neach of the regions, the heterogeneous GNN model is comparable to or\noutperforms previously-benchmarked single-species SDMs as well as a\nfeed-forward neural network baseline model.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "GNN",
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling",
    "url": "http://arxiv.org/abs/2502.03480v1",
    "authors": [
      "Diana Koldasbayeva",
      "Alexey Zaytsev"
    ],
    "published": "2025-01-27",
    "abstract": "Species Distribution Models (SDMs) often suffer from spatial autocorrelation\n(SAC), leading to biased performance estimates. We tested cross-validation (CV)\nstrategies - random splits, spatial blocking with varied distances,\nenvironmental (ENV) clustering, and a novel spatio-temporal method - under two\nproposed training schemes: LAST FOLD, widely used in spatial CV at the cost of\ndata loss, and RETRAIN, which maximizes data usage but risks reintroducing SAC.\nLAST FOLD consistently yielded lower errors and stronger correlations. Spatial\nblocking at an optimal distance (SP 422) and ENV performed best, achieving\nSpearman and Pearson correlations of 0.485 and 0.548, respectively, although\nENV may be unsuitable for long-term forecasts involving major environmental\nshifts. A spatio-temporal approach yielded modest benefits in our moderately\nvariable dataset, but may excel with stronger temporal changes. These findings\nhighlight the need to align CV approaches with the spatial and temporal\nstructure of SDM data, ensuring rigorous validation and reliable predictive\noutcomes.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Applying the maximum entropy principle to neural networks enhances multi-species distribution models",
    "url": "http://arxiv.org/abs/2412.19217v3",
    "authors": [
      "Maxime Ryckewaert",
      "Diego Marcos",
      "Christophe Botella",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2024-12-26",
    "abstract": "The rapid expansion of citizen science initiatives has led to a significant\ngrowth of biodiversity databases, and particularly presence-only (PO)\nobservations. PO data are invaluable for understanding species distributions\nand their dynamics, but their use in a Species Distribution Model (SDM) is\ncurtailed by sampling biases and the lack of information on absences. Poisson\npoint processes are widely used for SDMs, with Maxent being one of the most\npopular methods. Maxent maximises the entropy of a probability distribution\nacross sites as a function of predefined transformations of variables, called\nfeatures. In contrast, neural networks and deep learning have emerged as a\npromising technique for automatic feature extraction from complex input\nvariables. Arbitrarily complex transformations of input variables can be\nlearned from the data efficiently through backpropagation and stochastic\ngradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses\nneural networks to automatically learn shared features among species, using the\nmaximum entropy principle. To do so, it employs a normalised Poisson loss where\nfor each species, presence probabilities across sites are modelled by a neural\nnetwork. We evaluate DeepMaxent on a benchmark dataset known for its spatial\nsampling biases, using PO data for calibration and presence-absence (PA) data\nfor validation across six regions with different biological groups and\ncovariates. Our results indicate that DeepMaxent performs better than Maxent\nand other leading SDMs across all regions and taxonomic groups. The method\nperforms particularly well in regions of uneven sampling, demonstrating\nsubstantial potential to increase SDM performances. In particular, our approach\nyields more accurate predictions than traditional single-species models, which\nopens up new possibilities for methodological enhancement.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MiTREE: Multi-input Transformer Ecoregion Encoder for Species Distribution Modelling",
    "url": "http://arxiv.org/abs/2412.18995v1",
    "authors": [
      "Theresa Chen",
      "Yao-Yi Chiang"
    ],
    "published": "2024-12-25",
    "abstract": "Climate change poses an extreme threat to biodiversity, making it imperative\nto efficiently model the geographical range of different species. The\navailability of large-scale remote sensing images and environmental data has\nfacilitated the use of machine learning in Species Distribution Models (SDMs),\nwhich aim to predict the presence of a species at any given location.\nTraditional SDMs, reliant on expert observation, are labor-intensive, but\nadvancements in remote sensing and citizen science data have facilitated\nmachine learning approaches to SDM development. However, these models often\nstruggle with leveraging spatial relationships between different inputs -- for\ninstance, learning how climate data should inform the data present in satellite\nimagery -- without upsampling or distorting the original inputs. Additionally,\nlocation information and ecological characteristics at a location play a\ncrucial role in predicting species distribution models, but these aspects have\nnot yet been incorporated into state-of-the-art approaches. In this work, we\nintroduce MiTREE: a multi-input Vision-Transformer-based model with an\necoregion encoder. MiTREE computes spatial cross-modal relationships without\nupsampling as well as integrates location and ecological context. We evaluate\nour model on the SatBird Summer and Winter datasets, the goal of which is to\npredict bird species encounter rates, and we find that our approach improves\nupon state-of-the-art baselines.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Spatial Clustering of Citizen Science Data Improves Downstream Species Distribution Models",
    "url": "http://arxiv.org/abs/2412.15559v3",
    "authors": [
      "Nahian Ahmed",
      "Mark Roth",
      "Tyler A. Hallman",
      "W. Douglas Robinson",
      "Rebecca A. Hutchinson"
    ],
    "published": "2024-12-20",
    "abstract": "Citizen science biodiversity data present great opportunities for ecology and\nconservation across vast spatial and temporal scales. However, the\nopportunistic nature of these data lacks the sampling structure required by\nmodeling methodologies that address a pervasive challenge in ecological data\ncollection: imperfect detection, i.e., the likelihood of under-observing\nspecies on field surveys. Occupancy modeling is an example of an approach that\naccounts for imperfect detection by explicitly modeling the observation process\nseparately from the biological process of habitat selection. This produces\nspecies distribution models that speak to the pattern of the species on a\nlandscape after accounting for imperfect detection in the data, rather than the\npattern of species observations corrupted by errors. To achieve this benefit,\noccupancy models require multiple surveys of a site across which the site's\nstatus (i.e., occupied or not) is assumed constant. Since citizen science data\nare not collected under the required repeated-visit protocol, observations may\nbe grouped into sites post hoc. Existing approaches for constructing sites\ndiscard some observations and/or consider only geographic distance and not\nenvironmental similarity. In this study, we compare ten approaches for site\nconstruction in terms of their impact on downstream species distribution models\nfor 31 bird species in Oregon, using observations recorded in the eBird\ndatabase. We find that occupancy models built on sites constructed by spatial\nclustering algorithms perform better than existing alternatives.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "FathomVerse: A community science dataset for ocean animal discovery",
    "url": "http://arxiv.org/abs/2412.01701v1",
    "authors": [
      "Genevieve Patterson",
      "Joost Daniels",
      "Benjamin Woodward",
      "Kevin Barnard",
      "Giovanna Sainz",
      "Lonny Lundsten",
      "Kakani Katija"
    ],
    "published": "2024-12-02",
    "abstract": "Can computer vision help us explore the ocean? The ultimate challenge for\ncomputer vision is to recognize any visual phenomena, more than only the\nobjects and animals humans encounter in their terrestrial lives. Previous\ndatasets have explored everyday objects and fine-grained categories humans see\nfrequently. We present the FathomVerse v0 detection dataset to push the limits\nof our field by exploring animals that rarely come in contact with people in\nthe deep sea. These animals present a novel vision challenge.\n  The FathomVerse v0 dataset consists of 3843 images with 8092 bounding boxes\nfrom 12 distinct morphological groups recorded at two locations on the deep\nseafloor that are new to computer vision. It features visually perplexing\nscenarios such as an octopus intertwined with a sea star, and confounding\ncategories like vampire squids and sea spiders. This dataset can push forward\nresearch on topics like fine-grained transfer learning, novel category\ndiscovery, species distribution modeling, and carbon cycle analysis, all of\nwhich are important to the care and husbandry of our planet.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Joint Spatiotemporal Modeling of Zooplankton and Whale Abundance in a Dynamic Marine Environment",
    "url": "http://arxiv.org/abs/2411.06001v1",
    "authors": [
      "Bokgyeong Kang",
      "Erin M. Schliep",
      "Alan E. Gelfand",
      "Christopher W. Clark",
      "Christine A. Hudak",
      "Charles A. Mayo",
      "Ryan Schosberg",
      "Tina M. Yack",
      "Robert S. Schick"
    ],
    "published": "2024-11-08",
    "abstract": "North Atlantic right whales are an endangered species; their entire\npopulation numbers approximately 372 individuals, and they are subject to major\nanthropogenic threats. They feed on zooplankton species whose distribution\nshifts in a dynamic and warming oceanic environment. Because right whales in\nturn follow their shifting food resource, it is necessary to jointly study the\ndistribution of whales and their prey. The innovative joint species\ndistribution modeling (JSDM) contribution here is different from anything in\nthe large JDSM literature, reflecting the processes and data we have to work\nwith. Specifically, our JSDM supplies a geostatistical model for expected\namount of zooplankton collected at a site. We require a point pattern model for\nthe intensity of right whale abundance. The two process models are joined\nthrough a latent conditional-marginal specification. Further, each species has\ntwo data sources to inform their respective distributions and these sources\nrequire novel data fusion. What emerges is a complex multi-level model. Through\nsimulation we demonstrate the ability of our joint specification to identify\nmodel unknowns and learn better about the species distributions than modeling\nthem individually. We then apply our modeling to real data from Cape Cod Bay,\nMassachusetts in the U.S.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-Scale and Multimodal Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2411.04016v1",
    "authors": [
      "Nina van Tiel",
      "Robin Zbinden",
      "Emanuele Dalsasso",
      "Benjamin Kellenberger",
      "Lo\u00efc Pellissier",
      "Devis Tuia"
    ],
    "published": "2024-11-06",
    "abstract": "Species distribution models (SDMs) aim to predict the distribution of species\nby relating occurrence data with environmental variables. Recent applications\nof deep learning to SDMs have enabled new avenues, specifically the inclusion\nof spatial data (environmental rasters, satellite images) as model predictors,\nallowing the model to consider the spatial context around each species'\nobservations. However, the appropriate spatial extent of the images is not\nstraightforward to determine and may affect the performance of the model, as\nscale is recognized as an important factor in SDMs. We develop a modular\nstructure for SDMs that allows us to test the effect of scale in both single-\nand multi-scale settings. Furthermore, our model enables different scales to be\nconsidered for different modalities, using a late fusion approach. Results on\nthe GeoLifeCLEF 2023 benchmark indicate that considering multimodal data and\nlearning multi-scale representations leads to more accurate models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Hybrid Spatial Representations for Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2410.10937v2",
    "authors": [
      "Shiran Yuan",
      "Hao Zhao"
    ],
    "published": "2024-10-14",
    "abstract": "We address an important problem in ecology called Species Distribution\nModeling (SDM), whose goal is to predict whether a species exists at a certain\nposition on Earth. In particular, we tackle a challenging version of this task,\nwhere we learn from presence-only data in a community-sourced dataset, model a\nlarge number of species simultaneously, and do not use any additional\nenvironmental information. Previous work has used neural implicit\nrepresentations to construct models that achieve promising results. However,\nimplicit representations often generate predictions of limited spatial\nprecision. We attribute this limitation to their inherently global formulation\nand inability to effectively capture local feature variations. This issue is\nespecially pronounced with presence-only data and a large number of species. To\naddress this, we propose a hybrid embedding scheme that combines both implicit\nand explicit embeddings. Specifically, the explicit embedding is implemented\nwith a multiresolution hashgrid, enabling our models to better capture local\ninformation. Experiments demonstrate that our results exceed other works by a\nlarge margin on various standard benchmarks, and that the hybrid representation\nis better than both purely implicit and explicit ones. Qualitative\nvisualizations and comprehensive ablation studies reveal that our hybrid\nrepresentation successfully addresses the two main challenges. Our code is\nopen-sourced at https://github.com/Shiran-Yuan/HSR-SDM.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MALPOLON: A Framework for Deep Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2409.18102v1",
    "authors": [
      "Theo Larcher",
      "Lukas Picek",
      "Benjamin Deneu",
      "Titouan Lorieul",
      "Maximilien Servajean",
      "Alexis Joly"
    ],
    "published": "2024-09-26",
    "abstract": "This paper describes a deep-SDM framework, MALPOLON. Written in Python and\nbuilt upon the PyTorch library, this framework aims to facilitate training and\ninferences of deep species distribution models (deep-SDM) and sharing for users\nwith only general Python language skills (e.g., modeling ecologists) who are\ninterested in testing deep learning approaches to build new SDMs. More advanced\nusers can also benefit from the framework's modularity to run more specific\nexperiments by overriding existing classes while taking advantage of\npress-button examples to train neural networks on multiple classification tasks\nusing custom or provided raw and pre-processed datasets. The framework is\nopen-sourced on GitHub and PyPi along with extensive documentation and examples\nof use in various scenarios. MALPOLON offers straightforward installation,\nYAML-based configuration, parallel computing, multi-GPU utilization, baseline\nand foundational models for benchmarking, and extensive\ntutorials/documentation, aiming to enhance accessibility and performance\nscalability for ecologists and researchers.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Integrating systematic surveys with historical data to model the distribution of Ornithodoros turicata americanus, a vector of epidemiological concern in North America",
    "url": "http://arxiv.org/abs/2409.12761v1",
    "authors": [
      "Sebastian Botero-Canola",
      "Carson Torhorst",
      "Nicholas Canino",
      "Lorenza Beati",
      "Kathleen C. O Hara",
      "Angela M. James",
      "Samantha M. Wisely"
    ],
    "published": "2024-09-19",
    "abstract": "Globally, vector-borne diseases are increasing in distribution and frequency,\naffecting humans, domestic animals and livestock, and wildlife. Science-based\nmanagement and prevention of these diseases requires a sound understanding of\nthe distribution and environmental requirements of the vectors and hosts\ninvolved in disease transmission. Integrated Species Distribution Models (ISDM)\naccount for diverse data types through hierarchical modeling and represent a\nsignificant advancement in species distribution modeling that have not yet been\nleveraged in disease ecology. We used this approach, as implemented in the\nrecently developed R package RISDM, to assess the distribution of the soft tick\nsubspecies Ornithodoros turicata americanus. We created an ISDM for O. t.\namericanus, using systematically collected field data and historical records of\nthis tick species in the southeastern US, to predict its distribution and\nassess potential correlations with environmental variables. Given the novelty\nof this method, we compared the results to a conventional Maxent SDM and\nvalidated the results through data partitioning using true skills statistics\n(TSS), sensitivity, and area under the ROC curve (AUC) metrics. We found that a\ncombination of climatic variables describing seasonality and temperature\nextremes, along with the amount of sand in the soil, determined the predicted\nintensity of occurrence of this tick species. When projected in geographic\nspace, this distribution model predicted 62% of Florida as suitable habitat for\nthis tick species. The ISDM presented a higher TSS and AUC than the Maxent\nconventional model, while sensitivity was similar between both models. Our case\nexample shows the utility of ISDMs in disease ecology studies and highlights\nthe broad range of geographic suitability for this important disease vector.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Trophic Cascades and Habitat Suitability in Udanti Sitnadi Tiger Reserve: Impacts of Prey Depletion and Climate Change on Predator Prey Dynamics",
    "url": "http://arxiv.org/abs/2409.00193v1",
    "authors": [
      "Krishnendu Basak",
      "Chiranjib Chaudhuri",
      "M Suraj",
      "Moiz Ahmed"
    ],
    "published": "2024-08-30",
    "abstract": "This study investigates the trophic cascades and habitat suitability in\nUdanti Sitnadi Tiger Reserve (USTR), highlighting the roles of apex predators,\nsubordinate predators, and prey species in maintaining ecosystem balance. Using\nthe Trophic Species Distribution Model (SDM), we explored prey-predator\ninteractions and habitat suitability, revealing that tigers, due to prey\ndepletion, increasingly rely on cattle, while leopards adapt by preying on\nsmaller species. The study emphasizes the need for prey augmentation and\nhabitat restoration to support apex predators. Additionally, climate change\nprojections for 2021-2040 and 2081-2100 under CMIP6 scenarios SSP245 and SSP585\nindicate significant regional habitat shifts, necessitating adaptive management\nstrategies. Kuladighat is projected to face habitat contraction, while Sitanadi\nmay experience habitat expansion. Effective conservation efforts such as\nhabitat restoration, prey augmentation and predator recovery are the most\nimportant steps needed to maintain the purpose of a Tiger reserve and\nconservation potential of Udanti-Sonabeda Tiger Conservation Unit (TCU). To\nachieve these dynamics, focusing on community participation, anti-poaching\nmeasures, and scientific recommendations are the most crucial components to\nfocus on. This comprehensive analysis underscores the critical role of targeted\nconservation activities in prey-depleted landscapes to ensure the long-term\nsurvival of tigers and the overall health of forest ecosystems, enhancing\nbiodiversity and mitigating human-wildlife conflicts in USTR.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Dogs on forest trails; Understanding ecology of Striped Hyena and wild Canids in the presence of free-ranging dogs in Udanti-Sitanadi Tiger Reserve, Central India using Joint Distribution and Deep Neural Networks",
    "url": "http://arxiv.org/abs/2409.00185v1",
    "authors": [
      "Chiranjib Chaudhuri",
      "Krishnendu Basak",
      "M Suraj",
      "Moiz Ahmed",
      "Amit Kumar"
    ],
    "published": "2024-08-30",
    "abstract": "This study uses Joint Species Distribution Models (JSDMs) and Deep Neural\nNetworks (DNNs) to explore how wild carnivores and free-ranging dogs interact\nin the Udanti-Sitanadi Tiger Reserve (USTR) in Central India. The research\nfocuses on key species like the Striped Hyena, Grey Wolf, Golden Jackal, and\nIndian Fox, revealing significant overlaps in habitat with free-ranging dogs,\nespecially in densely populated areas like the Sitanadi region of the tiger\nreserve. These overlaps pose serious risks to wildlife through competition for\nresources, predation, and the spread of diseases. The study shows that the\nStriped Hyena prefers gentle slopes and forested areas, while the Grey Wolf\ntends to avoid cropland and thrives in regions with higher rainfall that\nsupports a stable prey base. The Golden Jackal, more adaptable than the others,\nfavors west-facing slopes and stable temperatures, whereas the Indian Fox is\nmainly found in the less disturbed, mountainous Kuladighat region.\nAdditionally, the study highlights the potential impacts of climate change,\npredicting that the Grey Wolf could face habitat extinction under more severe\nscenarios. These findings underscore the urgent need for conservation\nstrategies tailored to address both dog wild carnivore interactions and the\ngrowing challenges posed by climate change, focusing on protecting the critical\nhabitats of vulnerable species like the Striped Hyena and Grey Wolf.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Generating Binary Species Range Maps",
    "url": "http://arxiv.org/abs/2408.15956v1",
    "authors": [
      "Filip Dorm",
      "Christian Lange",
      "Scott Loarie",
      "Oisin Mac Aodha"
    ],
    "published": "2024-08-28",
    "abstract": "Accurately predicting the geographic ranges of species is crucial for\nassisting conservation efforts. Traditionally, range maps were manually created\nby experts. However, species distribution models (SDMs) and, more recently,\ndeep learning-based variants offer a potential automated alternative. Deep\nlearning-based SDMs generate a continuous probability representing the\npredicted presence of a species at a given location, which must be binarized by\nsetting per-species thresholds to obtain binary range maps. However, selecting\nappropriate per-species thresholds to binarize these predictions is non-trivial\nas different species can require distinct thresholds. In this work, we evaluate\ndifferent approaches for automatically identifying the best thresholds for\nbinarizing range maps using presence-only data. This includes approaches that\nrequire the generation of additional pseudo-absence data, along with ones that\nonly require presence data. We also propose an extension of an existing\npresence-only technique that is more robust to outliers. We perform a detailed\nevaluation of different thresholding techniques on the tasks of binary range\nestimation and large-scale fine-grained visual classification, and we\ndemonstrate improved performance over existing pseudo-absence free approaches\nusing our method.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "GeoPlant: Spatial Plant Species Prediction Dataset",
    "url": "http://arxiv.org/abs/2408.13928v2",
    "authors": [
      "Lukas Picek",
      "Christophe Botella",
      "Maximilien Servajean",
      "C\u00e9sar Leblanc",
      "R\u00e9mi Palard",
      "Th\u00e9o Larcher",
      "Benjamin Deneu",
      "Diego Marcos",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2024-08-25",
    "abstract": "The difficulty of monitoring biodiversity at fine scales and over large areas\nlimits ecological knowledge and conservation efforts. To fill this gap, Species\nDistribution Models (SDMs) predict species across space from spatially explicit\nfeatures. Yet, they face the challenge of integrating the rich but\nheterogeneous data made available over the past decade, notably millions of\nopportunistic species observations and standardized surveys, as well as\nmultimodal remote sensing data. In light of that, we have designed and\ndeveloped a new European-scale dataset for SDMs at high spatial resolution\n(10--50m), including more than 10k species (i.e., most of the European flora).\nThe dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive\nPresence-Absence survey records, all accompanied by diverse environmental\nrasters (e.g., elevation, human footprint, and soil) traditionally used in\nSDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10\nm resolution, a 20-year time series of climatic variables, and satellite time\nseries from the Landsat program. In addition to the data, we provide an openly\naccessible SDM benchmark (hosted on Kaggle), which has already attracted an\nactive community and a set of strong baselines for single predictor/modality\nand multimodal approaches. All resources, e.g., the dataset, pre-trained\nmodels, and baseline methods (in the form of notebooks), are available on\nKaggle, allowing one to start with our dataset literally with two mouse clicks.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Fast fitting of phylogenetic mixed-effects models",
    "url": "http://arxiv.org/abs/2408.05333v2",
    "authors": [
      "Bert van der Veen",
      "Robert Brian O'Hara"
    ],
    "published": "2024-08-09",
    "abstract": "Mixed-effects models are among the most commonly used statistical methods for\nthe exploration of multispecies data. In recent years, also Joint Species\nDistribution Models and Generalized Linear Latent Variale Models have gained in\npopularity when the goal is to incorporate residual covariation between species\nthat cannot be explained due to measured environmental covariates. Few software\nimplementations of such models exist that can additionally incorporate\nphylogenetic information, and those that exist tend to utilize Markov chain\nMonte Carlo methods for estimation, so that model fitting takes a long time. In\nthis article we develop new methods for quickly and flexibly fitting\nphylogenetic mixed-effects models, potentially incorporating residual\ncovariation between species using latent variables, with the possibility to\nestimate the strength of phylogenetic structuring in species responses per\nenvironmental covariate, and while incorporating correlation between different\ncovariate effects. By combining Variational approximations, a sparse\napproximation to the phylogenetic precision matrix, and parallel computation,\nphylogenetic mixed-effects models can be fitted much more quickly than the\ncurrent state-of-the-art. Two simulation studies demonstrate that the proposed\ncombination of approximations is fast and enjoys high accuracy. We explore\nsensitivity of the approximation to the ordering of species with a real world\ndataset of wood-decaying fungi.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Implication of modelling choices on connectivity estimation: A comparative analysis",
    "url": "http://arxiv.org/abs/2407.09564v1",
    "authors": [
      "Marie Soret",
      "Sylvain Moulherat",
      "Maxime Lenormand",
      "Sandra Luque"
    ],
    "published": "2024-07-05",
    "abstract": "We focus on connectivity methods used to understand and predict how\nlandscapes and habitats facilitate or impede the movement and dispersal of\nspecies. Our objective is to compare the implication of methodological choices\nat three stages of the modelling framework: landscape characterisation,\nconnectivity estimation, and connectivity assessment. What are the convergences\nand divergences of different modelling approaches? What are the implications of\ntheir combined results for landscape planning? We implemented two landscape\ncharacterisation approaches: expert opinion and species distribution model\n(SDM); four connectivity estimation models: Euclidean distance, least-cost\npaths (LCP), circuit theory, and stochastic movement simulation (SMS); and two\nconnectivity indices: flux and area-weighted flux (dPCflux). We compared\noutcomes such as movement maps and habitat prioritisation for a rural landscape\nin southwestern France. Landscape characterisation is the main factor\ninfluencing connectivity assessment. The movement maps reflect the models'\nassumptions: LCP produced narrow beams reflecting the optimal pathways; whereas\ncircuit theory and SMS produced wider estimation reflecting movement\nstochasticity, with SMS integrating behavioural drivers. The indices\nhighlighted different aspects: dPCflux the surface of suitable habitats and\nflux their proximity. We recommend focusing on landscape characterisation\nbefore engaging further in the modelling framework. We emphasise the importance\nof stochasticity and behavioural drivers in connectivity, which can be\nreflected using circuit theory, SMS or other stochastic individual-based\nmodels. We stress the importance of using multiple indices to capture the\nmulti-factorial aspect of connectivity.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning",
    "url": "http://arxiv.org/abs/2406.15658v3",
    "authors": [
      "Nemin Wu",
      "Qian Cao",
      "Zhangyu Wang",
      "Zeping Liu",
      "Yanlin Qi",
      "Jielu Zhang",
      "Joshua Ni",
      "Xiaobai Yao",
      "Hongxu Ma",
      "Lan Mu",
      "Stefano Ermon",
      "Tanuja Ganu",
      "Akshay Nambi",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published": "2024-06-21",
    "abstract": "Spatial representation learning (SRL) aims at learning general-purpose neural\nnetwork representations from various types of spatial data (e.g., points,\npolylines, polygons, networks, images, etc.) in their native formats. Learning\ngood spatial representations is a fundamental problem for various downstream\napplications such as species distribution modeling, weather forecasting,\ntrajectory generation, geographic question answering, etc. Even though SRL has\nbecome the foundation of almost all geospatial artificial intelligence (GeoAI)\nresearch, we have not yet seen significant efforts to develop an extensive deep\nlearning framework and benchmark to support SRL model development and\nevaluation. To fill this gap, we propose TorchSpatial, a learning framework and\nbenchmark for location (point) encoding, which is one of the most fundamental\ndata types of spatial representation learning. TorchSpatial contains three key\ncomponents: 1) a unified location encoding framework that consolidates 15\ncommonly recognized location encoders, ensuring scalability and reproducibility\nof the implementations; 2) the LocBench benchmark tasks encompassing 7\ngeo-aware image classification and 10 geo-aware image regression datasets; 3) a\ncomprehensive suite of evaluation metrics to quantify geo-aware model's overall\nperformance as well as their geographic bias, with a novel Geo-Bias Score\nmetric. Finally, we provide a detailed analysis and insights into the model\nperformance and geographic bias of different location encoders. We believe\nTorchSpatial will foster future advancement of spatial representation learning\nand spatial fairness in GeoAI research. The TorchSpatial model framework and\nLocBench benchmark are available at https://github.com/seai-lab/TorchSpatial,\nand the Geo-Bias Score evaluation framework is available at\nhttps://github.com/seai-lab/PyGBS.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Cumulant-based approximation for fast and efficient prediction for species distribution",
    "url": "http://arxiv.org/abs/2405.14456v1",
    "authors": [
      "Osamu Komori",
      "Yusuke Saigusa",
      "Shinto Eguchi",
      "Yasuhiro Kubota"
    ],
    "published": "2024-05-23",
    "abstract": "Species distribution modeling plays an important role in estimating the\nhabitat suitability of species using environmental variables. For this purpose,\nMaxent and the Poisson point process are popular and powerful methods\nextensively employed across various ecological and biological sciences.\nHowever, the computational speed becomes prohibitively slow when using huge\nbackground datasets, which is often the case with fine-resolution data or\nglobal-scale estimations. To address this problem, we propose a computationally\nefficient species distribution model using a cumulant-based approximation (CBA)\napplied to the loss function of $\\gamma$-divergence. Additionally, we introduce\na sequential estimating algorithm with an $L_1$ penalty to select important\nenvironmental variables closely associated with species distribution. The\nregularized geometric-mean method, derived from the CBA, demonstrates high\ncomputational efficiency and estimation accuracy. Moreover, by applying CBA to\nMaxent, we establish that Maxent and Fisher linear discriminant analysis are\nequivalent under a normality assumption. This equivalence leads to an highly\nefficient computational method for estimating species distribution. The\neffectiveness of our proposed methods is illustrated through simulation studies\nand by analyzing data on 226 species from the National Centre for Ecological\nAnalysis and Synthesis and 709 Japanese vascular plant species. The\ncomputational efficiency of the proposed methods is significantly improved\ncompared to Maxent, while maintaining comparable estimation accuracy. A R\npackage {\\tt CBA} is also prepared to provide all programming codes used in\nsimulation studies and real data analysis.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "A Comparison of Joint Species Distribution Models for Percent Cover Data",
    "url": "http://arxiv.org/abs/2403.11562v1",
    "authors": [
      "Pekka Korhonen",
      "Francis K. C. Hui",
      "Jenni Niku",
      "Sara Taskinen",
      "Bert van der Veen"
    ],
    "published": "2024-03-18",
    "abstract": "1. Joint species distribution models (JSDMs) have gained considerable\ntraction among ecologists over the past decade, due to their capacity to answer\na wide range of questions at both the species- and the community-level. The\nfamily of generalized linear latent variable models in particular has proven\npopular for building JSDMs, being able to handle many response types including\npresence-absence data, biomass, overdispersed and/or zero-inflated counts.\n  2. We extend latent variable models to handle percent cover data, with\nvegetation, sessile invertebrate, and macroalgal cover data representing the\nprime examples of such data arising in community ecology.\n  3. Sparsity is a commonly encountered challenge with percent cover data.\nResponses are typically recorded as percentages covered per plot, though some\nspecies may be completely absent or present, i.e., have 0% or 100% cover\nrespectively, rendering the use of beta distribution inadequate.\n  4. We propose two JSDMs suitable for percent cover data, namely a hurdle beta\nmodel and an ordered beta model. We compare the two proposed approaches to a\nbeta distribution for shifted responses, transformed presence-absence data, and\nan ordinal model for percent cover classes. Results demonstrate the hurdle beta\nJSDM was generally the most accurate at retrieving the latent variables and\npredicting ecological percent cover data.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Imbalance-aware Presence-only Loss Function for Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2403.07472v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Marc Ru\u00dfwurm",
      "Devis Tuia"
    ],
    "published": "2024-03-12",
    "abstract": "In the face of significant biodiversity decline, species distribution models\n(SDMs) are essential for understanding the impact of climate change on species\nhabitats by connecting environmental conditions to species occurrences.\nTraditionally limited by a scarcity of species observations, these models have\nsignificantly improved in performance through the integration of larger\ndatasets provided by citizen science initiatives. However, they still suffer\nfrom the strong class imbalance between species within these datasets, often\nresulting in the penalization of rare species--those most critical for\nconservation efforts. To tackle this issue, this study assesses the\neffectiveness of training deep learning models using a balanced presence-only\nloss function on large citizen science-based datasets. We demonstrate that this\nimbalance-aware loss function outperforms traditional loss functions across\nvarious datasets and tasks, particularly in accurately modeling rare species\nwith limited observations.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Infinite joint species distribution models",
    "url": "http://arxiv.org/abs/2402.13384v2",
    "authors": [
      "Federica Stolf",
      "David B. Dunson"
    ],
    "published": "2024-02-20",
    "abstract": "Joint species distribution models are popular in ecology for modeling\ncovariate effects on species occurrence, while characterizing cross-species\ndependence. Data consist of multivariate binary indicators of the occurrences\nof different species in each sample, along with sample-specific covariates. A\nkey problem is that current models implicitly assume that the list of species\nunder consideration is predefined and finite, while for highly diverse groups\nof organisms, it is impossible to anticipate which species will be observed in\na study and discovery of unknown species is common. This article proposes a new\nmodeling paradigm for statistical ecology, which generalizes traditional\nmultivariate probit models to accommodate large numbers of rare species and new\nspecies discovery. We discuss theoretical properties of the proposed modeling\nparadigm and implement efficient algorithms for posterior computation.\nSimulation studies and applications to fungal biodiversity data provide\ncompelling support for the new modeling class.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Novel community data in ecology -- properties and prospects",
    "url": "http://arxiv.org/abs/2401.10860v1",
    "authors": [
      "Florian Hartig",
      "Nerea Abrego",
      "Alex Bush",
      "Jonathan M. Chase",
      "Gurutzeta Guillera-Arroita",
      "Mathew A. Leibold",
      "Otso Ovaskainen",
      "Lo\u00efc Pellissier",
      "Maximilian Pichler",
      "Giovanni Poggiato",
      "Laura Pollock",
      "Sara Si-Moussi",
      "Wilfried Thuiller",
      "Duarte S. Viana",
      "David I. Warton",
      "Damaris Zurell",
      "Douglas W. Yu"
    ],
    "published": "2024-01-19",
    "abstract": "New technologies for acquiring biological information such as eDNA, acoustic\nor optical sensors, make it possible to generate spatial community observations\nat unprecedented scales. The potential of these novel community data to\nstandardize community observations at high spatial, temporal, and taxonomic\nresolution and at large spatial scale ('many rows and many columns') has been\nwidely discussed, but so far, there has been little integration of these data\nwith ecological models and theory. Here, we review these developments and\nhighlight emerging solutions, focusing on statistical methods for analyzing\nnovel community data, in particular joint species distribution models; the new\necological questions that can be answered with these data; and the potential\nimplications of these developments for policy and conservation.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "A sequential Monte Carlo algorithm for data assimilation problems in ecology",
    "url": "http://arxiv.org/abs/2401.06515v1",
    "authors": [
      "Kwaku Peprah Adjei",
      "Rob Cooke",
      "Nick Isaac",
      "Robert B. O'Hara"
    ],
    "published": "2024-01-12",
    "abstract": "1. Temporal trends in species distributions are necessary for monitoring\nchanges in biodiversity, which aids policymakers and conservationists in making\ninformed decisions. Dynamic species distribution models are often fitted to\necological time series data using Markov Chain Monte Carlo algorithms to\nproduce these temporal trends. However, the fitted models can be time-consuming\nto produce and run, making it inefficient to refit them as new observations\nbecome available.\n  2. We propose an algorithm that updates model parameters and the latent state\ndistribution (e.g. true occupancy) using the saved information from a\npreviously fitted model. This algorithm capitalises on the strength of\nimportance sampling to generate new posterior samples of interest by updating\nthe model output. The algorithm was validated with simulation studies on linear\nGaussian state space models and occupancy models, and we applied the framework\nto Crested Tits in Switzerland and Yellow Meadow Ants in the UK.\n  3. We found that models updated with the proposed algorithm captured the true\nmodel parameters and latent state values as good as the models refitted to the\nexpanded dataset. Moreover, the updated models were much faster to run and\npreserved the trajectory of the derived quantities.\n  4. The proposed approach serves as an alternative to conventional methods for\nupdating state-space models (SSMs), and it is most beneficial when the fitted\nSSMs have a long run time. Overall, we provide a Monte Carlo algorithm to\nefficiently update complex models, a key issue in developing biodiversity\nmodels and indicators.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts",
    "url": "http://arxiv.org/abs/2401.05470v1",
    "authors": [
      "Joaquim Estopinan",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "Fran\u00e7ois Munoz",
      "Alexis Joly"
    ],
    "published": "2024-01-10",
    "abstract": "The post-2020 global biodiversity framework needs ambitious, research-based\ntargets. Estimating the accelerated extinction risk due to climate change is\ncritical. The International Union for Conservation of Nature (IUCN) measures\nthe extinction risk of species. Automatic methods have been developed to\nprovide information on the IUCN status of under-assessed taxa. However, these\ncompensatory methods are based on current species characteristics, mainly\ngeographical, which precludes their use in future projections. Here, we\nevaluate a novel method for classifying the IUCN status of species benefiting\nfrom the generalisation power of species distribution models based on deep\nlearning. Our method matches state-of-the-art classification performance while\nrelying on flexible SDM-based features that capture species' environmental\npreferences. Cross-validation yields average accuracies of 0.61 for status\nclassification and 0.78 for binary classification. Climate change will reshape\nfuture species distributions. Under the species-environment equilibrium\nhypothesis, SDM projections approximate plausible future outcomes. Two extremes\nof species dispersal capacity are considered: unlimited or null. The projected\nspecies distributions are translated into features feeding our IUCN\nclassification method. Finally, trends in threatened species are analysed over\ntime and i) by continent and as a function of average ii) latitude or iii)\naltitude. The proportion of threatened species is increasing globally, with\ncritical rates in Africa, Asia and South America. Furthermore, the proportion\nof threatened species is predicted to peak around the two Tropics, at the\nEquator, in the lowlands and at altitudes of 800-1,500 m.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale",
    "url": "http://arxiv.org/abs/2401.04691v1",
    "authors": [
      "Joaquim Estopinan",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly",
      "Fran\u00e7ois Munoz"
    ],
    "published": "2024-01-09",
    "abstract": "Although increasing threats on biodiversity are now widely recognised, there\nare no accurate global maps showing whether and where species assemblages are\nat risk. We hereby assess and map at kilometre resolution the conservation\nstatus of the iconic orchid family, and discuss the insights conveyed at\nmultiple scales. We introduce a new Deep Species Distribution Model trained on\n1M occurrences of 14K orchid species to predict their assemblages at global\nscale and at kilometre resolution. We propose two main indicators of the\nconservation status of the assemblages: (i) the proportion of threatened\nspecies, and (ii) the status of the most threatened species in the assemblage.\nWe show and analyze the variation of these indicators at World scale and in\nrelation to currently protected areas in Sumatra island. Global and interactive\nmaps available online show the indicators of conservation status of orchid\nassemblages, with sharp spatial variations at all scales. The highest level of\nthreat is found at Madagascar and the neighbouring islands. In Sumatra, we\nfound good correspondence of protected areas with our indicators, but\nsupplementing current IUCN assessments with status predictions results in\nalarming levels of species threat across the island. Recent advances in deep\nlearning enable reliable mapping of the conservation status of species\nassemblages on a global scale. As an umbrella taxon, orchid family provides a\nreference for identifying vulnerable ecosystems worldwide, and prioritising\nconservation actions both at international and local levels.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning",
    "url": "http://arxiv.org/abs/2401.02989v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Benjamin Kellenberger",
      "Lloyd Hughes",
      "Devis Tuia"
    ],
    "published": "2024-01-03",
    "abstract": "Species distribution modeling is a highly versatile tool for understanding\nthe intricate relationship between environmental conditions and species\noccurrences. However, the available data often lacks information on confirmed\nspecies absence and is limited to opportunistically sampled, presence-only\nobservations. To overcome this limitation, a common approach is to employ\npseudo-absences, which are specific geographic locations designated as negative\nsamples. While pseudo-absences are well-established for single-species\ndistribution models, their application in the context of multi-species neural\nnetworks remains underexplored. Notably, the significant class imbalance\nbetween species presences and pseudo-absences is often left unaddressed.\nMoreover, the existence of different types of pseudo-absences (e.g., random and\ntarget-group background points) adds complexity to the selection process.\nDetermining the optimal combination of pseudo-absences types is difficult and\ndepends on the characteristics of the data, particularly considering that\ncertain types of pseudo-absences can be used to mitigate geographic biases. In\nthis paper, we demonstrate that these challenges can be effectively tackled by\nintegrating pseudo-absences in the training of multi-species neural networks\nthrough modifications to the loss function. This adjustment involves assigning\ndifferent weights to the distinct terms of the loss function, thereby\naddressing both the class imbalance and the choice of pseudo-absence types.\nAdditionally, we propose a strategy to set these loss weights using spatial\nblock cross-validation with presence-only data. We evaluate our approach using\na benchmark dataset containing independent presence-absence data from six\ndifferent regions and report improved results when compared to competing\napproaches.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  }
]