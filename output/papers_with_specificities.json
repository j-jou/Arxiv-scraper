[
  {
<<<<<<< HEAD
=======
    "title": "Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening",
    "url": "http://arxiv.org/abs/2512.02643v1",
    "authors": [
      "Yongchuan Cui",
      "Peng Liu",
      "Yi Zeng"
    ],
    "published": "2025-12-02",
    "abstract": "Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts",
    "url": "http://arxiv.org/abs/2512.02517v1",
    "authors": [
      "Jiaqi Liu",
      "Ronghao Fu",
      "Lang Sun",
      "Haoran Liu",
      "Xiao Yang",
      "Weipeng Zhang",
      "Xu Na",
      "Zhuoran Duan",
      "Bo Yang"
    ],
    "published": "2025-12-02",
    "abstract": "The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2512.01665v1",
    "authors": [
      "Zhicheng Zhao",
      "Yin Huang",
      "Lingma Sun",
      "Chenglong Li",
      "Jin Tang"
    ],
    "published": "2025-12-01",
    "abstract": "Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images",
    "url": "http://arxiv.org/abs/2512.00718v1",
    "authors": [
      "Deliang Wang",
      "Peng Liu"
    ],
    "published": "2025-11-30",
    "abstract": "Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?",
    "url": "http://arxiv.org/abs/2511.21523v2",
    "authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "St\u00e9phane May",
      "S\u00e9bastien Lef\u00e8vre"
    ],
    "published": "2025-11-26",
    "abstract": "Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs. All codes and pretrained models are available at https://github.com/pierreadorni/EoS-FM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning",
    "url": "http://arxiv.org/abs/2511.21420v1",
    "authors": [
      "Futian Wang",
      "Mengqi Wang",
      "Xiao Wang",
      "Haowen Wang",
      "Jin Tang"
    ],
    "published": "2025-11-26",
    "abstract": "Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search",
    "url": "http://arxiv.org/abs/2511.20460v2",
    "authors": [
      "Yunqi Zhou",
      "Chengjie Jiang",
      "Chun Yuan",
      "Jing Li"
    ],
    "published": "2025-11-25",
    "abstract": "With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2511.20302v2",
    "authors": [
      "Shilei Cao",
      "Ziyang Gong",
      "Hehai Lin",
      "Yang Liu",
      "Jiashun Cheng",
      "Xiaoxing Hu",
      "Haoyuan Liang",
      "Guowen Li",
      "Chengwei Qin",
      "Hong Cheng",
      "Xue Yang",
      "Juepeng Zheng",
      "Haohuan Fu"
    ],
    "published": "2025-11-25",
    "abstract": "In Remote Sensing (RS), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key approach to activate the generalizable representation ability of foundation models for downstream tasks. However, existing specialized PEFT methods often fail when applied to large-scale Earth observation tasks, as they are unable to fully handle the multifaceted and unpredictable domain gaps (\\eg, spatial, semantic, and frequency shifts) inherent in RS data. To overcome this, we propose CrossEarth-Gate, which introduces two primary contributions. First, we establish a comprehensive RS module toolbox to address multifaceted domain gaps, comprising spatial, semantic, and frequency modules. Second, we develop a Fisher-guided adaptive selection mechanism that operates on this toolbox. This selection is guided by Fisher Information to quantify each module's importance by measuring its contribution to the task-specific gradient flow. It dynamically activates only the most critical modules at the appropriate layers, guiding the gradient flow to maximize adaptation effectiveness and efficiency. Comprehensive experiments validate the efficacy and generalizability of our method, where CrossEarth-Gate achieves state-of-the-art performance across 16 cross-domain benchmarks for RS semantic segmentation. The code of the work will be released.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting",
    "url": "http://arxiv.org/abs/2511.20004v1",
    "authors": [
      "Peining Zhang",
      "Hongchen Qin",
      "Haochen Zhang",
      "Ziqi Guo",
      "Guiling Wang",
      "Jinbo Bi"
    ],
    "published": "2025-11-25",
    "abstract": "This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors",
    "url": "http://arxiv.org/abs/2511.18264v2",
    "authors": [
      "Ruijie Fan",
      "Junyan Ye",
      "Huan Chen",
      "Zilong Huang",
      "Xiaolei Wang",
      "Weijia Li"
    ],
    "published": "2025-11-23",
    "abstract": "Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
    "url": "http://arxiv.org/abs/2511.17442v1",
    "authors": [
      "Binger Chen",
      "Tacettin Emre B\u00f6k",
      "Behnood Rasti",
      "Volker Markl",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-11-21",
    "abstract": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "CORONA-Fields: Leveraging Foundation Models for Classification of Solar Wind Phenomena",
    "url": "http://arxiv.org/abs/2511.09843v1",
    "authors": [
      "Daniela Martin",
      "Jinsu Hong",
      "Connor O'Brien",
      "Valmir P Moraes Filho",
      "Jasmine R. Kobayashi",
      "Evangelia Samara",
      "Joseph Gallego"
    ],
    "published": "2025-11-13",
    "abstract": "Space weather at Earth, driven by the solar activity, poses growing risks to satellites around our planet as well as to critical ground-based technological infrastructure. Major space weather contributors are the solar wind and coronal mass ejections whose variable density, speed, temperature, and magnetic field make the automated classification of those structures challenging. In this work, we adapt a foundation model for solar physics, originally trained on Solar Dynamics Observatory imagery, to create embeddings suitable for solar wind structure analysis. These embeddings are concatenated with the spacecraft position and solar magnetic connectivity encoded using Fourier features which generates a neural field-based model. The full deep learning architecture is fine-tuned bridging the gap between remote sensing and in situ observations. Labels are derived from Parker Solar Probe measurements, forming a downstream classification task that maps plasma properties to solar wind structures. Although overall classification performance is modest, likely due to coarse labeling, class imbalance, and limited transferability of the pretrained model, this study demonstrates the feasibility of leveraging foundation model embeddings for in situ solar wind tasks. As a first proof-of-concept, it lays the groundwork for future improvements toward more reliable space weather predictions. The code and configuration files used in this study are publicly available to support reproducibility.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental Modelling",
    "url": "http://arxiv.org/abs/2511.11706v3",
    "authors": [
      "Julia Peters",
      "Karin Mora",
      "Miguel D. Mahecha",
      "Chaonan Ji",
      "David Montero",
      "Clemens Mosig",
      "Guido Kraemer"
    ],
    "published": "2025-11-12",
    "abstract": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping",
    "url": "http://arxiv.org/abs/2511.08156v1",
    "authors": [
      "Chenying Liu",
      "Wei Huang",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-11",
    "abstract": "Land Use and Land Cover (LULC) mapping is a fundamental task in Earth Observation (EO). However, current LULC models are typically developed for a specific modality and a fixed class taxonomy, limiting their generability and broader applicability. Recent advances in foundation models (FMs) offer promising opportunities for building universal models. Yet, task-agnostic FMs often require fine-tuning for downstream applications, whereas task-specific FMs rely on massive amounts of labeled data for training, which is costly and impractical in the remote sensing (RS) domain. To address these challenges, we propose LandSegmenter, an LULC FM framework that resolves three-stage challenges at the input, model, and output levels. From the input side, to alleviate the heavy demand on labeled data for FM training, we introduce LAnd Segment (LAS), a large-scale, multi-modal, multi-source dataset built primarily with globally sampled weak labels from existing LULC products. LAS provides a scalable, cost-effective alternative to manual annotation, enabling large-scale FM training across diverse LULC domains. For model architecture, LandSegmenter integrates an RS-specific adapter for cross-modal feature extraction and a text encoder for semantic awareness enhancement. At the output stage, we introduce a class-wise confidence-guided fusion strategy to mitigate semantic omissions and further improve LandSegmenter's zero-shot performance. We evaluate LandSegmenter on six precisely annotated LULC datasets spanning diverse modalities and class taxonomies. Extensive transfer learning and zero-shot experiments demonstrate that LandSegmenter achieves competitive or superior performance, particularly in zero-shot settings when transferred to unseen datasets. These results highlight the efficacy of our proposed framework and the utility of weak supervision for building task-specific FMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "GeoCrossBench: Cross-Band Generalization for Remote Sensing",
    "url": "http://arxiv.org/abs/2511.02831v1",
    "authors": [
      "Hakob Tamazyan",
      "Ani Vanyan",
      "Alvard Barseghyan",
      "Anna Khosrovyan",
      "Evan Shelhamer",
      "Hrant Khachatrian"
    ],
    "published": "2025-11-04",
    "abstract": "The number and diversity of remote sensing satellites grows over time, while the vast majority of labeled data comes from older satellites. As the foundation models for Earth observation scale up, the cost of (re-)training to support new satellites grows too, so the generalization capabilities of the models towards new satellites become increasingly important. In this work we introduce GeoCrossBench, an extension of the popular GeoBench benchmark with a new evaluation protocol: it tests the in-distribution performance; generalization to satellites with no band overlap; and generalization to satellites with additional bands with respect to the training set. We also develop a self-supervised extension of ChannelViT, ChiViT, to improve its cross-satellite performance. First, we show that even the best foundation models for remote sensing (DOFA, TerraFM) do not outperform general purpose models like DINOv3 in the in-distribution setting. Second, when generalizing to new satellites with no band overlap, all models suffer 2-4x drop in performance, and ChiViT significantly outperforms the runner-up DINOv3. Third, the performance of all tested models drops on average by 5-25\\% when given additional bands during test time. Finally, we show that fine-tuning just the last linear layer of these models using oracle labels from all bands can get relatively consistent performance across all satellites, highlighting that the benchmark is far from being saturated. We publicly release the code and the datasets to encourage the development of more future-proof remote sensing models with stronger cross-satellite generalization.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping",
    "url": "http://arxiv.org/abs/2510.27219v1",
    "authors": [
      "Renjie Ji",
      "Xue Wang",
      "Chao Niu",
      "Wen Zhang",
      "Yong Mei",
      "Kun Tan"
    ],
    "published": "2025-10-31",
    "abstract": "Hyperspectral imaging (HSI) is a vital tool for fine-grained land-use and land-cover (LULC) mapping. However, the inherent heterogeneity of HSI data has long posed a major barrier to developing generalized models via joint training. Although HSI foundation models have shown promise for different downstream tasks, the existing approaches typically overlook the critical guiding role of sensor meta-attributes, and struggle with multi-sensor training, limiting their transferability. To address these challenges, we propose SpecAware, which is a novel hyperspectral spectral-content aware foundation model for unifying multi-sensor learning for HSI mapping. We also constructed the Hyper-400K dataset to facilitate this research, which is a new large-scale, high-quality benchmark dataset with over 400k image patches from diverse airborne AVIRIS sensors. The core of SpecAware is a two-step hypernetwork-driven encoding process for HSI data. Firstly, we designed a meta-content aware module to generate a unique conditional input for each HSI patch, tailored to each spectral band of every sample by fusing the sensor meta-attributes and its own image content. Secondly, we designed the HyperEmbedding module, where a sample-conditioned hypernetwork dynamically generates a pair of matrix factors for channel-wise encoding, consisting of adaptive spatial pattern extraction and latent semantic feature re-projection. Thus, SpecAware gains the ability to perceive and interpret spatial-spectral features across diverse scenes and sensors. This, in turn, allows SpecAware to adaptively process a variable number of spectral channels, establishing a unified framework for joint pre-training. Extensive experiments on six datasets demonstrate that SpecAware can learn superior feature representations, excelling in land-cover semantic segmentation classification, change detection, and scene classification.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges",
    "url": "http://arxiv.org/abs/2510.22964v1",
    "authors": [
      "Liling Yang",
      "Ning Chen",
      "Jun Yue",
      "Yidan Liu",
      "Jiayi Ma",
      "Pedram Ghamisi",
      "Antonio Plaza",
      "Leyuan Fang"
    ],
    "published": "2025-10-27",
    "abstract": "Foundation models have transformed natural language processing and computer vision, and their impact is now reshaping remote sensing image analysis. With powerful generalization and transfer learning capabilities, they align naturally with the multimodal, multi-resolution, and multi-temporal characteristics of remote sensing data. To address unique challenges in the field, multimodal geospatial foundation models (GFMs) have emerged as a dedicated research frontier. This survey delivers a comprehensive review of multimodal GFMs from a modality-driven perspective, covering five core visual and vision-language modalities. We examine how differences in imaging physics and data representation shape interaction design, and we analyze key techniques for alignment, integration, and knowledge transfer to tackle modality heterogeneity, distribution shifts, and semantic gaps. Advances in training paradigms, architectures, and task-specific adaptation strategies are systematically assessed alongside a wealth of emerging benchmarks. Representative multimodal visual and vision-language GFMs are evaluated across ten downstream tasks, with insights into their architectures, performance, and application scenarios. Real-world case studies, spanning land cover mapping, agricultural monitoring, disaster response, climate studies, and geospatial intelligence, demonstrate the practical potential of GFMs. Finally, we outline pressing challenges in domain generalization, interpretability, efficiency, and privacy, and chart promising avenues for future research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "WaveMAE: Wavelet decomposition Masked Auto-Encoder for Remote Sensing",
    "url": "http://arxiv.org/abs/2510.22697v1",
    "authors": [
      "Vittorio Bernuzzi",
      "Leonardo Rossi",
      "Tomaso Fontanini",
      "Massimo Bertozzi",
      "Andrea Prati"
    ],
    "published": "2025-10-26",
    "abstract": "Self-supervised learning (SSL) has recently emerged as a key strategy for building foundation models in remote sensing, where the scarcity of annotated data limits the applicability of fully supervised approaches. In this work, we introduce WaveMAE, a masked autoencoding framework tailored for multispectral satellite imagery. Unlike conventional pixel-based reconstruction, WaveMAE leverages a multi-level Discrete Wavelet Transform (DWT) to disentangle frequency components and guide the encoder toward learning scale-aware high-frequency representations. We further propose a Geo-conditioned Positional Encoding (GPE), which incorporates geographical priors via Spherical Harmonics, encouraging embeddings that respect both semantic and geospatial structure. To ensure fairness in evaluation, all methods are pretrained on the same dataset (fMoW-S2) and systematically evaluated on the diverse downstream tasks of the PANGAEA benchmark, spanning semantic segmentation, regression, change detection, and multilabel classification. Extensive experiments demonstrate that WaveMAE achieves consistent improvements over prior state-of-the-art approaches, with substantial gains on segmentation and regression benchmarks. The effectiveness of WaveMAE pretraining is further demonstrated by showing that even a lightweight variant, containing only 26.4% of the parameters, achieves state-of-the-art performance. Our results establish WaveMAE as a strong and geographically informed foundation model for multispectral remote sensing imagery.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration",
    "url": "http://arxiv.org/abs/2510.17670v2",
    "authors": [
      "Yehonathan Refael",
      "Amit Aides",
      "Aviad Barzilai",
      "George Leifman",
      "Genady Beryozkin",
      "Vered Silverman",
      "Bolous Jaber",
      "Tomer Shekel"
    ],
    "published": "2025-10-20",
    "abstract": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by detecting objects from arbitrary text queries. However, their zero-shot performance in specialized domains like Remote Sensing (RS) is often compromised by the inherent ambiguity of natural language, limiting critical downstream applications. For instance, an OVD model may struggle to distinguish between fine-grained classes such as \"fishing boat\" and \"yacht\" since their embeddings are similar and often inseparable. This can hamper specific user goals, such as monitoring illegal fishing, by producing irrelevant detections. To address this, we propose a cascaded approach that couples the broad generalization of a large pre-trained OVD model with a lightweight few-shot classifier. Our method first employs the zero-shot model to generate high-recall object proposals. These proposals are then refined for high precision by a compact classifier trained in real-time on only a handful of user-annotated examples - drastically reducing the high costs of RS imagery annotation.The core of our framework is FLAME, a one-step active learning strategy that selects the most informative samples for training. FLAME identifies, on the fly, uncertain marginal candidates near the decision boundary using density estimation, followed by clustering to ensure sample diversity. This efficient sampling technique achieves high accuracy without costly full-model fine-tuning and enables instant adaptation, within less then a minute, which is significantly faster than state-of-the-art alternatives.Our method consistently surpasses state-of-the-art performance on RS benchmarks, establishing a practical and resource-efficient framework for adapting foundation models to specific user needs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Do Satellite Tasks Need Special Pretraining?",
    "url": "http://arxiv.org/abs/2510.17014v1",
    "authors": [
      "Ani Vanyan",
      "Alvard Barseghyan",
      "Hakob Tamazyan",
      "Tigran Galstyan",
      "Vahan Huroyan",
      "Naira Hovakimyan",
      "Hrant Khachatrian"
    ],
    "published": "2025-10-19",
    "abstract": "Foundation models have advanced machine learning across various modalities, including images. Recently multiple teams trained foundation models specialized for remote sensing applications. This line of research is motivated by the distinct characteristics of remote sensing imagery, specific applications and types of robustness useful for satellite image analysis. In this work we systematically challenge the idea that specific foundation models are more useful than general-purpose vision foundation models, at least in the small scale. First, we design a simple benchmark that measures generalization of remote sensing models towards images with lower resolution for two downstream tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID, an ImageNet-scale satellite imagery dataset, with several modifications specific to remote sensing. We show that none of those pretrained models bring consistent improvements upon general-purpose baselines at the ViT-B scale.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations",
    "url": "http://arxiv.org/abs/2510.13774v1",
    "authors": [
      "Dominik J. M\u00fchlematter",
      "Lin Che",
      "Ye Hong",
      "Martin Raubal",
      "Nina Wiedemann"
    ],
    "published": "2025-10-15",
    "abstract": "Forecasting urban phenomena such as housing prices and public health indicators requires the effective integration of various geospatial data. Current methods primarily utilize task-specific models, while recent foundation models for spatial representations often support only limited modalities and lack multimodal fusion capabilities. To overcome these challenges, we present UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal Fusion (SMF). The framework employs modality-specific encoders to process different types of inputs, including street view imagery, remote sensing data, cartographic maps, and points of interest (POIs) data. These multimodal inputs are integrated via a Transformer-based fusion module that learns unified representations. An extensive evaluation across 41 tasks in 56 cities worldwide demonstrates UrbanFusion's strong generalization and predictive performance compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms prior foundation models on location-encoding, 2) allows multimodal input during inference, and 3) generalizes well to regions unseen during training. UrbanFusion can flexibly utilize any subset of available modalities for a given location during both pretraining and inference, enabling broad applicability across diverse data availability scenarios. All source code is available at https://github.com/DominikM198/UrbanFusion.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping",
    "url": "http://arxiv.org/abs/2510.11576v2",
    "authors": [
      "Walid Elbarz",
      "Mohamed Bourriz",
      "Hicham Hajji",
      "Hamd Ait Abdelali",
      "Fran\u00e7ois Bourzeix"
    ],
    "published": "2025-10-13",
    "abstract": "Foundation models are transforming Earth observation, but their potential for hyperspectral crop mapping remains underexplored. This study benchmarks three foundation models for cereal crop mapping using hyperspectral imagery: HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth dataset (a large multitemporal hyperspectral archive). Models were fine-tuned on manually labeled data from a training region and evaluated on an independent test region. Performance was measured with overall accuracy (OA), average accuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%), DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of 93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved 91%, highlighting the importance of model architecture for strong generalization across geographic regions and sensor platforms. These results provide a systematic evaluation of foundation models for operational hyperspectral crop mapping and outline directions for future model development.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework",
    "url": "http://arxiv.org/abs/2510.10084v1",
    "authors": [
      "Meijun Zhou",
      "Gang Mei",
      "Zhengjing Ma",
      "Nengxiong Xu",
      "Jianbing Peng"
    ],
    "published": "2025-10-11",
    "abstract": "Tracking the spatiotemporal evolution of large-scale landslide scars is critical for understanding the evolution mechanisms and failure precursors, enabling effective early-warning. However, most existing studies have focused on single-phase or pre- and post-failure dual-phase landslide identification. Although these approaches delineate post-failure landslide boundaries, it is challenging to track the spatiotemporal evolution of landslide scars. To address this problem, this study proposes a novel and universal framework for tracking the spatiotemporal evolution of large-scale landslide scars using a vision foundation model. The key idea behind the proposed framework is to reconstruct discrete optical remote sensing images into a continuous video sequence. This transformation enables a vision foundation model, which is developed for video segmentation, to be used for tracking the evolution of landslide scars. The proposed framework operates within a knowledge-guided, auto-propagation, and interactive refinement paradigm to ensure the continuous and accurate identification of landslide scars. The proposed framework was validated through application to two representative cases: the post-failure Baige landslide and the active Sela landslide (2017-2025). Results indicate that the proposed framework enables continuous tracking of landslide scars, capturing both failure precursors critical for early warning and post-failure evolution essential for assessing secondary hazards and long-term stability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Tracking"
    ]
  },
  {
    "title": "The View From Space: Navigating Instrumentation Differences with EOFMs",
    "url": "http://arxiv.org/abs/2510.03316v1",
    "authors": [
      "Ryan P. Demilt",
      "Nicholas LaHaye",
      "Karis Tenneson"
    ],
    "published": "2025-10-01",
    "abstract": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as tools for processing the massive volumes of remotely sensed and other earth observation data, and for delivering impact on the many essential earth monitoring tasks. An emerging trend posits using the outputs of pre-trained models as 'embeddings' which summarize high dimensional data to be used for generic tasks such as similarity search and content-specific queries. However, most EOFM models are trained only on single modalities of data and then applied or benchmarked by matching bands across different modalities. It is not clear from existing work what impact diverse sensor architectures have on the internal representations of the present suite of EOFMs. We show in this work that the representation space of EOFMs is highly sensitive to sensor architecture and that understanding this difference gives a vital perspective on the pitfalls of current EOFM design and signals for how to move forward as model developers, users, and a community guided by robust remote-sensing science.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "GeoLink: Empowering Remote Sensing Foundation Model with OpenStreetMap Data",
    "url": "http://arxiv.org/abs/2509.26016v1",
    "authors": [
      "Lubian Bai",
      "Xiuyuan Zhang",
      "Siqi Zhang",
      "Zepeng Zhang",
      "Haoyu Wang",
      "Wei Qin",
      "Shihong Du"
    ],
    "published": "2025-09-30",
    "abstract": "Integrating ground-level geospatial data with rich geographic context, like OpenStreetMap (OSM), into remote sensing (RS) foundation models (FMs) is essential for advancing geospatial intelligence and supporting a broad spectrum of tasks. However, modality gap between RS and OSM data, including differences in data structure, content, and spatial granularity, makes effective synergy highly challenging, and most existing RS FMs focus on imagery alone. To this end, this study presents GeoLink, a multimodal framework that leverages OSM data to enhance RS FM during both the pretraining and downstream task stages. Specifically, GeoLink enhances RS self-supervised pretraining using multi-granularity learning signals derived from OSM data, guided by cross-modal spatial correlations for information interaction and collaboration. It also introduces image mask-reconstruction to enable sparse input for efficient pretraining. For downstream tasks, GeoLink generates both unimodal and multimodal fine-grained encodings to support a wide range of applications, from common RS interpretation tasks like land cover classification to more comprehensive geographic tasks like urban function zone mapping. Extensive experiments show that incorporating OSM data during pretraining enhances the performance of the RS image encoder, while fusing RS and OSM data in downstream tasks improves the FM's adaptability to complex geographic scenarios. These results underscore the potential of multimodal synergy in advancing high-level geospatial artificial intelligence. Moreover, we find that spatial correlation plays a crucial role in enabling effective multimodal geospatial data integration. Code, checkpoints, and using examples are released at https://github.com/bailubin/GeoLink_NeurIPS2025",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Environment-Aware Satellite Image Generation with Diffusion Models",
    "url": "http://arxiv.org/abs/2509.24875v1",
    "authors": [
      "Nikos Kostagiolas",
      "Pantelis Georgiades",
      "Yannis Panagakis",
      "Mihalis A. Nicolaou"
    ],
    "published": "2025-09-29",
    "abstract": "Diffusion-based foundation models have recently garnered much attention in the field of generative modeling due to their ability to generate images of high quality and fidelity. Although not straightforward, their recent application to the field of remote sensing signaled the first successful trials towards harnessing the large volume of publicly available datasets containing multimodal information. Despite their success, existing methods face considerable limitations: they rely on limited environmental context, struggle with missing or corrupted data, and often fail to reliably reflect user intentions in generated outputs. In this work, we propose a novel diffusion model conditioned on environmental context, that is able to generate satellite images by conditioning from any combination of three different control signals: a) text, b) metadata, and c) visual data. In contrast to previous works, the proposed method is i) to our knowledge, the first of its kind to condition satellite image generation on dynamic environmental conditions as part of its control signals, and ii) incorporating a metadata fusion strategy that models attribute embedding interactions to account for partially corrupt and/or missing observations. Our method outperforms previous methods both qualitatively (robustness to missing metadata, higher responsiveness to control inputs) and quantitatively (higher fidelity, accuracy, and quality of generations measured using 6 different metrics) in the trials of single-image and temporal generation. The reported results support our hypothesis that conditioning on environmental context can improve the performance of foundation models for satellite imagery, and render our model a promising candidate for usage in downstream tasks. The collected 3-modal dataset is to our knowledge, the first publicly-available dataset to combine data from these three different mediums.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "FUSAR-KLIP: Towards Multimodal Foundation Models for Remote Sensing",
    "url": "http://arxiv.org/abs/2509.23927v2",
    "authors": [
      "Yi Yang",
      "Xiaokun Zhang",
      "Qingchen Fang",
      "Jing Liu",
      "Ziqi Ye",
      "Rui Li",
      "Li Liu",
      "Haipeng Wang"
    ],
    "published": "2025-09-28",
    "abstract": "Cross-modal artificial intelligence has garnered widespread attention in recent years, achieving significant progress in the study of natural images. However, existing methods are mostly designed for RGB imagery, leaving a significant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with its all-day, all-weather imaging capabilities, plays an irreplaceable role in remote sensing scene understanding. To address this gap, this paper proposes FUSAR-KLIP, the first universal SAR multimodal foundational model, along with reusable data and evaluation baselines. Specifically: (1) This work introduces the critical yet long-overlooked attribute of geographic information into remote sensing research, constructing FUSAR-GEOVL-1M (the first large-scale SAR dataset with complete geographic projection properties), covering multiple satellite platforms, 120,000 images, and 135 cities. (2) Aligned structured text is generated through a hierarchical cognitive chain-of-thought (HCoT), providing more than one million multi-dimensional semantic annotations of landforms, regional functions, target attributes, and spatial relationships. (3) We design a Self-Consistent Iterative Optimization mechanism that continuously enhances cross-modal alignment through a self-supervised closed loop of contrastive, matching, and reconstruction learning on a transferable multimodal encoder. (4) A unified evaluation benchmark is established across 11 representative downstream vision and vision-language tasks, with comparisons against 14 leading foundation models, where FUSAR-KLIP demonstrates leading performance, particularly in object counting and land-cover classification. We expect that FUSAR-KLIP's large-scale multimodal data, transferable model architecture, and comprehensive experimental benchmark will significantly advance the development of SAR multimodal baseline models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation",
    "url": "http://arxiv.org/abs/2509.21894v1",
    "authors": [
      "Yixiao Liu",
      "Yizhou Yang",
      "Jinwen Li",
      "Jun Tao",
      "Ruoyu Li",
      "Xiangkun Wang",
      "Min Zhu",
      "Junlong Cheng"
    ],
    "published": "2025-09-26",
    "abstract": "Remote Sensing Change Detection (RSCD) typically identifies changes in land cover or surface conditions by analyzing multi-temporal images. Currently, most deep learning-based methods primarily focus on learning unimodal visual information, while neglecting the rich semantic information provided by multimodal data such as text. To address this limitation, we propose a novel Language-Guided Change Detection model (LG-CD). This model leverages natural language prompts to direct the network's attention to regions of interest, significantly improving the accuracy and robustness of change detection. Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature extractor to capture multi-scale pyramid features from high-resolution to low-resolution across bi-temporal remote sensing images. Subsequently, multi-layer adapters are employed to fine-tune the model for downstream tasks, ensuring its effectiveness in remote sensing change detection. Additionally, we design a Text Fusion Attention Module (TFAM) to align visual and textual information, enabling the model to focus on target change regions using text prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented, which deeply integrates visual and semantic information through a cross-attention mechanism to produce highly accurate change detection masks. Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate that LG-CD consistently outperforms state-of-the-art change detection methods. Furthermore, our approach provides new insights into achieving generalized change detection by leveraging multimodal information.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A Sentinel-3 foundation model for ocean colour",
    "url": "http://arxiv.org/abs/2509.21273v1",
    "authors": [
      "Geoffrey Dawson",
      "Remy Vandaele",
      "Andrew Taylor",
      "David Moffat",
      "Helen Tamura-Wicks",
      "Sarah Jackson",
      "Rosie Lickorish",
      "Paolo Fraccaro",
      "Hywel Williams",
      "Chunbo Luo",
      "Anne Jones"
    ],
    "published": "2025-09-25",
    "abstract": "Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive unlabelled datasets, have the potential to drastically change AI applications in ocean science, where labelled data are often sparse and expensive to collect. In this work, we describe a new foundation model using the Prithvi-EO Vision Transformer architecture which has been pre-trained to reconstruct data from the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the model by fine-tuning on two downstream marine earth observation tasks. We first assess model performance compared to current baseline models used to quantify chlorophyll concentration. We then evaluate the FMs ability to refine remote sensing-based estimates of ocean primary production. Our results demonstrate the utility of self-trained FMs for marine monitoring, in particular for making use of small amounts of high quality labelled data and in capturing detailed spatial patterns of ocean colour whilst matching point observations. We conclude that this new generation of geospatial AI models has the potential to provide more robust, data-driven insights into ocean ecosystems and their role in global climate processes.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2509.18711v2",
    "authors": [
      "Ke Li",
      "Di Wang",
      "Ting Wang",
      "Fuyu Dong",
      "Yiming Zhang",
      "Luyao Zhang",
      "Xiangyu Wang",
      "Shaofeng Li",
      "Quan Wang"
    ],
    "published": "2025-09-23",
    "abstract": "Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Visual Instruction Pretraining for Domain-Specific Foundation Models",
    "url": "http://arxiv.org/abs/2509.17562v2",
    "authors": [
      "Yuxuan Li",
      "Yicheng Zhang",
      "Wenhao Tang",
      "Yimian Dai",
      "Ming-Ming Cheng",
      "Xiang Li",
      "Jian Yang"
    ],
    "published": "2025-09-22",
    "abstract": "Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at https://github.com/zcablii/ViTP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation",
    "url": "http://arxiv.org/abs/2509.15795v1",
    "authors": [
      "Tianyang Wang",
      "Xi Xiao",
      "Gaofei Chen",
      "Hanzhang Chi",
      "Qi Zhang",
      "Guo Cheng",
      "Yingrui Ji"
    ],
    "published": "2025-09-19",
    "abstract": "Segment Anything Model (SAM) has demonstrated impressive zero-shot segmentation capabilities across natural image domains, but it struggles to generalize to the unique challenges of remote sensing data, such as complex terrain, multi-scale objects, and temporal dynamics. In this paper, we introduce TASAM, a terrain and temporally-aware extension of SAM designed specifically for high-resolution remote sensing image segmentation. TASAM integrates three lightweight yet effective modules: a terrain-aware adapter that injects elevation priors, a temporal prompt generator that captures land-cover changes over time, and a multi-scale fusion strategy that enhances fine-grained object delineation. Without retraining the SAM backbone, our approach achieves substantial performance gains across three remote sensing benchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and task-specific models with minimal computational overhead. Our results highlight the value of domain-adaptive augmentation for foundation models and offer a scalable path toward more robust geospatial segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts",
    "url": "http://arxiv.org/abs/2509.14104v1",
    "authors": [
      "Leonard Hackel",
      "Tom Burgert",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-09-17",
    "abstract": "Self-supervised learning through masked autoencoders has attracted great attention for remote sensing (RS) foundation model (FM) development, enabling improved representation learning across diverse sensors and downstream tasks. However, existing RS FMs often either suffer from substantial computational complexity during both training and inference or exhibit limited representational capacity. These issues restrict their practical applicability in RS. To address this limitation, we propose an adaptation for enhancing the efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism into the FM. The integration of Soft MoEs into the FM allows modality-specific expert specialization alongside shared cross-sensor representation learning. To demonstrate the effectiveness of our adaptation, we apply it on the Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic descriptor-driven sampling strategy for the construction of a representative and diverse training set to train our CSMoE model. Extensive experiments on scene classification, semantic segmentation, and content-based image retrieval demonstrate that our adaptation yields a reduction in computational requirements while maintaining or improving representational performance. Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off between representational capacity, accuracy, and computational efficiency. On average, CSMoE achieves more than twice the computational efficiency of existing RS FMs, while maintaining competitive performance across all experiments. These results show the effectiveness of the proposed adaptation for creating computationally efficient RS FMs. The code for the model, the training set creation, and the model weights will be available at https://git.tu-berlin.de/rsim/csmoe.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2509.09572v1",
    "authors": [
      "Sijun Dong",
      "Yuxuan Hu",
      "LiBo Wang",
      "Geng Chen",
      "Xiaoliang Meng"
    ],
    "published": "2025-09-11",
    "abstract": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia",
    "url": "http://arxiv.org/abs/2509.08303v1",
    "authors": [
      "M. Warizmi Wafiq",
      "Peter Cutter",
      "Ate Poortinga",
      "Daniel Marc G. dela Torre",
      "Karis Tenneson",
      "Vanna Teck",
      "Enikoe Bihari",
      "Chanarun Saisaward",
      "Weraphong Suaruang",
      "Andrea McMahon",
      "Andi Vika Faradiba Muin",
      "Karno B. Batiran",
      "Chairil A",
      "Nurul Qomar",
      "Arya Arismaya Metananda",
      "David Ganz",
      "David Saah"
    ],
    "published": "2025-09-10",
    "abstract": "Oil palm cultivation remains one of the leading causes of deforestation in Indonesia. To better track and address this challenge, detailed and reliable mapping is needed to support sustainability efforts and emerging regulatory frameworks. We present an open-access geospatial dataset of oil palm plantations and related land cover types in Indonesia, produced through expert labeling of high-resolution satellite imagery from 2020 to 2024. The dataset provides polygon-based, wall-to-wall annotations across a range of agro-ecological zones and includes a hierarchical typology that distinguishes oil palm planting stages as well as similar perennial crops. Quality was ensured through multi-interpreter consensus and field validation. The dataset was created using wall-to-wall digitization over large grids, making it suitable for training and benchmarking both conventional convolutional neural networks and newer geospatial foundation models. Released under a CC-BY license, it fills a key gap in training data for remote sensing and aims to improve the accuracy of land cover types mapping. By supporting transparent monitoring of oil palm expansion, the resource contributes to global deforestation reduction goals and follows FAIR data principles.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement",
    "url": "http://arxiv.org/abs/2508.20954v1",
    "authors": [
      "Amir Jmal",
      "Chaima Chtourou",
      "Mahdi Louati",
      "Abdelaziz Kallel",
      "Houda Khmila"
    ],
    "published": "2025-08-28",
    "abstract": "In the context of proven climate change, maintaining olive biodiversity through early anomaly detection and treatment using remote sensing technology is crucial, offering effective management solutions. This paper presents an innovative approach to olive tree segmentation from satellite images. By leveraging foundational models and advanced segmentation techniques, the study integrates the Segment Anything Model (SAM) to accurately identify and segment olive trees in agricultural plots. The methodology includes SAM segmentation and corrections based on trees alignement in the field and a learanble constraint about the shape and the size. Our approach achieved a 98\\% accuracy rate, significantly surpassing the initial SAM performance of 82\\%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Anomaly Detection"
    ]
  },
  {
    "title": "Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory",
    "url": "http://arxiv.org/abs/2508.18829v1",
    "authors": [
      "Takayuki Ishikawa",
      "Carmelo Bonannella",
      "Bas J. W. Lerink",
      "Marc Ru\u00dfwurm"
    ],
    "published": "2025-08-26",
    "abstract": "National Forest Inventory (NFI)s serve as the primary source of forest information, providing crucial tree species distribution data. However, maintaining these inventories requires labor-intensive on-site campaigns. Remote sensing approaches, particularly when combined with machine learning, offer opportunities to update NFIs more frequently and at larger scales. While the use of Satellite Image Time Series has proven effective for distinguishing tree species through seasonal canopy reflectance patterns, current approaches rely primarily on Random Forest classifiers with hand-designed features and phenology-based metrics. Using deep features from an available pre-trained remote sensing foundation models offers a complementary strategy. These pre-trained models leverage unannotated global data and are meant to used for general-purpose applications and can then be efficiently fine-tuned with smaller labeled datasets for specific classification tasks. This work systematically investigates how deep features improve tree species classification accuracy in the Netherlands with few annotated data. Data-wise, we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites data and SRTM data using Google Earth Engine. Our results demonstrate that fine-tuning a publicly available remote sensing time series foundation model outperforms the current state-of-the-art in NFI classification in the Netherlands by a large margin of up to 10% across all datasets. This demonstrates that classic hand-defined harmonic features are too simple for this task and highlights the potential of using deep AI features for data-limited application like NFI classification. By leveraging openly available satellite data and pre-trained models, this approach significantly improves classification accuracy compared to traditional methods and can effectively complement existing forest inventory processes.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images",
    "url": "http://arxiv.org/abs/2508.18067v1",
    "authors": [
      "Kaiyu Li",
      "Xiangyong Cao",
      "Ruixun Liu",
      "Shihong Wang",
      "Zixuan Jiang",
      "Zhi Wang",
      "Deyu Meng"
    ],
    "published": "2025-08-25",
    "abstract": "Semantic segmentation of remote sensing (RS) images is pivotal for comprehensive Earth observation, but the demand for interpreting new object categories, coupled with the high expense of manual annotation, poses significant challenges. Although open-vocabulary semantic segmentation (OVSS) offers a promising solution, existing frameworks designed for natural images are insufficient for the unique complexities of RS data. They struggle with vast scale variations and fine-grained details, and their adaptation often relies on extensive, costly annotations. To address this critical gap, this paper introduces SegEarth-OV, the first framework for annotation-free open-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp, a universal upsampler that robustly restores high-resolution spatial details from coarse features, correcting distorted target shapes without any task-specific post-training. We also present a simple yet effective Global Bias Alleviation operation to subtract the inherent global context from patch features, significantly enhancing local semantic fidelity. These components empower SegEarth-OV to effectively harness the rich semantics of pre-trained VLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the framework's universality to other challenging RS modalities like SAR images, where large-scale VLMs are unavailable and expensive to create, we introduce AlignEarth, which is a distillation-based strategy and can efficiently transfer semantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the need to build SAR foundation models from scratch and enabling universal OVSS across diverse sensor types. Extensive experiments on both optical and SAR datasets validate that SegEarth-OV can achieve dramatic improvements over the SOTA methods, establishing a robust foundation for annotation-free and open-world Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing",
    "url": "http://arxiv.org/abs/2508.12409v3",
    "authors": [
      "Liang Lv",
      "Di Wang",
      "Jing Zhang",
      "Lefei Zhang"
    ],
    "published": "2025-08-17",
    "abstract": "Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS) analysis by leveraging unlabeled data through pseudo-labeling and consistency learning. However, existing S4 studies often rely on small-scale datasets and models, limiting their practical applicability. To address this, we propose S5, the first scalable framework for semi-supervised semantic segmentation in RS, which unlocks the potential of vast unlabeled Earth observation data typically underutilized due to costly pixel-level annotations. Built upon existing large-scale RS datasets, S5 introduces a data selection strategy that integrates entropy-based filtering and diversity expansion, resulting in the RS4P-1M dataset. Using this dataset, we systematically scale up S4 into a new pretraining paradigm, S4 pre-training (S4P), to pretrain RS foundation models (RSFMs) of varying sizes on this extensive corpus, significantly boosting their performance on land cover segmentation and object detection tasks. Furthermore, during fine-tuning, we incorporate a Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which enables efficient adaptation to multiple RS benchmarks with fewer parameters. This approach improves the generalization and versatility of RSFMs across diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance across all benchmarks, underscoring the viability of scaling semi-supervised learning for RS applications. All datasets, code, and models will be released at https://github.com/MiliLab/S5",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection",
    "url": "http://arxiv.org/abs/2508.11167v2",
    "authors": [
      "Jianhong Han",
      "Yupei Wang",
      "Liang Chen"
    ],
    "published": "2025-08-15",
    "abstract": "Unsupervised domain adaptation methods have been widely explored to bridge domain gaps. However, in real-world remote-sensing scenarios, privacy and transmission constraints often preclude access to source domain data, which limits their practical applicability. Recently, Source-Free Object Detection (SFOD) has emerged as a promising alternative, aiming at cross-domain adaptation without relying on source data, primarily through a self-training paradigm. Despite its potential, SFOD frequently suffers from training collapse caused by noisy pseudo-labels, especially in remote sensing imagery with dense objects and complex backgrounds. Considering that limited target domain annotations are often feasible in practice, we propose a Vision foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised framework for SFOD in remote sensing images. VG-DETR integrates a Vision Foundation Model (VFM) into the training pipeline in a \"free lunch\" manner, leveraging a small amount of labeled target data to mitigate pseudo-label noise while improving the detector's feature-extraction capability. Specifically, we introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's semantic priors to further assess the reliability of the generated pseudo-labels. By recovering potentially correct predictions from low-confidence outputs, our strategy improves pseudo-label quality and quantity. In addition, a dual-level VFM-guided alignment method is proposed, which aligns detector features with VFM embeddings at both the instance and image levels. Through contrastive learning among fine-grained prototypes and similarity matching between feature maps, this dual-level alignment further enhances the robustness of feature representations against domain gaps. Extensive experiments demonstrate that VG-DETR achieves superior performance in source-free remote sensing detection tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.10568v1",
    "authors": [
      "Humza Naveed",
      "Xina Zeng",
      "Mitch Bryson",
      "Nagita Mehrseresht"
    ],
    "published": "2025-08-14",
    "abstract": "Foundational models have achieved significant success in diverse domains of computer vision. They learn general representations that are easily transferable to tasks not seen during training. One such foundational model is Segment anything model (SAM), which can accurately segment objects in images. We propose adapting the SAM encoder via fine-tuning for remote sensing change detection (RSCD) along with spatial-temporal feature enhancement (STFE) and multi-scale decoder fusion (MSDF) to detect changes robustly at multiple scales. Additionally, we propose a novel cross-entropy masking (CEM) loss to handle high class imbalance in change detection datasets. Our method outperforms state-of-the-art (SOTA) methods on four change detection datasets, Levir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on a large complex S2Looking dataset. The code is available at: https://github.com/humza909/SAM-CEM-CD",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss",
    "url": "http://arxiv.org/abs/2508.09453v1",
    "authors": [
      "Abdul Matin",
      "Tanjim Bin Faruk",
      "Shrideep Pallickara",
      "Sangmi Lee Pallickara"
    ],
    "published": "2025-08-13",
    "abstract": "The proliferation of foundation models, pretrained on large-scale unlabeled datasets, has emerged as an effective approach in creating adaptable and reusable architectures that can be leveraged for various downstream tasks using satellite observations. However, their direct application to hyperspectral remote sensing remains challenging due to inherent spectral disparities and the scarcity of available observations. In this work, we present HyperKD, a novel knowledge distillation framework that enables transferring learned representations from a teacher model into a student model for effective development of a foundation model on hyperspectral images. Unlike typical knowledge distillation frameworks, which use a complex teacher to guide a simpler student, HyperKD enables an inverse form of knowledge transfer across different types of spectral data, guided by a simpler teacher model. Building upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi foundational model into a student tailored for EnMAP hyperspectral imagery. HyperKD addresses the inverse domain adaptation problem with spectral gaps by introducing a feature-based strategy that includes spectral range-based channel alignment, spatial feature-guided masking, and an enhanced loss function tailored for hyperspectral images. HyperKD bridges the substantial spectral domain gap, enabling the effective use of pretrained foundation models for geospatial applications. Extensive experiments show that HyperKD significantly improves representation learning in MAEs, leading to enhanced reconstruction fidelity and more robust performance on downstream tasks such as land cover classification, crop type identification, and soil organic carbon prediction, underpinning the potential of knowledge distillation frameworks in remote sensing analytics with hyperspectral imagery.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks",
    "url": "http://arxiv.org/abs/2508.03566v1",
    "authors": [
      "Xinyu Xiong",
      "Zihuang Wu",
      "Lei Zhang",
      "Lei Lu",
      "Ming Li",
      "Guanbin Li"
    ],
    "published": "2025-08-05",
    "abstract": "Recent studies have highlighted the potential of adapting the Segment Anything Model (SAM) for various downstream tasks. However, constructing a more powerful and generalizable encoder to further enhance performance remains an open challenge. In this work, we propose SAM2-UNeXT, an advanced framework that builds upon the core principles of SAM2-UNet while extending the representational capacity of SAM2 through the integration of an auxiliary DINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue layer, our approach enables more accurate segmentation with a simple architecture, relaxing the need for complex decoder designs. Extensive experiments conducted on four benchmarks, including dichotomous image segmentation, camouflaged object detection, marine animal segmentation, and remote sensing saliency detection, demonstrate the superior performance of our proposed method. The code is available at https://github.com/WZH0120/SAM2-UNeXT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models",
    "url": "http://arxiv.org/abs/2508.01731v1",
    "authors": [
      "Yuxiang Zhang",
      "Wei Li",
      "Mengmeng Zhang",
      "Jiawei Han",
      "Ran Tao",
      "Shunlin Liang"
    ],
    "published": "2025-08-03",
    "abstract": "Recent advances in Remote Sensing Foundation Models (RSFMs) have led to significant breakthroughs in the field. While many RSFMs have been pretrained with massive optical imagery, more multispectral/hyperspectral data remain lack of the corresponding foundation models. To leverage the advantages of spectral imagery in earth observation, we explore whether existing RSFMs can be effectively adapted to process diverse spectral modalities without requiring extensive spectral pretraining. In response to this challenge, we proposed SpectralX, an innovative parameter-efficient fine-tuning framework that adapt existing RSFMs as backbone while introducing a two-stage training approach to handle various spectral inputs, thereby significantly improving domain generalization performance. In the first stage, we employ a masked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) to extract attribute tokens from both spatial and spectral dimensions. Simultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA) that dynamically aggregates multi-attribute expert knowledge while performing layer-wise fine-tuning. With semantic segmentation as downstream task in the second stage, we insert an Attribute-refined Adapter (Are-adapter) into the first stage framework. By iteratively querying low-level semantic features with high-level representations, the model learns to focus on task-beneficial attributes, enabling customized adjustment of RSFMs. Following this two-phase adaptation process, SpectralX is capable of interpreting spectral imagery from new regions or seasons. The codes will be available from the website: https://github.com/YuxiangZhang-BIT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources",
    "url": "http://arxiv.org/abs/2508.00627v1",
    "authors": [
      "Paul Tresson",
      "Pierre Le Coz",
      "Hadrien Tulet",
      "Anthony Malkassian",
      "Maxime R\u00e9jou M\u00e9chain"
    ],
    "published": "2025-08-01",
    "abstract": "Remote sensing has entered a new era with the rapid development of artificial intelligence approaches. However, the implementation of deep learning has largely remained restricted to specialists and has been impractical because it often requires (i) large reference datasets for model training and validation; (ii) substantial computing resources; and (iii) strong coding skills. Here, we introduce IAMAP, a user-friendly QGIS plugin that addresses these three challenges in an easy yet flexible way. IAMAP builds on recent advancements in self-supervised learning strategies, which now provide robust feature extractors, often referred to as foundation models. These generalist models can often be reliably used in few-shot or zero-shot scenarios (i.e., with little to no fine-tuning). IAMAP's interface allows users to streamline several key steps in remote sensing image analysis: (i) extracting image features using a wide range of deep learning architectures; (ii) reducing dimensionality with built-in algorithms; (iii) performing clustering on features or their reduced representations; (iv) generating feature similarity maps; and (v) calibrating and validating supervised machine learning models for prediction. By enabling non-AI specialists to leverage the high-quality features provided by recent deep learning approaches without requiring GPU capacity or extensive reference datasets, IAMAP contributes to the democratization of computationally efficient and energy-conscious deep learning methods.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model",
    "url": "http://arxiv.org/abs/2507.22675v2",
    "authors": [
      "Meiqi Hu",
      "Lingzhi Lu",
      "Chengxi Han",
      "Xiaoping Liu"
    ],
    "published": "2025-07-30",
    "abstract": "Recently, large foundation models trained on vast datasets have demonstrated exceptional capabilities in feature extraction and general feature representation. The ongoing advancements in deep learning-driven large models have shown great promise in accelerating unsupervised change detection methods, thereby enhancing the practical applicability of change detection technologies. Building on this progress, this paper introduces MergeSAM, an innovative unsupervised change detection method for high-resolution remote sensing imagery, based on the Segment Anything Model (SAM). Two novel strategies, MaskMatching and MaskSplitting, are designed to address real-world complexities such as object splitting, merging, and other intricate changes. The proposed method fully leverages SAM's object segmentation capabilities to construct multitemporal masks that capture complex changes, embedding the spatial structure of land cover into the change detection process.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning",
    "url": "http://arxiv.org/abs/2507.20776v1",
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Yingchao Feng",
      "Kaiwen Wei",
      "Wenxin Yin",
      "Wenhui Diao",
      "Mengyu Wang",
      "Hanbo Bi",
      "Kaiyue Kang",
      "Tong Ling",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2025-07-28",
    "abstract": "Remote sensing (RS) images from multiple modalities and platforms exhibit diverse details due to differences in sensor characteristics and imaging perspectives. Existing vision-language research in RS largely relies on relatively homogeneous data sources. Moreover, they still remain limited to conventional visual perception tasks such as classification or captioning. As a result, these methods fail to serve as a unified and standalone framework capable of effectively handling RS imagery from diverse sources in real-world applications. To address these issues, we propose RingMo-Agent, a model designed to handle multi-modal and multi-platform data that performs perception and reasoning tasks based on user textual instructions. Compared with existing models, RingMo-Agent 1) is supported by a large-scale vision-language dataset named RS-VL3M, comprising over 3 million image-text pairs, spanning optical, SAR, and infrared (IR) modalities collected from both satellite and UAV platforms, covering perception and challenging reasoning tasks; 2) learns modality adaptive representations by incorporating separated embedding layers to construct isolated features for heterogeneous modalities and reduce cross-modal interference; 3) unifies task modeling by introducing task-specific tokens and employing a token-based high-dimensional hidden state decoding mechanism designed for long-horizon spatial tasks. Extensive experiments on various RS vision-language tasks demonstrate that RingMo-Agent not only proves effective in both visual understanding and sophisticated analytical tasks, but also exhibits strong generalizability across different platforms and sensing modalities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation",
    "url": "http://arxiv.org/abs/2507.16716v1",
    "authors": [
      "Yiguo He",
      "Junjie Zhu",
      "Yiying Li",
      "Xiaoyu Zhang",
      "Chunping Qiu",
      "Jun Wang",
      "Qiangjuan Huang",
      "Ke Yang"
    ],
    "published": "2025-07-22",
    "abstract": "The application of Vision-language foundation models (VLFMs) to remote sensing (RS) imagery has garnered significant attention due to their superior capability in various downstream tasks. A key challenge lies in the scarcity of high-quality, large-scale, image-text paired training data. Recently, several works introduced extensive image-text datasets for RS and trained their VLFMs. However, due to the rudimentary methods used for generating captions, the quality of datasets is suboptimal, requiring larger volumes of training data, while only yielding modest performance improvements. In this paper, we propose a two-stage method named MpGI(Multi-Perspective Generation and Integration) for generating high-quality text captions for RS images. Firstly, we generate distinct and detailed descriptions from different perspectives using Rule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs generation methods. Next, we utilize Large Language Models (LLMs) to integrate these diverse descriptions into comprehensive captions, capturing details from multiple perspectives. Finally, we have created the HQRS-IT-210K dataset, including about 210,000 RS images and 1.3 million captions. We fine-tuned two VLFMs using our dataset: CLIP, a discriminative model, and CoCa, an image-to-text generative model. This process resulted in our proposed HQRS-CLIP and RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed the previous SOTA RS CLIP model in various downstream tasks while using only 4.2\\% of the training data. RS-CoCa outperforms other advanced approaches across benchmark datasets and can generate captions for RS images that rival or even exceed manual annotations. Dataset, pre-trained models, and codes will be released at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "CLIP"
    ],
    "applications": []
  },
  {
    "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
    "url": "http://arxiv.org/abs/2507.13812v1",
    "authors": [
      "Yingying Zhang",
      "Lixiang Ru",
      "Kang Wu",
      "Lei Yu",
      "Lei Liang",
      "Yansheng Li",
      "Jingdong Chen"
    ],
    "published": "2025-07-18",
    "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly advanced various Earth observation tasks, such as urban planning, environmental monitoring, and natural disaster management. However, most existing approaches generally require the training of separate backbone networks for each data modality, leading to redundancy and inefficient parameter utilization. Moreover, prevalent pre-training methods typically apply self-supervised learning (SSL) techniques from natural images without adequately accommodating the characteristics of remote sensing (RS) images, such as the complicated semantic distribution within a single RS image. In this work, we present SkySense V2, a unified MM-RSFM that employs a single transformer backbone to handle multiple modalities. This backbone is pre-trained with a novel SSL strategy tailored to the distinct traits of RS data. In particular, SkySense V2 incorporates an innovative adaptive patch merging module and learnable modality prompt tokens to address challenges related to varying resolutions and limited feature diversity across modalities. In additional, we incorporate the mixture of experts (MoE) module to further enhance the performance of the foundation model. SkySense V2 demonstrates impressive generalization abilities through an extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense by an average of 1.8 points.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Deploying Geospatial Foundation Models in the Real World: Lessons from WorldCereal",
    "url": "http://arxiv.org/abs/2508.00858v1",
    "authors": [
      "Christina Butsko",
      "Kristof Van Tricht",
      "Gabriel Tseng",
      "Giorgia Milli",
      "David Rolnick",
      "Ruben Cartuyvels",
      "Inbal Becker Reshef",
      "Zoltan Szantoi",
      "Hannah Kerner"
    ],
    "published": "2025-07-16",
    "abstract": "The increasing availability of geospatial foundation models has the potential to transform remote sensing applications such as land cover classification, environmental monitoring, and change detection. Despite promising benchmark results, the deployment of these models in operational settings is challenging and rare. Standardized evaluation tasks often fail to capture real-world complexities relevant for end-user adoption such as data heterogeneity, resource constraints, and application-specific requirements. This paper presents a structured approach to integrate geospatial foundation models into operational mapping systems. Our protocol has three key steps: defining application requirements, adapting the model to domain-specific data and conducting rigorous empirical testing. Using the Presto model in a case study for crop mapping, we demonstrate that fine-tuning a pre-trained model significantly improves performance over conventional supervised methods. Our results highlight the model's strong spatial and temporal generalization capabilities. Our protocol provides a replicable blueprint for practitioners and lays the groundwork for future research to operationalize foundation models in diverse remote sensing applications. Application of the protocol to the WorldCereal global crop-mapping system showcases the framework's scalability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Continental-scale habitat distribution modelling with multimodal earth observation foundation models",
    "url": "http://arxiv.org/abs/2507.09732v2",
    "authors": [
      "Sara Si-Moussi",
      "Stephan Hennekens",
      "Sander Mucher",
      "Stan Los",
      "Yoann Cartier",
      "Borja Jim\u00e9nez-Alfaro",
      "Fabio Attorre",
      "Jens-Christian Svenning",
      "Wilfried Thuiller"
    ],
    "published": "2025-07-13",
    "abstract": "Habitats integrate the abiotic conditions, vegetation composition and structure that support biodiversity and sustain nature's contributions to people. Most habitats face mounting pressures from human activities, which requires accurate, high-resolution habitat mapping for effective conservation and restoration. Yet, current habitat maps often fall short in thematic or spatial resolution because they must (1) model several mutually exclusive habitat types that co-occur across landscapes and (2) cope with severe class imbalance that complicates exhaustive multi-class training. Here, we evaluated how high-resolution remote sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat mapping across large geographical extents at fine spatial and thematic resolution. Using vegetation plots from the European Vegetation Archive, we modelled the distribution of Level 3 EUNIS habitat types across Europe and assessed multiple modelling strategies against independent validation datasets. Strategies that exploited the hierarchical nature of habitat classifications resolved classification ambiguities, especially in fragmented habitats. Integrating satellite-borne multispectral and radar imagery, particularly through Earth Observation (EO) Foundation models (EO-FMs), enhanced within-formation discrimination and overall performance. Finally, ensemble machine learning that corrects class imbalance boosted predictive accuracy even further. Our methodological framework is transferable beyond Europe and adaptable to other classification systems. Future research should advance temporal modelling of habitat dynamics, extend to habitat segmentation and quality assessment, and exploit next-generation EO data paired with higher-quality in situ observations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges",
    "url": "http://arxiv.org/abs/2507.09562v1",
    "authors": [
      "Yidong Jiang"
    ],
    "published": "2025-07-13",
    "abstract": "The Segment Anything Model (SAM) has revolutionized image segmentation through its innovative prompt-based approach, yet the critical role of prompt engineering in its success remains underexplored. This paper presents the first comprehensive survey focusing specifically on prompt engineering techniques for SAM and its variants. We systematically organize and analyze the rapidly growing body of work in this emerging field, covering fundamental methodologies, practical applications, and key challenges. Our review reveals how prompt engineering has evolved from simple geometric inputs to sophisticated multimodal approaches, enabling SAM's adaptation across diverse domains including medical imaging and remote sensing. We identify unique challenges in prompt optimization and discuss promising research directions. This survey fills an important gap in the literature by providing a structured framework for understanding and advancing prompt engineering in foundation models for segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion",
    "url": "http://arxiv.org/abs/2507.09081v1",
    "authors": [
      "Zhenyu Yu",
      "Mohd Yamani Idna Idris",
      "Hua Wang",
      "Pei Wang",
      "Junyi Chen",
      "Kun Wang"
    ],
    "published": "2025-07-11",
    "abstract": "Quantitative remote sensing inversion aims to estimate continuous surface variables-such as biomass, vegetation indices, and evapotranspiration-from satellite observations, supporting applications in ecosystem monitoring, carbon accounting, and land management. With the evolution of remote sensing systems and artificial intelligence, traditional physics-based paradigms are giving way to data-driven and foundation model (FM)-based approaches. This paper systematically reviews the methodological evolution of inversion techniques, from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods (e.g., deep learning, multimodal fusion), and further to foundation models (e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application scenarios, and limitations of each paradigm, with emphasis on recent FM advances in self-supervised pretraining, multi-modal integration, and cross-task adaptation. We also highlight persistent challenges in physical interpretability, domain generalization, limited supervision, and uncertainty quantification. Finally, we envision the development of next-generation foundation models for remote sensing inversion, emphasizing unified modeling capacity, cross-domain generalization, and physical interpretability.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "BioAnalyst: A Foundation Model for Biodiversity",
    "url": "http://arxiv.org/abs/2507.09080v2",
    "authors": [
      "Athanasios Trantas",
      "Martino Mensio",
      "Stylianos Stasinos",
      "Sebastian Gribincea",
      "Taimur Khan",
      "Damian Podareanu",
      "Aliene van der Veen"
    ],
    "published": "2025-07-11",
    "abstract": "Multimodal Foundation Models (FMs) offer a path to learn general-purpose representations from heterogeneous ecological data, easily transferable to downstream tasks. However, practical biodiversity modelling remains fragmented; separate pipelines and models are built for each dataset and objective, which limits reuse across regions and taxa. In response, we present BioAnalyst, to our knowledge the first multimodal Foundation Model tailored to biodiversity analysis and conservation planning in Europe at $0.25^{\\circ}$ spatial resolution targeting regional to national-scale applications. BioAnalyst employs a transformer-based architecture, pre-trained on extensive multimodal datasets that align species occurrence records with remote sensing indicators, climate and environmental variables. Post pre-training, the model is adapted via lightweight roll-out fine-tuning to a range of downstream tasks, including joint species distribution modelling, biodiversity dynamics and population trend forecasting. The model is evaluated on two representative downstream use cases: (i) joint species distribution modelling and with 500 vascular plant species (ii) monthly climate linear probing with temperature and precipitation data. Our findings show that BioAnalyst can provide a strong baseline both for biotic and abiotic tasks, acting as a macroecological simulator with a yearly forecasting horizon and monthly resolution, offering the first application of this type of modelling in the biodiversity domain. We have open-sourced the model weights, training and fine-tuning pipelines to advance AI-driven ecological research.",
    "categories": [
      "fish_plankton",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MAPEX: Modality-Aware Pruning of Experts for Remote Sensing Foundation Models",
    "url": "http://arxiv.org/abs/2507.07527v1",
    "authors": [
      "Joelle Hanna",
      "Linus Scheibenreif",
      "Damian Borth"
    ],
    "published": "2025-07-10",
    "abstract": "Remote sensing data is commonly used for tasks such as flood mapping, wildfire detection, or land-use studies. For each task, scientists carefully choose appropriate modalities or leverage data from purpose-built instruments. Recent work on remote sensing foundation models pre-trains computer vision models on large amounts of remote sensing data. These large-scale models tend to focus on specific modalities, often optical RGB or multispectral data. For many important applications, this introduces a mismatch between the application modalities and the pre-training data. Moreover, the large size of foundation models makes them expensive and difficult to fine-tune on typically small datasets for each task. We address this mismatch with MAPEX, a remote sensing foundation model based on mixture-of-modality experts. MAPEX is pre-trained on multi-modal remote sensing data with a novel modality-conditioned token routing mechanism that elicits modality-specific experts. To apply the model on a specific task, we propose a modality aware pruning technique, which only retains experts specialized for the task modalities. This yields efficient modality-specific models while simplifying fine-tuning and deployment for the modalities of interest. We experimentally validate MAPEX on diverse remote sensing datasets and show strong performance compared to fully supervised training and state-of-the-art remote sensing foundation models. Code is available at https://github.com/HSG-AIML/MAPEX.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models",
    "url": "http://arxiv.org/abs/2507.06231v1",
    "authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Bowen Chen",
      "Jiafan Zhang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-07-08",
    "abstract": "Referring Remote Sensing Image Segmentation provides a flexible and fine-grained framework for remote sensing scene analysis via vision-language collaborative interpretation. Current approaches predominantly utilize a three-stage pipeline encompassing dual-modal encoding, cross-modal interaction, and pixel decoding. These methods demonstrate significant limitations in managing complex semantic relationships and achieving precise cross-modal alignment, largely due to their coupled processing mechanism that conflates target localization with boundary delineation. This architectural coupling amplifies error propagation under semantic ambiguity while restricting model generalizability and interpretability. To address these issues, we propose RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow into a collaborative dual-stage framework: coarse localization followed by fine segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with SAM's segmentation generalizability through strategic foundation model collaboration. Specifically, CLIP is employed as the dual-modal encoder to activate target features within its pre-aligned semantic space and generate localization prompts. To mitigate CLIP's misactivation challenges in multi-entity scenarios described by referring texts, a cascaded second-order prompter is devised, which enhances precision through implicit reasoning via decomposition of text embeddings into complementary semantic subspaces. These optimized semantic prompts subsequently direct the SAM to generate pixel-level refined masks, thereby completing the semantic transmission pipeline. Extensive experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2 surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex semantic interpretation. Code is available at: https://github.com/KyanChen/RSRefSeg2.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "From General to Specialized: The Need for Foundational Models in Agriculture",
    "url": "http://arxiv.org/abs/2507.05390v2",
    "authors": [
      "Vishal Nedungadi",
      "Xingguo Xiong",
      "Aike Potze",
      "Ron Van Bree",
      "Tao Lin",
      "Marc Ru\u00dfwurm",
      "Ioannis N. Athanasiadis"
    ],
    "published": "2025-07-07",
    "abstract": "Food security remains a global concern as population grows and climate change intensifies, demanding innovative solutions for sustainable agricultural productivity. Recent advances in foundation models have demonstrated remarkable performance in remote sensing and climate sciences, and therefore offer new opportunities for agricultural monitoring. However, their application in challenges related to agriculture-such as crop type mapping, crop phenology estimation, and crop yield estimation-remains under-explored. In this work, we quantitatively evaluate existing foundational models to assess their effectivity for a representative set of agricultural tasks. From an agricultural domain perspective, we describe a requirements framework for an ideal agricultural foundation model (CropFM). We then survey and compare existing general-purpose foundational models in this framework and empirically evaluate two exemplary of them in three representative agriculture specific tasks. Finally, we highlight the need for a dedicated foundational model tailored specifically to agriculture.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Time2Agri: Temporal Pretext Tasks for Agricultural Monitoring",
    "url": "http://arxiv.org/abs/2507.04366v1",
    "authors": [
      "Moti Rattan Gupta",
      "Anupam Sobti"
    ],
    "published": "2025-07-06",
    "abstract": "Self Supervised Learning(SSL) has emerged as a prominent paradigm for label-efficient learning, and has been widely utilized by remote sensing foundation models(RSFMs). Recent RSFMs including SatMAE, DoFA, primarily rely on masked autoencoding(MAE), contrastive learning or some combination of them. However, these pretext tasks often overlook the unique temporal characteristics of agricultural landscape, namely nature's cycle. Motivated by this gap, we propose three novel agriculture-specific pretext tasks, namely Time-Difference Prediction(TD), Temporal Frequency Prediction(FP), and Future-Frame Prediction(FF). Comprehensive evaluation on SICKLE dataset shows FF achieves 69.6% IoU on crop mapping and FP reduces yield prediction error to 30.7% MAPE, outperforming all baselines, and TD remains competitive on most tasks. Further, we also scale FF to the national scale of India, achieving 54.2% IoU outperforming all baselines on field boundary delineation on FTW India dataset.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation",
    "url": "http://arxiv.org/abs/2507.00356v1",
    "authors": [
      "Zhiwei Yi",
      "Xin Cheng",
      "Jingyu Ma",
      "Ruifei Zhu",
      "Junwei Tian",
      "Yuanxiu Zhou",
      "Xinge Zhao",
      "Hongzhe Li"
    ],
    "published": "2025-07-01",
    "abstract": "Deep learning methods have significantly advanced the development of intelligent rinterpretation in remote sensing (RS), with foundational model research based on large-scale pre-training paradigms rapidly reshaping various domains of Earth Observation (EO). However, compared to the open accessibility and high spatiotemporal coverage of medium-resolution data, the limited acquisition channels for ultra-high-resolution optical RS imagery have constrained the progress of high-resolution remote sensing vision foundation models (RSVFM). As the world's largest sub-meter-level commercial RS satellite constellation, the Jilin-1 constellation possesses abundant sub-meter-level image resources. This study proposes CGEarthEye, a RSVFM framework specifically designed for Jilin-1 satellite characteristics, comprising five backbones with different parameter scales with totaling 2.1 billion parameters. To enhance the representational capacity of the foundation model, we developed JLSSD, the first 15-million-scale multi-temporal self-supervised learning (SSL) dataset featuring global coverage with quarterly temporal sampling within a single year, constructed through multi-level representation clustering and sampling strategies. The framework integrates seasonal contrast, augmentation-based contrast, and masked patch token contrastive strategies for pre-training. Comprehensive evaluations across 10 benchmark datasets covering four typical RS tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art (SOTA) performance. Further analysis reveals CGEarthEye's superior characteristics in feature visualization, model convergence, parameter efficiency, and practical mapping applications. This study anticipates that the exceptional representation capabilities of CGEarthEye will facilitate broader and more efficient applications of Jilin-1 data in traditional EO application.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition",
    "url": "http://arxiv.org/abs/2506.20174v2",
    "authors": [
      "Man Duc Chuc"
    ],
    "published": "2025-06-25",
    "abstract": "Foundation models are rapidly transforming Earth Observation data mining by enabling generalizable and scalable solutions for key tasks such as scene classification and semantic segmentation. While most efforts in the geospatial domain have focused on developing large models trained from scratch using massive Earth Observation datasets, an alternative strategy that remains underexplored is the reuse and combination of existing pretrained models. In this study, we investigate whether foundation models pretrained on remote sensing and general vision datasets can be effectively combined to improve performance across a diverse set of key Earth Observation tasks. Using the GEO-Bench benchmark, we evaluate several prominent models, including Prithvi, Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions, sensor modalities, and task types. The results show that feature-level ensembling of smaller pretrained models can match or exceed the performance of much larger models, while requiring less training time and computational resources. Moreover, the study highlights the potential of applying knowledge distillation to transfer the strengths of ensembles into more compact models, offering a practical path for deploying foundation models in real-world Earth Observation applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2506.19585v1",
    "authors": [
      "Gencer Sumbul",
      "Chang Xu",
      "Emanuele Dalsasso",
      "Devis Tuia"
    ],
    "published": "2025-06-24",
    "abstract": "From optical sensors to microwave radars, leveraging the complementary strengths of remote sensing (RS) sensors is crucial for achieving dense spatio-temporal monitoring of our planet. In contrast, recent deep learning models, whether task-specific or foundational, are often specific to single sensors or to fixed combinations: adapting such models to different sensory inputs requires both architectural changes and re-training, limiting scalability and generalization across multiple RS sensors. On the contrary, a single model able to modulate its feature representations to accept diverse sensors as input would pave the way to agile and flexible multi-sensor RS data processing. To address this, we introduce SMARTIES, a generic and versatile foundation model lifting sensor-specific/dependent efforts and enabling scalability and generalization to diverse RS sensors: SMARTIES projects data from heterogeneous sensors into a shared spectrum-aware space, enabling the use of arbitrary combinations of bands both for training and inference. To obtain sensor-agnostic representations, we train a single, unified transformer model reconstructing masked multi-sensor data with cross-sensor token mixup. On both single- and multi-modal tasks across diverse sensors, SMARTIES outperforms previous models that rely on sensor-specific pretraining. Our code and pretrained models are available at https://gsumbul.github.io/SMARTIES.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high Spatial Resolution Land Cover Classification",
    "url": "http://arxiv.org/abs/2506.15565v2",
    "authors": [
      "Junhao Wu",
      "Aboagye-Ntow Stephen",
      "Chuyuan Wang",
      "Gang Chen",
      "Xin Huang"
    ],
    "published": "2025-06-18",
    "abstract": "Ultra-high Spatial Resolution (UHSR) Land Cover Classification is increasingly important for urban analysis, enabling fine-scale planning, ecological monitoring, and infrastructure management. It identifies land cover types on sub-meter remote sensing imagery, capturing details such as building outlines, road networks, and distinct boundaries. However, most existing methods focus on 1 m imagery and rely heavily on large-scale annotations, while UHSR data remain scarce and difficult to annotate, limiting practical applicability. To address these challenges, we introduce Baltimore Atlas, a UHSR land cover classification framework that reduces reliance on large-scale training data and delivers high-accuracy results. Baltimore Atlas builds on three key ideas: (1) Baltimore Atlas Dataset, a 0.3 m resolution dataset based on aerial imagery of Baltimore City; (2) FreqWeaver Adapter, a parameter-efficient adapter that transfers SAM2 to this domain, leveraging foundation model knowledge to reduce training data needs while enabling fine-grained detail and structural modeling; (3) Uncertainty-Aware Teacher Student Framework, a semi-supervised framework that exploits unlabeled data to further reduce training dependence and improve generalization across diverse scenes. Using only 5.96% of total model parameters, our approach achieves a 1.78% IoU improvement over existing parameter-efficient tuning strategies and a 3.44% IoU gain compared to state-of-the-art high-resolution remote sensing segmentation methods on the Baltimore Atlas Dataset.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models",
    "url": "http://arxiv.org/abs/2506.08780v1",
    "authors": [
      "Isaac Corley",
      "Lakshay Sharma",
      "Ruth Crasto"
    ],
    "published": "2025-06-10",
    "abstract": "The Landsat program offers over 50 years of globally consistent Earth imagery. However, the lack of benchmarks for this data constrains progress towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and LC100-L. We establish baseline and standardized evaluation methods across both common architectures and Landsat foundation models pretrained on the SSL4EO-L dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract better representations for downstream tasks in comparison to ImageNet, including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and BigEarthNet-L.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation",
    "url": "http://arxiv.org/abs/2506.08772v2",
    "authors": [
      "Jiayi Song",
      "Kaiyu Li",
      "Xiangyong Cao",
      "Deyu Meng"
    ],
    "published": "2025-06-10",
    "abstract": "Semantic segmentation in remote sensing images is crucial for various applications, yet its performance is heavily reliant on large-scale, high-quality pixel-wise annotations, which are notoriously expensive and time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a promising alternative to mitigate this data dependency. However, existing SSS methods often struggle with the inherent distribution mismatch between limited labeled data and abundant unlabeled data, leading to suboptimal generalization. To alleviate this issue, we attempt to introduce the Vision Foundation Models (VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs possess robust generalization capabilities that can effectively bridge this distribution gap and provide strong semantic priors for SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework that leverages the powerful semantic knowledge embedded in VFMs to guide semi-supervised learning in remote sensing. Specifically, RS-MTDF employs multiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing feature-level distillation to align student features with their robust representations. To further enhance discriminative power, the distilled knowledge is seamlessly fused into the student decoder. Extensive experiments on three challenging remote sensing datasets demonstrate that RS-MTDF consistently achieves state-of-the-art performance. Notably, our method outperforms existing approaches across various label ratios on LoveDA and secures the highest IoU in the majority of semantic categories. These results underscore the efficacy of multi-teacher VFM guidance in significantly enhancing both generalization and semantic understanding for remote sensing segmentation. Ablation studies further validate the contribution of each proposed module.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
    "url": "http://arxiv.org/abs/2505.21375v2",
    "authors": [
      "Fengxiang Wang",
      "Mingshuo Chen",
      "Yueying Li",
      "Di Wang",
      "Haotian Wang",
      "Zonghao Guo",
      "Zefan Wang",
      "Boqi Shan",
      "Long Lan",
      "Yulin Wang",
      "Hongzhen Wang",
      "Wenjing Yang",
      "Bo Du",
      "Jing Zhang"
    ],
    "published": "2025-05-27",
    "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA (avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: Background Token Pruning and Anchored Token Selection, to reduce the memory footprint while preserving key semantics.Integrating these techniques, we introduce GeoLLaVA-8K, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping",
    "url": "http://arxiv.org/abs/2505.21357v2",
    "authors": [
      "Wenyuan Li",
      "Shunlin Liang",
      "Keyan Chen",
      "Yongzhe Chen",
      "Han Ma",
      "Jianglei Xu",
      "Yichuan Ma",
      "Shikang Guan",
      "Husheng Fang",
      "Zhenwei Shi"
    ],
    "published": "2025-05-27",
    "abstract": "Accurate crop mapping fundamentally relies on modeling multi-scale spatiotemporal patterns, where spatial scales range from individual field textures to landscape-level context, and temporal scales capture both short-term phenological transitions and full growing-season dynamics. Transformer-based remote sensing foundation models (RSFMs) offer promising potential for crop mapping due to their innate ability for unified spatiotemporal processing. However, current RSFMs remain suboptimal for crop mapping: they either employ fixed spatiotemporal windows that ignore the multi-scale nature of crop systems or completely disregard temporal information by focusing solely on spatial patterns. To bridge these gaps, we present AgriFM, a multi-source remote sensing foundation model specifically designed for agricultural crop mapping. Our approach begins by establishing the necessity of simultaneous hierarchical spatiotemporal feature extraction, leading to the development of a modified Video Swin Transformer architecture where temporal down-sampling is synchronized with spatial scaling operations. This modified backbone enables efficient unified processing of long time-series satellite inputs. AgriFM leverages temporally rich data streams from three satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is pre-trained on a global representative dataset comprising over 25 million image samples supervised by land cover products. The resulting framework incorporates a versatile decoder architecture that dynamically fuses these learned spatiotemporal representations, supporting diverse downstream tasks. Comprehensive evaluations demonstrate AgriFM's superior performance over conventional deep learning approaches and state-of-the-art general-purpose RSFMs across all downstream tasks. Codes will be available at https://github.com/flyakon/AgriFM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2505.19447v2",
    "authors": [
      "Hengtong Shen",
      "Haiyan Gu",
      "Haitao Li",
      "Yi Yang",
      "Agen Qiu"
    ],
    "published": "2025-05-26",
    "abstract": "Self-Supervised Learning (SSL) enables us to pre-train foundation models without costly labeled data. Among SSL methods, Contrastive Learning (CL) methods are better at obtaining accurate semantic representations in noise interference. However, due to the significant domain gap, while CL methods have achieved great success in many computer vision tasks, they still require specific adaptation for Remote Sensing (RS) images. To this end, we present a novel self-supervised method called PerA, which produces all-purpose RS features through semantically Perfectly Aligned sample pairs. Specifically, PerA obtains features from sampled views by applying spatially disjoint masks to augmented images rather than random cropping. Our framework provides high-quality features by ensuring consistency between teacher and student and predicting learnable mask tokens. Compared to previous contrastive methods, our method demonstrates higher memory efficiency and can be trained with larger batches due to its sparse inputs. Additionally, the proposed method demonstrates remarkable adaptability to uncurated RS data and reduce the impact of the potential semantic inconsistency. We also collect an unlabeled pre-training dataset, which contains about 5 million RS images. We conducted experiments on multiple downstream task datasets and achieved performance comparable to previous state-of-the-art methods with a limited model scale, demonstrating the effectiveness of our approach. We hope this work will contribute to practical remote sensing interpretation works.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models",
    "url": "http://arxiv.org/abs/2505.16793v2",
    "authors": [
      "Xiang Li",
      "Yong Tao",
      "Siyuan Zhang",
      "Siwei Liu",
      "Zhitong Xiong",
      "Chunbo Luo",
      "Lu Liu",
      "Mykola Pechenizkiy",
      "Xiao Xiang Zhu",
      "Tianjin Huang"
    ],
    "published": "2025-05-22",
    "abstract": "Earth observation foundation models have shown strong generalization across multiple Earth observation tasks, but their robustness under real-world perturbations remains underexplored. To bridge this gap, we introduce REOBench, the first comprehensive benchmark for evaluating the robustness of Earth observation foundation models across six tasks and twelve types of image corruptions, including both appearance-based and geometric perturbations. To ensure realistic and fine-grained evaluation, our benchmark focuses on high-resolution optical remote sensing images, which are widely used in critical applications such as urban planning and disaster response. We conduct a systematic evaluation of a broad range of models trained using masked image modeling, contrastive learning, and vision-language pre-training paradigms. Our results reveal that (1) existing Earth observation foundation models experience significant performance degradation when exposed to input corruptions. (2) The severity of degradation varies across tasks, model architectures, backbone sizes, and types of corruption, with performance drop varying from less than 1% to over 20%. (3) Vision-language models show enhanced robustness, particularly in multimodal tasks. REOBench underscores the vulnerability of current Earth observation foundation models to real-world corruptions and provides actionable insights for developing more robust and reliable models. Code and data are publicly available at https://github.com/lx709/REOBench.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation",
    "url": "http://arxiv.org/abs/2505.16540v1",
    "authors": [
      "Inbal Cohen",
      "Boaz Meivar",
      "Peihan Tu",
      "Shai Avidan",
      "Gal Oren"
    ],
    "published": "2025-05-22",
    "abstract": "Segment Anything Models (SAM) have achieved remarkable success in object segmentation tasks across diverse datasets. However, these models are predominantly trained on large-scale semantic segmentation datasets, which introduce a bias toward object shape rather than texture cues in the image. This limitation is critical in domains such as medical imaging, material classification, and remote sensing, where texture changes define object boundaries. In this study, we investigate SAM's bias toward semantics over textures and introduce a new texture-aware foundation model, TextureSAM, which performs superior segmentation in texture-dominant scenarios. To achieve this, we employ a novel fine-tuning approach that incorporates texture augmentation techniques, incrementally modifying training images to emphasize texture features. By leveraging a novel texture-alternation of the ADE20K dataset, we guide TextureSAM to prioritize texture-defined regions, thereby mitigating the inherent shape bias present in the original SAM model. Our extensive experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation datasets. The code and texture-augmented dataset will be publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2505.15334v1",
    "authors": [
      "Bernardin Ligan",
      "Khalide Jbilou",
      "Fahd Kalloubi",
      "Ahmed Ratnani"
    ],
    "published": "2025-05-21",
    "abstract": "Foundation models have achieved great success across diverse domains, including remote sensing (RS), thanks to their versatility and strong generalization abilities. However, most RS foundation models are designed for multispectral data, while hyperspectral imagery (HSI) - with its hundreds of spectral bands - remains less explored. Fine-tuning such models for downstream tasks is also challenging, often demanding considerable memory and storage. In this paper, we propose an efficient framework to fine-tune SpectralGPT, a multispectral foundation model, for hyperspectral image classification (HSIC). We explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including Low-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank Kronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for low-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce KronA+, which applies a similar mechanism to the Kronecker matrices. We evaluate our approach on five datasets from different sensors, showing competitive performance with state-of-the-art HSI models. Our full fine-tuning (FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral foundation model on some datasets while requiring only a quarter of the training epochs. Under the same number of epochs, KronA+ reaches similar performance with far fewer trainable parameters - just 0.056 percent - and adds only approximately 0.2 megabytes of storage, making it the most effective PEFT method tested.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation",
    "url": "http://arxiv.org/abs/2505.15147v1",
    "authors": [
      "Quanwei Liu",
      "Tao Huang",
      "Yanni Dong",
      "Jiaqi Yang",
      "Wei Xiang"
    ],
    "published": "2025-05-21",
    "abstract": "Remote sensing images (RSIs) capture both natural and human-induced changes on the Earth's surface, serving as essential data for environmental monitoring, urban planning, and resource management. Semantic segmentation (SS) of RSIs enables the fine-grained interpretation of surface features, making it a critical task in remote sensing analysis. With the increasing diversity and volume of RSIs collected by sensors on various platforms, traditional processing methods struggle to maintain efficiency and accuracy. In response, deep learning (DL) has emerged as a transformative approach, enabling substantial advances in remote sensing image semantic segmentation (RSISS) by automating feature extraction and improving segmentation accuracy across diverse modalities. This paper revisits the evolution of DL-based RSISS by categorizing existing approaches into four stages: the early pixel-based methods, the prevailing patch-based and tile-based techniques, and the emerging image-based strategies enabled by foundation models. We analyze these developments from the perspective of feature extraction and learning strategies, revealing the field's progression from pixel-level to tile-level and from unimodal to multimodal segmentation. Furthermore, we conduct a comprehensive evaluation of nearly 40 advanced techniques on a unified dataset to quantitatively characterize their performance and applicability. This review offers a holistic view of DL-based SS for RS, highlighting key advancements, comparative insights, and open challenges to guide future research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives",
    "url": "http://arxiv.org/abs/2505.14361v1",
    "authors": [
      "Xingxing Weng",
      "Chao Pang",
      "Gui-Song Xia"
    ],
    "published": "2025-05-20",
    "abstract": "Vision-language modeling (VLM) aims to bridge the information gap between images and natural language. Under the new paradigm of first pre-training on massive image-text pairs and then fine-tuning on task-specific data, VLM in the remote sensing domain has made significant progress. The resulting models benefit from the absorption of extensive general knowledge and demonstrate strong performance across a variety of remote sensing data analysis tasks. Moreover, they are capable of interacting with users in a conversational manner. In this paper, we aim to provide the remote sensing community with a timely and comprehensive review of the developments in VLM using the two-stage paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing: contrastive learning, visual instruction tuning, and text-conditioned image generation. For each category, we detail the commonly used network architecture and pre-training objectives. Second, we conduct a thorough review of existing works, examining foundation models and task-specific adaptation methods in contrastive-based VLM, architectural upgrades, training strategies and model capabilities in instruction-based VLM, as well as generative foundation models with their representative downstream applications. Third, we summarize datasets used for VLM pre-training, fine-tuning, and evaluation, with an analysis of their construction methodologies (including image sources and caption generation) and key properties, such as scale and task adaptability. Finally, we conclude this survey with insights and discussions on future research directions: cross-modal representation alignment, vague requirement comprehension, explanation-driven model reliability, continually scalable model capabilities, and large-scale datasets featuring richer modalities and greater challenges.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts",
    "url": "http://arxiv.org/abs/2505.14088v1",
    "authors": [
      "Xi Chen",
      "Shen Yan",
      "Juelin Zhu",
      "Chen Chen",
      "Yu Liu",
      "Maojun Zhang"
    ],
    "published": "2025-05-20",
    "abstract": "We introduce Land-MoE, a novel approach for multispectral land cover classification (MLCC). Spectral shift, which emerges from disparities in sensors and geospatial conditions, poses a significant challenge in this domain. Existing methods predominantly rely on domain adaptation and generalization strategies, often utilizing small-scale models that exhibit limited performance. In contrast, Land-MoE addresses these issues by hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts, to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner. Specifically, Land-MoE comprises two key modules: the mixture of low-rank token experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages rank-differentiated tokens to generate diverse feature adjustments for individual instances within multispectral images. By dynamically combining learnable low-rank token experts of varying ranks, it enhances the robustness against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on the refined features. This process enables the model to effectively capture frequency band information that is strongly correlated with semantic essence, while simultaneously suppressing frequency noise irrelevant to the task. Comprehensive experiments on MLCC tasks involving cross-sensor and cross-geospatial setups demonstrate that Land-MoE outperforms existing methods by a large margin. Additionally, the proposed approach has also achieved state-of-the-art performance in domain generalization semantic segmentation tasks of RGB remote sensing images.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models",
    "url": "http://arxiv.org/abs/2505.12108v2",
    "authors": [
      "Jiancheng Pan",
      "Shiye Lei",
      "Yuqian Fu",
      "Jiahao Li",
      "Yanxing Liu",
      "Yuze Sun",
      "Xiao He",
      "Long Peng",
      "Xiaomeng Huang",
      "Bo Zhao"
    ],
    "published": "2025-05-17",
    "abstract": "Remote sensing image (RSI) interpretation typically faces challenges due to the scarcity of labeled data, which limits the performance of RSI interpretation tasks. To tackle this challenge, we propose EarthSynth, a diffusion-based generative foundation model that enables synthesizing multi-category, cross-satellite labeled Earth observation for downstream RSI interpretation tasks. To the best of our knowledge, EarthSynth is the first to explore multi-task generation for remote sensing, tackling the challenge of limited generalization in task-oriented synthesis for RSI interpretation. EarthSynth, trained on the EarthSynth-180K dataset, employs the Counterfactual Composition training strategy with a three-dimensional batch-sample selection mechanism to improve training data diversity and enhance category control. Furthermore, a rule-based method of R-Filter is proposed to filter more informative synthetic data for downstream tasks. We evaluate our EarthSynth on scene classification, object detection, and semantic segmentation in open-world scenarios. There are significant improvements in open-vocabulary understanding tasks, offering a practical solution for advancing RSI interpretation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing",
    "url": "http://arxiv.org/abs/2505.11121v1",
    "authors": [
      "Mathis J\u00fcrgen Adler",
      "Leonard Hackel",
      "Gencer Sumbul",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-05-16",
    "abstract": "The development of foundation models through pretraining of vision-language models (VLMs) has recently attracted great attention in remote sensing (RS). VLM pretraining aims to learn image and language alignments from a large number of image-text pairs. Each pretraining image is often associated with multiple captions containing redundant information due to repeated or semantically similar phrases, resulting in increased pretraining and inference time. To overcome this, we introduce a weighted feature aggregation (WFA) strategy for VLM pretraining in RS. Our strategy aims to extract and exploit complementary information from multiple captions per image while reducing redundancies through feature aggregation with importance weighting. To calculate adaptive importance weights for different captions of each image, we propose two techniques: (i) non-parametric uniqueness and (ii) learning-based attention. In the first technique, importance weights are calculated based on the bilingual evaluation understudy (BLEU) scores of the captions to emphasize unique sentences and reduce the influence of repetitive ones. In the second technique, importance weights are learned through an attention mechanism instead of relying on hand-crafted features. The effectiveness of the proposed WFA strategy with the two techniques is analyzed in terms of downstream performance on text-to-image retrieval in RS. Experimental results show that the proposed strategy enables efficient and effective pretraining of VLMs in RS. Based on the experimental analysis, we derive guidelines for selecting appropriate techniques depending on downstream task requirements and resource constraints. The code of this work is publicly available at https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach",
    "url": "http://arxiv.org/abs/2505.03299v1",
    "authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "St\u00e9phane May",
      "S\u00e9bastien Lef\u00e8vre"
    ],
    "published": "2025-05-06",
    "abstract": "Foundation models constitute a significant advancement in computer vision: after a single, albeit costly, training phase, they can address a wide array of tasks. In the field of Earth observation, over 75 remote sensing vision foundation models have been developed in the past four years. However, none has consistently outperformed the others across all available downstream tasks. To facilitate their comparison, we propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning on each one. This method is based on what we call \"capabilities encoding.\" The utility of this novel approach is twofold: we demonstrate its potential to simplify the selection of a foundation model for a given new task, and we employ it to offer a fresh perspective on the existing literature, suggesting avenues for future research. Codes are available at https://github.com/pierreadorni/capabilities-encoding.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery",
    "url": "http://arxiv.org/abs/2505.02829v1",
    "authors": [
      "Jerome Quenum",
      "Wen-Han Hsieh",
      "Tsung-Han Wu",
      "Ritwik Gupta",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published": "2025-05-05",
    "abstract": "Segmentation models can recognize a pre-defined set of objects in images. However, models that can reason over complex user queries that implicitly refer to multiple objects of interest are still in their infancy. Recent advances in reasoning segmentation--generating segmentation masks from complex, implicit query text--demonstrate that vision-language models can operate across an open domain and produce reasonable outputs. However, our experiments show that such models struggle with complex remote-sensing imagery. In this work, we introduce LISAt, a vision-language model designed to describe complex remote-sensing scenes, answer questions about them, and segment objects of interest. We trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES, with 27,615 annotations over 9,205 images, and a multimodal pretraining dataset, PreGRES, containing over 1 million question-answer pairs. LISAt outperforms existing geospatial foundation models such as RS-GPT4V by over 10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 % (gIoU). Our model, datasets, and code are available at https://lisat-bair.github.io/LISAt/",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "From Spaceborne to Airborne: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation",
    "url": "http://arxiv.org/abs/2505.03844v2",
    "authors": [
      "Solene Debuysere",
      "Nicolas Trouve",
      "Nathan Letheule",
      "Olivier Leveque",
      "Elise Colin"
    ],
    "published": "2025-05-05",
    "abstract": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERA's extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model \\cite{Baqu2019SethiR}. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERA's physics-based simulator EMPRISE \\cite{empriseem_ai_images}. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning",
    "url": "http://arxiv.org/abs/2505.01558v1",
    "authors": [
      "Anan Yaghmour",
      "Melba M. Crawford",
      "Saurabh Prasad"
    ],
    "published": "2025-05-02",
    "abstract": "Remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. Advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. Domain adaptation offers a promising solution to improve model generalization. This paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. We further provide new mathematical insights into MAE-based generative learning for domain-invariant feature learning. Experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes",
    "url": "http://arxiv.org/abs/2504.20303v2",
    "authors": [
      "Junlin Guo",
      "James R. Zimmer-Dauphinee",
      "Jordan M. Nieusma",
      "Siqi Lu",
      "Quan Liu",
      "Ruining Deng",
      "Can Cui",
      "Jialin Yue",
      "Yizhe Lin",
      "Tianyuan Yao",
      "Juming Xiong",
      "Junchao Zhu",
      "Chongyu Qu",
      "Yuechen Yang",
      "Mitchell Wilkes",
      "Xiao Wang",
      "Parker VanValkenburgh",
      "Steven A. Wernke",
      "Yuankai Huo"
    ],
    "published": "2025-04-28",
    "abstract": "By mapping sites at large scales using remotely sensed data, archaeologists can generate unique insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change. Remote sensing surveys complement field-based approaches, and their reach can be especially great when combined with deep learning and computer vision techniques. However, conventional supervised deep learning methods face challenges in annotating fine-grained archaeological features at scale. While recent vision foundation models have shown remarkable success in learning large-scale remote sensing data with minimal annotations, most off-the-shelf solutions are designed for RGB images rather than multi-spectral satellite imagery, such as the 8-band data used in our study. In this paper, we introduce DeepAndes, a transformer-based vision foundation model trained on three million multi-spectral satellite images, specifically tailored for Andean archaeology. DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery, marking the first foundation model designed explicitly for the Andes region. We evaluate its image understanding performance through imbalanced image classification, image instance retrieval, and pixel-level semantic segmentation tasks. Our experiments show that DeepAndes achieves superior F1 scores, mean average precision, and Dice scores in few-shot learning scenarios, significantly outperforming models trained from scratch or pre-trained on smaller datasets. This underscores the effectiveness of large-scale self-supervised pre-training in archaeological remote sensing. Codes will be available on https://github.com/geopacha/DeepAndes.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis",
    "url": "http://arxiv.org/abs/2504.19223v3",
    "authors": [
      "Alexander Baumann",
      "Leonardo Ayala",
      "Silvia Seidlitz",
      "Jan Sellner",
      "Alexander Studier-Fischer",
      "Berkin \u00d6zdemir",
      "Lena Maier-Hein",
      "Slobodan Ilic"
    ],
    "published": "2025-04-27",
    "abstract": "Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing. However, variability in channel dimensionality and captured wavelengths among spectral cameras impede the development of AI-driven methodologies, leading to camera-specific models with limited generalizability and inadequate cross-camera applicability. To address this bottleneck, we introduce CARL, a model for Camera-Agnostic Representation Learning across RGB, multispectral, and hyperspectral imaging modalities. To enable the conversion of a spectral image with any channel dimensionality to a camera-agnostic representation, we introduce a novel spectral encoder, featuring a self-attention-cross-attention mechanism, to distill salient spectral information into learned spectral representations. Spatio-spectral pre-training is achieved with a novel feature-based self-supervision strategy tailored to CARL. Large-scale experiments across the domains of medical imaging, autonomous driving, and satellite imaging demonstrate our model's unique robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. The scalability and versatility of the proposed approach position our model as a backbone for future spectral foundation models.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data",
    "url": "http://arxiv.org/abs/2504.18770v1",
    "authors": [
      "Manuel Weber",
      "Carly Beneke"
    ],
    "published": "2025-04-26",
    "abstract": "We propose PyViT-FUSE, a foundation model for earth observation data explicitly designed to handle multi-modal imagery by learning to fuse an arbitrary number of mixed-resolution input bands into a single representation through an attention mechanism. The learned patch tokens are further processed by a stack of vision transformers with a novel pyramidal structure. We train the model on a globally sampled dataset in a self-supervised manner, leveraging core concepts of the SwAV algorithm. We show the interpretability of the fusion mechanism by visualization of the attention scores and the models applicability to downstream tasks.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology",
    "url": "http://arxiv.org/abs/2504.18256v3",
    "authors": [
      "Elena Plekhanova",
      "Damien Robert",
      "Johannes Dollinger",
      "Emilia Arens",
      "Philipp Brun",
      "Jan Dirk Wegner",
      "Niklaus Zimmermann"
    ],
    "published": "2025-04-25",
    "abstract": "With the exacerbation of the biodiversity and climate crises, macroecological pursuits such as global biodiversity mapping become more urgent. Remote sensing offers a wealth of Earth observation data for ecological studies, but the scarcity of labeled datasets remains a major challenge. Recently, self-supervised learning has enabled learning representations from unlabeled data, triggering the development of pretrained geospatial models with generalizable features. However, these models are often trained on datasets biased toward areas of high human activity, leaving entire ecological regions underrepresented. Additionally, while some datasets attempt to address seasonality through multi-date imagery, they typically follow calendar seasons rather than local phenological cycles. To better capture vegetation seasonality at a global scale, we propose a simple phenology-informed sampling strategy and introduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we train an existing model with a season-contrastive objective. We compare representations learned from SSL4Eco against other datasets on diverse ecological downstream tasks and demonstrate that our straightforward sampling method consistently improves representation quality, highlighting the importance of dataset construction. The model pretrained on SSL4Eco reaches state of the art performance on 7 out of 8 downstream tasks spanning (multi-label) classification and regression. We release our code, data, and model weights to support macroecological and computer vision research at https://github.com/PlekhanovaElena/ssl4eco.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2504.17397v2",
    "authors": [
      "Francesc Marti-Escofet",
      "Benedikt Blumenstiel",
      "Linus Scheibenreif",
      "Paolo Fraccaro",
      "Konrad Schindler"
    ],
    "published": "2025-04-24",
    "abstract": "Earth observation (EO) is crucial for monitoring environmental changes, responding to disasters, and managing natural resources. In this context, foundation models facilitate remote sensing image analysis to retrieve relevant geoinformation accurately and efficiently. However, as these models grow in size, fine-tuning becomes increasingly challenging due to the associated computational resources and costs, limiting their accessibility and scalability. Furthermore, full fine-tuning can lead to forgetting pre-trained features and even degrade model generalization. To address this, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution. In this paper, we conduct extensive experiments with various foundation model architectures and PEFT techniques to evaluate their effectiveness on five different EO datasets. Our results provide a comprehensive comparison, offering insights into when and how PEFT methods support the adaptation of pre-trained geospatial models. We demonstrate that PEFT techniques match or even exceed full fine-tuning performance and enhance model generalisation to unseen geographic regions, while reducing training time and memory requirements. Additional experiments investigate the effect of architecture choices such as the decoder type or the use of metadata, suggesting UNet decoders and fine-tuning without metadata as the recommended configuration. We have integrated all evaluated foundation models and techniques into the open-source package TerraTorch to support quick, scalable, and cost-effective model adaptation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "A Genealogy of Foundation Models in Remote Sensing",
    "url": "http://arxiv.org/abs/2504.17177v2",
    "authors": [
      "Kevin Lane",
      "Morteza Karimzadeh"
    ],
    "published": "2025-04-24",
    "abstract": "Foundation models have garnered increasing attention for representation learning in remote sensing. Many such foundation models adopt approaches that have demonstrated success in computer vision with minimal domain-specific modification. However, the development and application of foundation models in this field are still burgeoning, as there are a variety of competing approaches for how to most effectively leverage remotely sensed data. This paper examines these approaches, along with their roots in the computer vision field. This is done to characterize potential advantages and pitfalls, while outlining future directions to further improve remote sensing-specific foundation models. We discuss the quality of the learned representations and methods to alleviate the need for massive compute resources. We first examine single-sensor remote foundation models to introduce concepts and provide context, and then place emphasis on incorporating the multi-sensor aspect of Earth observations into foundation models. In particular, we explore the extent to which existing approaches leverage multiple sensors in training foundation models in relation to multi-modal foundation models. Finally, we identify opportunities for further harnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote sensing observations.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatelliteCalculator: A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion",
    "url": "http://arxiv.org/abs/2504.13442v1",
    "authors": [
      "Zhenyu Yu",
      "Mohd. Yamani Idna Idris",
      "Pei Wang"
    ],
    "published": "2025-04-18",
    "abstract": "Quantitative remote sensing inversion plays a critical role in environmental monitoring, enabling the estimation of key ecological variables such as vegetation indices, canopy structure, and carbon stock. Although vision foundation models have achieved remarkable progress in classification and segmentation tasks, their application to physically interpretable regression remains largely unexplored. Furthermore, the multi-spectral nature and geospatial heterogeneity of remote sensing data pose significant challenges for generalization and transferability. To address these issues, we introduce SatelliteCalculator, the first vision foundation model tailored for quantitative remote sensing inversion. By leveraging physically defined index formulas, we automatically construct a large-scale dataset of over one million paired samples across eight core ecological indicators. The model integrates a frozen Swin Transformer backbone with a prompt-guided architecture, featuring cross-attentive adapters and lightweight task-specific MLP decoders. Experiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator achieves competitive accuracy across all tasks while significantly reducing inference cost. Our results validate the feasibility of applying foundation models to quantitative inversion, and provide a scalable framework for task-adaptive remote sensing estimation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "SAM-Based Building Change Detection with Distribution-Aware Fourier Adaptation and Edge-Constrained Warping",
    "url": "http://arxiv.org/abs/2504.12619v1",
    "authors": [
      "Yun-Cheng Li",
      "Sen Lei",
      "Yi-Tao Zhao",
      "Heng-Chao Li",
      "Jun Li",
      "Antonio Plaza"
    ],
    "published": "2025-04-17",
    "abstract": "Building change detection remains challenging for urban development, disaster assessment, and military reconnaissance. While foundation models like Segment Anything Model (SAM) show strong segmentation capabilities, SAM is limited in the task of building change detection due to domain gap issues. Existing adapter-based fine-tuning approaches face challenges with imbalanced building distribution, resulting in poor detection of subtle changes and inaccurate edge extraction. Additionally, bi-temporal misalignment in change detection, typically addressed by optical flow, remains vulnerable to background noises. This affects the detection of building changes and compromises both detection accuracy and edge recognition. To tackle these challenges, we propose a new SAM-Based Network with Distribution-Aware Fourier Adaptation and Edge-Constrained Warping (FAEWNet) for building change detection. FAEWNet utilizes the SAM encoder to extract rich visual features from remote sensing images. To guide SAM in focusing on specific ground objects in remote sensing scenes, we propose a Distribution-Aware Fourier Aggregated Adapter to aggregate task-oriented changed information. This adapter not only effectively addresses the domain gap issue, but also pays attention to the distribution of changed buildings. Furthermore, to mitigate noise interference and misalignment in height offset estimation, we design a novel flow module that refines building edge extraction and enhances the perception of changed buildings. Our state-of-the-art results on the LEVIR-CD, S2Looking and WHU-CD datasets highlight the effectiveness of FAEWNet. The code is available at https://github.com/SUPERMAN123000/FAEWNet.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning",
    "url": "http://arxiv.org/abs/2504.11999v1",
    "authors": [
      "Mengyu Wang",
      "Hanbo Bi",
      "Yingchao Feng",
      "Linlin Xin",
      "Shuo Gong",
      "Tianqi Wang",
      "Zhiyuan Yan",
      "Peijin Wang",
      "Wenhui Diao",
      "Xian Sun"
    ],
    "published": "2025-04-16",
    "abstract": "Vision foundation models in remote sensing have been extensively studied due to their superior generalization on various downstream tasks. Synthetic Aperture Radar (SAR) offers all-day, all-weather imaging capabilities, providing significant advantages for Earth observation. However, establishing a foundation model for SAR image interpretation inevitably encounters the challenges of insufficient information utilization and poor interpretability. In this paper, we propose a remote sensing foundation model based on complex-valued SAR data, which simulates the polarimetric decomposition process for pre-training, i.e., characterizing pixel scattering intensity as a weighted combination of scattering bases and scattering coefficients, thereby endowing the foundation model with physical interpretability. Specifically, we construct a series of scattering queries, each representing an independent and meaningful scattering basis, which interact with SAR features in the scattering query decoder and output the corresponding scattering coefficient. To guide the pre-training process, polarimetric decomposition loss and power self-supervision loss are constructed. The former aligns the predicted coefficients with Yamaguchi coefficients, while the latter reconstructs power from the predicted coefficients and compares it to the input image's power. The performance of our foundation model is validated on six typical downstream tasks, achieving state-of-the-art results. Notably, the foundation model can extract stable feature representations and exhibits strong generalization, even in data-scarce conditions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization",
    "url": "http://arxiv.org/abs/2504.10727v1",
    "authors": [
      "Darryl Hannan",
      "John Cooper",
      "Dylan White",
      "Timothy Doster",
      "Henry Kvinge",
      "Yijing Watkins"
    ],
    "published": "2025-04-14",
    "abstract": "Multimodal large language models (MLLMs) have altered the landscape of computer vision, obtaining impressive results across a wide range of tasks, especially in zero-shot settings. Unfortunately, their strong performance does not always transfer to out-of-distribution domains, such as earth observation (EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks, such as image captioning and scene understanding, while failing at tasks that require more fine-grained spatial reasoning, such as object localization. However, MLLMs are advancing rapidly and insights quickly become out-dated. In this work, we analyze more recent MLLMs that have been explicitly trained to include fine-grained spatial reasoning capabilities, benchmarking them on EO object localization tasks. We demonstrate that these models are performant in certain settings, making them well suited for zero-shot scenarios. Additionally, we provide a detailed discussion focused on prompt selection, ground sample distance (GSD) optimization, and analyzing failure cases. We hope that this work will prove valuable as others evaluate whether an MLLM is well suited for a given EO localization task and how to optimize it.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks",
    "url": "http://arxiv.org/abs/2504.10403v3",
    "authors": [
      "Yan Zhu",
      "Jingyang Zhu",
      "Ting Wang",
      "Yuanming Shi",
      "Chunxiao Jiang",
      "Khaled Ben Letaief"
    ],
    "published": "2025-04-14",
    "abstract": "Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation",
    "url": "http://arxiv.org/abs/2504.06962v2",
    "authors": [
      "Thomas Kerdreux",
      "Alexandre Tuel",
      "Quentin Febvre",
      "Alexis Mouche",
      "Bertrand Chapron"
    ],
    "published": "2025-04-09",
    "abstract": "Self-supervised learning (SSL) has enabled the development of vision foundation models for Earth Observation (EO), demonstrating strong transferability across diverse remote sensing tasks. While prior work has focused on network architectures and training strategies, the role of dataset curation, especially in balancing and diversifying pre-training datasets, remains underexplored. In EO, this challenge is amplified by the redundancy and heavy-tailed distributions common in satellite imagery, which can lead to biased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to improve SSL pre-training by maximizing dataset diversity and balance. Our method iteratively refines the training set without requiring a pre-existing feature extractor, making it well-suited for domains where curated datasets are limited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode (WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by ocean observations. We train models from scratch on the entire Sentinel-1 WV archive spanning 10 years. Across three downstream tasks, our results show that dynamic pruning improves both computational efficiency and representation quality, leading to stronger transferability.\n  We also release the weights of OceanSAR-1, the first model in the OceanSAR family, a series of foundation models for ocean observation and analysis using SAR imagery, at github.com/galeio-research/OceanSAR-models/.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation",
    "url": "http://arxiv.org/abs/2504.06220v4",
    "authors": [
      "Xiaoxing Hu",
      "Ziyang Gong",
      "Yupei Wang",
      "Yuru Jia",
      "Fei Lin",
      "Dexiang Gao",
      "Ke An",
      "Jianhong Han",
      "Zhuoran Sun",
      "Gen Luo",
      "Gen Luo",
      "Xue Yang"
    ],
    "published": "2025-04-08",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt powerful Foundation Models (FMs) to diverse downstream tasks while preserving and unleashing their inherent capabilities. However, we have observed that existing PEFT methods, which are often designed with natural imagery in mind, struggle when applied to Remote Sensing (RS) scenarios. This is primarily due to their inability to handle artifact influences, a problem particularly severe in RS image features. To tackle this challenge, we introduce Earth-Adapter, the first PEFT method specifically designed for RS artifacts conquering. Earth-Adapter introduces a novel Mixture of Frequency Adaptation process that combines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT). By utilizing DFT, Earth-Adapter can decompose features into different frequency components, precisely separating artifacts from original features. The MoA then dynamically assigns weights to each adapter expert, allowing for the combination of features across various frequency domains. These simple-yet-effective approaches enable Earth-Adapter to more efficiently overcome the disturbances caused by artifacts than previous PEFT methods, significantly enhancing the FMs' performance on RS scenarios. Experiments on Domain Adaptation (DA), and Domain Generalization (DG) semantic segmentation benchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline Rein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG benchmarks. Our code will be released at https://github.com/VisionXLab/Earth-Adapter.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via Eliminate Before Align and Keyword Explicit Reasoning",
    "url": "http://arxiv.org/abs/2504.05644v1",
    "authors": [
      "Yan Zhang",
      "Zhong Ji",
      "Changxu Meng",
      "Yanwei Pang",
      "Jungong Han"
    ],
    "published": "2025-04-08",
    "abstract": "Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR), which aims at searching for the corresponding targets based on the given query. Among these efforts, the application of Foundation Models (FMs), such as CLIP, to the domain of remote sensing has yielded encouraging outcomes. However, existing FM based methodologies neglect the negative impact of weakly correlated sample pairs and fail to account for the key distinctions among remote sensing texts, leading to biased and superficial exploration of sample pairs. To address these challenges, we propose an approach named iEBAKER (an Improved Eliminate Before Align strategy with Keyword Explicit Reasoning framework) for RSITR. Specifically, we propose an innovative Eliminate Before Align (EBA) strategy to filter out the weakly correlated sample pairs, thereby mitigating their deviations from optimal embedding space during alignment.Further, two specific schemes are introduced from the perspective of whether local similarity and global similarity affect each other. On this basis, we introduce an alternative Sort After Reversed Retrieval (SAR) strategy, aims at optimizing the similarity matrix via reverse retrieval. Additionally, we incorporate a Keyword Explicit Reasoning (KER) module to facilitate the beneficial impact of subtle key concept distinctions. Without bells and whistles, our approach enables a direct transition from FM to RSITR task, eliminating the need for additional pretraining on remote sensing data. Extensive experiments conducted on three popular benchmark datasets demonstrate that our proposed iEBAKER method surpasses the state-of-the-art models while requiring less training data. Our source code will be released at https://github.com/zhangy0822/iEBAKER.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": []
  },
  {
    "title": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation",
    "url": "http://arxiv.org/abs/2504.03166v1",
    "authors": [
      "Hanbo Bi",
      "Yingchao Feng",
      "Boyuan Tong",
      "Mengyu Wang",
      "Haichen Yu",
      "Yongqiang Mao",
      "Hao Chang",
      "Wenhui Diao",
      "Peijin Wang",
      "Yue Yu",
      "Hanyang Peng",
      "Yehong Zhang",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2025-04-04",
    "abstract": "The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Tracking"
    ]
  },
  {
    "title": "A Decade of Deep Learning for Remote Sensing Spatiotemporal Fusion: Advances, Challenges, and Opportunities",
    "url": "http://arxiv.org/abs/2504.00901v2",
    "authors": [
      "Enzhe Sun",
      "Yongchuan Cui",
      "Peng Liu",
      "Jining Yan"
    ],
    "published": "2025-04-01",
    "abstract": "Remote sensing spatiotemporal fusion (STF) addresses the fundamental trade-off between temporal and spatial resolution by combining high temporal-low spatial and high spatial-low temporal imagery. This paper presents the first comprehensive survey of deep learning advances in remote sensing STF over the past decade. We establish a systematic taxonomy of deep learning architectures including Convolutional Neural Networks (CNNs), Transformers, Generative Adversarial Networks (GANs), diffusion models, and sequence models, revealing significant growth in deep learning adoption for STF tasks. Our analysis reveals that CNN-based methods dominate spatial feature extraction, while Transformer architectures show superior performance in capturing long-range temporal dependencies. GAN and diffusion models demonstrate exceptional capability in detail reconstruction, substantially outperforming traditional methods in structural similarity and spectral fidelity. Through comprehensive experiments on seven benchmark datasets comparing ten representative methods, we validate these findings and quantify the performance trade-offs between different approaches. We identify five critical challenges: time-space conflicts, limited generalization across datasets, computational efficiency for large-scale processing, multi-source heterogeneous fusion, and insufficient benchmark diversity. The survey highlights promising opportunities in foundation models, hybrid architectures, and self-supervised learning approaches that could address current limitations and enable multimodal applications. The specific models, datasets, and other information mentioned in this article have been collected in: https://github.com/yc-cui/Deep-Learning-Spatiotemporal-Fusion-Survey.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer",
      "GAN",
      "Diffusion Models"
    ],
    "applications": []
  },
  {
    "title": "FlexiMo: A Flexible Remote Sensing Foundation Model",
    "url": "http://arxiv.org/abs/2503.23844v1",
    "authors": [
      "Xuyang Li",
      "Chenyu Li",
      "Pedram Ghamisi",
      "Danfeng Hong"
    ],
    "published": "2025-03-31",
    "abstract": "The rapid expansion of multi-source satellite imagery drives innovation in Earth observation, opening unprecedented opportunities for Remote Sensing Foundation Models to harness diverse data. However, many existing models remain constrained by fixed spatial resolutions and patch sizes, limiting their ability to fully exploit the heterogeneous spatial characteristics inherent in satellite imagery. To address these challenges, we propose FlexiMo, a flexible remote sensing foundation model that endows the pre-trained model with the flexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a spatial resolution-aware module that employs a parameter-free alignment embedding mechanism to dynamically recalibrate patch embeddings based on the input image's resolution and dimensions. This design not only preserves critical token characteristics and ensures multi-scale feature fidelity but also enables efficient feature extraction without requiring modifications to the underlying network architecture. In addition, FlexiMo incorporates a lightweight channel adaptation module that leverages prior spectral information from sensors. This mechanism allows the model to process images with varying numbers of channels while maintaining the data's intrinsic physical properties. Extensive experiments on diverse multimodal, multi-resolution, and multi-scale datasets demonstrate that FlexiMo significantly enhances model generalization and robustness. In particular, our method achieves outstanding performance across a range of downstream tasks, including scene classification, land cover classification, urban building segmentation, and cloud detection. By enabling parameter-efficient and physically consistent adaptation, FlexiMo paves the way for more adaptable and effective foundation models in real-world remote sensing applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Efficient Adaptation For Remote Sensing Visual Grounding",
    "url": "http://arxiv.org/abs/2503.23083v3",
    "authors": [
      "Hasan Moughnieh",
      "Mohamad Chalhoub",
      "Hasan Nasrallah",
      "Cristiano Nattero",
      "Paolo Campanella",
      "Giovanni Nico",
      "Ali J. Ghandour"
    ],
    "published": "2025-03-29",
    "abstract": "Adapting pre-trained models has become an effective strategy in artificial intelligence, offering a scalable and efficient alternative to training models from scratch. In the context of remote sensing (RS), where visual grounding(VG) remains underexplored, this approach enables the deployment of powerful vision-language models to achieve robust cross-modal understanding while significantly reducing computational overhead. To address this, we applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for RS-specific VG tasks. Specifically, we evaluated LoRA placement across different modules in Grounding DINO and used BitFit and adapters to fine-tune the OFA foundation model pre-trained on general-purpose VG datasets. This approach achieved performance comparable to or surpassing current State Of The Art (SOTA) models while significantly reducing computational costs. This study highlights the potential of PEFT techniques to advance efficient and precise multi-modal analysis in RS, offering a practical and cost-effective alternative to full model training.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery",
    "url": "http://arxiv.org/abs/2503.22516v1",
    "authors": [
      "Samira Alkaee Taleghan",
      "Morteza Karimzadeh",
      "Andrew P. Barrett",
      "Walter N. Meier",
      "Farnoush Banaei-Kashani"
    ],
    "published": "2025-03-28",
    "abstract": "Accurate segmentation of sea ice types is essential for mapping and operational forecasting of sea ice conditions for safe navigation and resource extraction in ice-covered waters, as well as for understanding polar climate processes. While deep learning methods have shown promise in automating sea ice segmentation, they often rely on extensive labeled datasets which require expert knowledge and are time-consuming to create. Recently, foundation models (FMs) have shown excellent results for segmenting remote sensing images by utilizing pre-training on large datasets using self-supervised techniques. However, their effectiveness for sea ice segmentation remains unexplored, especially given sea ice's complex structures, seasonal changes, and unique spectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagery characteristics including banding and scalloping noise, and varying ice backscatter characteristics, which are often missing in standard remote sensing pre-training datasets. In particular, SAR images over polar regions are acquired using different modes than used to capture the images at lower latitudes by the same sensors that form training datasets for FMs. This study evaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1 SAR imagery, focusing on their seasonal and spatial generalization. Among the selected models, Prithvi-600M outperforms the baseline models, while CROMA achieves a very similar performance in F1-score. Our contributions include offering a systematic methodology for selecting FMs for sea ice data analysis, a comprehensive benchmarking study on performances of FMs for sea ice segmentation with tailored performance metrics, and insights into existing gaps and future directions for improving domain-specific models in polar applications using SAR data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "A Survey on Remote Sensing Foundation Models: From Vision to Multimodality",
    "url": "http://arxiv.org/abs/2503.22081v1",
    "authors": [
      "Ziyue Huang",
      "Hongxi Yan",
      "Qiqi Zhan",
      "Shuai Yang",
      "Mingming Zhang",
      "Chenkai Zhang",
      "YiMing Lei",
      "Zeming Liu",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "published": "2025-03-28",
    "abstract": "The rapid advancement of remote sensing foundation models, particularly vision and multimodal models, has significantly enhanced the capabilities of intelligent geospatial data interpretation. These models combine various data modalities, such as optical, radar, and LiDAR imagery, with textual and geographic information, enabling more comprehensive analysis and understanding of remote sensing data. The integration of multiple modalities allows for improved performance in tasks like object detection, land cover classification, and change detection, which are often challenged by the complex and heterogeneous nature of remote sensing data. However, despite these advancements, several challenges remain. The diversity in data types, the need for large-scale annotated datasets, and the complexity of multimodal fusion techniques pose significant obstacles to the effective deployment of these models. Moreover, the computational demands of training and fine-tuning multimodal models require significant resources, further complicating their practical application in remote sensing image interpretation tasks. This paper provides a comprehensive review of the state-of-the-art in vision and multimodal foundation models for remote sensing, focusing on their architecture, training methods, datasets and application scenarios. We discuss the key challenges these models face, such as data alignment, cross-modal transfer learning, and scalability, while also identifying emerging research directions aimed at overcoming these limitations. Our goal is to provide a clear understanding of the current landscape of remote sensing foundation models and inspire future research that can push the boundaries of what these models can achieve in real-world applications. The list of resources collected by the paper can be found in the https://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2503.21841v1",
    "authors": [
      "Jingtao Li",
      "Yingyi Liu",
      "Xinyu Wang",
      "Yunning Peng",
      "Chen Sun",
      "Shaoyu Wang",
      "Zhendong Sun",
      "Tian Ke",
      "Xiao Jiang",
      "Tangwei Lu",
      "Anran Zhao",
      "Yanfei Zhong"
    ],
    "published": "2025-03-27",
    "abstract": "Advanced interpretation of hyperspectral remote sensing images benefits many precise Earth observation tasks. Recently, visual foundation models have promoted the remote sensing interpretation but concentrating on RGB and multispectral images. Due to the varied hyperspectral channels,existing foundation models would face image-by-image tuning situation, imposing great pressure on hardware and time resources. In this paper, we propose a tuning-free hyperspectral foundation model called HyperFree, by adapting the existing visual prompt engineering. To process varied channel numbers, we design a learned weight dictionary covering full-spectrum from $0.4 \\sim 2.5 \\, \u03bc\\text{m}$, supporting to build the embedding layer dynamically. To make the prompt design more tractable, HyperFree can generate multiple semantic-aware masks for one prompt by treating feature distance as semantic-similarity. After pre-training HyperFree on constructed large-scale high-resolution hyperspectral images, HyperFree (1 prompt) has shown comparable results with specialized models (5 shots) on 5 tasks and 11 datasets.Code and dataset are accessible at https://rsidea.whu.edu.cn/hyperfree.htm.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "First On-Orbit Demonstration of a Geospatial Foundation Model",
    "url": "http://arxiv.org/abs/2512.01181v1",
    "authors": [
      "Andrew Du",
      "Roberto Del Prete",
      "Alejandro Mousist",
      "Nick Manser",
      "Fabrice Marre",
      "Andrew Barton",
      "Carl Seubert",
      "Gabriele Meoni",
      "Tat-Jun Chin"
    ],
    "published": "2025-12-01",
    "abstract": "Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale",
    "url": "http://arxiv.org/abs/2512.02055v1",
    "authors": [
      "Mirela G. Tulbure",
      "Julio Caineta",
      "Mark Broich",
      "Mollie D. Gaines",
      "Philippe Rufin",
      "Leon-Friedrich Thomas",
      "Hamed Alemohammad",
      "Jan Hemmerling",
      "Patrick Hostert"
    ],
    "published": "2025-11-27",
    "abstract": "Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.\n  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.\n  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
    "url": "http://arxiv.org/abs/2511.15658v1",
    "authors": [
      "Naomi Simumba",
      "Nils Lehmann",
      "Paolo Fraccaro",
      "Hamed Alemohammad",
      "Geeth De Mel",
      "Salman Khan",
      "Manil Maskey",
      "Nicolas Longepe",
      "Xiao Xiang Zhu",
      "Hannah Kerner",
      "Juan Bernabe-Moreno",
      "Alexander Lacoste"
    ],
    "published": "2025-11-19",
    "abstract": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.\n  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation",
    "url": "http://arxiv.org/abs/2511.10370v1",
    "authors": [
      "Kai-Hendrik Cohrs",
      "Zuzanna Osika",
      "Maria Gonzalez-Calabuig",
      "Vishal Nedungadi",
      "Ruben Cartuyvels",
      "Steffen Knoblauch",
      "Joppe Massant",
      "Shruti Nath",
      "Patrick Ebel",
      "Vasileios Sitokonstantinou"
    ],
    "published": "2025-11-13",
    "abstract": "Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands",
    "url": "http://arxiv.org/abs/2511.10177v1",
    "authors": [
      "Tishya Chhabra",
      "Manisha Bajpai",
      "Walter Zesk",
      "Skylar Tibbits"
    ],
    "published": "2025-11-13",
    "abstract": "We present an initial evaluation of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on shoreline delineation of small sandy islands using satellite images. We curated and labeled a dataset of 225 multispectral images of two Maldivian islands, which we publicly release, and fine-tuned both the 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Our experiments show that even with as few as 5 training images, the models achieve high performance (F1 of 0.94, IoU of 0.79). Our results demonstrate the strong transfer learning capability of Prithvi, underscoring the potential of such models to support coastal monitoring in data-poor regions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2",
    "url": "http://arxiv.org/abs/2511.05461v1",
    "authors": [
      "Olivier Dietrich",
      "Merlin Alfredsson",
      "Emilia Arens",
      "Nando Metzger",
      "Torben Peters",
      "Linus Scheibenreif",
      "Jan Dirk Wegner",
      "Konrad Schindler"
    ],
    "published": "2025-11-07",
    "abstract": "Natural disasters demand rapid damage assessment to guide humanitarian response. Here, we investigate whether medium-resolution Earth observation images from the Copernicus program can support building damage assessment, complementing very-high resolution imagery with often limited availability. We introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the established xBD benchmark. In a series of experiments, we demonstrate that building damage can be detected and mapped rather well in many disaster scenarios, despite the moderate 10$\\,$m ground sampling distance. We also find that, for damage mapping at that resolution, architectural sophistication does not seem to bring much advantage: more complex model architectures tend to struggle with generalization to unseen disasters, and geospatial foundation models bring little practical benefit. Our results suggest that Copernicus images are a viable data source for rapid, wide-area damage assessment and could play an important role alongside VHR imagery. We release the xBD-S12 dataset, code, and trained models to support further research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation",
    "url": "http://arxiv.org/abs/2511.04766v1",
    "authors": [
      "Dhenenjay Yadav",
      "Rohan Sawai"
    ],
    "published": "2025-11-06",
    "abstract": "Foundation models (FMs) offer powerful representations for geospatial analysis, but adapting them effectively remains challenging. Standard adaptation methods, whether full fine-tuning or efficient frozen-backbone approaches, typically employ decoders with fixed regularization strategies, failing to account for the significant heterogeneity in satellite imagery. We introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder architecture designed to address this limitation. DARN integrates three key innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and (3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide theoretical justifications linking DARN's optimization to stationary point convergence and its mechanism to adaptive information bottlenecks. Empirically, DARN demonstrates exceptional performance across both major adaptation paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering substantial advantages crucial for real-world deployment: superior out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms), enhanced robustness (17% relative reduction in corruption error), and improved performance on minority classes. DARN offers a more intelligent, robust, and efficient approach to leveraging FMs in critical geospatial applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability",
    "url": "http://arxiv.org/abs/2511.04474v1",
    "authors": [
      "Wenwen Li",
      "Sizhe Wang",
      "Hyunho Lee",
      "Chenyan Lu",
      "Sujit Roy",
      "Rahul Ramachandran",
      "Chia-Yu Hsu"
    ],
    "published": "2025-11-06",
    "abstract": "Landslides cause severe damage to lives, infrastructure, and the environment, making accurate and timely mapping essential for disaster preparedness and response. However, conventional deep learning models often struggle when applied across different sensors, regions, or under conditions of limited training data. To address these challenges, we present a three-axis analytical framework of sensor, label, and domain for adapting geospatial foundation models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a series of experiments, we show that it consistently outperforms task-specific CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other GeoFMs (TerraMind, SatMAE). The model, built on global pretraining, self-supervision, and adaptable fine-tuning, proved resilient to spectral variation, maintained accuracy under label scarcity, and generalized more reliably across diverse datasets and geographic settings. Alongside these strengths, we also highlight remaining challenges such as computational cost and the limited availability of reusable AI-ready training data for landslide research. Overall, our study positions GeoFMs as a step toward more robust and scalable approaches for landslide risk reduction and environmental monitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing",
    "url": "http://arxiv.org/abs/2510.26609v1",
    "authors": [
      "Shayan Nejadshamsi",
      "Yuanyuan Zhang",
      "Shadi Zaki",
      "Brock Porth",
      "Lysa Porth",
      "Vahab Khoshdel"
    ],
    "published": "2025-10-30",
    "abstract": "Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing), a deep learning model designed for high-resolution, intra-field canola yield prediction. CYPRESS leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, CYPRESS demonstrates superior performance over existing deep learning-based yield prediction models, highlighting the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, CYPRESS offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi",
    "url": "http://arxiv.org/abs/2510.25954v1",
    "authors": [
      "Lynn Metz",
      "Rachel Haggard",
      "Michael Moszczynski",
      "Samer Asbah",
      "Chris Mwase",
      "Patricia Khomani",
      "Tyler Smith",
      "Hannah Cooper",
      "Annie Mwale",
      "Arbaaz Muslim",
      "Gautam Prasad",
      "Mimi Sun",
      "Tomer Shekel",
      "Joydeep Paul",
      "Anna Carter",
      "Shravya Shetty",
      "Dylan Green"
    ],
    "published": "2025-10-29",
    "abstract": "The reliability of routine health data in low and middle-income countries (LMICs) is often constrained by reporting delays and incomplete coverage, necessitating the exploration of novel data sources and analytics. Geospatial Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse spatial, temporal, and behavioral data into mathematical embeddings that can be efficiently used for downstream prediction tasks. This study evaluated the predictive performance of three GeoFM embedding sources - Google Population Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite imagery), and mobile phone call detail records (CDR) - for modeling 15 routine health programmatic outputs in Malawi, and compared their utility to traditional geospatial interpolation methods. We used XGBoost models on data from 552 health catchment areas (January 2021-May 2023), assessing performance with R2, and using an 80/20 training and test data split with 5-fold cross-validation used in training. While predictive performance was mixed, the embedding-based approaches improved upon baseline geostatistical methods in 13 of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three embedding sources produced the most robust predictions, achieving average 5-fold cross validated R2 values for indicators like population density (0.63), new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64, 0.68, and 0.55, respectively. Prediction was poor for prediction targets with low primary data availability, such as TB and malnutrition cases. These results demonstrate that GeoFM embeddings imbue a modest predictive improvement for select health and demographic outcomes in an LMIC context. We conclude that the integration of multiple GeoFM sources is an efficient and valuable tool for supplementing and strengthening constrained routine health information systems.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures",
    "url": "http://arxiv.org/abs/2511.00073v1",
    "authors": [
      "Harald Kristen",
      "Daniel Kulmer",
      "Manuela Hirschmugl"
    ],
    "published": "2025-10-29",
    "abstract": "Rapid climate change and other disturbances in alpine ecosystems demand frequent habitat monitoring, yet manual mapping remains prohibitively expensive for the required temporal resolution. We employ deep learning for change detection using long-term alpine habitat data from Gesaeuse National Park, Austria, addressing a major gap in applying geospatial foundation models (GFMs) to complex natural environments with fuzzy class boundaries and highly imbalanced classes. We compare two paradigms: post-classification change detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the transformer ChangeViT against U-Net baselines. Using high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus U-Net's 41% for multi-class habitat change, while both reach 67% for binary change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's 23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy. Although overall accuracies are lower than in more homogeneous landscapes, they reflect realistic performance for complex alpine habitats. Future work will integrate object-based post-processing and physical constraints to enhance applicability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Global Chlorophyll-\\textit{a} Retrieval algorithm from Sentinel 2 Using Residual Deep Learning and Novel Machine Learning Water Classification",
    "url": "http://arxiv.org/abs/2510.24124v1",
    "authors": [
      "Yotam Sherf",
      "Bar Efrati",
      "Gabriel Rozman",
      "Moshe Harel"
    ],
    "published": "2025-10-28",
    "abstract": "We present the Global Water Classifier (GWC), a supervised, geospatially extensive Machine Learning (ML) classifier trained on Sen2Cor corrected Sentinel-2 surface reflectance data. Using nearly 100 globally distributed inland water bodies, GWC distinguishes water across Chlorophyll-a (Chla) levels from non-water spectra (clouds, sun glint, snow, ice, aquatic vegetation, land and sediments) and shows geographically stable performance.\n  Building on this foundation model, we perform Chla retrieval based on a matchup Sentinel-2 reflectance data with the United States Geological Survey (USGS) AquaMatch in-situ dataset, covering diverse geographical and hydrological conditions.\n  We train an XGBoost regressor on 13626 matchup points. The positive labeled scenes by the GWC consistently outperform the negatives and produce more accurate Chla retrieval values, which confirms the classifiers advantage in reducing various interferences.\n  Next, residual analysis of the regression predictions revealed structured errors, motivating a residual CNN (RCNN) correction stage. We add a CNN residual stage trained on normalized residuals, which yield substantial improvement. Our algorithm was tested on 867 water bodies with over 2,000 predictions and Chla values up to 1000~mg$/m^{3}$, achieving $R^2$ = 0.79, MAE = 13.52~mg$/m^{3}$, and slope = 0.91, demonstrating robust, scalable, and globally transferable performance without additional tuning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping",
    "url": "http://arxiv.org/abs/2510.23364v1",
    "authors": [
      "Hyeongkyun Kim",
      "Orestis Oikonomou"
    ],
    "published": "2025-10-27",
    "abstract": "Flood susceptibility mapping (FSM) is vital for disaster prevention but remains challenging in data-scarce regions where hydrodynamic models require dense geophysical inputs. This work introduces ZeroFlood, a geospatial foundation model framework for data-efficient FSM. The approach fine-tunes Geospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning, enabling flood prediction from basic Earth observation data such as Sentinel-1 or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich regions, ZeroFlood bridges data availability gaps through cross-modal representation learning. Experiments with TerraMind and Prithvi GFMs show that TiM enhances model robustness, with the TerraMind-Large configuration achieving an F1 score of 67.21. The results demonstrate the feasibility of foundation-model-based FSM as a scalable and data-efficient solution for flood risk management.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction",
    "url": "http://arxiv.org/abs/2510.18773v1",
    "authors": [
      "Jannis Fleckenstein",
      "David Kreismann",
      "Tamara Rosemary Govindasamy",
      "Thomas Brunschwiler",
      "Etienne Vos",
      "Mattia Rigotti"
    ],
    "published": "2025-10-21",
    "abstract": "As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data, yet conventional machine learning models with limited data often produce inaccurate predictions, particularly in underserved areas. Geospatial foundation models trained on global unstructured data offer a promising alternative by demonstrating strong generalization and requiring only minimal fine-tuning. In this study, an empirical ground truth of urban heat patterns is established by quantifying cooling effects from green spaces and benchmarking them against model predictions to evaluate the model's accuracy. The foundation model is subsequently fine-tuned to predict land surface temperatures under future climate scenarios, and its practical value is demonstrated through a simulated inpainting that highlights its role for mitigation support. The results indicate that foundation models offer a powerful way for evaluating urban heat island mitigation strategies in data-scarce regions to support more climate-resilient cities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning",
    "url": "http://arxiv.org/abs/2510.18318v3",
    "authors": [
      "Aaron Bell",
      "Amit Aides",
      "Amr Helmy",
      "Arbaaz Muslim",
      "Aviad Barzilai",
      "Aviv Slobodkin",
      "Bolous Jaber",
      "David Schottlander",
      "George Leifman",
      "Joydeep Paul",
      "Mimi Sun",
      "Nadav Sherman",
      "Natalie Williams",
      "Per Bjornsson",
      "Roy Lee",
      "Ruth Alcantara",
      "Thomas Turnbull",
      "Tomer Shekel",
      "Vered Silverman",
      "Yotam Gigi",
      "Adam Boulanger",
      "Alex Ottenwess",
      "Ali Ahmadalipour",
      "Anna Carter",
      "Behzad Vahedi",
      "Charles Elliott",
      "David Andre",
      "Elad Aharoni",
      "Gia Jung",
      "Hassler Thurston",
      "Jacob Bien",
      "Jamie McPike",
      "Jessica Sapick",
      "Juliet Rothenberg",
      "Kartik Hegde",
      "Kel Markert",
      "Kim Philipp Jablonski",
      "Luc Houriez",
      "Monica Bharel",
      "Phing VanLee",
      "Reuven Sayag",
      "Sebastian Pilarski",
      "Shelley Cazares",
      "Shlomi Pasternak",
      "Siduo Jiang",
      "Thomas Colthurst",
      "Yang Chen",
      "Yehonathan Refael",
      "Yochai Blau",
      "Yuval Carny",
      "Yael Maguire",
      "Avinatan Hassidim",
      "James Manyika",
      "Tim Thelin",
      "Genady Beryozkin",
      "Gautam Prasad",
      "Luke Barrington",
      "Yossi Matias",
      "Niv Efron",
      "Shravya Shetty"
    ],
    "published": "2025-10-21",
    "abstract": "Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of geospatial AI models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet. This approach is built upon foundation models across three key domains--Planet-scale Imagery, Population, and Environment--and an intelligent Gemini-powered reasoning engine. We present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together, they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities. To handle complex, multi-step queries, we developed a Gemini-powered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools. On a new benchmark of real-world crisis scenarios, our agent demonstrates the ability to deliver critical and timely insights, effectively bridging the gap between raw geospatial data and actionable understanding.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence",
    "url": "http://arxiv.org/abs/2510.16555v1",
    "authors": [
      "Qiongyan Wang",
      "Xingchen Zou",
      "Yutian Jiang",
      "Haomin Wen",
      "Jiaheng Wei",
      "Qingsong Wen",
      "Yuxuan Liang"
    ],
    "published": "2025-10-18",
    "abstract": "Rapid urbanization intensifies the demand for Urban General Intelligence (UGI), referring to AI systems that can understand and reason about complex urban environments. Recent studies have built urban foundation models using supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit persistent geospatial bias, producing regionally skewed predictions and limited generalization. To this end, we propose Urban-R1, a reinforcement learning-based post-training framework that aligns MLLMs with the objectives of UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize reasoning across geographic groups and employs urban region profiling as a proxy task to provide measurable rewards from multimodal urban data. Extensive experiments across diverse regions and tasks show that Urban-R1 effectively mitigates geo-bias and improves cross-region generalization, outperforming both SFT-trained and closed-source models. Our results highlight reinforcement learning alignment as a promising pathway toward equitable and trustworthy urban intelligence.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning",
    "url": "http://arxiv.org/abs/2510.09894v1",
    "authors": [
      "Junyuan Liu",
      "Quan Qin",
      "Guangsheng Dong",
      "Xinglei Wang",
      "Jiazhuang Feng",
      "Zichao Zeng",
      "Tao Cheng"
    ],
    "published": "2025-10-10",
    "abstract": "General-purpose spatial representations are essential for building transferable geospatial foundation models (GFMs). Among them, the AlphaEarth Foundation (AE) represents a major step toward a global, unified representation of the Earth's surface, learning 10-meter embeddings from multi-source Earth Observation (EO) data that capture rich physical and environmental patterns across diverse landscapes. However, such EO-driven representations remain limited in capturing the functional and socioeconomic dimensions of cities, as they primarily encode physical and spectral patterns rather than human activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched Representation Learning), a lightweight framework that adapts AlphaEarth to human-centered urban analysis through multimodal alignment guided by Points of Interest (POIs). AETHER aligns AE embeddings with textual representations of POIs, enriching physically grounded EO features with semantic cues about urban functions and socioeconomic contexts. In Greater London, AETHER achieves consistent gains over the AE baseline, with a 7.2% relative improvement in land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler divergence for socioeconomic mapping. Built upon pretrained AE, AETHER leverages a lightweight multimodal alignment to enrich it with human-centered semantics while remaining computationally efficient and scalable for urban applications. By coupling EO with human-centered semantics, it advances geospatial foundation models toward general-purpose urban representations that integrate both physical form and functional meaning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Foundation Models for Astrobiology: Paper I -- Workshop and Overview",
    "url": "http://arxiv.org/abs/2510.08636v1",
    "authors": [
      "Ryan Felton",
      "Caleb Scharf",
      "Stuart Bartlett",
      "Nathalie A. Cabrol",
      "Victoria Da Poian",
      "Diana Gentry",
      "Jian Gong",
      "Adrienne Hoarfrost",
      "Manil Maskey",
      "Floyd Nichols",
      "Conor A. Nixon",
      "Tejas Panambur",
      "Joseph Pasterski",
      "Anton S. Petrov",
      "Anirudh Prabhu",
      "Brenda Thomson",
      "Hamed Valizadegan",
      "Kimberley Warren-Rhodes",
      "David Wettergreen",
      "Michael L. Wong",
      "Anastasia Yanchilina"
    ],
    "published": "2025-10-08",
    "abstract": "Advances in machine learning over the past decade have resulted in a proliferation of algorithmic applications for encoding, characterizing, and acting on complex data that may contain many high dimensional features. Recently, the emergence of deep-learning models trained across very large datasets has created a new paradigm for machine learning in the form of Foundation Models. Foundation Models are programs trained on very large and broad datasets with an extensive number of parameters. Once built, these powerful, and flexible, models can be utilized in less resource-intensive ways to build many different, downstream applications that can integrate previously disparate, multimodal data. The development of these applications can be done rapidly and with a much lower demand for machine learning expertise. And the necessary infrastructure and models themselves are already being established within agencies such as NASA and ESA. At NASA this work is across several divisions of the Science Mission Directorate including the NASA Goddard and INDUS Large Language Models and the Prithvi Geospatial Foundation Model. And ESA initiatives to bring Foundation Models to Earth observations has led to the development of TerraMind. A workshop was held by the NASA Ames Research Center and the SETI Institute, in February 2025, to investigate the potential of Foundation Models for astrobiological research and to determine what steps would be needed to build and utilize such a model or models. This paper shares the findings and recommendations of that workshop, and describes clear near-term, and future opportunities in the development of a Foundation Model (or Models) for astrobiology applications. These applications would include a biosignature, or life characterization, task, a mission development and operations task, and a natural language task for integrating and supporting astrobiology research needs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment",
    "url": "http://arxiv.org/abs/2510.05617v1",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Iffanice Houndayi",
      "Rym Oualha",
      "Mohamed Aziz Cherif",
      "Kobby Panford-Quainoo",
      "Arnu Pretorius"
    ],
    "published": "2025-10-07",
    "abstract": "Open-access multispectral imagery from missions like Landsat 8-9 and Sentinel-2 has fueled the development of geospatial foundation models (GFMs) for humanitarian and environmental applications. Yet, their deployment remains limited by (i) the absence of automated geospatial data pipelines and (ii) the large size of fine-tuned models. Existing GFMs lack workflows for processing raw satellite imagery, and downstream adaptations often retain the full complexity of the original encoder.\n  We present InstaGeo, an open-source, end-to-end framework that addresses these challenges by integrating: (1) automated data curation to transform raw imagery into model-ready datasets; (2) task-specific model distillation to derive compact, compute-efficient models; and (3) seamless deployment as interactive web-map applications. Using InstaGeo, we reproduced datasets from three published studies and trained models with marginal mIoU differences of -0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for desert locust prediction. The distilled models are up to 8x smaller than standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal accuracy loss.\n  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp improvement over prior baselines. Moreover, InstaGeo enables users to progress from raw data to model deployment within a single working day.\n  By unifying data preparation, model compression, and deployment, InstaGeo transforms research-grade GFMs into practical, low-carbon tools for real-time, large-scale Earth observation. This approach shifts geospatial AI toward data quality and application-driven innovation. Source code, datasets, and model checkpoints are available at: https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Geospatial Machine Learning Libraries",
    "url": "http://arxiv.org/abs/2510.02572v2",
    "authors": [
      "Adam J. Stewart",
      "Caleb Robinson",
      "Arindam Banerjee"
    ],
    "published": "2025-10-02",
    "abstract": "Recent advances in machine learning have been supported by the emergence of domain-specific software libraries, enabling streamlined workflows and increased reproducibility. For geospatial machine learning (GeoML), the availability of Earth observation data has outpaced the development of domain libraries to handle its unique challenges, such as varying spatial resolutions, spectral properties, temporal cadence, data coverage, coordinate systems, and file formats. This chapter presents a comprehensive overview of GeoML libraries, analyzing their evolution, core functionalities, and the current ecosystem. It also introduces popular GeoML libraries such as TorchGeo, eo-learn, and Raster Vision, detailing their architecture, supported data types, and integration with ML frameworks. Additionally, it discusses common methodologies for data preprocessing, spatial--temporal joins, benchmarking, and the use of pretrained models. Through a case study in crop type mapping, it demonstrates practical applications of these tools. Best practices in software design, licensing, and testing are highlighted, along with open challenges and future directions, particularly the rise of foundation models and the need for governance in open-source geospatial software. Our aim is to guide practitioners, developers, and researchers in navigating and contributing to the rapidly evolving GeoML landscape.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model",
    "url": "http://arxiv.org/abs/2509.16617v1",
    "authors": [
      "David Kreismann"
    ],
    "published": "2025-09-20",
    "abstract": "As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data. However, predictive analytics methods based on conventional machine learning models and limited data infrastructure often provide inaccurate predictions, especially in underserved areas. In this context, geospatial foundation models trained on unstructured global data demonstrate strong generalization and require minimal fine-tuning, offering an alternative for predictions where traditional approaches are limited. This study fine-tunes a geospatial foundation model to predict urban land surface temperatures under future climate scenarios and explores its response to land cover changes using simulated vegetation strategies. The fine-tuned model achieved pixel-wise downscaling errors below 1.74 \u00b0C and aligned with ground truth patterns, demonstrating an extrapolation capacity up to 3.62 \u00b0C.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines",
    "url": "http://arxiv.org/abs/2509.18182v1",
    "authors": [
      "Isabelle Tingzon",
      "Yoji Toriumi",
      "Caroline Gevaert"
    ],
    "published": "2025-09-18",
    "abstract": "Detailed structural building information is used to estimate potential damage from hazard events like cyclones, floods, and landslides, making them critical for urban resilience planning and disaster risk reduction. However, such information is often unavailable in many small island developing states (SIDS) in climate-vulnerable regions like the Caribbean. To address this data gap, we present an AI-driven workflow to automatically infer rooftop attributes from high-resolution satellite imagery, with Saint Vincent and the Grenadines as our case study. Here, we compare the utility of geospatial foundation models combined with shallow classifiers against fine-tuned deep learning models for rooftop classification. Furthermore, we assess the impact of incorporating additional training data from neighboring SIDS to improve model performance. Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof material classification, respectively. Combined with local capacity building, our work aims to provide SIDS with novel capabilities to harness AI and Earth Observation (EO) data to enable more efficient, evidence-based urban governance.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Scalable Geospatial Data Generation Using AlphaEarth Foundations Model",
    "url": "http://arxiv.org/abs/2508.11739v1",
    "authors": [
      "Luc Houriez",
      "Sebastian Pilarski",
      "Behzad Vahedi",
      "Ali Ahmadalipour",
      "Teo Honda Scully",
      "Nicholas Aflitto",
      "David Andre",
      "Caroline Jaffe",
      "Martha Wedner",
      "Rich Mazzola",
      "Josh Jeffery",
      "Ben Messinger",
      "Sage McGinley-Smith",
      "Sarah Russell"
    ],
    "published": "2025-08-15",
    "abstract": "High-quality labeled geospatial datasets are essential for extracting insights and understanding our planet. Unfortunately, these datasets often do not span the entire globe and are limited to certain geographic regions where data was collected. Google DeepMind's recently released AlphaEarth Foundations (AEF) provides an information-dense global geospatial representation designed to serve as a useful input across a wide gamut of tasks. In this article we propose and evaluate a methodology which leverages AEF to extend geospatial labeled datasets beyond their initial geographic regions. We show that even basic models like random forests or logistic regression can be used to accomplish this task. We investigate a case study of extending LANDFIRE's Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for EvtPhys, model predictions align with ground truth. Trained models achieve 81% and 73% classification accuracy on EvtPhys validation sets in the USA and Canada, despite discussed limitations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "PlaceFM: A Training-free Geospatial Foundation Model of Places using Large-Scale Point of Interest Data",
    "url": "http://arxiv.org/abs/2507.02921v2",
    "authors": [
      "Mohammad Hashemi",
      "Hossein Amiri",
      "Andreas Zufle"
    ],
    "published": "2025-06-25",
    "abstract": "With the rapid growth and continual updates of geospatial data from diverse sources, geospatial foundation model pre-training for urban representation learning has emerged as a key research direction for advancing data-driven urban planning. Spatial structure is fundamental to effective geospatial intelligence systems; however, existing foundation models often lack the flexibility to reason about places, context-rich regions spanning multiple spatial granularities that may consist of many spatially and semantically related points of interest. To address this gap, we propose PlaceFM, a geospatial foundation model that captures place representations through a training-free, clustering-based approach. PlaceFM summarizes the entire point of interest graph constructed from U.S. Foursquare data, producing general-purpose region embeddings while automatically identifying places of interest. These embeddings can be directly integrated into geolocation data pipelines to support a variety of urban downstream tasks. Without the need for costly pre-training, PlaceFM provides a scalable and efficient solution for multi-granular geospatial analysis. Extensive experiments on two real-world prediction tasks, ZIP code-level population density and housing prices, demonstrate that PlaceFM not only outperforms most state-of-the-art graph-based geospatial foundation models but also achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation is available at https://github.com/mohammadhashemii/PlaceFM.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning",
    "url": "http://arxiv.org/abs/2506.17302v1",
    "authors": [
      "Yijun Lin",
      "Theresa Chen",
      "Colby Brungard",
      "Grunwald Sabine",
      "Sue Ives",
      "Matt Macander",
      "Timm Nawrocki",
      "Yao-Yi Chiang",
      "Nic Jelinski"
    ],
    "published": "2025-06-17",
    "abstract": "Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and localized simulations, remains a critical yet underdeveloped task, despite the region's ecological importance and extensive permafrost coverage. As permafrost thaw accelerates due to climate change, it threatens infrastructure stability and key ecosystem services, such as soil carbon storage. High-resolution soil maps are essential for characterizing permafrost distribution, identifying vulnerable areas, and informing adaptation strategies. We present MISO, a vision-based machine learning (ML) model to produce statewide fine-scale soil maps for near-surface permafrost and soil taxonomy. The model integrates a geospatial foundation model for visual feature extraction, implicit neural representations for continuous spatial prediction, and contrastive learning for multimodal alignment and geo-location awareness. We compare MISO with Random Forest (RF), a traditional ML model that has been widely used in soil mapping applications. Spatial cross-validation and regional analysis across Permafrost Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better to remote, unseen locations and achieves higher recall than RF, which is critical for monitoring permafrost thaw and related environmental processes. These findings demonstrate the potential of advanced ML approaches for fine-scale soil mapping and provide practical guidance for future soil sampling and infrastructure planning in permafrost-affected landscapes. The project will be released at https://github.com/knowledge-computing/Peatland-permafrost.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places",
    "url": "http://arxiv.org/abs/2506.14570v1",
    "authors": [
      "Mohammad Hashemi",
      "Andreas Zufle"
    ],
    "published": "2025-06-17",
    "abstract": "Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation",
    "url": "http://arxiv.org/abs/2506.13599v1",
    "authors": [
      "Yuwei Du",
      "Jie Feng",
      "Jian Yuan",
      "Yong Li"
    ],
    "published": "2025-06-16",
    "abstract": "Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered \\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation (\\textbf{CAMS}), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. \\textbf{CAMS} comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that \\textbf{CAMS} achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, \\textbf{CAMS} generates more realistic and plausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "HyBiomass: Global Hyperspectral Imagery Benchmark Dataset for Evaluating Geospatial Foundation Models in Forest Aboveground Biomass Estimation",
    "url": "http://arxiv.org/abs/2506.11314v1",
    "authors": [
      "Aaron Banze",
      "Timoth\u00e9e Stassin",
      "Nassim Ait Ali Braham",
      "R\u0131dvan Salih Kuzu",
      "Simon Besnard",
      "Michael Schmitt"
    ],
    "published": "2025-06-12",
    "abstract": "Comprehensive evaluation of geospatial foundation models (Geo-FMs) requires benchmarking across diverse tasks, sensors, and geographic regions. However, most existing benchmark datasets are limited to segmentation or classification tasks, and focus on specific geographic areas. To address this gap, we introduce a globally distributed dataset for forest aboveground biomass (AGB) estimation, a pixel-wise regression task. This benchmark dataset combines co-located hyperspectral imagery (HSI) from the Environmental Mapping and Analysis Program (EnMAP) satellite and predictions of AGB density estimates derived from the Global Ecosystem Dynamics Investigation lidars, covering seven continental regions. Our experimental results on this dataset demonstrate that the evaluated Geo-FMs can match or, in some cases, surpass the performance of a baseline U-Net, especially when fine-tuning the encoder. We also find that the performance difference between the U-Net and Geo-FMs depends on the dataset size for each region and highlight the importance of the token patch size in the Vision Transformer backbone for accurate predictions in pixel-wise regression tasks. By releasing this globally distributed hyperspectral benchmark dataset, we aim to facilitate the development and evaluation of Geo-FMs for HSI applications. Leveraging this dataset additionally enables research into geographic bias and generalization capacity of Geo-FMs. The dataset and source code will be made publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Geospatial Foundation Models to Enable Progress on Sustainable Development Goals",
    "url": "http://arxiv.org/abs/2505.24528v2",
    "authors": [
      "Pedram Ghamisi",
      "Weikang Yu",
      "Xiaokang Zhang",
      "Aldino Rizaldy",
      "Jian Wang",
      "Chufeng Zhou",
      "Richard Gloaguen",
      "Gustau Camps-Valls"
    ],
    "published": "2025-05-30",
    "abstract": "Foundation Models (FMs) are large-scale, pre-trained artificial intelligence (AI) systems that have revolutionized natural language processing and computer vision, and are now advancing geospatial analysis and Earth Observation (EO). They promise improved generalization across tasks, scalability, and efficient adaptation with minimal labeled data. However, despite the rapid proliferation of geospatial FMs, their real-world utility and alignment with global sustainability goals remain underexplored. We introduce SustainFM, a comprehensive benchmarking framework grounded in the 17 Sustainable Development Goals with extremely diverse tasks ranging from asset wealth prediction to environmental hazard detection. This study provides a rigorous, interdisciplinary assessment of geospatial FMs and offers critical insights into their role in attaining sustainability goals. Our findings show: (1) While not universally superior, FMs often outperform traditional approaches across diverse tasks and datasets. (2) Evaluating FMs should go beyond accuracy to include transferability, generalization, and energy efficiency as key criteria for their responsible use. (3) FMs enable scalable, SDG-grounded solutions, offering broad utility for tackling complex sustainability challenges. Critically, we advocate for a paradigm shift from model-centric development to impact-driven deployment, and emphasize metrics such as energy efficiency, robustness to domain shifts, and ethical considerations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations",
    "url": "http://arxiv.org/abs/2505.17136v1",
    "authors": [
      "Yuhan Ji",
      "Song Gao",
      "Ying Nie",
      "Ivan Maji\u0107",
      "Krzysztof Janowicz"
    ],
    "published": "2025-05-22",
    "abstract": "Applying AI foundation models directly to geospatial datasets remains challenging due to their limited ability to represent and reason with geographical entities, specifically vector-based geometries and natural language descriptions of complex spatial relations. To address these issues, we investigate the extent to which a well-known-text (WKT) representation of geometries and their spatial relations (e.g., topological predicates) are preserved during spatial reasoning when the geospatial vector data are passed to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt engineering-based, and everyday language-based evaluation. Our experiment results demonstrate that both the embedding-based and prompt engineering-based approaches to geospatial question-answering tasks with GPT models can achieve an accuracy of over 0.6 on average for the identification of topological spatial relations between two geometries. Among the evaluated models, GPT-4 with few-shot prompting achieved the highest performance with over 0.66 accuracy on topological spatial relation inference. Additionally, GPT-based reasoner is capable of properly comprehending inverse topological spatial relations and including an LLM-generated geometry can enhance the effectiveness for geographic entity retrieval. GPT-4 also exhibits the ability to translate certain vernacular descriptions about places into formal topological relations, and adding the geometry-type or place-type context in prompts may improve inference accuracy, but it varies by instance. The performance of these spatial reasoning tasks offers valuable insights for the refinement of LLMs with geographical knowledge towards the development of geo-foundation models capable of geospatial reasoning.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GAIA: A Foundation Model for Operational Atmospheric Dynamics",
    "url": "http://arxiv.org/abs/2505.18179v2",
    "authors": [
      "Ata Akbari Asanjan",
      "Olivia Alexander",
      "Tom Berg",
      "Stephen Peng",
      "Jad Makki",
      "Clara Zhang",
      "Matt Yang",
      "Disha Shidham",
      "Srija Chakraborty",
      "William Bender",
      "Cara Crawford",
      "Arun Ravindran",
      "Olivier Raiman",
      "David Potere",
      "David Bell"
    ],
    "published": "2025-05-15",
    "abstract": "We introduce GAIA (Geospatial Artificial Intelligence for Atmospheres), a hybrid self-supervised geospatial foundation model that fuses Masked Autoencoders (MAE) with self-distillation with no labels (DINO) to generate semantically rich representations from global geostationary satellite imagery. Pre-trained on 15 years of globally-merged infrared observations (2001-2015), GAIA learns disentangled representations that capture atmospheric dynamics rather than trivial diurnal patterns, as evidenced by distributed principal component structure and temporal coherence analysis. We demonstrate robust reconstruction capabilities across varying data availability (30-95% masking), achieving superior gap-filling performance on real missing data patterns. When transferred to downstream tasks, GAIA consistently outperforms an MAE-only baseline: improving atmospheric river segmentation (F1: 0.58 vs 0.52), enhancing tropical cyclone detection (storm-level recall: 81% vs 75%, early detection: 29% vs 17%), and maintaining competitive precipitation estimation performance. Analysis reveals that GAIA's hybrid objectives encourage learning of spatially coherent, object-centric features distributed across multiple principal components rather than concentrated representations focused on reconstruction. This work demonstrates that combining complementary self-supervised objectives yields more transferable representations for diverse atmospheric modeling tasks. Model weights and code are available at: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2505.08723v1",
    "authors": [
      "Xiaolei Qin",
      "Di Wang",
      "Jing Zhang",
      "Fengxiang Wang",
      "Xin Su",
      "Bo Du",
      "Liangpei Zhang"
    ],
    "published": "2025-05-13",
    "abstract": "Satellite image time series (SITS) provide continuous observations of the Earth's surface, making them essential for applications such as environmental management and disaster assessment. However, existing spatiotemporal foundation models rely on plain vision transformers, which encode entire temporal sequences without explicitly capturing multiscale spatiotemporal relationships between land objects. This limitation hinders their effectiveness in downstream tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision transformer foundation model tailored for SITS analysis. At its core, we introduce a spatiotemporal gyroscope attention mechanism that dynamically captures evolving multiscale patterns across both time and space. For pre-training, we curate MillionST, a large-scale dataset of one million images from 100,000 geographic locations, each captured across 10 temporal phases over five years, encompassing diverse geospatial changes and seasonal variations. Leveraging this dataset, we adapt masked image modeling to pre-train TiMo, enabling it to effectively learn and encode generalizable spatiotemporal representations.Extensive experiments across multiple spatiotemporal tasks-including deforestation monitoring, land cover segmentation, crop type classification, and flood detection-demonstrate TiMo's superiority over state-of-the-art methods. Code, model, and dataset will be released at https://github.com/MiliLab/TiMo.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "url": "http://arxiv.org/abs/2505.03368v2",
    "authors": [
      "Stef De Sabbata",
      "Stefano Mizzaro",
      "Kevin Roitero"
    ],
    "published": "2025-05-06",
    "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and \"reasoning\" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call \"how LLMs think about geographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "Autoencoder",
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data",
    "url": "http://arxiv.org/abs/2504.11172v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Paolo Fraccaro",
      "Valerio Marsocci",
      "Johannes Jakubik",
      "Stefano Maurogiovanni",
      "Mikolaj Czerkawski",
      "Rocco Sedona",
      "Gabriele Cavallaro",
      "Thomas Brunschwiler",
      "Juan Bernabe-Moreno",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-04-15",
    "abstract": "Large-scale foundation models in Earth Observation can learn versatile, label-efficient representations by leveraging massive amounts of unlabeled data. However, existing public datasets are often limited in scale, geographic coverage, or sensor variety. We introduce TerraMesh, a new globally diverse, multimodal dataset combining optical, synthetic aperture radar, elevation, and land-cover modalities in an Analysis-Ready Data format. TerraMesh includes over 9~million samples with eight spatiotemporal aligned modalities, enabling large-scale pre-training. We provide detailed data processing steps, comprehensive statistics, and empirical evidence demonstrating improved model performance when pre-trained on TerraMesh. The dataset is hosted at https://huggingface.co/datasets/ibm-esa-geospatial/TerraMesh.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
    "url": "http://arxiv.org/abs/2504.11171v4",
    "authors": [
      "Johannes Jakubik",
      "Felix Yang",
      "Benedikt Blumenstiel",
      "Erik Scheurer",
      "Rocco Sedona",
      "Stefano Maurogiovanni",
      "Jente Bosmans",
      "Nikolaos Dionelis",
      "Valerio Marsocci",
      "Niklas Kopp",
      "Rahul Ramachandran",
      "Paolo Fraccaro",
      "Thomas Brunschwiler",
      "Gabriele Cavallaro",
      "Juan Bernabe-Moreno",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-04-15",
    "abstract": "We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces \"Thinking-in-Modalities\" (TiM) -- the capability of generating additional artificial data during finetuning and inference to improve the model output -- and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "TerraTorch: The Geospatial Foundation Models Toolkit",
    "url": "http://arxiv.org/abs/2503.20563v1",
    "authors": [
      "Carlos Gomes",
      "Benedikt Blumenstiel",
      "Joao Lucas de Sousa Almeida",
      "Pedro Henrique de Oliveira",
      "Paolo Fraccaro",
      "Francesc Marti Escofet",
      "Daniela Szwarcman",
      "Naomi Simumba",
      "Romeo Kienzler",
      "Bianca Zadrozny"
    ],
    "published": "2025-03-26",
    "abstract": "TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial Foundation Models built on PyTorch Lightning and tailored for satellite, weather, and climate data. It integrates domain-specific data modules, pre-defined tasks, and a modular model factory that pairs any backbone with diverse decoder heads. These components allow researchers and practitioners to fine-tune supported models in a no-code fashion by simply editing a training configuration. By consolidating best practices for model development and incorporating the automated hyperparameter optimization extension Iterate, TerraTorch reduces the expertise and time required to fine-tune or benchmark models on new Earth Observation use cases. Furthermore, TerraTorch directly integrates with GEO-Bench, allowing for systematic and reproducible benchmarking of Geospatial Foundation Models. TerraTorch is open sourced under Apache 2.0, available at https://github.com/IBM/terratorch, and can be installed via pip install terratorch.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2503.18540v1",
    "authors": [
      "Guneet Mutreja",
      "Philipp Schuegraf",
      "Ksenia Bittner"
    ],
    "published": "2025-03-24",
    "abstract": "Recent advances in self-supervised learning have led to the development of foundation models that have significantly advanced performance in various computer vision tasks. However, despite their potential, these models often overlook the crucial role of high-resolution digital surface models (DSMs) in understanding urban environments, particularly for building-level analysis, which is essential for applications like digital twins. To address this gap, we introduce HiRes-FusedMIM, a novel pre-trained model specifically designed to leverage the rich information contained within high-resolution RGB and DSM data. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling (SimMIM) architecture with a multi-objective loss function that combines reconstruction and contrastive objectives, enabling it to learn powerful, joint representations from both modalities. We conducted a comprehensive evaluation of HiRes-FusedMIM on a diverse set of downstream tasks, including classification, semantic segmentation, and instance segmentation. Our results demonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art geospatial methods on several building-related datasets, including WHU Aerial and LoveDA, demonstrating its effectiveness in capturing and leveraging fine-grained building information; 2) Incorporating DSMs during pre-training consistently improves performance compared to using RGB data alone, highlighting the value of elevation information for building-level analysis; 3) The dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB and DSM data, significantly outperforms a single-encoder model on the Vaihingen segmentation task, indicating the benefits of learning specialized representations for each modality. To facilitate further research and applications in this direction, we will publicly release the trained model weights.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations",
    "url": "http://arxiv.org/abs/2503.16683v1",
    "authors": [
      "Zeping Liu",
      "Fan Zhang",
      "Junfeng Jiao",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published": "2025-03-20",
    "abstract": "Advancements in vision and language foundation models have inspired the development of geo-foundation models (GeoFMs), enhancing performance across diverse geospatial tasks. However, many existing GeoFMs primarily focus on overhead remote sensing (RS) data while neglecting other data modalities such as ground-level imagery. A key challenge in multimodal GeoFM development is to explicitly model geospatial relationships across modalities, which enables generalizability across tasks, spatial scales, and temporal contexts. To address these limitations, we propose GAIR, a novel multimodal GeoFM architecture integrating overhead RS data, street view (SV) imagery, and their geolocation metadata. We utilize three factorized neural encoders to project an SV image, its geolocation, and an RS image into the embedding space. The SV image needs to be located within the RS image's spatial footprint but does not need to be at its geographic center. In order to geographically align the SV image and RS image, we propose a novel implicit neural representations (INR) module that learns a continuous RS image representation and looks up the RS embedding at the SV image's geolocation. Next, these geographically aligned SV embedding, RS embedding, and location embedding are trained with contrastive learning objectives from unlabeled data. We evaluate GAIR across 10 geospatial tasks spanning RS image-based, SV image-based, and location embedding-based benchmarks. Experimental results demonstrate that GAIR outperforms state-of-the-art GeoFMs and other strong baselines, highlighting its effectiveness in learning generalizable and transferable geospatial representations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data",
    "url": "http://arxiv.org/abs/2503.12843v3",
    "authors": [
      "Haozhe Si",
      "Yuxuan Wan",
      "Minh Do",
      "Deepak Vasisht",
      "Han Zhao",
      "Hendrik F. Hamann"
    ],
    "published": "2025-03-17",
    "abstract": "Geospatial raster data, such as that collected by satellite-based imaging systems at different times and spectral bands, hold immense potential for enabling a wide range of high-impact applications. This potential stems from the rich information that is spatially and temporally contextualized across multiple channels and sensing modalities. Recent work has adapted existing self-supervised learning approaches for such geospatial data. However, they fall short of scalable model architectures, leading to inflexibility and computational inefficiencies when faced with an increasing number of channels and modalities. To address these limitations, we introduce Low-rank Efficient Spatial-Spectral Vision Transformer with three key innovations: i) the LESS Attention Block that approximates high-dimensional spatial-spectral attention through Kronecker's product of the low-dimensional spatial and spectral attention components; ii) the Continuous Positional-Channel Embedding Layer that preserves both the continuity and physical characteristics of each spatial-spectral patch; and iii) the Perception Field Mask that exploits local spatial dependencies by constraining attention to neighboring patches. To evaluate the proposed innovations, we construct GFM-Bench, which serves as a comprehensive benchmark for such geospatial raster data. We pretrain LESS ViT using a Hyperspectral Masked Autoencoder framework with integrated positional and channel masking strategies. Experimental results demonstrate that our proposed method achieves competitive performance against state-of-the-art multi-modal geospatial foundation models while outperforming them on cross-satellite generalization tasks with higher computational efficiency. The flexibility and extensibility of our framework make it a promising direction for future geospatial data analysis tasks that involve a wide range of modalities and channels.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection",
    "url": "http://arxiv.org/abs/2503.09493v2",
    "authors": [
      "Romain Thoreau",
      "Valerio Marsocci",
      "Dawa Derksen"
    ],
    "published": "2025-03-12",
    "abstract": "As large-scale heterogeneous data sets become increasingly available, adapting foundation models at low cost has become a key issue. Seminal works in natural language processing, e.g. Low-Rank Adaptation (LoRA), leverage the low \"intrinsic rank\" of parameter updates during adaptation. In this paper, we argue that incorporating stronger inductive biases in both data and models can enhance the adaptation of Geospatial Foundation Models (GFMs), pretrained on RGB satellite images, to other types of optical satellite data. Specifically, the pretrained parameters of GFMs serve as a strong prior for the spatial structure of multispectral images. For this reason, we introduce DEFLECT (Deflecting Embeddings for Finetuning Latent representations for Earth and Climate Tasks), a novel strategy for adapting GFMs to multispectral satellite imagery with very few additional parameters. DEFLECT improves the representation capabilities of the extracted features, particularly enhancing spectral information, which is essential for geoscience and environmental-related tasks. We demonstrate the effectiveness of our method across three different GFMs and five diverse datasets, ranging from forest monitoring to marine environment segmentation. Compared to competing methods, DEFLECT achieves on-par or higher accuracy with 5-10$\\times$ fewer parameters for classification and segmentation tasks. The code will be made publicly available.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?",
    "url": "http://arxiv.org/abs/2503.07890v2",
    "authors": [
      "Yuru Jia",
      "Valerio Marsocci",
      "Ziyang Gong",
      "Xue Yang",
      "Maarten Vergauwen",
      "Andrea Nascetti"
    ],
    "published": "2025-03-10",
    "abstract": "Self-supervised learning (SSL) has revolutionized representation learning in Remote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage vast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs primarily employ objectives like contrastive learning or masked image modeling, owing to their proven success in learning transferable representations. However, generative diffusion models, which demonstrate the potential to capture multi-grained semantics essential for RS tasks during image generation, remain underexplored for discriminative applications. This prompts the question: can generative diffusion models also excel and serve as GFMs with sufficient discriminative power? In this work, we answer this question with SatDiFuser, a framework that transforms a diffusion-based generative geospatial foundation model into a powerful pretraining tool for discriminative RS. By systematically analyzing multi-stage, noise-dependent diffusion features, we develop three fusion strategies to effectively leverage these diverse representations. Extensive experiments on remote sensing benchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving gains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in classification, demonstrating the capacity of diffusion-based generative foundation models to rival or exceed discriminative GFMs. The source code is available at: https://github.com/yurujaja/SatDiFuser.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Image Generation"
    ]
  },
  {
    "title": "Lossy Neural Compression for Geospatial Analytics: A Review",
    "url": "http://arxiv.org/abs/2503.01505v2",
    "authors": [
      "Carlos Gomes",
      "Isabelle Wittmann",
      "Damien Robert",
      "Johannes Jakubik",
      "Tim Reichelt",
      "Michele Martone",
      "Stefano Maurogiovanni",
      "Rikard Vinge",
      "Jonas Hurst",
      "Erik Scheurer",
      "Rocco Sedona",
      "Thomas Brunschwiler",
      "Stefan Kesselheim",
      "Matej Batic",
      "Philip Stier",
      "Jan Dirk Wegner",
      "Gabriele Cavallaro",
      "Edzer Pebesma",
      "Michael Marszalek",
      "Miguel A Belenguer-Plomer",
      "Kennedy Adriko",
      "Paolo Fraccaro",
      "Romeo Kienzler",
      "Rania Briq",
      "Sabrina Benassou",
      "Michele Lazzarini",
      "Conrad M Albrecht"
    ],
    "published": "2025-03-03",
    "abstract": "Over the past decades, there has been an explosion in the amount of available Earth Observation (EO) data. The unprecedented coverage of the Earth's surface and atmosphere by satellite imagery has resulted in large volumes of data that must be transmitted to ground stations, stored in data centers, and distributed to end users. Modern Earth System Models (ESMs) face similar challenges, operating at high spatial and temporal resolutions, producing petabytes of data per simulated day. Data compression has gained relevance over the past decade, with neural compression (NC) emerging from deep learning and information theory, making EO data and ESM outputs ideal candidates due to their abundance of unlabeled data. In this review, we outline recent developments in NC applied to geospatial data. We introduce the fundamental concepts of NC including seminal works in its traditional applications to image and video compression domains with focus on lossy compression. We discuss the unique characteristics of EO and ESM data, contrasting them with \"natural images\", and explain the additional challenges and opportunities they present. Moreover, we review current applications of NC across various EO modalities and explore the limited efforts in ESM compression to date. The advent of self-supervised learning (SSL) and foundation models (FM) has advanced methods to efficiently distill representations from vast unlabeled data. We connect these developments to NC for EO, highlighting the similarities between the two fields and elaborate on the potential of transferring compressed feature representations for machine--to--machine communication. Based on insights drawn from this review, we devise future directions relevant to applications in EO and ESM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SSL4EO-S12 v1.1: A Multimodal, Multiseasonal Dataset for Pretraining, Updated",
    "url": "http://arxiv.org/abs/2503.00168v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Nassim Ait Ali Braham",
      "Conrad M Albrecht",
      "Stefano Maurogiovanni",
      "Paolo Fraccaro"
    ],
    "published": "2025-02-28",
    "abstract": "This technical report presents SSL4EO-S12 v1.1, a multimodal, multitemporal Earth Observation dataset designed for pretraining large-scale foundation models. Building on the success of SSL4EO-S12 v1.0, the new version addresses the previous challenges of data misalignment and a limited data structure for low-barrier, analysis-ready EO processing. SSL4EO-S12 v1.1 covers the world's 10,000 largest cities and its surroundings within a 50 km radius across four seasons, resulting in a diverse collection of nearly one million patches. SSL4EO-S12 v1.1 packages the data in Zarr file format for cloud-efficient loading and representation of meta-information such as including cloud masks and geolocation. Released under the CC-BY-4.0 license, SSL4EO-S12 v1.1 facilitates open research and provides a robust foundation for future advancements in self-supervised learning and geospatial analysis. The dataset is available online through https://datapub.fz-juelich.de/ssl4eo-s12, and we provided additional resources at https://github.com/DLR-MF-DAS/SSL4EO-S12-v1.1.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "How Does the Spatial Distribution of Pre-training Data Affect Geospatial Foundation Models?",
    "url": "http://arxiv.org/abs/2501.12535v1",
    "authors": [
      "Mirali Purohit",
      "Gedeon Muhawenayo",
      "Esther Rolf",
      "Hannah Kerner"
    ],
    "published": "2025-01-21",
    "abstract": "Foundation models have made rapid advances in many domains including Earth observation, where Geospatial Foundation Models (GFMs) can help address global challenges such as climate change, agriculture, and disaster response. Previous work on GFMs focused on tailoring model architecture and pre-text tasks, and did not investigate the impact of pre-training data selection on model performance. However, recent works from other domains show that the pre-training data distribution is an important factor influencing the performance of the foundation models. With this motivation, our research explores how the geographic distribution of pre-training data affects the performance of GFMs. We evaluated several pre-training data distributions by sampling different compositions from a global data pool. Our experiments with two GFMs on downstream tasks indicate that balanced and globally representative data compositions often outperform region-specific sampling, highlighting the importance of diversity and global coverage in pre-training data. Our results suggest that the most appropriate data sampling technique may depend on the specific GFM architecture. These findings will support the development of robust GFMs by incorporating quality pre-training data distributions, ultimately improving machine learning solutions for Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models",
    "url": "http://arxiv.org/abs/2501.00316v2",
    "authors": [
      "Mahir Labib Dihan",
      "Md Tanvir Hassan",
      "Md Tanvir Parvez",
      "Md Hasebul Hasan",
      "Md Almash Alam",
      "Muhammad Aamir Cheema",
      "Mohammed Eunus Ali",
      "Md Rizwan Parvez"
    ],
    "published": "2024-12-31",
    "abstract": "Recent advancements in foundation models have improved autonomous tool usage and reasoning, but their capabilities in map-based reasoning remain underexplored. To address this, we introduce MapEval, a benchmark designed to assess foundation models across three distinct tasks - textual, API-based, and visual reasoning - through 700 multiple-choice questions spanning 180 cities and 54 countries, covering spatial relationships, navigation, travel planning, and real-world map interactions. Unlike prior benchmarks that focus on simple location queries, MapEval requires models to handle long-context reasoning, API interactions, and visual map analysis, making it the most comprehensive evaluation framework for geospatial AI. On evaluation of 30 foundation models, including Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro, none surpass 67% accuracy, with open-source models performing significantly worse and all models lagging over 20% behind human performance. These results expose critical gaps in spatial inference, as models struggle with distances, directions, route planning, and place-specific reasoning, highlighting the need for better geospatial AI to bridge the gap between foundation models and real-world navigation. All the resources are available at: https://mapeval.github.io/.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2412.04204v2",
    "authors": [
      "Valerio Marsocci",
      "Yuru Jia",
      "Georges Le Bellier",
      "David Kerekes",
      "Liang Zeng",
      "Sebastian Hafner",
      "Sebastian Gerard",
      "Eric Brune",
      "Ritu Yadav",
      "Ali Shibli",
      "Heng Fang",
      "Yifang Ban",
      "Maarten Vergauwen",
      "Nicolas Audebert",
      "Andrea Nascetti"
    ],
    "published": "2024-12-05",
    "abstract": "Geospatial Foundation Models (GFMs) have emerged as powerful tools for extracting representations from Earth observation data, but their evaluation remains inconsistent and narrow. Existing works often evaluate on suboptimal downstream datasets and tasks, that are often too easy or too narrow, limiting the usefulness of the evaluations to assess the real-world applicability of GFMs. Additionally, there is a distinct lack of diversity in current evaluation protocols, which fail to account for the multiplicity of image resolutions, sensor types, and temporalities, which further complicates the assessment of GFM performance. In particular, most existing benchmarks are geographically biased towards North America and Europe, questioning the global applicability of GFMs. To overcome these challenges, we introduce PANGAEA, a standardized evaluation protocol that covers a diverse set of datasets, tasks, resolutions, sensor modalities, and temporalities. It establishes a robust and widely applicable benchmark for GFMs. We evaluate the most popular GFMs openly available on this benchmark and analyze their performance across several domains. In particular, we compare these models to supervised baselines (e.g. UNet and vanilla ViT), and assess their effectiveness when faced with limited labeled data. Our findings highlight the limitations of GFMs, under different scenarios, showing that they do not consistently outperform supervised models. PANGAEA is designed to be highly extensible, allowing for the seamless inclusion of new datasets, models, and tasks in future research. By releasing the evaluation code and benchmark, we aim to enable other researchers to replicate our experiments and build upon our work, fostering a more principled evaluation protocol for large pre-trained geospatial models. The code is available at https://github.com/VMarsocci/pangaea-bench.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications",
    "url": "http://arxiv.org/abs/2412.02732v2",
    "authors": [
      "Daniela Szwarcman",
      "Sujit Roy",
      "Paolo Fraccaro",
      "\u00deorsteinn El\u00ed G\u00edslason",
      "Benedikt Blumenstiel",
      "Rinki Ghosal",
      "Pedro Henrique de Oliveira",
      "Joao Lucas de Sousa Almeida",
      "Rocco Sedona",
      "Yanghui Kang",
      "Srija Chakraborty",
      "Sizhe Wang",
      "Carlos Gomes",
      "Ankur Kumar",
      "Myscon Truong",
      "Denys Godwin",
      "Hyunho Lee",
      "Chia-Yu Hsu",
      "Ata Akbari Asanjan",
      "Besart Mujeci",
      "Disha Shidham",
      "Trevor Keenan",
      "Paulo Arevalo",
      "Wenwen Li",
      "Hamed Alemohammad",
      "Pontus Olofsson",
      "Christopher Hain",
      "Robert Kennedy",
      "Bianca Zadrozny",
      "David Bell",
      "Gabriele Cavallaro",
      "Campbell Watson",
      "Manil Maskey",
      "Rahul Ramachandran",
      "Juan Bernabe Moreno"
    ],
    "published": "2024-12-03",
    "abstract": "This technical report presents Prithvi-EO-2.0, a new geospatial foundation model that offers significant improvements over its predecessor, Prithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's Harmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M and 600M parameter models incorporate temporal and location embeddings for enhanced performance across various geospatial tasks. Through extensive benchmarking with GEO-Bench, the 600M version outperforms the previous Prithvi-EO model by 8\\% across a range of tasks. It also outperforms six other geospatial foundation models when benchmarked on remote sensing tasks from different domains and resolutions (i.e. from 0.1m to 15m). The results demonstrate the versatility of the model in both classical earth observation and high-resolution applications. Early involvement of end-users and subject matter experts (SMEs) are among the key factors that contributed to the project's success. In particular, SME involvement allowed for constant feedback on model and dataset design, as well as successful customization for diverse SME-led applications in disaster response, land use and crop mapping, and ecosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and IBM terratorch, with additional resources on GitHub. The project exemplifies the Trusted Open Science approach embraced by all involved organizations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatVision-TOA: A Geospatial Foundation Model for Coarse-Resolution All-Sky Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2411.17000v1",
    "authors": [
      "Caleb S. Spradlin",
      "Jordan A. Caraballo-Vega",
      "Jian Li",
      "Mark L. Carroll",
      "Jie Gong",
      "Paul M. Montesano"
    ],
    "published": "2024-11-26",
    "abstract": "Foundation models have the potential to transform the landscape of remote sensing (RS) data analysis by enabling large computer vision models to be pre-trained on vast amounts of remote sensing data. These models can then be fine-tuned with small amounts of labeled training and applied to a variety of applications. Most existing foundation models are designed for high spatial resolution, cloud-free satellite imagery or photos, limiting their applicability in scenarios that require frequent temporal monitoring or broad spectral profiles. As a result, foundation models trained solely on cloud-free images have limited utility for applications that involve atmospheric variables or require atmospheric corrections. We introduce SatVision-TOA, a novel foundation model pre-trained on 14-band MODIS L1B Top-Of-Atmosphere (TOA) radiance imagery, addressing the need for models pre-trained to handle moderate- and coarse-resolution all-sky remote sensing data. The SatVision-TOA model is pre-trained using a Masked-Image-Modeling (MIM) framework and the SwinV2 architecture, and learns detailed contextual representations through self-supervised learning without the need for labels. It is a 3 billion parameter model that is trained on 100 million images. To our knowledge this is the largest foundation model trained solely on satellite RS imagery. Results show that SatVision-TOA achieves superior performance over baseline methods on downstream tasks such as 3D cloud retrieval. Notably, the model achieves a mean intersection over union (mIOU) of 0.46, a substantial improvement over the baseline mIOU of 0.22. Additionally, the rate of false negative results in the fine-tuning task were reduced by over 50% compared to the baseline. Our work advances pre-trained vision modeling for multispectral RS by learning from a variety of atmospheric and aerosol conditions to improve cloud and land surface monitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "General Geospatial Inference with a Population Dynamics Foundation Model",
    "url": "http://arxiv.org/abs/2411.07207v5",
    "authors": [
      "Mohit Agarwal",
      "Mimi Sun",
      "Chaitanya Kamath",
      "Arbaaz Muslim",
      "Prithul Sarker",
      "Joydeep Paul",
      "Hector Yee",
      "Marcin Sieniek",
      "Kim Jablonski",
      "Swapnil Vispute",
      "Atul Kumar",
      "Yael Mayer",
      "David Fork",
      "Sheila de Guia",
      "Jamie McPike",
      "Adam Boulanger",
      "Tomer Shekel",
      "David Schottlander",
      "Yao Xiao",
      "Manjit Chakravarthy Manukonda",
      "Yun Liu",
      "Neslihan Bulut",
      "Sami Abu-el-haija",
      "Bryan Perozzi",
      "Monica Bharel",
      "Von Nguyen",
      "Luke Barrington",
      "Niv Efron",
      "Yossi Matias",
      "Greg Corrado",
      "Krish Eswaran",
      "Shruthi Prabhakara",
      "Shravya Shetty",
      "Gautam Prasad"
    ],
    "published": "2024-11-11",
    "abstract": "Supporting the health and well-being of dynamic populations around the world requires governmental agencies, organizations and researchers to understand and reason over complex relationships between human behavior and local contexts in order to identify high-risk groups and strategically allocate limited resources. Traditional approaches to these classes of problems often entail developing manually curated, task-specific features and models to represent human behavior and the natural and built environment, which can be challenging to adapt to new, or even, related tasks. To address this, we introduce a Population Dynamics Foundation Model (PDFM) that aims to capture the relationships between diverse data modalities and is applicable to a broad range of geospatial tasks. We first construct a geo-indexed dataset for postal codes and counties across the United States, capturing rich aggregated information on human behavior from maps, busyness, and aggregated search trends, and environmental factors such as weather and air quality. We then model this data and the complex relationships between locations using a graph neural network, producing embeddings that can be adapted to a wide range of downstream tasks using relatively simple models. We evaluate the effectiveness of our approach by benchmarking it on 27 downstream tasks spanning three distinct domains: health indicators, socioeconomic factors, and environmental measurements. The approach achieves state-of-the-art performance on all 27 geospatial interpolation tasks, and on 25 out of the 27 extrapolation and super-resolution tasks. We combined the PDFM with a state-of-the-art forecasting foundation model, TimesFM, to predict unemployment and poverty, achieving performance that surpasses fully supervised forecasting. The full set of embeddings and sample code are publicly available for researchers.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "MapSAM: Adapting Segment Anything Model for Automated Feature Detection in Historical Maps",
    "url": "http://arxiv.org/abs/2411.06971v1",
    "authors": [
      "Xue Xia",
      "Daiwei Zhang",
      "Wenxuan Song",
      "Wei Huang",
      "Lorenz Hurni"
    ],
    "published": "2024-11-11",
    "abstract": "Automated feature detection in historical maps can significantly accelerate the reconstruction of the geospatial past. However, this process is often constrained by the time-consuming task of manually digitizing sufficient high-quality training data. The emergence of visual foundation models, such as the Segment Anything Model (SAM), offers a promising solution due to their remarkable generalization capabilities and rapid adaptation to new data distributions. Despite this, directly applying SAM in a zero-shot manner to historical map segmentation poses significant challenges, including poor recognition of certain geospatial features and a reliance on input prompts, which limits its ability to be fully automated. To address these challenges, we introduce MapSAM, a parameter-efficient fine-tuning strategy that adapts SAM into a prompt-free and versatile solution for various downstream historical map segmentation tasks. Specifically, we employ Weight-Decomposed Low-Rank Adaptation (DoRA) to integrate domain-specific knowledge into the image encoder. Additionally, we develop an automatic prompt generation process, eliminating the need for manual input. We further enhance the positional prompt in SAM, transforming it into a higher-level positional-semantic prompt, and modify the cross-attention mechanism in the mask decoder with masked attention for more effective feature aggregation. The proposed MapSAM framework demonstrates promising performance across two distinct historical map segmentation tasks: one focused on linear features and the other on areal features. Experimental results show that it adapts well to various features, even when fine-tuned with extremely limited data (e.g. 10 shots).",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Multimodal Contrastive Learning of Urban Space Representations from POI Data",
    "url": "http://arxiv.org/abs/2411.06229v1",
    "authors": [
      "Xinglei Wang",
      "Tao Cheng",
      "Stephen Law",
      "Zichao Zeng",
      "Lu Yin",
      "Junyuan Liu"
    ],
    "published": "2024-11-09",
    "abstract": "Existing methods for learning urban space representations from Point-of-Interest (POI) data face several limitations, including issues with geographical delineation, inadequate spatial information modelling, underutilisation of POI semantic attributes, and computational inefficiencies. To address these issues, we propose CaLLiPer (Contrastive Language-Location Pre-training), a novel representation learning model that directly embeds continuous urban spaces into vector representations that can capture the spatial and semantic distribution of urban environment. This model leverages a multimodal contrastive learning objective, aligning location embeddings with textual POI descriptions, thereby bypassing the need for complex training corpus construction and negative sampling. We validate CaLLiPer's effectiveness by applying it to learning urban space representations in London, UK, where it demonstrates 5-15% improvement in predictive performance for land use classification and socioeconomic mapping tasks compared to state-of-the-art methods. Visualisations of the learned representations further illustrate our model's advantages in capturing spatial variations in urban semantics with high accuracy and fine resolution. Additionally, CaLLiPer achieves reduced training time, showcasing its efficiency and scalability. This work provides a promising pathway for scalable, semantically rich urban space representation learning that can support the development of geospatial foundation models. The implementation code is available at https://github.com/xlwang233/CaLLiPer.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2410.22629v3",
    "authors": [
      "Ziyang Gong",
      "Zhixiang Wei",
      "Di Wang",
      "Xiaoxing Hu",
      "Xianzheng Ma",
      "Hongruixuan Chen",
      "Yuru Jia",
      "Yupeng Deng",
      "Zhenming Ji",
      "Xiangwei Zhu",
      "Xue Yang",
      "Naoto Yokoya",
      "Jing Zhang",
      "Bo Du",
      "Junchi Yan",
      "Liangpei Zhang"
    ],
    "published": "2024-10-30",
    "abstract": "The field of Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. Despite the substantial domain gaps in RS images that are characterized by variabilities such as location, wavelength, and sensor type, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies targeting the RSDG issue, especially for semantic segmentation tasks, where existing models are developed for specific unknown domains, struggling with issues of underfitting on other unknown scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 cross-domain settings across various regions, spectral bands, platforms, and climates, providing a comprehensive framework for testing the generalizability of future RSDG models. Extensive experiments on this benchmark demonstrate the superiority of CrossEarth over existing state-of-the-art methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery",
    "url": "http://arxiv.org/abs/2410.19965v1",
    "authors": [
      "Philipe Dias",
      "Aristeidis Tsaris",
      "Jordan Bowman",
      "Abhishek Potnis",
      "Jacob Arndt",
      "H. Lexie Yang",
      "Dalton Lunga"
    ],
    "published": "2024-10-25",
    "abstract": "While the pretraining of Foundation Models (FMs) for remote sensing (RS) imagery is on the rise, models remain restricted to a few hundred million parameters. Scaling models to billions of parameters has been shown to yield unprecedented benefits including emergent abilities, but requires data scaling and computing resources typically not available outside industry R&D labs. In this work, we pair high-performance computing resources including Frontier supercomputer, America's first exascale system, and high-resolution optical RS data to pretrain billion-scale FMs. Our study assesses performance of different pretrained variants of vision Transformers across image classification, semantic segmentation and object detection benchmarks, which highlight the importance of data scaling for effective model scaling. Moreover, we discuss construction of a novel TIU pretraining dataset, model initialization, with data and pretrained models intended for public release. By discussing technical challenges and details often lacking in the related literature, this work is intended to offer best practices to the geospatial community toward efficient training and benchmarking of larger FMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "On the Generalizability of Foundation Models for Crop Type Mapping",
    "url": "http://arxiv.org/abs/2409.09451v4",
    "authors": [
      "Yi-Chia Chang",
      "Adam J. Stewart",
      "Favyen Bastani",
      "Piper Wolters",
      "Shreya Kannan",
      "George R. Huber",
      "Jingtong Wang",
      "Arindam Banerjee"
    ],
    "published": "2024-09-14",
    "abstract": "Foundation models pre-trained using self-supervised learning have shown powerful transfer learning capabilities on various downstream tasks, including language understanding, text generation, and image recognition. The Earth observation (EO) field has produced several foundation models pre-trained directly on multispectral satellite imagery for applications like precision agriculture, wildfire and drought monitoring, and natural disaster response. However, few studies have investigated the ability of these models to generalize to new geographic locations, and potential concerns of geospatial bias -- models trained on data-rich developed nations not transferring well to data-scarce developing nations -- remain. We evaluate three popular EO foundation models, SSL4EO-S12, SatlasPretrain, and ImageNet, on five crop classification datasets across five continents. Results show that pre-trained weights designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform general pre-trained weights like ImageNet. While only 100 labeled images are sufficient for achieving high overall accuracy, 900 images are required to mitigate class imbalance and improve average accuracy.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability",
    "url": "http://arxiv.org/abs/2409.00489v1",
    "authors": [
      "Chia-Yu Hsu",
      "Wenwen Li",
      "Sizhe Wang"
    ],
    "published": "2024-08-31",
    "abstract": "Research on geospatial foundation models (GFMs) has become a trending topic in geospatial artificial intelligence (AI) research due to their potential for achieving high generalizability and domain adaptability, reducing model training costs for individual researchers. Unlike large language models, such as ChatGPT, constructing visual foundation models for image analysis, particularly in remote sensing, encountered significant challenges such as formulating diverse vision tasks into a general problem framework. This paper evaluates the recently released NASA-IBM GFM Prithvi for its predictive performance on high-level image analysis tasks across multiple benchmark datasets. Prithvi was selected because it is one of the first open-source GFMs trained on time-series of high-resolution remote sensing imagery. A series of experiments were designed to assess Prithvi's performance as compared to other pre-trained task-specific AI models in geospatial image analysis. New strategies, including band adaptation, multi-scale feature generation, and fine-tuning techniques, are introduced and integrated into an image analysis pipeline to enhance Prithvi's domain adaptation capability and improve model performance. In-depth analyses reveal Prithvi's strengths and weaknesses, offering insights for both improving Prithvi and developing future visual foundation models for geospatial tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Evaluating the Effectiveness of Large Language Models in Representing and Understanding Movement Trajectories",
    "url": "http://arxiv.org/abs/2409.00335v1",
    "authors": [
      "Yuhan Ji",
      "Song Gao"
    ],
    "published": "2024-08-31",
    "abstract": "This research focuses on assessing the ability of AI foundation models in representing the trajectories of movements. We utilize one of the large language models (LLMs) (i.e., GPT-J) to encode the string format of trajectories and then evaluate the effectiveness of the LLM-based representation for trajectory data analysis. The experiments demonstrate that while the LLM-based embeddings can preserve certain trajectory distance metrics (i.e., the correlation coefficients exceed 0.74 between the Cosine distance derived from GPT-J embeddings and the Hausdorff and Dynamic Time Warping distances on raw trajectories), challenges remain in restoring numeric values and retrieving spatial neighbors in movement trajectory analytics. In addition, the LLMs can understand the spatiotemporal dependency contained in trajectories and have good accuracy in location prediction tasks. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using LLMs.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Self-Supervised Representation Learning for Geospatial Objects: A Survey",
    "url": "http://arxiv.org/abs/2408.12133v2",
    "authors": [
      "Yile Chen",
      "Weiming Huang",
      "Kaiqi Zhao",
      "Yue Jiang",
      "Gao Cong"
    ],
    "published": "2024-08-22",
    "abstract": "The proliferation of various data sources in urban and territorial environments has significantly facilitated the development of geospatial artificial intelligence (GeoAI) across a wide range of geospatial applications. However, geospatial data, which is inherently linked to geospatial objects, often exhibits data heterogeneity that necessitates specialized fusion and representation strategies while simultaneously being inherently sparse in labels for downstream tasks. Consequently, there is a growing demand for techniques that can effectively leverage geospatial data without heavy reliance on task-specific labels and model designs. This need aligns with the principles of self-supervised learning (SSL), which has garnered increasing attention for its ability to learn effective and generalizable representations directly from data without extensive labeled supervision. This paper presents a comprehensive and up-to-date survey of SSL techniques specifically applied to or developed for geospatial objects in three primary vector geometric types: Point, Polyline, and Polygon. We systematically categorize various SSL techniques into predictive and contrastive methods, and analyze their adaptation to different data types for representation learning across various downstream tasks. Furthermore, we examine the emerging trends in SSL for geospatial objects, particularly the gradual advancements towards geospatial foundation models. Finally, we discuss key challenges in current research and outline promising directions for future investigation. By offering a structured analysis of existing studies, this paper aims to inspire continued progress in integrating SSL with geospatial objects, and the development of geospatial foundation models in a longer term.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Causally Informed Pretraining Approach for Multimodal Foundation Models: Applications in Remote Sensing",
    "url": "http://arxiv.org/abs/2407.19660v3",
    "authors": [
      "Praveen Ravirathinam",
      "Ankush Khandelwal",
      "Rahul Ghosh",
      "Vipin Kumar"
    ],
    "published": "2024-07-29",
    "abstract": "Self-supervised learning has emerged as a powerful paradigm for pretraining foundation models using large-scale data. Existing pretraining approaches predominantly rely on masked reconstruction or next-token prediction strategies, demonstrating strong performance across various downstream tasks, including geoscience applications. However, these approaches do not fully capture the causal interplay between different geospatial and environmental variables. To address this limitation, we propose Causally Informed Variable-Step Forecasting (CI-VSF), a novel pretraining task that models forecasting as a conditional generation task, where driver variables (e.g., weather) inform the prediction of response variables (e.g., satellite imagery). We demonstrate that pretraining in such a fashion leads to enhanced performance when finetuned on both prediction (e.g., crop mapping, missing image prediction, soil moisture estimation) and forecasting (e.g., future image forecasting, soil moisture forecasting) downstream tasks when compared to other pretraining approaches. While we use remote sensing as our main application to demonstrate the efficacy of our proposed pretraining strategy over existing paradigms, it is applicable to any domain that involves known causal relationships amongst a set of variables.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Fine-tuning of Geospatial Foundation Models for Aboveground Biomass Estimation",
    "url": "http://arxiv.org/abs/2406.19888v1",
    "authors": [
      "Michal Muszynski",
      "Levente Klein",
      "Ademir Ferreira da Silva",
      "Anjani Prasad Atluri",
      "Carlos Gomes",
      "Daniela Szwarcman",
      "Gurkanwar Singh",
      "Kewen Gu",
      "Maciel Zortea",
      "Naomi Simumba",
      "Paolo Fraccaro",
      "Shraddha Singh",
      "Steve Meliksetian",
      "Campbell Watson",
      "Daiki Kimura",
      "Harini Srinivasan"
    ],
    "published": "2024-06-28",
    "abstract": "Global vegetation structure mapping is critical for understanding the global carbon cycle and maximizing the efficacy of nature-based carbon sequestration initiatives. Moreover, vegetation structure mapping can help reduce the impacts of climate change by, for example, guiding actions to improve water security, increase biodiversity and reduce flood risk. Global satellite measurements provide an important set of observations for monitoring and managing deforestation and degradation of existing forests, natural forest regeneration, reforestation, biodiversity restoration, and the implementation of sustainable agricultural practices. In this paper, we explore the effectiveness of fine-tuning of a geospatial foundation model to estimate above-ground biomass (AGB) using space-borne data collected across different eco-regions in Brazil. The fine-tuned model architecture consisted of a Swin-B transformer as the encoder (i.e., backbone) and a single convolutional layer for the decoder head. All results were compared to a U-Net which was trained as the baseline model Experimental results of this sparse-label prediction task demonstrate that the fine-tuned geospatial foundation model with a frozen encoder has comparable performance to a U-Net trained from scratch. This is despite the fine-tuned model having 13 times less parameters requiring optimization, which saves both time and compute resources. Further, we explore the transfer-learning capabilities of the geospatial foundation models by fine-tuning on satellite imagery with sparse labels from different eco-regions in Brazil.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Geode: A Zero-shot Geospatial Question-Answering Agent with Explicit Reasoning and Precise Spatio-Temporal Retrieval",
    "url": "http://arxiv.org/abs/2407.11014v1",
    "authors": [
      "Devashish Vikas Gupta",
      "Azeez Syed Ali Ishaqui",
      "Divya Kiran Kadiyala"
    ],
    "published": "2024-06-26",
    "abstract": "Large language models (LLMs) have shown promising results in learning and contextualizing information from different forms of data. Recent advancements in foundational models, particularly those employing self-attention mechanisms, have significantly enhanced our ability to comprehend the semantics of diverse data types. One such area that could highly benefit from multi-modality is in understanding geospatial data, which inherently has multiple modalities. However, current Natural Language Processing (NLP) mechanisms struggle to effectively address geospatial queries. Existing pre-trained LLMs are inadequately equipped to meet the unique demands of geospatial data, lacking the ability to retrieve precise spatio-temporal data in real-time, thus leading to significantly reduced accuracy in answering complex geospatial queries. To address these limitations, we introduce Geode--a pioneering system designed to tackle zero-shot geospatial question-answering tasks with high precision using spatio-temporal data retrieval. Our approach represents a significant improvement in addressing the limitations of current LLM models, demonstrating remarkable improvement in geospatial question-answering abilities compared to existing state-of-the-art pre-trained models.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Evaluating and Benchmarking Foundation Models for Earth Observation and Geospatial AI",
    "url": "http://arxiv.org/abs/2406.18295v1",
    "authors": [
      "Nikolaos Dionelis",
      "Casper Fibaek",
      "Luke Camilleri",
      "Andreas Luyts",
      "Jente Bosmans",
      "Bertrand Le Saux"
    ],
    "published": "2024-06-26",
    "abstract": "When we are primarily interested in solving several problems jointly with a given prescribed high performance accuracy for each target application, then Foundation Models should for most cases be used rather than problem-specific models. We focus on the specific Computer Vision application of Foundation Models for Earth Observation (EO) and geospatial AI. These models can solve important problems we are tackling, including for example land cover classification, crop type mapping, flood segmentation, building density estimation, and road regression segmentation. In this paper, we show that for a limited number of labelled data, Foundation Models achieve improved performance compared to problem-specific models. In this work, we also present our proposed evaluation benchmark for Foundation Models for EO. Benchmarking the generalization performance of Foundation Models is important as it has become difficult to standardize a fair comparison across the many different models that have been proposed recently. We present the results using our evaluation benchmark for EO Foundation Models and show that Foundation Models are label efficient in the downstream tasks and help us solve problems we are tackling in EO and remote sensing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "GFM4MPM: Towards Geospatial Foundation Models for Mineral Prospectivity Mapping",
    "url": "http://arxiv.org/abs/2406.12756v1",
    "authors": [
      "Angel Daruna",
      "Vasily Zadorozhnyy",
      "Georgina Lukoczki",
      "Han-Pang Chiu"
    ],
    "published": "2024-06-18",
    "abstract": "Machine Learning (ML) for Mineral Prospectivity Mapping (MPM) remains a challenging problem as it requires the analysis of associations between large-scale multi-modal geospatial data and few historical mineral commodity observations (positive labels). Recent MPM works have explored Deep Learning (DL) as a modeling tool with more representation capacity. However, these overparameterized methods may be more prone to overfitting due to their reliance on scarce labeled data. While a large quantity of unlabeled geospatial data exists, no prior MPM works have considered using such information in a self-supervised manner. Our MPM approach uses a masked image modeling framework to pretrain a backbone neural network in a self-supervised manner using unlabeled geospatial data alone. After pretraining, the backbone network provides feature extraction for downstream MPM tasks. We evaluated our approach alongside existing methods to assess mineral prospectivity of Mississippi Valley Type (MVT) and Clastic-Dominated (CD) Lead-Zinc deposits in North America and Australia. Our results demonstrate that self-supervision promotes robustness in learned features, improving prospectivity predictions. Additionally, we leverage explainable artificial intelligence techniques to demonstrate that individual predictions can be interpreted from a geological perspective.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Towards Vision-Language Geo-Foundation Model: A Survey",
    "url": "http://arxiv.org/abs/2406.09385v1",
    "authors": [
      "Yue Zhou",
      "Litong Feng",
      "Yiping Ke",
      "Xue Jiang",
      "Junchi Yan",
      "Xue Yang",
      "Wayne Zhang"
    ],
    "published": "2024-06-13",
    "abstract": "Vision-Language Foundation Models (VLFMs) have made remarkable progress on various multimodal tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding. However, most methods rely on training with general image datasets, and the lack of geospatial data leads to poor performance on earth observation. Numerous geospatial image-text pair datasets and VLFMs fine-tuned on them have been proposed recently. These new approaches aim to leverage large-scale, multimodal geospatial data to build versatile intelligent models with diverse geo-perceptive capabilities, which we refer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper thoroughly reviews VLGFMs, summarizing and analyzing recent developments in the field. In particular, we introduce the background and motivation behind the rise of VLGFMs, highlighting their unique research significance. Then, we systematically summarize the core technologies employed in VLGFMs, including data construction, model architectures, and applications of various multimodal geospatial tasks. Finally, we conclude with insights, issues, and discussions regarding future research directions. To the best of our knowledge, this is the first comprehensive literature review of VLGFMs. We keep tracing related works at https://github.com/zytx121/Awesome-VLGFM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SeeFar: Satellite Agnostic Multi-Resolution Dataset for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2406.06776v1",
    "authors": [
      "James Lowman",
      "Kelly Liu Zheng",
      "Roydon Fraser",
      "Jesse Van Griensven The",
      "Mojtaba Valipour"
    ],
    "published": "2024-06-10",
    "abstract": "SeeFar is an evolving collection of multi-resolution satellite images from public and commercial satellites. We specifically curated this dataset for training geospatial foundation models, unconstrained by satellite type. In recent years, advances in technology have made satellite imagery more accessible than ever. More earth-observing satellites have been launched in the last five years than in the previous fifty. Modern commercial satellites now offer up to 100 times the spatial resolution of public access satellites. However, the high cost and limited historical availability of commercial satellite imagery is a barrier to the training of foundational models, impacting what images can be used during inference. The SeeFar dataset represents a step towards training models that are satellite-agnostic by combining multi-resolution commercial and public access pre-processed images. This will enable users to utilize historical data alongside higher-resolution, more expensive satellite imagery, offering greater flexibility during inference. To achieve this, we describe a process for standardizing data from diverse satellite sources, normalizing different data formats, and aligning spectral bands to enhance interoperability. The SeeFar dataset includes images at a resolution of 384x384 pixels, spanning four spectral bands (Blue, Green, Red, and Near-Infrared) and expanding spatial resolutions (starting with 30, 10, 1.5, and 1.0 meters), all in cloud-optimized GeoTIFF format. It also provides consistent and comprehensive metadata to enhance data transparency and reliability. By aggregating data from multiple sources, SeeFar makes processed and consistent satellite data accessible to a wider range of users - from researchers to policymakers - fostering competition and innovation in satellite imagery analysis. The dataset is available at \\url{coastalcarbon.ai/seefar}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery",
    "url": "http://arxiv.org/abs/2405.02512v2",
    "authors": [
      "Yohei Nakayama",
      "Jiawei Su",
      "Luis M. Pazos-Out\u00f3n"
    ],
    "published": "2024-05-03",
    "abstract": "Recent advancements in foundation models have significantly impacted various fields, including natural language processing, computer vision, and multi-modal tasks. One area that stands to benefit greatly is Earth observation, where these models can efficiently process large-scale, unlabeled geospatial data. In this work we extend the SwinMAE model to integrate temporal information for satellite time-series data. The architecture employs a hierarchical 3D Masked Autoencoder (MAE) with Video Swin Transformer blocks to effectively capture multi-scale spatio-temporal dependencies in satellite imagery. To enhance transfer learning, we incorporate both encoder and decoder pretrained weights, along with skip connections to preserve scale-specific information. This forms an architecture similar to SwinUNet with an additional temporal component. Our approach shows significant performance improvements over existing state-of-the-art foundation models for all the evaluated downstream tasks: land cover segmentation, building density prediction, flood mapping, wildfire scar mapping and multi-temporal crop segmentation. Particularly, in the land cover segmentation task of the PhilEO Bench dataset, it outperforms other geospatial foundation models with a 10.4% higher accuracy.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "NLP-enabled Trajectory Map-matching in Urban Road Networks using a Transformer-based Encoder-decoder",
    "url": "http://arxiv.org/abs/2404.12460v4",
    "authors": [
      "Sevin Mohammadi",
      "Andrew W. Smyth"
    ],
    "published": "2024-04-18",
    "abstract": "Vehicular trajectory data from geolocation telematics is vital for analyzing urban mobility patterns. Map-matching aligns noisy, sparsely sampled GPS trajectories with digital road maps to reconstruct accurate vehicle paths. Traditional methods rely on geometric proximity, topology, and shortest-path heuristics, but they overlook two key factors: (1) drivers may prefer routes based on local road characteristics rather than shortest paths, revealing learnable shared preferences, and (2) GPS noise varies spatially due to multipath effects. These factors can reduce the effectiveness of conventional methods in complex scenarios and increase the effort required for heuristic-based implementations. This study introduces a data-driven, deep learning-based map-matching framework, formulating the task as machine translation, inspired by NLP. Specifically, a transformer-based encoder-decoder model learns contextual representations of noisy GPS points to infer trajectory behavior and road structures in an end-to-end manner. Trained on large-scale trajectory data, the method improves path estimation accuracy. Experiments on synthetic trajectories show that this approach outperforms conventional methods by integrating contextual awareness. Evaluation on real-world GPS traces from Manhattan, New York, achieves 75% accuracy in reconstructing navigated routes. These results highlight the effectiveness of transformers in capturing drivers' trajectory behaviors, spatial dependencies, and noise patterns, offering a scalable, robust solution for map-matching. This work contributes to advancing trajectory-driven foundation models for geospatial modeling and urban mobility applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Pretraining Billion-scale Geospatial Foundational Models on Frontier",
    "url": "http://arxiv.org/abs/2404.11706v1",
    "authors": [
      "Aristeidis Tsaris",
      "Philipe Ambrozio Dias",
      "Abhishek Potnis",
      "Junqi Yin",
      "Feiyi Wang",
      "Dalton Lunga"
    ],
    "published": "2024-04-17",
    "abstract": "As AI workloads increase in scope, generalization capability becomes challenging for small task-specific models and their demand for large amounts of labeled training samples increases. On the contrary, Foundation Models (FMs) are trained with internet-scale unlabeled data via self-supervised learning and have been shown to adapt to various tasks with minimal fine-tuning. Although large FMs have demonstrated significant impact in natural language processing and computer vision, efforts toward FMs for geospatial applications have been restricted to smaller size models, as pretraining larger models requires very large computing resources equipped with state-of-the-art hardware accelerators. Current satellite constellations collect 100+TBs of data a day, resulting in images that are billions of pixels and multimodal in nature. Such geospatial data poses unique challenges opening up new opportunities to develop FMs. We investigate billion scale FMs and HPC training profiles for geospatial applications by pretraining on publicly available data. We studied from end-to-end the performance and impact in the solution by scaling the model size. Our larger 3B parameter size model achieves up to 30% improvement in top1 scene classification accuracy when comparing a 100M parameter model. Moreover, we detail performance experiments on the Frontier supercomputer, America's first exascale system, where we study different model and data parallel approaches using PyTorch's Fully Sharded Data Parallel library. Specifically, we study variants of the Vision Transformer architecture (ViT), conducting performance analysis for ViT models with size up to 15B parameters. By discussing throughput and performance bottlenecks under different parallelism configurations, we offer insights on how to leverage such leadership-class HPC resources when developing large models for geospatial imagery applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2404.01260v1",
    "authors": [
      "Boran Han",
      "Shuai Zhang",
      "Xingjian Shi",
      "Markus Reichstein"
    ],
    "published": "2024-04-01",
    "abstract": "In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa",
    "url": "http://arxiv.org/abs/2403.06860v2",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Mukhtar Opeyemi Yusuf",
      "Kobby Panford-Quainoo",
      "Arnu Pretorius"
    ],
    "published": "2024-03-11",
    "abstract": "Desert locust swarms present a major threat to agriculture and food security. Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures. We curated a dataset from the United Nations Food and Agriculture Organization's (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images. Our approach employed custom deep learning models (three-dimensional and LSTM-based recurrent convolutional networks), along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023. These models notably outperformed existing baselines, with the Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding from our research is that multi-spectral earth observation images alone are sufficient for effective locust breeding ground prediction without the need to explicitly incorporate climatic or environmental features.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2403.02059v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Viktoria Moor",
      "Romeo Kienzler",
      "Thomas Brunschwiler"
    ],
    "published": "2024-03-04",
    "abstract": "Image retrieval enables an efficient search through vast amounts of satellite imagery and returns similar images to a query. Deep learning models can identify images across various semantic concepts without the need for annotations. This work proposes to use Geospatial Foundation Models, like Prithvi, for remote sensing image retrieval with multiple benefits: i) the models encode multi-spectral satellite data and ii) generalize without further fine-tuning. We introduce two datasets to the retrieval task and observe a strong performance: Prithvi processes six bands and achieves a mean Average Precision of 97.62% on BigEarthNet-43 and 44.51% on ForestNet-12, outperforming other RGB-based models. Further, we evaluate three compression methods with binarized embeddings balancing retrieval speed and accuracy. They match the retrieval speed of much shorter hash codes while maintaining the same accuracy as floating-point embeddings but with a 32-fold compression. The code is available at https://github.com/IBM/remote-sensing-image-retrieval.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Large Language Models are Geographically Biased",
    "url": "http://arxiv.org/abs/2402.02680v2",
    "authors": [
      "Rohin Manvi",
      "Samar Khanna",
      "Marshall Burke",
      "David Lobell",
      "Stefano Ermon"
    ],
    "published": "2024-02-05",
    "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\u03c1$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\u03c1$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping",
    "url": "http://arxiv.org/abs/2401.08787v1",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Sizhe Wang",
      "Yezhou Yang",
      "Hyunho Lee",
      "Anna Liljedahl",
      "Chandi Witharana",
      "Yili Yang",
      "Brendan M. Rogers",
      "Samantha T. Arundel",
      "Matthew B. Jones",
      "Kenton McHenry",
      "Patricia Solis"
    ],
    "published": "2024-01-16",
    "abstract": "This paper assesses trending AI foundation models, especially emerging computer vision foundation models and their performance in natural landscape feature segmentation. While the term foundation model has quickly garnered interest from the geospatial domain, its definition remains vague. Hence, this paper will first introduce AI foundation models and their defining characteristics. Built upon the tremendous success achieved by Large Language Models (LLMs) as the foundation models for language tasks, this paper discusses the challenges of building foundation models for geospatial artificial intelligence (GeoAI) vision tasks. To evaluate the performance of large AI vision models, especially Meta's Segment Anything Model (SAM), we implemented different instance segmentation pipelines that minimize the changes to SAM to leverage its power as a foundation model. A series of prompt strategies was developed to test SAM's performance regarding its theoretical upper bound of predictive accuracy, zero-shot performance, and domain adaptability through fine-tuning. The analysis used two permafrost feature datasets, ice-wedge polygons and retrogressive thaw slumps because (1) these landform features are more challenging to segment than manmade features due to their complicated formation mechanisms, diverse forms, and vague boundaries; (2) their presence and changes are important indicators for Arctic warming and climate change. The results show that although promising, SAM still has room for improvement to support AI-augmented terrain mapping. The spatial and domain generalizability of this finding is further validated using a more general dataset EuroCrop for agricultural field mapping. Finally, we discuss future research directions that strengthen SAM's applicability in challenging geospatial domains.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2512.03257v1",
    "authors": [
      "Mark Moussa",
      "Andre Williams",
      "Seth Roffe",
      "Douglas Morton"
    ],
    "published": "2025-12-02",
    "abstract": "Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.\n  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.\n  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Fusion or Confusion? Assessing the impact of visible-thermal image fusion for automated wildlife detection",
    "url": "http://arxiv.org/abs/2511.22768v2",
    "authors": [
      "Camille Dionne-Pierre",
      "Samuel Foucher",
      "J\u00e9r\u00f4me Th\u00e9au",
      "J\u00e9r\u00f4me Lema\u00eetre",
      "Patrick Charbonneau",
      "Maxime Brousseau",
      "Mathieu Varin"
    ],
    "published": "2025-11-27",
    "abstract": "Efficient wildlife monitoring methods are necessary for biodiversity conservation and management. The combination of remote sensing, aerial imagery and deep learning offer promising opportunities to renew or improve existing survey methods. The complementary use of visible (VIS) and thermal infrared (TIR) imagery can add information compared to a single-source image and improve results in an automated detection context. However, the alignment and fusion process can be challenging, especially since visible and thermal images usually have different fields of view (FOV) and spatial resolutions. This research presents a case study on the great blue heron (Ardea herodias) to evaluate the performances of synchronous aerial VIS and TIR imagery to automatically detect individuals and nests using a YOLO11n model. Two VIS-TIR fusion methods were tested and compared: an early fusion approach and a late fusion approach, to determine if the addition of the TIR image gives any added value compared to a VIS-only model. VIS and TIR images were automatically aligned using a deep learning model. A principal component analysis fusion method was applied to VIS-TIR image pairs to form the early fusion dataset. A classification and regression tree was used to process the late fusion dataset, based on the detection from the VIS-only and TIR-only trained models. Across all classes, both late and early fusion improved the F1 score compared to the VIS-only model. For the main class, occupied nest, the late fusion improved the F1 score from 90.2 (VIS-only) to 93.0%. This model was also able to identify false positives from both sources with 90% recall. Although fusion methods seem to give better results, this approach comes with a limiting TIR FOV and alignment constraints that eliminate data. Using an aircraft-mounted very high-resolution visible sensor could be an interesting option for operationalizing surveys.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Hierarchical Semi-Supervised Active Learning for Remote Sensing",
    "url": "http://arxiv.org/abs/2511.18058v2",
    "authors": [
      "Wei Huang",
      "Zhitong Xiong",
      "Chenying Liu",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-22",
    "abstract": "The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be publicly available.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2511.16322v1",
    "authors": [
      "Ching-Heng Cheng",
      "Chih-Chung Hsu"
    ],
    "published": "2025-11-20",
    "abstract": "Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection",
    "url": "http://arxiv.org/abs/2511.16143v1",
    "authors": [
      "Quanqing Ma",
      "Jiaen Chen",
      "Peng Wang",
      "Yao Zheng",
      "Qingzhan Zhao",
      "Yuchen Zheng"
    ],
    "published": "2025-11-20",
    "abstract": "Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "SpectralTrain: A Universal Framework for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2511.16084v1",
    "authors": [
      "Meihua Zhou",
      "Liping Yu",
      "Jiawei Cai",
      "Wai Kin Fung",
      "Ruiguo Hu",
      "Jiarui Zhao",
      "Wenzhuo Liu",
      "Nan Wan"
    ],
    "published": "2025-11-20",
    "abstract": "Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images",
    "url": "http://arxiv.org/abs/2511.13552v1",
    "authors": [
      "Sining Chen",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-17",
    "abstract": "Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Mapping the Vanishing and Transformation of Urban Villages in China",
    "url": "http://arxiv.org/abs/2511.13507v1",
    "authors": [
      "Wenyu Zhang",
      "Yao Tong",
      "Yiqiu Liu",
      "Rui Cao"
    ],
    "published": "2025-11-17",
    "abstract": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)",
    "url": "http://arxiv.org/abs/2511.11882v2",
    "authors": [
      "Simon Durand",
      "Samuel Foucher",
      "Alexandre Delplanque",
      "Jo\u00eblle Taillon",
      "J\u00e9r\u00f4me Th\u00e9au"
    ],
    "published": "2025-11-14",
    "abstract": "Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production",
    "url": "http://arxiv.org/abs/2511.11880v1",
    "authors": [
      "David Montero",
      "Miguel D. Mahecha",
      "Francesco Martinuzzi",
      "C\u00e9sar Aybar",
      "Anne Klosterhalfen",
      "Alexander Knohl",
      "Jes\u00fas Anaya",
      "Clemens Mosig",
      "Sebastian Wieneke"
    ],
    "published": "2025-11-14",
    "abstract": "Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "PGDM: Physically guided diffusion model for land surface temperature downscaling",
    "url": "http://arxiv.org/abs/2511.05964v1",
    "authors": [
      "Huanyu Zhang",
      "Bo-Hui Tang",
      "Tian Hu",
      "Yun Jiang",
      "Zhao-Liang Li"
    ],
    "published": "2025-11-08",
    "abstract": "Land surface temperature (LST) is a fundamental parameter in thermal infrared remote sensing, while current LST products are often constrained by the trade-off between spatial and temporal resolutions. To mitigate this limitation, numerous studies have been conducted to enhance the resolutions of LST data, with a particular emphasis on the spatial dimension (commonly known as LST downscaling). Nevertheless, a comprehensive benchmark dataset tailored for this task remains scarce. In addition, existing downscaling models face challenges related to accuracy, practical usability, and the capability to self-evaluate their uncertainties. To overcome these challenges, this study first compiled three representative datasets, including one dataset over mainland China containing 22,909 image patches for model training and evaluation, as well as two datasets covering 40 heterogeneous regions worldwide for external evaluation. Subsequently, grounded in the surface energy balance (SEB)-based geophysical reasoning, we proposed the physically guided diffusion model (PGDM) for LST downscaling. In this framework, the downscaling task was formulated as an inference problem, aiming to sample from the posterior distribution of high-spatial-resolution (HR) LST conditioned on low-spatial-resolution (LR) LST observations and a suite of HR geophysical priors. Comprehensive evaluations demonstrate the effectiveness of PGDM, which generates high-quality downscaling results and outperforms existing representative interpolation, kernel-driven, hybrid, and deep learning approaches. Finally, by exploiting the inherent stochasticity of PGDM, the scene-level standard deviation of multiple generations was computed, revealing a strong positive linear correlation with the actual downscaling error...",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data",
    "url": "http://arxiv.org/abs/2511.04304v1",
    "authors": [
      "Robin Spanier",
      "Thorsten Hoeser",
      "Claudia Kuenzer"
    ],
    "published": "2025-11-06",
    "abstract": "The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning",
    "url": "http://arxiv.org/abs/2511.03004v1",
    "authors": [
      "Dakota Hester",
      "Vitor S. Martins",
      "Lucas B. Ferreira",
      "Thainara M. A. Lima"
    ],
    "published": "2025-11-04",
    "abstract": "Deep learning semantic segmentation methods have shown promising performance for very high 1-m resolution land cover classification, but the challenge of collecting large volumes of representative training data creates a significant barrier to widespread adoption of such models for meter-scale land cover mapping over large areas. In this study, we present a novel label-efficient approach for statewide 1-m land cover classification using only 1,000 annotated reference image patches with self-supervised deep learning. We use the \"Bootstrap Your Own Latent\" pre-training strategy with a large amount of unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to pre-train a ResNet-101 convolutional encoder. The learned encoder weights were subsequently transferred into multiple deep semantic segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then fine-tuned using very small training dataset sizes with cross-validation (250, 500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall accuracy and 75.58% macro F1 score using an ensemble of the best performing U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more than 123 billion pixels over the state of Mississippi, USA. Detailed qualitative and quantitative analysis revealed accurate mapping of open water and forested areas, while highlighting challenges in accurate delineation between cropland, herbaceous, and barren land cover types. These results show that self-supervised learning is an effective strategy for reducing the need for large volumes of manually annotated data, directly addressing a major limitation to high spatial resolution land cover mapping at scale.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "ResNet"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Automatic Extraction of Road Networks by using Teacher-Student Adaptive Structural Deep Belief Network and Its Application to Landslide Disaster",
    "url": "http://arxiv.org/abs/2511.05567v1",
    "authors": [
      "Shin Kamada",
      "Takumi Ichimura"
    ],
    "published": "2025-11-04",
    "abstract": "An adaptive structural learning method of Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) has been developed as one of prominent deep learning models. The neuron generation-annihilation algorithm in RBM and layer generation algorithm in DBN make an optimal network structure for given input during the learning. In this paper, our model is applied to an automatic recognition method of road network system, called RoadTracer. RoadTracer can generate a road map on the ground surface from aerial photograph data. A novel method of RoadTracer using the Teacher-Student based ensemble learning model of Adaptive DBN is proposed, since the road maps contain many complicated features so that a model with high representation power to detect should be required. The experimental results showed the detection accuracy of the proposed model was improved from 40.0\\% to 89.0\\% on average in the seven major cities among the test dataset. In addition, we challenged to apply our method to the detection of available roads when landslide by natural disaster is occurred, in order to rapidly obtain a way of transportation. For fast inference, a small size of the trained model was implemented on a small embedded edge device as lightweight deep learning. We reported the detection results for the satellite image before and after the rainfall disaster in Japan.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery",
    "url": "http://arxiv.org/abs/2510.27224v1",
    "authors": [
      "Mahmoud El Hussieni",
      "Bahad\u0131r K. G\u00fcnt\u00fcrk",
      "Hasan F. Ate\u015f",
      "O\u011fuz Hano\u011flu"
    ],
    "published": "2025-10-31",
    "abstract": "Accurate building instance segmentation and height classification are critical for urban planning, 3D city modeling, and infrastructure monitoring. This paper presents a detailed analysis of YOLOv11, the recent advancement in the YOLO series of deep learning models, focusing on its application to joint building extraction and discrete height classification from satellite imagery. YOLOv11 builds on the strengths of earlier YOLO models by introducing a more efficient architecture that better combines features at different scales, improves object localization accuracy, and enhances performance in complex urban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000 annotated buildings across 12 cities -- we evaluate YOLOv11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLOv11 achieves strong instance segmentation performance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust classification accuracy across five predefined height tiers. The model excels in handling occlusions, complex building shapes, and class imbalance, particularly for rare high-rise structures. Comparative analysis confirms that YOLOv11 outperforms earlier multitask frameworks in both detection accuracy and inference speed, making it well-suited for real-time, large-scale urban mapping. This research highlights YOLOv11's potential to advance semantic urban reconstruction through streamlined categorical height modeling, offering actionable insights for future developments in remote sensing and geospatial intelligence.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2",
    "url": "http://arxiv.org/abs/2510.26653v1",
    "authors": [
      "Daniela Martin",
      "Joseph Gallego"
    ],
    "published": "2025-10-30",
    "abstract": "Accurate estimation of sea ice drift is critical for Arctic navigation, climate research, and operational forecasting. While optical flow, a computer vision technique for estimating pixel wise motion between consecutive images, has advanced rapidly in computer vision, its applicability to geophysical problems and to satellite SAR imagery remains underexplored. Classical optical flow methods rely on mathematical models and strong assumptions about motion, which limit their accuracy in complex scenarios. Recent deep learning based approaches have substantially improved performance and are now the standard in computer vision, motivating their application to sea ice drift estimation. We present the first large scale benchmark of 48 deep learning optical flow models on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the Arctic. Our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods, which have substantially improved motion estimation accuracy compared to classical methods, can be effectively transferred to polar remote sensing. Optical flow produces spatially continuous drift fields, providing motion estimates for every image pixel rather than at sparse buoy locations, offering new opportunities for navigation and climate modeling.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning",
    "url": "http://arxiv.org/abs/2510.24321v1",
    "authors": [
      "Ivica Dimitrovski",
      "Vlatko Spasev",
      "Ivan Kitanovski"
    ],
    "published": "2025-10-28",
    "abstract": "Remote sensing applications increasingly rely on deep learning for scene classification. However, their performance is often constrained by the scarcity of labeled data and the high cost of annotation across diverse geographic and sensor domains. While recent vision-language models like CLIP have shown promise by learning transferable representations at scale by aligning visual and textual modalities, their direct application to remote sensing remains suboptimal due to significant domain gaps and the need for task-specific semantic adaptation. To address this critical challenge, we systematically explore prompt learning as a lightweight and efficient adaptation strategy for few-shot remote sensing image scene classification. We evaluate several representative methods, including Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints. These approaches reflect complementary design philosophies: from static context optimization to conditional prompts for enhanced generalization, multi-modal prompts for joint vision-language adaptation, and semantically regularized prompts for stable learning without forgetting. We benchmark these prompt-learning methods against two standard baselines: zero-shot CLIP with hand-crafted prompts and a linear probe trained on frozen CLIP features. Through extensive experiments on multiple benchmark remote sensing datasets, including cross-dataset generalization tests, we demonstrate that prompt learning consistently outperforms both baselines in few-shot scenarios. Notably, Prompting with Self-Regulating Constraints achieves the most robust cross-domain performance. Our findings underscore prompt learning as a scalable and efficient solution for bridging the domain gap in satellite and aerial imagery, providing a strong foundation for future research in this field.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM",
      "CLIP"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Review of End-to-End Precipitation Prediction Using Remote Sensing Data: from Divination to Machine Learning",
    "url": "http://arxiv.org/abs/2510.22855v1",
    "authors": [
      "Yugong Zeng",
      "Jonathan Wu"
    ],
    "published": "2025-10-26",
    "abstract": "Precipitation prediction has undergone a profound transformation -- from early symbolic and empirical methods rooted in divination and observation, to modern technologies based on atmospheric physics and artificial intelligence. This review traces the historical and technological evolution of precipitation forecasting, presenting a survey about end-to-end precipitation prediction technologies that spans ancient practices, the foundations of meteorological science, the rise of numerical weather prediction (NWP), and the emergence of machine learning (ML) and deep learning (DL) models. We first explore traditional and indigenous forecasting methods, then describe the development of physical modeling and statistical frameworks that underpin contemporary operational forecasting. Particular emphasis is placed on recent advances in neural network-based approaches, including automated deep learning, interpretability-driven design, and hybrid physical-data models. By compositing research across multiple eras and paradigms, this review not only depicts the history of end-to-end precipitation prediction but also outlines future directions in next generation forecasting systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enpowering Your Pansharpening Models with Generalizability: Unified Distribution is All You Need",
    "url": "http://arxiv.org/abs/2510.22217v1",
    "authors": [
      "Yongchuan Cui",
      "Peng Liu",
      "Hui Zhang"
    ],
    "published": "2025-10-25",
    "abstract": "Existing deep learning-based models for remote sensing pansharpening exhibit exceptional performance on training datasets. However, due to sensor-specific characteristics and varying imaging conditions, these models suffer from substantial performance degradation when applied to unseen satellite data, lacking generalizability and thus limiting their applicability. We argue that the performance drops stem primarily from distributional discrepancies from different sources and the key to addressing this challenge lies in bridging the gap between training and testing distributions. To validate the idea and further achieve a \"train once, deploy forever\" capability, this paper introduces a novel and intuitive approach to enpower any pansharpening models with generalizability by employing a unified distribution strategy (UniPAN). Specifically, we construct a distribution transformation function that normalizes the pixels sampled from different sources to conform to an identical distribution. The deep models are trained on the transformed domain, and during testing on new datasets, the new data are also transformed to match the training distribution. UniPAN aims to train and test the model on a unified and consistent distribution, thereby enhancing its generalizability. Extensive experiments validate the efficacy of UniPAN, demonstrating its potential to significantly enhance the performance of deep pansharpening models across diverse satellite sensors. Codes: https://github.com/yc-cui/UniPAN.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters",
    "url": "http://arxiv.org/abs/2510.19329v1",
    "authors": [
      "Panagiotis Agrafiotis",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-10-22",
    "abstract": "Accurate, detailed, and regularly updated bathymetry, coupled with complex semantic content, is essential for under-mapped shallow-water environments facing increasing climatological and anthropogenic pressures. However, existing approaches that derive either depth or seabed classes from remote sensing imagery treat these tasks in isolation, forfeiting the mutual benefits of their interaction and hindering the broader adoption of deep learning methods. To address these limitations, we introduce Seabed-Net, a unified multi-task framework that simultaneously predicts bathymetry and pixel-based seabed classification from remote sensing imagery of various resolutions. Seabed-Net employs dual-branch encoders for bathymetry estimation and pixel-based seabed classification, integrates cross-task features via an Attention Feature Fusion module and a windowed Swin-Transformer fusion block, and balances objectives through dynamic task uncertainty weighting. In extensive evaluations at two heterogeneous coastal sites, it consistently outperforms traditional empirical models and traditional machine learning regression methods, achieving up to 75\\% lower RMSE. It also reduces bathymetric RMSE by 10-30\\% compared to state-of-the-art single-task and multi-task baselines and improves seabed classification accuracy up to 8\\%. Qualitative analyses further demonstrate enhanced spatial consistency, sharper habitat boundaries, and corrected depth biases in low-contrast regions. These results confirm that jointly modeling depth with both substrate and seabed habitats yields synergistic gains, offering a robust, open solution for integrated shallow-water mapping. Code and pretrained weights are available at https://github.com/pagraf/Seabed-Net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance",
    "url": "http://arxiv.org/abs/2510.16445v1",
    "authors": [
      "Chien Thai",
      "Mai Xuan Trang",
      "Huong Ninh",
      "Hoang Hiep Ly",
      "Anh Son Le"
    ],
    "published": "2025-10-18",
    "abstract": "Detecting rotated objects accurately and efficiently is a significant challenge in computer vision, particularly in applications such as aerial imagery, remote sensing, and autonomous driving. Although traditional object detection frameworks are effective for axis-aligned objects, they often underperform in scenarios involving rotated objects due to their limitations in capturing orientation variations. This paper introduces an improved loss function aimed at enhancing detection accuracy and robustness by leveraging the Gaussian bounding box representation and Bhattacharyya distance. In addition, we advocate for the use of an anisotropic Gaussian representation to address the issues associated with isotropic variance in square-like objects. Our proposed method addresses these challenges by incorporating a rotation-invariant loss function that effectively captures the geometric properties of rotated objects. We integrate this proposed loss function into state-of-the-art deep learning-based rotated object detection detectors, and extensive experiments demonstrated significant improvements in mean Average Precision metrics compared to existing methods. The results highlight the potential of our approach to establish new benchmark in rotated object detection, with implications for a wide range of applications requiring precise and reliable object localization irrespective of orientation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Deep Learning Based Domain Adaptation Methods in Remote Sensing: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2510.15615v1",
    "authors": [
      "Shuchang Lyu",
      "Qi Zhao",
      "Zheng Zhou",
      "Meng Li",
      "You Zhou",
      "Dingding Yao",
      "Guangliang Cheng",
      "Huiyu Zhou",
      "Zhenwei Shi"
    ],
    "published": "2025-10-17",
    "abstract": "Domain adaptation is a crucial and increasingly important task in remote sensing, aiming to transfer knowledge from a source domain a differently distributed target domain. It has broad applications across various real-world applications, including remote sensing element interpretation, ecological environment monitoring, and urban/rural planning. However, domain adaptation in remote sensing poses significant challenges due to differences in data, such as variations in ground sampling distance, imaging modes from various sensors, geographical landscapes, and environmental conditions. In recent years, deep learning has emerged as a powerful tool for feature representation and cross-domain knowledge transfer, leading to widespread adoption in remote sensing tasks. In this paper, we present a comprehensive survey of significant advancements in deep learning based domain adaptation for remote sensing. We first introduce the preliminary knowledge to clarify key concepts, mathematical notations, and the taxonomy of methodologies. We then organize existing algorithms from multiple perspectives, including task categorization, input mode, supervision paradigm, and algorithmic granularity, providing readers with a structured understanding of the field. Next, we review widely used datasets and summarize the performance of state-of-the-art methods to provide an overview of current progress. We also identify open challenges and potential directions to guide future research in domain adaptation for remote sensing. Compared to previous surveys, this work addresses a broader range of domain adaptation tasks in remote sensing, rather than concentrating on a few subfields. It also presents a systematic taxonomy, providing a more comprehensive and organized understanding of the field. As a whole, this survey can inspire the research community, foster understanding, and guide future work in the field.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Rethinking deep learning: linear regression remains a key benchmark in predicting terrestrial water storage",
    "url": "http://arxiv.org/abs/2510.10799v1",
    "authors": [
      "Wanshu Nie",
      "Sujay V. Kumar",
      "Junyu Chen",
      "Long Zhao",
      "Olya Skulovich",
      "Jinwoong Yoo",
      "Justin Pflug",
      "Shahryar Khalique Ahmad",
      "Goutam Konapala"
    ],
    "published": "2025-10-12",
    "abstract": "Recent advances in machine learning such as Long Short-Term Memory (LSTM) models and Transformers have been widely adopted in hydrological applications, demonstrating impressive performance amongst deep learning models and outperforming physical models in various tasks. However, their superiority in predicting land surface states such as terrestrial water storage (TWS) that are dominated by many factors such as natural variability and human driven modifications remains unclear. Here, using the open-access, globally representative HydroGlobe dataset - comprising a baseline version derived solely from a land surface model simulation and an advanced version incorporating multi-source remote sensing data assimilation - we show that linear regression is a robust benchmark, outperforming the more complex LSTM and Temporal Fusion Transformer for TWS prediction. Our findings highlight the importance of including traditional statistical models as benchmarks when developing and evaluating deep learning models. Additionally, we emphasize the critical need to establish globally representative benchmark datasets that capture the combined impact of natural variability and human interventions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data",
    "url": "http://arxiv.org/abs/2510.09845v1",
    "authors": [
      "Nicholas LaHaye",
      "Thilanka Munashinge",
      "Hugo Lee",
      "Xiaohua Pan",
      "Gonzalo Gonzalez Abad",
      "Hazem Mahmoud",
      "Jennifer Wei"
    ],
    "published": "2025-10-10",
    "abstract": "This work demonstrates the possibilities for improving wildfire and air quality management in the western United States by leveraging the unprecedented hourly data from NASA's TEMPO satellite mission and advances in self-supervised deep learning. Here we demonstrate the efficacy of deep learning for mapping the near real-time hourly spread of wildfire fronts and smoke plumes using an innovative self-supervised deep learning-system: successfully distinguishing smoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across the smoke and fire masks generated from different sensing modalities as well as significant improvement over operational products for the same cases.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Hyperspectral data augmentation with transformer-based diffusion models",
    "url": "http://arxiv.org/abs/2510.08363v1",
    "authors": [
      "Mattia Ferrari",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-09",
    "abstract": "The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models",
    "url": "http://arxiv.org/abs/2510.07008v1",
    "authors": [
      "Gianmarco Perantoni",
      "Giulio Weikmann",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-08",
    "abstract": "The temporal consistency of yearly land-cover maps is of great importance to model the evolution and change of the land cover over the years. In this paper, we focus the attention on a novel approach to classification of yearly satellite image time series (SITS) that combines deep learning with Bayesian modelling, using Hidden Markov Models (HMMs) integrated with Transformer Encoder (TE) based DNNs. The proposed approach aims to capture both i) intricate temporal correlations in yearly SITS and ii) specific patterns in multiyear crop type sequences. It leverages the cascade classification of an HMM layer built on top of the TE, discerning consistent yearly crop-type sequences. Validation on a multiyear crop type classification dataset spanning 47 crop types and six years of Sentinel-2 acquisitions demonstrates the importance of modelling temporal consistency in the predicted labels. HMMs enhance the overall performance and F1 scores, emphasising the effectiveness of the proposed approach.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Label-frugal satellite image change detection with generative virtual exemplar learning",
    "url": "http://arxiv.org/abs/2510.06926v1",
    "authors": [
      "Hichem Sahbi"
    ],
    "published": "2025-10-08",
    "abstract": "Change detection is a major task in remote sensing which consists in finding all the occurrences of changes in multi-temporal satellite or aerial images. The success of existing methods, and particularly deep learning ones, is tributary to the availability of hand-labeled training data that capture the acquisition conditions and the subjectivity of the user (oracle). In this paper, we devise a novel change detection algorithm, based on active learning. The main contribution of our work resides in a new model that measures how important is each unlabeled sample, and provides an oracle with only the most critical samples (also referred to as virtual exemplars) for further labeling. These exemplars are generated, using an invertible graph convnet, as the optimum of an adversarial loss that (i) measures representativity, diversity and ambiguity of the data, and thereby (ii) challenges (the most) the current change detection criteria, leading to a better re-estimate of these criteria in the subsequent iterations of active learning. Extensive experiments show the positive impact of our label-efficient learning model against comparative methods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Explaining raw data complexity to improve satellite onboard processing",
    "url": "http://arxiv.org/abs/2510.06858v2",
    "authors": [
      "Adrien Dorise",
      "Marjorie Bellizzi",
      "Adrien Girard",
      "Benjamin Francesconi",
      "St\u00e9phane May"
    ],
    "published": "2025-10-08",
    "abstract": "With increasing processing power, deploying AI models for remote sensing directly onboard satellites is becoming feasible. However, new constraints arise, mainly when using raw, unprocessed sensor data instead of preprocessed ground-based products. While current solutions primarily rely on preprocessed sensor images, few approaches directly leverage raw data. This study investigates the effects of utilising raw data on deep learning models for object detection and classification tasks. We introduce a simulation workflow to generate raw-like products from high-resolution L1 imagery, enabling systemic evaluation. Two object detection models (YOLOv11n and YOLOX-S) are trained on both raw and L1 datasets, and their performance is compared using standard detection metrics and explainability tools. Results indicate that while both models perform similarly at low to medium confidence thresholds, the model trained on raw data struggles with object boundary identification at high confidence levels. It suggests that adapting AI architectures with improved contouring methods can enhance object detection on raw images, improving onboard AI for remote sensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data",
    "url": "http://arxiv.org/abs/2510.05760v1",
    "authors": [
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-07",
    "abstract": "Deep learning has gained broad interest in remote sensing image scene classification thanks to the effectiveness of deep neural networks in extracting the semantics from complex data. However, deep networks require large amounts of training samples to obtain good generalization capabilities and are sensitive to errors in the training labels. This is a problem in remote sensing since highly reliable labels can be obtained at high costs and in limited amount. However, many sources of less reliable labeled data are available, e.g., obsolete digital maps. In order to train deep networks with larger datasets, we propose both the combination of single or multiple weak sources of labeled data with a small but reliable dataset to generate multisource labeled datasets and a novel training strategy where the reliability of each source is taken in consideration. This is done by exploiting the transition matrices describing the statistics of the errors of each source. The transition matrices are embedded into the labels and used during the training process to weigh each label according to the related source. The proposed method acts as a weighting scheme at gradient level, where each instance contributes with different weights to the optimization of different classes. The effectiveness of the proposed method is validated by experiments on different datasets. The results proved the robustness and capability of leveraging on unreliable source of labels of the proposed method.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images",
    "url": "http://arxiv.org/abs/2510.04916v1",
    "authors": [
      "Giulio Weikmann",
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-06",
    "abstract": "Deep learning has become increasingly important in remote sensing image classification due to its ability to extract semantic information from complex data. Classification tasks often include predefined label hierarchies that represent the semantic relationships among classes. However, these hierarchies are frequently overlooked, and most approaches focus only on fine-grained classification schemes. In this paper, we present a novel Semantics-Aware Hierarchical Consensus (SAHC) method for learning hierarchical features and relationships by integrating hierarchy-specific classification heads within a deep network architecture, each specialized in different degrees of class granularity. The proposed approach employs trainable hierarchy matrices, which guide the network through the learning of the hierarchical structure in a self-supervised manner. Furthermore, we introduce a hierarchical consensus mechanism to ensure consistent probability distributions across different hierarchical levels. This mechanism acts as a weighted ensemble being able to effectively leverage the inherent structure of the hierarchical classification task. The proposed SAHC method is evaluated on three benchmark datasets with different degrees of hierarchical complexity on different tasks, using distinct backbone architectures to effectively emphasize its adaptability. Experimental results show both the effectiveness of the proposed approach in guiding network learning and the robustness of the hierarchical consensus for remote sensing image classification tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification",
    "url": "http://arxiv.org/abs/2510.04628v1",
    "authors": [
      "Hao Liu",
      "Yunhao Gao",
      "Wei Li",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-06",
    "abstract": "Deep learning-based methods have achieved significant success in remote sensing Earth observation data analysis. Numerous feature fusion techniques address multimodal remote sensing image classification by integrating global and local features. However, these techniques often struggle to extract structural and detail features from heterogeneous and redundant multimodal images. With the goal of introducing frequency domain learning to model key and sparse detail features, this paper introduces the spatial-spectral-frequency interaction network (S$^2$Fin), which integrates pairwise fusion modules across the spatial, spectral, and frequency domains. Specifically, we propose a high-frequency sparse enhancement transformer that employs sparse spatial-spectral attention to optimize the parameters of the high-frequency filter. Subsequently, a two-level spatial-frequency fusion strategy is introduced, comprising an adaptive frequency channel module that fuses low-frequency structures with enhanced high-frequency details, and a high-frequency resonance mask that emphasizes sharp edges via phase similarity. In addition, a spatial-spectral attention fusion module further enhances feature extraction at intermediate layers of the network. Experiments on four benchmark multimodal datasets with limited labeled data demonstrate that S$^2$Fin performs superior classification, outperforming state-of-the-art methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks",
    "url": "http://arxiv.org/abs/2510.03725v1",
    "authors": [
      "Thomas Hallopeau",
      "Joris Gu\u00e9rin",
      "Laurent Demagistri",
      "Youssef Fouzai",
      "Renata Gracie",
      "Vanderlei Pascoal De Matos",
      "Helen Gurgel",
      "Nadine Dessay"
    ],
    "published": "2025-10-04",
    "abstract": "While deep learning methods for detecting informal settlements have already been developed, they have not yet fully utilized the potential offered by recent pretrained neural networks. We compare two types of pretrained neural networks for detecting the favelas of Rio de Janeiro: 1. Generic networks pretrained on large diverse datasets of unspecific images, 2. A specialized network pretrained on satellite imagery. While the latter is more specific to the target task, the former has been pretrained on significantly more images. Hence, this research investigates whether task specificity or data volume yields superior performance in urban informal settlement detection.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Overview of GeoLifeCLEF 2023: Species Composition Prediction with High Spatial Resolution at Continental Scale Using Remote Sensing",
    "url": "http://arxiv.org/abs/2509.25816v1",
    "authors": [
      "Christophe Botella",
      "Benjamin Deneu",
      "Diego Marcos",
      "Maximilien Servajean",
      "Theo Larcher",
      "Cesar Leblanc",
      "Joaquim Estopinan",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2025-09-30",
    "abstract": "Understanding the spatio-temporal distribution of species is a cornerstone of ecology and conservation. By pairing species observations with geographic and environmental predictors, researchers can model the relationship between an environment and the species which may be found there. To advance the state-of-the-art in this area with deep learning models and remote sensing data, we organized an open machine learning challenge called GeoLifeCLEF 2023. The training dataset comprised 5 million plant species observations (single positive label per sample) distributed across Europe and covering most of its flora, high-resolution rasters: remote sensing imagery, land cover, elevation, in addition to coarse-resolution data: climate, soil and human footprint variables. In this multi-label classification task, we evaluated models ability to predict the species composition in 22 thousand small plots based on standardized surveys. This paper presents an overview of the competition, synthesizes the approaches used by the participating teams, and analyzes the main results. In particular, we highlight the biases faced by the methods fitted to single positive labels when it comes to the multi-label evaluation, and the new and effective learning strategy combining single and multi-label data in training.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Spatial-Spectral Binarized Neural Network for Panchromatic and Multi-spectral Images Fusion",
    "url": "http://arxiv.org/abs/2509.23321v2",
    "authors": [
      "Yizhen Jiang",
      "Mengting Ma",
      "Anqi Zhu",
      "Xiaowen Ma",
      "Jiaxin Li",
      "Wei Zhang"
    ],
    "published": "2025-09-27",
    "abstract": "Remote sensing pansharpening aims to reconstruct spatial-spectral properties during the fusion of panchromatic (PAN) images and low-resolution multi-spectral (LR-MS) images, finally generating the high-resolution multi-spectral (HR-MS) images. Although deep learning-based models have achieved excellent performance, they often come with high computational complexity, which hinder their applications on resource-limited devices. In this paper, we explore the feasibility of applying the binary neural network (BNN) to pan-sharpening. Nevertheless, there are two main issues with binarizing pan-sharpening models: (i) the binarization will cause serious spectral distortion due to the inconsistent spectral distribution of the PAN/LR-MS images; (ii) the common binary convolution kernel is difficult to adapt to the multi-scale and anisotropic spatial features of remote sensing objects, resulting in serious degradation of contours. To address the above issues, we design the customized spatial-spectral binarized convolution (S2B-Conv), which is composed of the Spectral-Redistribution Mechanism (SRM) and Gabor Spatial Feature Amplifier (GSFA). Specifically, SRM employs an affine transformation, generating its scaling and bias parameters through a dynamic learning process. GSFA, which randomly selects different frequencies and angles within a preset range, enables to better handle multi-scale and-directional spatial features. A series of S2B-Conv form a brand-new binary network for pan-sharpening, dubbed as S2BNet. Extensive quantitative and qualitative experiments have shown our high-efficiency binarized pan-sharpening method can attain a promising performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification",
    "url": "http://arxiv.org/abs/2509.23310v1",
    "authors": [
      "Hao Liu",
      "Yongjie Zheng",
      "Yuhan Kang",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-09-27",
    "abstract": "Deep learning-based techniques for the analysis of multimodal remote sensing data have become popular due to their ability to effectively integrate complementary spatial, spectral, and structural information from different sensors. Recently, denoising diffusion probabilistic models (DDPMs) have attracted attention in the remote sensing community due to their powerful ability to capture robust and complex spatial-spectral distributions. However, pre-training multimodal DDPMs may result in modality imbalance, and effectively leveraging diffusion features to guide complementary diversity feature extraction remains an open question. To address these issues, this paper proposes a balanced diffusion-guided fusion (BDGF) framework that leverages multimodal diffusion features to guide a multi-branch network for land-cover classification. Specifically, we propose an adaptive modality masking strategy to encourage the DDPMs to obtain a modality-balanced rather than spectral image-dominated data distribution. Subsequently, these diffusion features hierarchically guide feature extraction among CNN, Mamba, and transformer networks by integrating feature fusion, group channel attention, and cross-attention mechanisms. Finally, a mutual learning strategy is developed to enhance inter-branch collaboration by aligning the probability entropy and feature similarity of individual subnetworks. Extensive experiments on four multimodal remote sensing datasets demonstrate that the proposed method achieves superior classification performance. The code is available at https://github.com/HaoLiu-XDU/BDGF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution",
    "url": "http://arxiv.org/abs/2510.00033v1",
    "authors": [
      "Usman Muhammad",
      "Jorma Laaksonen"
    ],
    "published": "2025-09-26",
    "abstract": "Hyperspectral single image super-resolution (SISR) is a challenging task due to the difficulty of restoring fine spatial details while preserving spectral fidelity across a wide range of wavelengths, which limits the performance of conventional deep learning models. To address this challenge, we introduce Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly integrated into standard 2D convolutional architectures to enhance both spatial resolution and spectral integrity. The SSUF combines spectral unmixing with spectral--spatial feature extraction and guides a ResNet-based convolutional neural network for improved reconstruction. In addition, we propose a custom Spatial-Spectral Gradient Loss function that integrates mean squared error with spatial and spectral gradient components, encouraging accurate reconstruction of both spatial and spectral features. Experiments on three public remote sensing hyperspectral datasets demonstrate that the proposed hybrid deep learning model achieves competitive performance while reducing model complexity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "ResNet"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images",
    "url": "http://arxiv.org/abs/2509.20918v1",
    "authors": [
      "Qinfeng Zhu",
      "Han Li",
      "Liang He",
      "Lei Fan"
    ],
    "published": "2025-09-25",
    "abstract": "Semantic segmentation of remote sensing imagery is a fundamental task in computer vision, supporting a wide range of applications such as land use classification, urban planning, and environmental monitoring. However, this task is often challenged by the high spatial resolution, complex scene structures, and diverse object scales present in remote sensing data. To address these challenges, various deep learning architectures have been proposed, including convolutional neural networks, Vision Transformers, and the recently introduced Vision Mamba. Vision Mamba features a global receptive field and low computational complexity, demonstrating both efficiency and effectiveness in image segmentation. However, its reliance on global scanning tends to overlook critical local features, such as textures and edges, which are essential for achieving accurate segmentation in remote sensing contexts. To tackle this limitation, we propose SwinMamba, a novel framework inspired by the Swin Transformer. SwinMamba integrates localized Mamba-style scanning within shifted windows with a global receptive field, to enhance the model's perception of both local and global features. Specifically, the first two stages of SwinMamba perform local scanning to capture fine-grained details, while its subsequent two stages leverage global scanning to fuse broader contextual information. In our model, the use of overlapping shifted windows enhances inter-region information exchange, facilitating more robust feature integration across the entire image. Extensive experiments on the LoveDA and ISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art methods, underscoring its effectiveness and potential as a superior solution for semantic segmentation of remotely sensed imagery.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
    "url": "http://arxiv.org/abs/2509.20234v3",
    "authors": [
      "Tom Burgert",
      "Oliver Stoll",
      "Paolo Rota",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-09-24",
    "abstract": "The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Optimal Transport Based Hyperspectral Unmixing for Highly Mixed Observations",
    "url": "http://arxiv.org/abs/2509.20417v1",
    "authors": [
      "D. Doutsas",
      "B. Figliuzzi"
    ],
    "published": "2025-09-24",
    "abstract": "We propose a novel approach based on optimal transport (OT) for tackling the problem of highly mixed data in blind hyperspectral unmixing. Our method constrains the distribution of the estimated abundance matrix to resemble a targeted Dirichlet distribution more closely. The novelty lies in using OT to measure the discrepancy between the targeted and true abundance distributions, which we incorporate as a regularization term in our optimization problem. We demonstrate the efficiency of our method through a case study involving an unsupervised deep learning approach. Our experiments show that the proposed approach allows for a better estimation of the endmembers in the presence of highly mixed data, while displaying robustness to the choice of target abundance distribution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy",
    "url": "http://arxiv.org/abs/2509.19665v2",
    "authors": [
      "Manuel Perez-Carrasco",
      "Maya Nasr",
      "Sebastien Roche",
      "Chris Chan Miller",
      "Zhan Zhang",
      "Core Francisco Park",
      "Eleanor Walker",
      "Cecilia Garraffo",
      "Douglas Finkbeiner",
      "Ritesh Gautam",
      "Steven Wofsy"
    ],
    "published": "2025-09-24",
    "abstract": "Effective cloud and cloud shadow detection is a critical prerequisite for accurate retrieval of concentrations of atmospheric methane or other trace gases in hyperspectral remote sensing. This challenge is especially pertinent for MethaneSAT and for its airborne companion mission, MethaneAIR. In this study, we use machine learning methods to address the cloud and cloud shadow detection problem for sensors with these high spatial resolutions instruments. Cloud and cloud shadows in remote sensing data need to be effectively screened out as they bias methane retrievals in remote sensing imagery and impact the quantification of emissions. We deploy and evaluate conventional techniques including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP), with advanced deep learning architectures, namely UNet and a Spectral Channel Attention Network (SCAN) method. Our results show that conventional methods struggle with spatial coherence and boundary definition, affecting the detection of clouds and cloud shadows. Deep learning models substantially improve detection quality: UNet performs best in preserving spatial structure, while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses UNet on MethaneSAT data, underscoring the benefits of incorporating spectral attention for satellite specific features. This in depth assessment of various disparate machine learning techniques demonstrates the strengths and effectiveness of advanced deep learning architectures in providing robust, scalable solutions for clouds and cloud shadow screening towards enhancing methane emission quantification capacity of existing and next generation hyperspectral missions. Our data and code is publicly available at https://doi.org/10.7910/DVN/IKLZOJ",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection",
      "Regression"
    ]
  },
  {
    "title": "Communications to Circulations: Real-Time 3D Wind Field Prediction Using 5G GNSS Signals and Deep Learning",
    "url": "http://arxiv.org/abs/2509.16068v3",
    "authors": [
      "Yuchen Ye",
      "Chaoxia Yuan",
      "Mingyu Li",
      "Aoqi Zhou",
      "Hong Liang",
      "Chunqing Shang",
      "Kezuan Wang",
      "Yifeng Zheng",
      "Cong Chen"
    ],
    "published": "2025-09-19",
    "abstract": "Accurate atmospheric wind field information is crucial for various applications, including weather forecasting, aviation safety, and disaster risk reduction. However, obtaining high spatiotemporal resolution wind data remains challenging due to limitations in traditional in-situ observations and remote sensing techniques, as well as the computational expense and biases of numerical weather prediction (NWP) models. This paper introduces G-WindCast, a novel deep learning framework that leverages signal strength variations from 5G Global Navigation Satellite System (GNSS) signals to forecast three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward Neural Networks (FNN) and Transformer networks to capture complex, nonlinear, and spatiotemporal relationships between GNSS-derived features and wind dynamics. Our preliminary results demonstrate promising accuracy in real-time wind forecasts (up to 30 minutes lead time). The model exhibits robustness across forecast horizons and different pressure levels, and its predictions for wind fields show superior agreement with ground-based radar wind profiler compared to concurrent European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5). Furthermore, we show that the system can maintain excellent performance for localized forecasting even with a significantly reduced number of GNSS stations (e.g., around 100), highlighting its cost-effectiveness and scalability. This interdisciplinary approach underscores the transformative potential of exploiting non-traditional data sources and deep learning for advanced environmental monitoring and real-time atmospheric applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland",
    "url": "http://arxiv.org/abs/2509.18176v1",
    "authors": [
      "Wendong Yao",
      "Saeed Azadnejad",
      "Binhua Huang",
      "Shane Donohue",
      "Soumyabrata Dev"
    ],
    "published": "2025-09-17",
    "abstract": "Monitoring ground displacement is crucial for urban infrastructure stability and mitigating geological hazards. However, forecasting future deformation from sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data remains a significant challenge. This paper introduces a novel deep learning framework that transforms these sparse point measurements into a dense spatio-temporal tensor. This methodological shift allows, for the first time, the direct application of advanced computer vision architectures to this forecasting problem. We design and implement a hybrid Convolutional Neural Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to simultaneously learn spatial patterns and temporal dependencies from the generated data tensor. The model's performance is benchmarked against powerful machine learning baselines, Light Gradient Boosting Machine and LASSO regression, using Sentinel-1 data from eastern Ireland. Results demonstrate that the proposed architecture provides significantly more accurate and spatially coherent forecasts, establishing a new performance benchmark for this task. Furthermore, an interpretability analysis reveals that baseline models often default to simplistic persistence patterns, highlighting the necessity of our integrated spatio-temporal approach to capture the complex dynamics of ground deformation. Our findings confirm the efficacy and potential of spatio-temporal deep learning for high-resolution deformation forecasting.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction",
    "url": "http://arxiv.org/abs/2509.10308v1",
    "authors": [
      "Joshua Dimasaka",
      "Christian Gei\u00df",
      "Robert Muir-Wood",
      "Emily So"
    ],
    "published": "2025-09-12",
    "abstract": "In the aftermath of disasters, many institutions worldwide face challenges in continually monitoring changes in disaster risk, limiting the ability of key decision-makers to assess progress towards the UN Sendai Framework for Disaster Risk Reduction 2015-2030. While numerous efforts have substantially advanced the large-scale modeling of hazard and exposure through Earth observation and data-driven methods, progress remains limited in modeling another equally important yet challenging element of the risk equation: physical vulnerability. To address this gap, we introduce Graph Categorical Structured Variational Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for modeling physical vulnerability by integrating deep learning, graph representation, and categorical probabilistic inference, using time-series satellite-derived datasets and prior expert belief systems. We introduce a weakly supervised first-order transition matrix that reflects the changes in the spatiotemporal distribution of physical vulnerability in two disaster-stricken and socioeconomically disadvantaged areas: (1) the cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the mudslide-affected city of Freetown in Sierra Leone. Our work reveals post-disaster regional dynamics in physical vulnerability, offering valuable insights into localized spatiotemporal auditing and sustainable strategies for post-disaster risk reduction.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery",
    "url": "http://arxiv.org/abs/2509.08949v1",
    "authors": [
      "Yibin Wang",
      "Wondimagegn Beshah",
      "Padmanava Dash",
      "Haifeng Wang"
    ],
    "published": "2025-09-10",
    "abstract": "The use of unmanned aerial systems (UASs) has increased tremendously in the current decade. They have significantly advanced remote sensing with the capability to deploy and image the terrain as per required spatial, spectral, temporal, and radiometric resolutions for various remote sensing applications. One of the major advantages of UAS imagery is that images can be acquired in cloudy conditions by flying the UAS under the clouds. The limitation to the technology is that the imagery is often sullied by cloud shadows. Images taken over water are additionally affected by sun glint. These are two pose serious issues for estimating water quality parameters from the UAS images. This study proposes a novel machine learning approach first to identify and extract regions with cloud shadows and sun glint and separate such regions from non-obstructed clear sky regions and sun-glint unaffected regions. The data was extracted from the images at pixel level to train an U-Net based deep learning model and best settings for model training was identified based on the various evaluation metrics from test cases. Using this evaluation, a high-quality image correction model was determined, which was used to recover the cloud shadow and sun glint areas in the images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2509.03961v1",
    "authors": [
      "Yijun Zhou",
      "Yikui Zhai",
      "Zilu Ying",
      "Tingfeng Xian",
      "Wenlve Zhou",
      "Zhiheng Zhou",
      "Xiaolin Tian",
      "Xudong Jia",
      "Hongsheng Zhang",
      "C. L. Philip Chen"
    ],
    "published": "2025-09-04",
    "abstract": "Although deep learning has advanced remote sensing change detection (RSCD), most methods rely solely on image modality, limiting feature representation, change pattern modeling, and generalization especially under illumination and noise disturbances. To address this, we propose MMChange, a multimodal RSCD method that combines image and text modalities to enhance accuracy and robustness. An Image Feature Refinement (IFR) module is introduced to highlight key regions and suppress environmental noise. To overcome the semantic limitations of image features, we employ a vision language model (VLM) to generate semantic descriptions of bitemporal images. A Textual Difference Enhancement (TDE) module then captures fine grained semantic shifts, guiding the model toward meaningful changes. To bridge the heterogeneity between modalities, we design an Image Text Feature Fusion (ITFF) module that enables deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and SYSUCD demonstrate that MMChange consistently surpasses state of the art methods across multiple metrics, validating its effectiveness for multimodal RSCD. Code is available at: https://github.com/yikuizhai/MMChange.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing",
    "url": "http://arxiv.org/abs/2509.03376v1",
    "authors": [
      "Hui Chen",
      "Liangyu Liu",
      "Xianchao Xiu",
      "Wanquan Liu"
    ],
    "published": "2025-09-03",
    "abstract": "Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote sensing images into a set of endmembers and their corresponding abundances. Despite significant progress in this field using deep learning, most methods fail to simultaneously characterize global dependencies and local consistency, making it difficult to preserve both long-range interactions and boundary details. This letter proposes a novel transformer-guided content-adaptive graph unmixing framework (T-CAGU), which overcomes these challenges by employing a transformer to capture global dependencies and introducing a content-adaptive graph neural network to enhance local relationships. Unlike previous work, T-CAGU integrates multiple propagation orders to dynamically learn the graph structure, ensuring robustness against noise. Furthermore, T-CAGU leverages a graph residual mechanism to preserve global information and stabilize training. Experimental results demonstrate its superiority over the state-of-the-art methods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Information transmission: Inferring change area from change moment in time series remote sensing images",
    "url": "http://arxiv.org/abs/2509.03112v1",
    "authors": [
      "Jialu Li",
      "Chen Wu",
      "Meiqi Hu"
    ],
    "published": "2025-09-03",
    "abstract": "Time series change detection is a critical task for exploring ecosystem dynamics using time series remote sensing images, because it can simultaneously indicate where and when change occur. While deep learning has shown excellent performance in this domain, it continues to approach change area detection and change moment identification as distinct tasks. Given that change area can be inferred from change moment, we propose a time series change detection network, named CAIM-Net (Change Area Inference from Moment Network), to ensure consistency between change area and change moment results. CAIM-Net infers change area from change moment based on the intrinsic relationship between time series analysis and spatial change detection. The CAIM-Net comprises three key steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction, and Fine Change Moment Extraction and Change Area Inference. In the Difference Extraction and Enhancement, a lightweight encoder with batch dimension stacking is designed to rapidly extract difference features. Subsequently, boundary enhancement convolution is applied to amplify these difference features. In the Coarse Change Moment Extraction, the enhanced difference features from the first step are used to spatiotemporal correlation analysis, and then two distinct methods are employed to determine coarse change moments. In the Fine Change Moment Extraction and Change Area Inference, a multiscale temporal Class Activation Mapping (CAM) module first increases the weight of the change-occurring moment from coarse change moments. Then the weighted change moment is used to infer change area based on the fact that pixels with the change moment must have undergone a change.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Time Series Analysis"
    ]
  },
  {
    "title": "HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision",
    "url": "http://arxiv.org/abs/2509.01882v2",
    "authors": [
      "Shubham Laxmikant Deshmukh",
      "Matthew Wilchek",
      "Feras A. Batarseh"
    ],
    "published": "2025-09-02",
    "abstract": "Ongoing advancements in computer vision, particularly in pattern recognition and scene classification, have enabled new applications in environmental monitoring. Deep learning now offers non-contact methods for assessing water quality and detecting contamination, both critical for disaster response and public health protection. This work introduces HydroVision, a deep learning-based scene classification framework that estimates optically active water quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored Dissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and Turbidity from standard Red-Green-Blue (RGB) images of surface water. HydroVision supports early detection of contamination trends and strengthens monitoring by regulatory agencies during external environmental stressors, industrial activities, and force majeure events. The model is trained on more than 500,000 seasonally varied images collected from the United States Geological Survey Hydrologic Imagery Visualization and Information System between 2022 and 2024. This approach leverages widely available RGB imagery as a scalable, cost-effective alternative to traditional multispectral and hyperspectral remote sensing. Four state-of-the-art convolutional neural networks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer are evaluated through transfer learning to identify the best-performing architecture. DenseNet121 achieves the highest validation performance, with an R2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for real-world water quality monitoring across diverse conditions. While the current model is optimized for well-lit imagery, future work will focus on improving robustness under low-light and obstructed scenarios to expand its operational utility.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "VGG",
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment",
    "url": "http://arxiv.org/abs/2509.01183v2",
    "authors": [
      "Bingnan Yang",
      "Mi Zhang",
      "Zhili Zhang",
      "Zhan Zhang",
      "Yuanxin Zhao",
      "Xiangyun Hu",
      "Jianya Gong"
    ],
    "published": "2025-09-01",
    "abstract": "High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at https://github.com/Yangbn97/SegAssess.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification",
    "url": "http://arxiv.org/abs/2509.00677v1",
    "authors": [
      "Qingyu Wang",
      "Xue Jiang",
      "Guozheng Xu"
    ],
    "published": "2025-08-31",
    "abstract": "Multimodal fusion has made great progress in the field of remote sensing image classification due to its ability to exploit the complementary spatial-spectral information. Deep learning methods such as CNN and Transformer have been widely used in these domains. State Space Models recently highlighted that prior methods suffer from quadratic computational complexity. As a result, modeling longer-range dependencies of spatial-spectral features imposes an overwhelming burden on the network. Mamba solves this problem by incorporating time-varying parameters into ordinary SSM and performing hardware optimization, but it cannot perform feature fusion directly. In order to make full use of Mamba's low computational burden and explore the potential of internal structure in multimodal feature fusion, we propose Cross State Fusion Mamba (CSFMamba) Network. Specifically, we first design the preprocessing module of remote sensing image information for the needs of Mamba structure, and combine it with CNN to extract multi-layer features. Secondly, a cross-state module based on Mamba operator is creatively designed to fully fuse the feature of the two modalities. The advantages of Mamba and CNN are combined by designing a more powerful backbone. We capture the fusion relationship between HSI and LiDAR modalities with stronger full-image understanding. The experimental results on two datasets of MUUFL and Houston2018 show that the proposed method outperforms the experimental results of Transformer under the premise of reducing the network training burden.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation",
    "url": "http://arxiv.org/abs/2509.00598v2",
    "authors": [
      "Boyi Li",
      "Ce Zhang",
      "Richard M. Timmerman",
      "Wenxuan Bao"
    ],
    "published": "2025-08-30",
    "abstract": "The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global-Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual-Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual-Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "DAOVI: Distortion-Aware Omnidirectional Video Inpainting",
    "url": "http://arxiv.org/abs/2509.00396v1",
    "authors": [
      "Ryosuke Seshimo",
      "Mariko Isogawa"
    ],
    "published": "2025-08-30",
    "abstract": "Omnidirectional videos that capture the entire surroundings are employed in a variety of fields such as VR applications and remote sensing. However, their wide field of view often causes unwanted objects to appear in the videos. This problem can be addressed by video inpainting, which enables the natural removal of such objects while preserving both spatial and temporal consistency. Nevertheless, most existing methods assume processing ordinary videos with a narrow field of view and do not tackle the distortion in equirectangular projection of omnidirectional videos. To address this issue, this paper proposes a novel deep learning model for omnidirectional video inpainting, called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI introduces a module that evaluates temporal motion information in the image space considering geodesic distance, as well as a depth-aware feature propagation module in the feature space that is designed to address the geometric distortion inherent to omnidirectional videos. The experimental results demonstrate that our proposed method outperforms existing methods both quantitatively and qualitatively.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "The point is the mask: scaling coral reef segmentation with weak supervision",
    "url": "http://arxiv.org/abs/2508.18958v1",
    "authors": [
      "Matteo Contini",
      "Victor Illien",
      "Sylvain Poulain",
      "Serge Bernard",
      "Julien Barde",
      "Sylvain Bonhommeau",
      "Alexis Joly"
    ],
    "published": "2025-08-26",
    "abstract": "Monitoring coral reefs at large spatial scales remains an open challenge, essential for assessing ecosystem health and informing conservation efforts. While drone-based aerial imagery offers broad spatial coverage, its limited resolution makes it difficult to reliably distinguish fine-scale classes, such as coral morphotypes. At the same time, obtaining pixel-level annotations over large spatial extents is costly and labor-intensive, limiting the scalability of deep learning-based segmentation methods for aerial imagery. We present a multi-scale weakly supervised semantic segmentation framework that addresses this challenge by transferring fine-scale ecological information from underwater imagery to aerial data. Our method enables large-scale coral reef mapping from drone imagery with minimal manual annotation, combining classification-based supervision, spatial interpolation and self-distillation techniques. We demonstrate the efficacy of the approach, enabling large-area segmentation of coral morphotypes and demonstrating flexibility for integrating new classes. This study presents a scalable, cost-effective methodology for high-resolution reef monitoring, combining low-cost data collection, weakly supervised deep learning and multi-scale remote sensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency",
    "url": "http://arxiv.org/abs/2508.18693v1",
    "authors": [
      "Zhitong Cheng",
      "Yiran Jiang",
      "Yulong Ge",
      "Yufeng Li",
      "Zhongheng Qin",
      "Rongzhi Lin",
      "Jianwei Ma"
    ],
    "published": "2025-08-26",
    "abstract": "Domain shift, characterized by degraded model performance during transition from labeled source domains to unlabeled target domains, poses a persistent challenge for deploying deep learning systems. Current unsupervised domain adaptation (UDA) methods predominantly rely on fine-tuning feature extractors - an approach limited by inefficiency, reduced interpretability, and poor scalability to modern architectures.\n  Our analysis reveals that models pretrained on large-scale data exhibit domain-invariant geometric patterns in their feature space, characterized by intra-class clustering and inter-class separation, thereby preserving transferable discriminative structures. These findings indicate that domain shifts primarily manifest as boundary misalignment rather than feature degradation.\n  Unlike fine-tuning entire pre-trained models - which risks introducing unpredictable feature distortions - we propose the Feature-space Planes Searcher (FPS): a novel domain adaptation framework that optimizes decision boundaries by leveraging these geometric patterns while keeping the feature encoder frozen. This streamlined approach enables interpretative analysis of adaptation while substantially reducing memory and computational costs through offline feature extraction, permitting full-dataset optimization in a single computation cycle.\n  Evaluations on public benchmarks demonstrate that FPS achieves competitive or superior performance to state-of-the-art methods. FPS scales efficiently with multimodal large models and shows versatility across diverse domains including protein structure prediction, remote sensing classification, and earthquake detection. We anticipate FPS will provide a simple, effective, and generalizable paradigm for transfer learning, particularly in domain adaptation tasks. .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Robust Small Methane Plume Segmentation in Satellite Imagery",
    "url": "http://arxiv.org/abs/2508.16282v1",
    "authors": [
      "Khai Duc Minh Tran",
      "Hoa Van Nguyen",
      "Aimuni Binti Muhammad Rawi",
      "Hareeshrao Athinarayanarao",
      "Ba-Ngu Vo"
    ],
    "published": "2025-08-22",
    "abstract": "This paper tackles the challenging problem of detecting methane plumes, a potent greenhouse gas, using Sentinel-2 imagery. This contributes to the mitigation of rapid climate change. We propose a novel deep learning solution based on U-Net with a ResNet34 encoder, integrating dual spectral enhancement techniques (Varon ratio and Sanchez regression) to optimise input features for heightened sensitivity. A key achievement is the ability to detect small plumes down to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing traditional methods limited to larger plumes. Experiments show our approach achieves a 78.39% F1-score on the validation set, demonstrating superior performance in sensitivity and precision over existing remote sensing techniques for automated methane monitoring, especially for small plumes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "IRSAMap:Towards Large-Scale, High-Resolution Land Cover Map Vectorization",
    "url": "http://arxiv.org/abs/2508.16272v1",
    "authors": [
      "Yu Meng",
      "Ligao Deng",
      "Zhihao Xi",
      "Jiansheng Chen",
      "Jingbo Chen",
      "Anzhi Yue",
      "Diyou Liu",
      "Kai Li",
      "Chenhao Wang",
      "Kaiyu Li",
      "Yupeng Deng",
      "Xian Sun"
    ],
    "published": "2025-08-22",
    "abstract": "With the enhancement of remote sensing image resolution and the rapid advancement of deep learning, land cover mapping is transitioning from pixel-level segmentation to object-based vector modeling. This shift demands more from deep learning models, requiring precise object boundaries and topological consistency. However, existing datasets face three main challenges: limited class annotations, small data scale, and lack of spatial structural information. To overcome these issues, we introduce IRSAMap, the first global remote sensing dataset for large-scale, high-resolution, multi-feature land cover vector mapping. IRSAMap offers four key advantages: 1) a comprehensive vector annotation system with over 1.8 million instances of 10 typical objects (e.g., buildings, roads, rivers), ensuring semantic and spatial accuracy; 2) an intelligent annotation workflow combining manual and AI-based methods to improve efficiency and consistency; 3) global coverage across 79 regions in six continents, totaling over 1,000 km; and 4) multi-task adaptability for tasks like pixel-level classification, building outline extraction, road centerline extraction, and panoramic segmentation. IRSAMap provides a standardized benchmark for the shift from pixel-based to object-based approaches, advancing geographic feature automation and collaborative modeling. It is valuable for global geographic information updates and digital twin construction. The dataset is publicly available at https://github.com/ucas-dlg/IRSAMap",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "CuMoLoS-MAE: A Masked Autoencoder for Remote Sensing Data Reconstruction",
    "url": "http://arxiv.org/abs/2508.14957v1",
    "authors": [
      "Anurup Naskar",
      "Nathanael Zhixin Wong",
      "Sara Shamekh"
    ],
    "published": "2025-08-20",
    "abstract": "Accurate atmospheric profiles from remote sensing instruments such as Doppler Lidar, Radar, and radiometers are frequently corrupted by low-SNR (Signal to Noise Ratio) gates, range folding, and spurious discontinuities. Traditional gap filling blurs fine-scale structures, whereas deep models lack confidence estimates. We present CuMoLoS-MAE, a Curriculum-Guided Monte Carlo Stochastic Ensemble Masked Autoencoder designed to (i) restore fine-scale features such as updraft and downdraft cores, shear lines, and small vortices, (ii) learn a data-driven prior over atmospheric fields, and (iii) quantify pixel-wise uncertainty. During training, CuMoLoS-MAE employs a mask-ratio curriculum that forces a ViT decoder to reconstruct from progressively sparser context. At inference, we approximate the posterior predictive by Monte Carlo over random mask realisations, evaluating the MAE multiple times and aggregating the outputs to obtain the posterior predictive mean reconstruction together with a finely resolved per-pixel uncertainty map. Together with high-fidelity reconstruction, this novel deep learning-based workflow enables enhanced convection diagnostics, supports real-time data assimilation, and improves long-term climate reanalysis.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives",
    "url": "http://arxiv.org/abs/2508.14558v1",
    "authors": [
      "Juepeng Zheng",
      "Zi Ye",
      "Yibin Wen",
      "Jianxi Huang",
      "Zhiwei Zhang",
      "Qingmei Li",
      "Qiong Hu",
      "Baodong Xu",
      "Lingyuan Zhao",
      "Haohuan Fu"
    ],
    "published": "2025-08-20",
    "abstract": "Powered by advances in multiple remote sensing sensors, the production of high spatial resolution images provides great potential to achieve cost-efficient and high-accuracy agricultural inventory and analysis in an automated way. Lots of studies that aim at providing an inventory of the level of each agricultural parcel have generated many methods for Agricultural Parcel and Boundary Delineation (APBD). This review covers APBD methods for detecting and delineating agricultural parcels and systematically reviews the past and present of APBD-related research applied to remote sensing images. With the goal to provide a clear knowledge map of existing APBD efforts, we conduct a comprehensive review of recent APBD papers to build a meta-data analysis, including the algorithm, the study site, the crop type, the sensor type, the evaluation method, etc. We categorize the methods into three classes: (1) traditional image processing methods (including pixel-based, edge-based and region-based); (2) traditional machine learning methods (such as random forest, decision tree); and (3) deep learning-based methods. With deep learning-oriented approaches contributing to a majority, we further discuss deep learning-based methods like semantic segmentation-based, object detection-based and Transformer-based methods. In addition, we discuss five APBD-related issues to further comprehend the APBD domain using remote sensing data, such as multi-sensor data in APBD task, comparisons between single-task learning and multi-task learning in the APBD domain, comparisons among different algorithms and different APBD tasks, etc. Finally, this review proposes some APBD-related applications and a few exciting prospects and potential hot topics in future APBD research. We hope this review help researchers who involved in APBD domain to keep track of its development and tendency.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning",
    "url": "http://arxiv.org/abs/2508.09555v1",
    "authors": [
      "Ahmet \u00d6ztel",
      "\u0130smet Karaca"
    ],
    "published": "2025-08-13",
    "abstract": "Objective - This study presents a biometric identification method based on topological invariants from 2D iris images, representing iris texture via formally defined digital homology and evaluating classification performance.\n  Methods - Each normalized iris image (48x482 pixels) is divided into grids (e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their ratio using a recent algorithm for homology groups in 2D digital images. The resulting invariants form a feature matrix used with logistic regression, KNN, and SVM (with PCA and 100 randomized repetitions). A convolutional neural network (CNN) is trained on raw images for comparison.\n  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy, outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The topological features showed high accuracy with low variance.\n  Conclusion - This is the first use of topological invariants from formal digital homology for iris recognition. The method offers a compact, interpretable, and accurate alternative to deep learning, useful when explainability or limited data is important. Beyond iris recognition, it can apply to other biometrics, medical imaging, materials science, remote sensing, and interpretable AI. It runs efficiently on CPU-only systems and produces robust, explainable features valuable for security-critical domains.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification",
      "Recognition",
      "Regression"
    ]
  },
  {
    "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion",
    "url": "http://arxiv.org/abs/2508.06485v1",
    "authors": [
      "Sofiane Bouaziz",
      "Adel Hafiane",
      "Raphael Canals",
      "Rachid Nedjai"
    ],
    "published": "2025-08-08",
    "abstract": "Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.05271v1",
    "authors": [
      "Xiaoyang Zhang",
      "Guodong Fan",
      "Guang-Yong Chen",
      "Zhen Hua",
      "Jinjiang Li",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "published": "2025-08-07",
    "abstract": "Change detection in remote sensing imagery plays a vital role in various engineering applications, such as natural disaster monitoring, urban expansion tracking, and infrastructure management. Despite the remarkable progress of deep learning in recent years, most existing methods still rely on spatial-domain modeling, where the limited diversity of feature representations hinders the detection of subtle change regions. We observe that frequency-domain feature modeling particularly in the wavelet domain an amplify fine-grained differences in frequency components, enhancing the perception of edge changes that are challenging to capture in the spatial domain. Thus, we propose a method called Wavelet-Guided Dual-Frequency Encoding (WGDF). Specifically, we first apply Discrete Wavelet Transform (DWT) to decompose the input images into high-frequency and low-frequency components, which are used to model local details and global structures, respectively. In the high-frequency branch, we design a Dual-Frequency Feature Enhancement (DFFE) module to strengthen edge detail representation and introduce a Frequency-Domain Interactive Difference (FDID) module to enhance the modeling of fine-grained changes. In the low-frequency branch, we exploit Transformers to capture global semantic relationships and employ a Progressive Contextual Difference Module (PCDM) to progressively refine change regions, enabling precise structural semantic characterization. Finally, the high- and low-frequency features are synergistically fused to unify local sensitivity with global discriminability. Extensive experiments on multiple remote sensing datasets demonstrate that WGDF significantly alleviates edge ambiguity and achieves superior detection accuracy and robustness compared to state-of-the-art methods. The code will be available at https://github.com/boshizhang123/WGDF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Deep learning framework for crater detection and identification on the Moon and Mars",
    "url": "http://arxiv.org/abs/2508.03920v1",
    "authors": [
      "Yihan Ma",
      "Zeyang Yu",
      "Rohitash Chandra"
    ],
    "published": "2025-08-05",
    "abstract": "Impact craters are among the most prominent geomorphological features on planetary surfaces and are of substantial significance in planetary science research. Their spatial distribution and morphological characteristics provide critical information on planetary surface composition, geological history, and impact processes. In recent years, the rapid advancement of deep learning models has fostered significant interest in automated crater detection. In this paper, we apply advancements in deep learning models for impact crater detection and identification. We use novel models, including Convolutional Neural Networks (CNNs) and variants such as YOLO and ResNet. We present a framework that features a two-stage approach where the first stage features crater identification using simple classic CNN, ResNet-50 and YOLO. In the second stage, our framework employs YOLO-based detection for crater localisation. Therefore, we detect and identify different types of craters and present a summary report with remote sensing data for a selected region. We consider selected regions for craters and identification from Mars and the Moon based on remote sensing data. Our results indicate that YOLO demonstrates the most balanced crater detection performance, while ResNet-50 excels in identifying large craters with high precision.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "ResNet"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Probabilistic Emissivity Retrieval from Hyperspectral Data via Physics-Guided Variational Inference",
    "url": "http://arxiv.org/abs/2508.08291v2",
    "authors": [
      "Joshua R. Tempelman",
      "Kevin Mitchell",
      "Adam J. Wachtor",
      "Eric B. Flynn"
    ],
    "published": "2025-08-05",
    "abstract": "Recent research has proven neural networks to be a powerful tool for performing hyperspectral imaging (HSI) target identification. However, many deep learning frameworks deliver a single material class prediction and operate on a per-pixel basis; such approaches are limited in their interpretability and restricted to predicting materials that are accessible in available training libraries. In this work, we present an inverse modeling approach in the form of a physics-conditioned generative model.A probabilistic latent-variable model learns the underlying distribution of HSI radiance measurements and produces the conditional distribution of the emissivity spectrum. Moreover, estimates of the HSI scene's atmosphere and background are used as a physically relevant conditioning mechanism to contextualize a given radiance measurement during the encoding and decoding processes. Furthermore, we employ an in-the-loop augmentation scheme and physics-based loss criteria to avoid bias towards a predefined training material set and to encourage the model to learn physically consistent inverse mappings. Monte-Carlo sampling of the model's conditioned posterior delivers a sought emissivity distribution and allows for interpretable uncertainty quantification. Moreover, a distribution-based material matching scheme is presented to return a set of likely material matches for an inferred emissivity distribution. Hence, we present a strategy to incorporate contextual information about a given HSI scene, capture the possible variation of underlying material spectra, and provide interpretable probability measures of a candidate material accounting for given remotely-sensed radiance measurement.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling",
    "url": "http://arxiv.org/abs/2508.03774v1",
    "authors": [
      "Rui Zhu",
      "Yuexing Peng",
      "Peng Wang",
      "George C. Alexandropoulos",
      "Wenbo Wang",
      "Wei Xiang"
    ],
    "published": "2025-08-05",
    "abstract": "Electromagnetic (EM) scattering modeling is critical for radar remote sensing, however, its inherent complexity introduces significant computational challenges. Traditional numerical solvers offer high accuracy, but suffer from scalability issues and substantial computational costs. Pure data-driven deep learning approaches, while efficient, lack physical constraints embedding during training and require extensive labeled data, limiting their applicability and generalization. To overcome these limitations, we propose a U-shaped Physics-Informed Network (U-PINet), the first fully deep-learning-based, physics-informed hierarchical framework for computational EM designed to ensure physical consistency while maximizing computational efficiency. Motivated by the hierarchical decomposition strategy in EM solvers and the inherent sparsity of local EM coupling, the U-PINet models the decomposition and coupling of near- and far-field interactions through a multiscale processing neural network architecture, while employing a physics-inspired sparse graph representation to efficiently model both self- and mutual- coupling among mesh elements of complex $3$-Dimensional (3D) objects. This principled approach enables end-to-end multiscale EM scattering modeling with improved efficiency, generalization, and physical consistency. Experimental results showcase that the U-PINet accurately predicts surface current distributions, achieving close agreement with traditional solver, while significantly reducing computational time and outperforming conventional deep learning baselines in both accuracy and robustness. Furthermore, our evaluations on radar cross section prediction tasks confirm the feasibility of the U-PINet for downstream EM scattering applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MGCR-Net:Multimodal Graph-Conditioned Vision-Language Reconstruction Network for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.01555v1",
    "authors": [
      "Chengming Wang",
      "Guodong Fan",
      "Jinjiang Li",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "published": "2025-08-03",
    "abstract": "With the advancement of remote sensing satellite technology and the rapid progress of deep learning, remote sensing change detection (RSCD) has become a key technique for regional monitoring. Traditional change detection (CD) methods and deep learning-based approaches have made significant contributions to change analysis and detection, however, many outstanding methods still face limitations in the exploration and application of multimodal data. To address this, we propose the multimodal graph-conditioned vision-language reconstruction network (MGCR-Net) to further explore the semantic interaction capabilities of multimodal data. Multimodal large language models (MLLM) have attracted widespread attention for their outstanding performance in computer vision, particularly due to their powerful visual-language understanding and dialogic interaction capabilities. Specifically, we design a MLLM-based optimization strategy to generate multimodal textual data from the original CD images, which serve as textual input to MGCR. Visual and textual features are extracted through a dual encoder framework. For the first time in the RSCD task, we introduce a multimodal graph-conditioned vision-language reconstruction mechanism, which is integrated with graph attention to construct a semantic graph-conditioned reconstruction module (SGCM), this module generates vision-language (VL) tokens through graph-based conditions and enables cross-dimensional interaction between visual and textual features via multihead attention. The reconstructed VL features are then deeply fused using the language vision transformer (LViT), achieving fine-grained feature alignment and high-level semantic interaction. Experimental results on four public datasets demonstrate that MGCR achieves superior performance compared to mainstream CD methods. Our code is available on https://github.com/cn-xvkong/MGCR",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "CGCCE-Net:Change-Guided Cross Correlation Enhancement Network for Remote Sensing Building Change Detection",
    "url": "http://arxiv.org/abs/2508.01549v1",
    "authors": [
      "ChengMing Wang"
    ],
    "published": "2025-08-03",
    "abstract": "Change detection encompasses a variety of task types, and the goal of building change detection (BCD) tasks is to accurately locate buildings and distinguish changed building areas. In recent years, various deep learning-based BCD methods have achieved significant success in detecting difference regions by using different change information enhancement techniques, effectively improving the precision of BCD tasks. To address the issue of BCD with special colors, we propose the change-guided cross correlation enhancement network (CGCCE-Net). We design the change-guided residual refinement (CGRR) Branch, which focuses on extending shallow texture features to multiple scale features obtained from PVT, enabling early attention and acquisition of special colors. Then, channel spatial attention is used in the deep features to achieve independent information enhancement. Additionally, we construct the global cross correlation module (GCCM) to facilitate semantic information interaction between bi-temporal images, establishing building and target recognition relationships between different images. Further semantic feature enhancement is achieved through the semantic cognitive enhancement module (SCEM), and finally, the cross fusion decoder (CFD) is used for change information fusion and image reconstruction. Extensive experiments on three public datasets demonstrate that our CGCCE-Net outperforms mainstream BCD methods with outstanding performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "SCANet: Split Coordinate Attention Network for Building Footprint Extraction",
    "url": "http://arxiv.org/abs/2507.20809v1",
    "authors": [
      "Chunshi Wang",
      "Bin Zhao",
      "Shuxue Ding"
    ],
    "published": "2025-07-28",
    "abstract": "Building footprint extraction holds immense significance in remote sensing image analysis and has great value in urban planning, land use, environmental protection and disaster assessment. Despite the progress made by conventional and deep learning approaches in this field, they continue to encounter significant challenges. This paper introduces a novel plug-and-play attention module, Split Coordinate Attention (SCA), which ingeniously captures spatially remote interactions by employing two spatial range of pooling kernels, strategically encoding each channel along x and y planes, and separately performs a series of split operations for each feature group, thus enabling more efficient semantic feature extraction. By inserting into a 2D CNN to form an effective SCANet, our SCANet outperforms recent SOTA methods on the public Wuhan University (WHU) Building Dataset and Massachusetts Building Dataset in terms of various metrics. Particularly SCANet achieves the best IoU, 91.61% and 75.49% for the two datasets. Our code is available at https://github.com/AiEson/SCANet",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit",
    "url": "http://arxiv.org/abs/2507.20623v1",
    "authors": [
      "Yang Zhao",
      "Shusheng Li",
      "Xueshang Feng"
    ],
    "published": "2025-07-28",
    "abstract": "As the development of lightweight deep learning algorithms, various deep neural network (DNN) models have been proposed for the remote sensing scene classification (RSSC) application. However, it is still challenging for these RSSC models to achieve optimal performance among model accuracy, inference latency, and energy consumption on resource-constrained edge devices. In this paper, we propose a lightweight RSSC framework, which includes a distilled global filter network (GFNet) model and an early-exit mechanism designed for edge devices to achieve state-of-the-art performance. Specifically, we first apply frequency domain distillation on the GFNet model to reduce model size. Then we design a dynamic early-exit model tailored for DNN models on edge devices to further improve model inference efficiency. We evaluate our E3C model on three edge devices across four datasets. Extensive experimental results show that it achieves an average of 1.3x speedup on model inference and over 40% improvement on energy efficiency, while maintaining high classification accuracy.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges",
    "url": "http://arxiv.org/abs/2507.18376v6",
    "authors": [
      "Xing Hu",
      "Haodong Chen",
      "Qianqian Duan",
      "Dawei Zhang"
    ],
    "published": "2025-07-24",
    "abstract": "With the global population increasing and arable land resources becoming increasingly limited, smart and precision agriculture have emerged as essential directions for sustainable agricultural development. Artificial intelligence (AI), particularly deep learning models, has been widely adopted in applications such as crop monitoring, pest detection, and yield prediction. Among recent generative models, diffusion models have demonstrated considerable potential in agricultural image processing, data augmentation, and remote sensing analysis. Compared to traditional generative adversarial networks (GANs), diffusion models exhibit greater training stability and superior image generation quality, effectively addressing challenges such as limited annotated datasets and imbalanced sample distributions in agricultural scenarios. This paper reviews recent advancements in the application of diffusion models within agriculture, focusing on their roles in crop disease and pest detection, remote sensing image enhancement, crop growth prediction, and agricultural resource management. Diffusion models have been found useful in improving tasks like image generation, denoising, and data augmentation in agriculture, especially when environmental noise or variability is present. While their high computational requirements and limited generalizability across domains remain concerns, the approach is gradually proving effective in real-world applications such as precision crop monitoring. As research progresses, these models may help support sustainable agriculture and address emerging challenges in food systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN",
      "Diffusion Models"
    ],
    "applications": [
      "Detection",
      "Image Generation",
      "Forecast"
    ]
  },
  {
    "title": "Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels for Building Detection",
    "url": "http://arxiv.org/abs/2507.16657v1",
    "authors": [
      "Shuang Song",
      "Yang Tang",
      "Rongjun Qin"
    ],
    "published": "2025-07-22",
    "abstract": "Deep learning has significantly advanced building segmentation in remote sensing, yet models struggle to generalize on data of diverse geographic regions due to variations in city layouts and the distribution of building types, sizes and locations. However, the amount of time-consuming annotated data for capturing worldwide diversity may never catch up with the demands of increasingly data-hungry models. Thus, we propose a novel approach: re-training models at test time using synthetic data tailored to the target region's city layout. This method generates geo-typical synthetic data that closely replicates the urban structure of a target area by leveraging geospatial data such as street network from OpenStreetMap. Using procedural modeling and physics-based rendering, very high-resolution synthetic images are created, incorporating domain randomization in building shapes, materials, and environmental illumination. This enables the generation of virtually unlimited training samples that maintain the essential characteristics of the target environment. To overcome synthetic-to-real domain gaps, our approach integrates geo-typical data into an adversarial domain adaptation framework for building segmentation. Experiments demonstrate significant performance enhancements, with median improvements of up to 12%, depending on the domain gap. This scalable and cost-effective method blends partial geographic knowledge with synthetic imagery, providing a promising solution to the \"model collapse\" issue in purely synthetic datasets. It offers a practical pathway to improving generalization in remote sensing building segmentation without extensive real-world annotations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing",
    "url": "http://arxiv.org/abs/2507.16228v1",
    "authors": [
      "Shreelekha Revankar",
      "Utkarsh Mall",
      "Cheng Perng Phoo",
      "Kavita Bala",
      "Bharath Hariharan"
    ],
    "published": "2025-07-22",
    "abstract": "Natural disasters cause devastating damage to communities and infrastructure every year. Effective disaster response is hampered by the difficulty of accessing affected areas during and after events. Remote sensing has allowed us to monitor natural disasters in a remote way. More recently there have been advances in computer vision and deep learning that help automate satellite imagery analysis, However, they remain limited by their narrow focus on specific disaster types, reliance on manual expert interpretation, and lack of datasets with sufficient temporal granularity or natural language annotations for tracking disaster progression. We present MONITRS, a novel multimodal dataset of more than 10,000 FEMA disaster events with temporal satellite imagery and natural language annotations from news articles, accompanied by geotagged locations, and question-answer pairs. We demonstrate that fine-tuning existing MLLMs on our dataset yields significant performance improvements for disaster monitoring tasks, establishing a new benchmark for machine learning-assisted disaster response systems. Code can be found at: https://github.com/ShreelekhaR/MONITRS",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery",
    "url": "http://arxiv.org/abs/2507.16849v1",
    "authors": [
      "Yi-Shan Chu",
      "Hsuan-Cheng Wei"
    ],
    "published": "2025-07-21",
    "abstract": "We propose a vision transformer (ViT)-based deep learning framework to refine disaster-affected area segmentation from remote sensing imagery, aiming to support and enhance the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The process starts with a small set of manually annotated regions. We then apply principal component analysis (PCA)-based feature space analysis and construct a confidence index (CI) to expand these labels, producing a weakly supervised training set. These expanded labels are then used to train ViT-based encoder-decoder models with multi-band inputs from Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder variants and multi-stage loss strategies to improve performance under limited supervision. During the evaluation, model predictions are compared with higher-resolution EVAP output to assess spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate that our framework improves the smoothness and reliability of segmentation results, offering a scalable approach for disaster mapping when accurate ground truth is unavailable.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
    "url": "http://arxiv.org/abs/2507.13852v1",
    "authors": [
      "Luigi Russo",
      "Francesco Mauro",
      "Babak Memar",
      "Alessandro Sebastianelli",
      "Silvia Liberata Ullo",
      "Paolo Gamba"
    ],
    "published": "2025-07-18",
    "abstract": "Building segmentation in urban areas is essential in fields such as urban planning, disaster response, and population mapping. Yet accurately segmenting buildings in dense urban regions presents challenges due to the large size and high resolution of satellite images. This study investigates the use of a Quanvolutional pre-processing to enhance the capability of the Attention U-Net model in the building segmentation. Specifically, this paper focuses on the urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR) imagery. In this work, Quanvolution was used to extract more informative feature maps that capture essential structural details in radar imagery, proving beneficial for accurate building segmentation. Preliminary results indicate that proposed methodology achieves comparable test accuracy to the standard Attention U-Net model while significantly reducing network parameters. This result aligns with findings from previous works, confirming that Quanvolution not only maintains model accuracy but also increases computational efficiency. These promising outcomes highlight the potential of quantum-assisted Deep Learning frameworks for large-scale building segmentation in urban environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A Deep-Learning Framework for Land-Sliding Classification from Remote Sensing Image",
    "url": "http://arxiv.org/abs/2507.12939v1",
    "authors": [
      "Hieu Tang",
      "Truong Vo",
      "Dong Pham",
      "Toan Nguyen",
      "Lam Pham",
      "Truong Nguyen"
    ],
    "published": "2025-07-17",
    "abstract": "The use of satellite imagery combined with deep learning to support automatic landslide detection is becoming increasingly widespread. However, selecting an appropriate deep learning architecture to optimize performance while avoiding overfitting remains a critical challenge. To address these issues, we propose a deep-learning based framework for landslide detection from remote sensing image in this paper. The proposed framework presents an effective combination of the online an offline data augmentation to tackle the imbalanced data, a backbone EfficientNet\\_Large deep learning model for extracting robust embedding features, and a post-processing SVM classifier to balance and enhance the classification performance. The proposed model achieved an F1-score of 0.8938 on the public test set of the Zindi challenge.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Precision Spatio-Temporal Feature Fusion for Robust Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2507.11523v1",
    "authors": [
      "Buddhi Wijenayake",
      "Athulya Ratnayake",
      "Praveen Sumanasekara",
      "Nichula Wasalathilaka",
      "Mathivathanan Piratheepan",
      "Roshan Godaliyadda",
      "Mervyn Ekanayake",
      "Vijitha Herath"
    ],
    "published": "2025-07-15",
    "abstract": "Remote sensing change detection is vital for monitoring environmental and urban transformations but faces challenges like manual feature extraction and sensitivity to noise. Traditional methods and early deep learning models, such as convolutional neural networks (CNNs), struggle to capture long-range dependencies and global context essential for accurate change detection in complex scenes. While Transformer-based models mitigate these issues, their computational complexity limits their applicability in high-resolution remote sensing. Building upon ChangeMamba architecture, which leverages state space models for efficient global context modeling, this paper proposes precision fusion blocks to capture channel-wise temporal variations and per-pixel differences for fine-grained change detection. An enhanced decoder pipeline, incorporating lightweight channel reduction mechanisms, preserves local details with minimal computational cost. Additionally, an optimized loss function combining Cross Entropy, Dice and Lovasz objectives addresses class imbalance and boosts Intersection-over-Union (IoU). Evaluations on SYSU-CD, LEVIR-CD+, and WHU-CD datasets demonstrate superior precision, recall, F1 score, IoU, and overall accuracy compared to state-of-the-art methods, highlighting the approach's robustness for remote sensing change detection. For complete transparency, the codes and pretrained models are accessible at https://github.com/Buddhi19/MambaCD.git",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images",
    "url": "http://arxiv.org/abs/2507.11143v1",
    "authors": [
      "Lam Pham",
      "Cam Le",
      "Hieu Tang",
      "Khang Truong",
      "Truong Nguyen",
      "Jasmin Lampert",
      "Alexander Schindler",
      "Martin Boyer",
      "Son Phan"
    ],
    "published": "2025-07-15",
    "abstract": "In recent years, landslide disasters have reported frequently due to the extreme weather events of droughts, floods , storms, or the consequence of human activities such as deforestation, excessive exploitation of natural resources. However, automatically observing landslide is challenging due to the extremely large observing area and the rugged topography such as mountain or highland. This motivates us to propose an end-to-end deep-learning-based model which explores the remote sensing images for automatically observing landslide events. By considering remote sensing images as the input data, we can obtain free resource, observe large and rough terrains by time. To explore the remote sensing images, we proposed a novel neural network architecture which is for two tasks of landslide detection and landslide segmentation. We evaluated our proposed model on three different benchmark datasets of LandSlide4Sense, Bijie, and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23, 93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense, Nepal datasets. These experimental results prove potential to integrate our proposed model into real-life landslide observation systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks",
    "url": "http://arxiv.org/abs/2507.10381v1",
    "authors": [
      "Aaryam Sharma"
    ],
    "published": "2025-07-14",
    "abstract": "Topological data analysis (TDA) is a relatively new field that is gaining rapid adoption due to its robustness and ability to effectively describe complex datasets by quantifying geometric information. In imaging contexts, TDA typically models data as filtered cubical complexes from which we can extract discriminative features using persistence homology. Meanwhile, convolutional neural networks (CNNs) have been shown to be biased towards texture based local features. To address this limitation, we propose a TDA feature engineering pipeline and a simple method to integrate topological features with deep learning models on remote sensing classification. Our method improves the performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving 99.33% accuracy, which surpasses all previously reported single-model accuracies, including those with larger architectures, such as ResNet50 (2x larger) and XL Vision Transformers (197x larger). We additionally show that our method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45 dataset. To our knowledge, this is the first application of TDA features in satellite scene classification with deep learning. This demonstrates that TDA features can be integrated with deep learning models, even on datasets without explicit topological structures, thereby increasing the applicability of TDA. A clean implementation of our method will be made publicly available upon publication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection",
    "url": "http://arxiv.org/abs/2507.09541v1",
    "authors": [
      "Zihao Xiong",
      "Fei Zhou",
      "Fengyi Wu",
      "Shuai Yuan",
      "Maixia Fu",
      "Zhenming Peng",
      "Jian Yang",
      "Yimian Dai"
    ],
    "published": "2025-07-13",
    "abstract": "Infrared small target detection plays a vital role in remote sensing, industrial monitoring, and various civilian applications. Despite recent progress powered by deep learning, many end-to-end convolutional models tend to pursue performance by stacking increasingly complex architectures, often at the expense of interpretability, parameter efficiency, and generalization. These models typically overlook the intrinsic sparsity prior of infrared small targets--an essential cue that can be explicitly modeled for both performance and efficiency gains. To address this, we revisit the model-based paradigm of Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network (DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware prior into a learnable architecture. Unlike conventional deep unfolding methods that rely on static, globally learned parameters, DRPCA-Net introduces a dynamic unfolding mechanism via a lightweight hypernetwork. This design enables the model to adaptively generate iteration-wise parameters conditioned on the input scene, thereby enhancing its robustness and generalization across diverse backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to better capture contextual variations within the background, leading to more accurate low-rank estimation and improved separation of small targets. Extensive experiments on multiple public infrared datasets demonstrate that DRPCA-Net significantly outperforms existing state-of-the-art methods in detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer",
    "url": "http://arxiv.org/abs/2507.08741v2",
    "authors": [
      "Tianlong Ai",
      "Tianzhu Liu",
      "Haochen Jiang",
      "Yanfeng Gu"
    ],
    "published": "2025-07-11",
    "abstract": "Hierarchical land cover and land use (LCLU) classification aims to assign pixel-wise labels with multiple levels of semantic granularity to remote sensing (RS) imagery. However, existing deep learning-based methods face two major challenges: 1) They predominantly adopt a flat classification paradigm, which limits their ability to generate end-to-end multi-granularity hierarchical predictions aligned with tree-structured hierarchies used in practice. 2) Most cross-domain studies focus on performance degradation caused by sensor or scene variations, with limited attention to transferring LCLU models to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop classification). These limitations hinder the flexibility and generalization of LCLU models in practical applications. To address these challenges, we propose HieraRS, a novel hierarchical interpretation paradigm that enables multi-granularity predictions and supports the efficient transfer of LCLU models to cross-domain tasks with heterogeneous tree-structured hierarchies. We introduce the Bidirectional Hierarchical Consistency Constraint Mechanism (BHCCM), which can be seamlessly integrated into mainstream flat classification models to generate hierarchical predictions, while improving both semantic consistency and classification accuracy. Furthermore, we present TransLU, a dual-branch cross-domain transfer framework comprising two key components: Cross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment (CDSA). TransLU supports dynamic category expansion and facilitates the effective adaptation of LCLU models to heterogeneous hierarchies. In addition, we construct MM-5B, a large-scale multi-modal hierarchical land use dataset featuring pixel-wise annotations. The code and MM-5B dataset will be released at: https://github.com/AI-Tianlong/HieraRS.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Ecological Legacies of Pre-Columbian Settlements Evident in Palm Clusters of Neotropical Mountain Forests",
    "url": "http://arxiv.org/abs/2507.06949v2",
    "authors": [
      "Sebastian Fajardo",
      "Sina Mohammadi",
      "Jonas Gregorio de Souza",
      "C\u00e9sar Ardila",
      "Alan Tapscott Baltar",
      "Shaddai Heidgen",
      "Maria Isabel Mayorga Hern\u00e1ndez",
      "Sylvia Mota de Oliveira",
      "Fernando Montejo",
      "Marco Moderato",
      "Vinicius Peripato",
      "Katy Puche",
      "Carlos Reina",
      "Juan Carlos Vargas",
      "Frank W. Takes",
      "Marco Madella"
    ],
    "published": "2025-07-09",
    "abstract": "Ancient populations markedly transformed Neotropical forests, yet the spatial extent of their ecological influence remains underexplored at high resolution. Here we present a deep learning and remote sensing based approach to estimate areas of pre-Columbian forest modification based on modern vegetation. We apply this method to high-resolution satellite imagery from the Sierra Nevada de Santa Marta, Colombia, as a demonstration of a scalable approach, to evaluate palm tree distributions in relation to archaeological infrastructure. Palms were significantly more abundant near archaeological sites with large infrastructure investment. The extent of the largest palm cluster indicates that ancient human-managed areas linked to major infrastructure sites may be up to two orders of magnitude bigger than indicated by current archaeological evidence alone. Our findings suggest that pre-Columbian populations influenced vegetation, fostering conditions conducive to palm proliferation, leaving a lasting ecological footprint. This may have lowered the logistical costs of establishing infrastructure-heavy settlements in less accessible locations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery",
    "url": "http://arxiv.org/abs/2507.01747v1",
    "authors": [
      "Nora Gourmelon",
      "Marcel Dreier",
      "Martin Mayr",
      "Thorsten Seehaus",
      "Dakota Pyles",
      "Matthias Braun",
      "Andreas Maier",
      "Vincent Christlein"
    ],
    "published": "2025-07-02",
    "abstract": "Glaciers are losing ice mass at unprecedented rates, increasing the need for accurate, year-round monitoring to understand frontal ablation, particularly the factors driving the calving process. Deep learning models can extract calving front positions from Synthetic Aperture Radar imagery to track seasonal ice losses at the calving fronts of marine- and lake-terminating glaciers. The current state-of-the-art model relies on ImageNet-pretrained weights. However, they are suboptimal due to the domain shift between the natural images in ImageNet and the specialized characteristics of remote sensing imagery, in particular for Synthetic Aperture Radar imagery. To address this challenge, we propose two novel self-supervised multimodal pretraining techniques that leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14 Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the dataset. Additionally, we introduce a novel hybrid model architecture that combines a Swin Transformer encoder with a residual Convolutional Neural Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean distance error of 293 m on the \"CAlving Fronts and where to Find thEm\" (CaFFe) benchmark dataset, outperforming the prior best model by 67 m. Evaluating an ensemble of the proposed model on a multi-annotator study of the benchmark dataset reveals a mean distance error of 75 m, approaching the human performance of 38 m. This advancement enables precise monitoring of seasonal changes in glacier calving fronts.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images",
    "url": "http://arxiv.org/abs/2507.01502v1",
    "authors": [
      "Ozan Durgut",
      "Beril Kallfelz-Sirmacek",
      "Cem Unsalan"
    ],
    "published": "2025-07-02",
    "abstract": "Global warming, loss of biodiversity, and air pollution are among the most significant problems facing Earth. One of the primary challenges in addressing these issues is the lack of monitoring forests to protect them. To tackle this problem, it is important to leverage remote sensing and computer vision methods to automate monitoring applications. Hence, automatic tree crown detection algorithms emerged based on traditional and deep learning methods. In this study, we first introduce two different tree crown detection methods based on these approaches. Then, we form a novel rule-based approach that integrates these two methods to enhance robustness and accuracy of tree crown detection results. While traditional methods are employed for feature extraction and segmentation of forested areas, deep learning methods are used to detect tree crowns in our method. With the proposed rule-based approach, we post-process these results, aiming to increase the number of detected tree crowns through neighboring trees and localized operations. We compare the obtained results with the proposed method in terms of the number of detected tree crowns and report the advantages, disadvantages, and areas for improvement of the obtained outcomes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions",
    "url": "http://arxiv.org/abs/2507.01123v1",
    "authors": [
      "Rahul A. Burange",
      "Harsh K. Shinde",
      "Omkar Mutyalwar"
    ],
    "published": "2025-07-01",
    "abstract": "Landslides pose severe threats to infrastructure, economies, and human lives, necessitating accurate detection and predictive mapping across diverse geographic regions. With advancements in deep learning and remote sensing, automated landslide detection has become increasingly effective. This study presents a comprehensive approach integrating multi-source satellite imagery and deep learning models to enhance landslide identification and prediction. We leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and Digital Elevation Model (DEM) layers to capture critical environmental features influencing landslide occurrences. Various geospatial analysis techniques are employed to assess the impact of terra in characteristics, vegetation cover, and rainfall on detection accuracy. Additionally, we evaluate the performance of multiple stateof-the-art deep learning segmentation models, including U-Net, DeepLabV3+, and Res-Net, to determine their effectiveness in landslide detection. The proposed framework contributes to the development of reliable early warning systems, improved disaster risk management, and sustainable land-use planning. Our findings provide valuable insights into the potential of deep learning and multi-source remote sensing in creating robust, scalable, and transferable landslide prediction models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Farm-Level, In-Season Crop Identification for India",
    "url": "http://arxiv.org/abs/2507.02972v1",
    "authors": [
      "Ishan Deshpande",
      "Amandeep Kaur Reehal",
      "Chandan Nath",
      "Renu Singh",
      "Aayush Patel",
      "Aishwarya Jayagopal",
      "Gaurav Singh",
      "Gaurav Aggarwal",
      "Amit Agarwal",
      "Prathmesh Bele",
      "Sridhar Reddy",
      "Tanya Warrier",
      "Kinjal Singh",
      "Ashish Tendulkar",
      "Luis Pazos Outon",
      "Nikita Saxena",
      "Agata Dondzik",
      "Dinesh Tewari",
      "Shruti Garg",
      "Avneet Singh",
      "Harsh Dhand",
      "Vaibhav Rajan",
      "Alok Talekar"
    ],
    "published": "2025-06-30",
    "abstract": "Accurate, timely, and farm-level crop type information is paramount for national food security, agricultural policy formulation, and economic planning, particularly in agriculturally significant nations like India. While remote sensing and machine learning have become vital tools for crop monitoring, existing approaches often grapple with challenges such as limited geographical scalability, restricted crop type coverage, the complexities of mixed-pixel and heterogeneous landscapes, and crucially, the robust in-season identification essential for proactive decision-making.\n  We present a framework designed to address the critical data gaps for targeted data driven decision making which generates farm-level, in-season, multi-crop identification at national scale (India) using deep learning. Our methodology leverages the strengths of Sentinel-1 and Sentinel-2 satellite imagery, integrated with national-scale farm boundary data. The model successfully identifies 12 major crops (which collectively account for nearly 90% of India's total cultivated area showing an agreement with national crop census 2023-24 of 94% in winter, and 75% in monsoon season). Our approach incorporates an automated season detection algorithm, which estimates crop sowing and harvest periods. This allows for reliable crop identification as early as two months into the growing season and facilitates rigorous in-season performance evaluation. Furthermore, we have engineered a highly scalable inference pipeline, culminating in what is, to our knowledge, the first pan-India, in-season, farm-level crop type data product. The system's effectiveness and scalability are demonstrated through robust validation against national agricultural statistics, showcasing its potential to deliver actionable, data-driven insights for transformative agricultural monitoring and management across India.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data",
    "url": "http://arxiv.org/abs/2506.22939v1",
    "authors": [
      "Ghufran A. Omran",
      "Wassan Saad Abduljabbar Hayale",
      "Ahmad AbdulQadir AlRababah",
      "Israa Ibraheem Al-Barazanchi",
      "Ravi Sekhar",
      "Pritesh Shah",
      "Sushma Parihar",
      "Harshavardhan Reddy Penubadi"
    ],
    "published": "2025-06-28",
    "abstract": "Scene categorization (SC) in remotely acquired images is an important subject with broad consequences in different fields, including catastrophe control, ecological observation, architecture for cities, and more. Nevertheless, its several apps, reaching a high degree of accuracy in SC from distant observation data has demonstrated to be difficult. This is because traditional conventional deep learning models require large databases with high variety and high levels of noise to capture important visual features. To address these problems, this investigation file introduces an innovative technique referred to as the Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type of scenes in remote sensing data. The investigation compares the execution of CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory (CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF), Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional Neural Networks Data Augmentation (CNN-DA). The results demonstrate that CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%, MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance of physical confirmation to ensure the efficiency of satellite data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake",
    "url": "http://arxiv.org/abs/2506.22338v1",
    "authors": [
      "Luigi Russo",
      "Deodato Tapete",
      "Silvia Liberata Ullo",
      "Paolo Gamba"
    ],
    "published": "2025-06-27",
    "abstract": "Building damage identification shortly after a disaster is crucial for guiding emergency response and recovery efforts. Although optical satellite imagery is commonly used for disaster mapping, its effectiveness is often hampered by cloud cover or the absence of pre-event acquisitions. To overcome these challenges, we introduce a novel multimodal deep learning (DL) framework for detecting building damage using single-date very high resolution (VHR) Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI) COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data. Our method integrates SAR image patches, OpenStreetMap (OSM) building footprints, digital surface model (DSM) data, and structural and exposure attributes from the Global Earthquake Model (GEM) to improve detection accuracy and contextual interpretation. Unlike existing approaches that depend on pre and post event imagery, our model utilizes only post event data, facilitating rapid deployment in critical scenarios. The framework effectiveness is demonstrated using a new dataset from the 2023 earthquake in Turkey, covering multiple cities with diverse urban settings. Results highlight that incorporating geospatial features significantly enhances detection performance and generalizability to previously unseen areas. By combining SAR imagery with detailed vulnerability and exposure information, our approach provides reliable and rapid building damage assessments without the dependency from available pre-event data. Moreover, the automated and scalable data generation process ensures the framework's applicability across diverse disaster-affected regions, underscoring its potential to support effective disaster management and recovery efforts. Code and data will be made available upon acceptance of the paper.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images",
    "url": "http://arxiv.org/abs/2506.21945v1",
    "authors": [
      "Naftaly Wambugu",
      "Ruisheng Wang",
      "Bo Guo",
      "Tianshu Yu",
      "Sheng Xu",
      "Mohammed Elhassan"
    ],
    "published": "2025-06-27",
    "abstract": "Land cover maps generated from semantic segmentation of high-resolution remotely sensed images have drawn mucon in the photogrammetry and remote sensing research community. Currently, massive fine-resolution remotely sensed (FRRS) images acquired by improving sensing and imaging technologies become available. However, accurate semantic segmentation of such FRRS images is greatly affected by substantial class disparities, the invisibility of key ground objects due to occlusion, and object size variation. Despite the extraordinary potential in deep convolutional neural networks (DCNNs) in image feature learning and representation, extracting sufficient features from FRRS images for accurate semantic segmentation is still challenging. These challenges demand the deep learning models to learn robust features and generate sufficient feature descriptors. Specifically, learning multi-contextual features to guarantee adequate coverage of varied object sizes from the ground scene and harnessing global-local contexts to overcome class disparities challenge even profound networks. Deeper networks significantly lose spatial details due to gradual downsampling processes resulting in poor segmentation results and coarse boundaries. This article presents a stacked deep residual network (SDRNet) for semantic segmentation from FRRS images. The proposed framework utilizes two stacked encoder-decoder networks to harness long-range semantics yet preserve spatial information and dilated residual blocks (DRB) between each encoder and decoder network to capture sufficient global dependencies thus improving segmentation performance. Our experimental results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate that the SDRNet performs effectively and competitively against current DCNNs in semantic segmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer",
    "url": "http://arxiv.org/abs/2506.21880v2",
    "authors": [
      "Yuansheng Li",
      "Yunhao Zou",
      "Linwei Chen",
      "Ying Fu"
    ],
    "published": "2025-06-27",
    "abstract": "Interferometric Hyperspectral Imaging (IHI) is a critical technique for large-scale remote sensing tasks due to its advantages in flux and spectral resolution. However, IHI is susceptible to complex errors arising from imaging steps, and its quality is limited by existing signal processing-based reconstruction algorithms. Two key challenges hinder performance enhancement: 1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific degradation components through learning-based methods. To address these challenges, we propose a novel IHI reconstruction pipeline. First, based on imaging physics and radiometric calibration data, we establish a simplified yet accurate IHI degradation model and a parameter estimation method. This model enables the synthesis of realistic IHI training datasets from hyperspectral images (HSIs), bridging the gap between IHI reconstruction and deep learning. Second, we design the Interferometric Hyperspectral Reconstruction Unfolding Transformer (IHRUT), which achieves effective spectral correction and detail restoration through a stripe-pattern enhancement mechanism and a spatial-spectral transformer architecture. Experimental results demonstrate the superior performance and generalization capability of our method.The code and are available at https://github.com/bit1120203554/IHRUT.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2506.21109v2",
    "authors": [
      "Luosheng Xu",
      "Dalin Zhang",
      "Zhaohui Song"
    ],
    "published": "2025-06-26",
    "abstract": "Remote sensing change detection is essential for monitoring urban expansion, disaster assessment, and resource management, offering timely, accurate, and large-scale insights into dynamic landscape transformations. While deep learning has revolutionized change detection, the increasing complexity and computational demands of modern models have not necessarily translated into significant accuracy gains. Instead of following this trend, this study explores a more efficient approach, focusing on lightweight models that maintain high accuracy while minimizing resource consumption, which is an essential requirement for on-satellite processing. To this end, we propose FlickCD, which means quick flick then get great results, pushing the boundaries of the performance-resource trade-off. FlickCD introduces an Enhanced Difference Module (EDM) to amplify critical feature differences between temporal phases while suppressing irrelevant variations such as lighting and weather changes, thereby reducing computational costs in the subsequent change decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion Blocks, leveraging Shifted Window Self-Attention (SWSA) and Efficient Global Self-Attention (EGSA) to effectively capture semantic information at multiple scales, preserving both coarse- and fine-grained changes. Extensive experiments on four benchmark datasets demonstrate that FlickCD reduces computational and storage overheads by more than an order of magnitude while achieving state-of-the-art (SOTA) performance or incurring only a minor (<1% F1) accuracy trade-off. The implementation code is publicly available at https://github.com/xulsh8/FlickCD.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Photon Absorption Remote Sensing (PARS): Comprehensive Absorption Imaging Enabling Label-Free Biomolecule Characterization and Mapping",
    "url": "http://arxiv.org/abs/2506.20069v1",
    "authors": [
      "Benjamin R. Ecclestone",
      "James A. Tummon Simmons",
      "James E. D. Tweel",
      "Deepak Dinakaran",
      "Parsin Haji Reza"
    ],
    "published": "2025-06-25",
    "abstract": "Label-free optical absorption microscopy techniques continue to evolve as promising tools for label-free histopathological imaging of cells and tissues. However, critical challenges relating to specificity and contrast, as compared to current gold-standard methods continue to hamper adoption. This work introduces Photon Absorption Remote Sensing (PARS), a new absorption microscope modality, which simultaneously captures the dominant de-excitation processes following an absorption event. In PARS, radiative (auto-fluorescence) and non-radiative (photothermal and photoacoustic) relaxation processes are collected simultaneously, providing enhanced specificity to a range of biomolecules. As an example, a multiwavelength PARS system featuring UV (266 nm) and visible (532 nm) excitation is applied to imaging human skin, and murine brain tissue samples. It is shown that PARS can directly characterize, differentiate, and unmix, clinically relevant biomolecules inside complex tissues samples using established statistical processing methods. Gaussian mixture models (GMM) are used to characterize clinically relevant biomolecules (e.g., white, and gray matter) based on their PARS signals, while non-negative least squares (NNLS) is applied to map the biomolecule abundance in murine brain tissues, without stained ground truth images or deep-learning methods. PARS unmixing and abundance estimates are directly validated and compared against chemically stained ground truth images, and deep learning based-image transforms. Overall, it is found that the PARS unique and rich contrast may provide comprehensive, and otherwise inaccessible, label-free characterization of molecular pathology, representing a new source of data to develop AI and machine learning methods for diagnostics and visualization.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Video Compression for Spatiotemporal Earth System Data",
    "url": "http://arxiv.org/abs/2506.19656v1",
    "authors": [
      "Oscar J. Pellicer-Valero",
      "Cesar Aybar",
      "Gustau Camps Valls"
    ],
    "published": "2025-06-24",
    "abstract": "Large-scale Earth system datasets, from high-resolution remote sensing imagery to spatiotemporal climate model outputs, exhibit characteristics analogous to those of standard videos. Their inherent spatial, temporal, and spectral redundancies can thus be readily exploited by established video compression techniques. Here, we present xarrayvideo, a Python library for compressing multichannel spatiotemporal datasets by encoding them as videos. Our approach achieves compression ratios of up to 250x while maintaining high fidelity by leveraging standard, well-optimized video codecs through ffmpeg. We demonstrate the library's effectiveness on four real-world multichannel spatiotemporal datasets: DynamicEarthNet (very high resolution Planet images), DeepExtremeCubes (high resolution Sentinel-2 images), ERA5 (weather reanalysis data), and the SimpleS2 dataset (high resolution multichannel Sentinel-2 images), achieving Peak Signal-to-Noise Ratios (PSNRs) of 55.86, 40.60, 46.58, and 43.23 dB at 0.1 bits per pixel per band (bpppb) and 65.91, 54.28, 62.90, and 55.04 dB at 1 bpppb. We are redistributing two of these datasets, DeepExtremeCubes (2.3 Tb) and DynamicEarthNet (525 Gb), in the machine-learning-ready and cloud-ready TACO format through HuggingFace at significantly reduced sizes (270 Gb and 8.5 Gb, respectively) without compromising quality (PSNR 55.77-56.65 and 60.15). No performance loss is observed when the compressed versions of these datasets are used in their respective deep learning-based downstream tasks (next step reflectance prediction and landcover segmentation). In conclusion, xarrayvideo presents an efficient solution for handling the rapidly growing size of Earth observation datasets, making advanced compression techniques accessible and practical to the Earth science community. The library is available for use at https://github.com/IPL-UV/xarrayvideo",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "MambaOutRS: A Hybrid CNN-Fourier Architecture for Remote Sensing Image Classification",
    "url": "http://arxiv.org/abs/2506.19561v1",
    "authors": [
      "Minjong Cheon",
      "Changbae Mun"
    ],
    "published": "2025-06-24",
    "abstract": "Recent advances in deep learning for vision tasks have seen the rise of State Space Models (SSMs) like Mamba, celebrated for their linear scalability. However, their adaptation to 2D visual data often necessitates complex modifications that may diminish efficiency. In this paper, we introduce MambaOutRS, a novel hybrid convolutional architecture for remote sensing image classification that re-evaluates the necessity of recurrent SSMs. MambaOutRS builds upon stacked Gated CNN blocks for local feature extraction and introduces a novel Fourier Filter Gate (FFG) module that operates in the frequency domain to capture global contextual information efficiently. Our architecture employs a four-stage hierarchical design and was extensively evaluated on challenging remote sensing datasets: UC Merced, AID, NWPU-RESISC45, and EuroSAT. MambaOutRS consistently achieved state-of-the-art (SOTA) performance across these benchmarks. Notably, our MambaOutRS-t variant (24.0M parameters) attained the highest F1-scores of 98.41\\% on UC Merced and 95.99\\% on AID, significantly outperforming existing baselines, including larger transformer models and Mamba-based architectures, despite using considerably fewer parameters. An ablation study conclusively demonstrates the critical role of the Fourier Filter Gate in enhancing the model's ability to capture global spatial patterns, leading to robust and accurate classification. These results strongly suggest that the complexities of recurrent SSMs can be effectively superseded by a judicious combination of gated convolutions for spatial mixing and frequency-based gates for spectral global context. Thus, MambaOutRS provides a compelling and efficient paradigm for developing high-performance deep learning models in remote sensing and other vision domains, particularly where computational efficiency is paramount.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Observation-driven correction of numerical weather prediction for marine winds",
    "url": "http://arxiv.org/abs/2512.03606v1",
    "authors": [
      "Matteo Peduto",
      "Qidong Yang",
      "Jonathan Giezendanner",
      "Devis Tuia",
      "Sherrie Wang"
    ],
    "published": "2025-12-03",
    "abstract": "Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Super-resolution of satellite-derived SST data via Generative Adversarial Networks",
    "url": "http://arxiv.org/abs/2511.22610v1",
    "authors": [
      "Claudia Fanelli",
      "Tiany Li",
      "Luca Biferale",
      "Bruno Buongiorno Nardelli",
      "Daniele Ciani",
      "Andrea Pisano",
      "Michele Buzzicotti"
    ],
    "published": "2025-11-27",
    "abstract": "In this work, we address the super-resolution problem of satellite-derived sea surface temperature (SST) using deep generative models. Although standard gap-filling techniques are effective in producing spatially complete datasets, they inherently smooth out fine-scale features that may be critical for a better understanding of the ocean dynamics. We investigate the use of deep learning models as Autoencoders (AEs) and generative models as Conditional-Generative Adversarial Networks (C-GANs), to reconstruct small-scale structures lost during interpolation. Our supervised -- model free -- training is based on SST observations of the Mediterranean Sea, with a focus on learning the conditional distribution of high-resolution fields given their low-resolution counterparts. We apply a tiling and merging strategy to deal with limited observational coverage and to ensure spatial continuity. Quantitative evaluations based on mean squared error metrics, spectral analysis, and gradient statistics show that while the AE reduces reconstruction error, it fails to recover high-frequency variability. In contrast, the C-GAN effectively restores the statistical properties of the true SST field at the cost of increasing the pointwise discrepancy with the ground truth observation. Our results highlight the potential of deep generative models to enhance the physical and statistical realism of gap-filled satellite data in oceanographic applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GAN",
      "Autoencoder"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Extratropical Atmospheric Circulation Response to ENSO in Deep Learning Pacific Pacemaker Experiments",
    "url": "http://arxiv.org/abs/2511.20899v1",
    "authors": [
      "Zhanxiang Hua",
      "Christina Karamperidou",
      "Zilu Meng"
    ],
    "published": "2025-11-25",
    "abstract": "Coupled atmosphere-ocean deep learning (DL) climate emulators are a new frontier but are known to exhibit weak ENSO variability, raising questions about their ability to simulate teleconnections. Here, we present the first Pacific pacemaker (PACE) experiments using a coupled DL emulator (DLESyM) to bypass this weak variability and isolate the atmospheric response to observed ENSO forcing. We find that while the emulator realistically captures internal atmospheric variability, it produces a significantly amplified forced teleconnection response to ENSO. This amplified response leads to biases in simulating extremes, notably an overestimation of atmospheric blocking frequency and duration with the underestimation of peak intensity. Our findings underscore that coupled DL climate models require in-depth and physically-grounded validation, analogous to traditional numerical models, to build confidence in their use for physical climate analysis.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting",
    "url": "http://arxiv.org/abs/2511.18732v1",
    "authors": [
      "Haoming Jia",
      "Yi Han",
      "Xiang Wang",
      "Huizan Wang",
      "Wei Wu",
      "Jianming Zheng",
      "Peikun Xiao"
    ],
    "published": "2025-11-24",
    "abstract": "Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "An Ecologically-Informed Deep Learning Framework for Interpretable and Validatable Habitat Mapping",
    "url": "http://arxiv.org/abs/2511.17627v1",
    "authors": [
      "Iv\u00e1n Felipe Benavides-Mart\u00ednez",
      "Cristiam Victoriano Portilla-Cabrera",
      "Katherine E. Mills",
      "Claire Enterline",
      "Jos\u00e9 Garc\u00e9s-Vargas",
      "Andrew J. Allyn",
      "Auroop R Ganguly"
    ],
    "published": "2025-11-18",
    "abstract": "Benthic habitat is challenging due to the environmental complexity of the seafloor, technological limitations, and elevated operational costs, especially in under-explored regions. This generates knowledge gaps for the sustainable management of hydrobiological resources and their nexus with society. We developed ECOSAIC (Ecological Compression via Orthogonal Specialized Autoencoders for Interpretable Classification), an Artificial Intelligence framework for automatic classification of benthic habitats through interpretable latent representations using a customizable autoencoder. ECOSAIC compresses n-dimensional feature space by optimizing specialization and orthogonality between domain-informed features. We employed two domain-informed categories: biogeochemical and hydrogeomorphological, that together integrate biological, physicochemical, hydrological and geomorphological, features, whose constraints on habitats have been recognized in ecology for a century. We applied the model to the Colombian Pacific Ocean and the results revealed 16 benthic habitats, expanding from mangroves to deep rocky areas up to 1000 m depth. The candidate habitats exhibited a strong correspondence between their environmental constraints, represented in latent space, and their expected species composition. This correspondence reflected meaningful ecological associations rather than purely statistical correlations, where the habitat's environmental offerings align semantically with the species' requirements. This approach could improve the management and conservation of benthic habitats, facilitating the development of functional maps that support marine planning, biodiversity conservation and fish stock assessment. We also hope it provides new insights into how ecological principles can inform AI frameworks, particularly given the substantial data limitations that characterize ecological research.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Advancing Ocean State Estimation with efficient and scalable AI",
    "url": "http://arxiv.org/abs/2511.06041v1",
    "authors": [
      "Yanfei Xiang",
      "Yuan Gao",
      "Hao Wu",
      "Quan Zhang",
      "Ruiqi Shu",
      "Xiao Zhou",
      "Xi Wu",
      "Xiaomeng Huang"
    ],
    "published": "2025-11-08",
    "abstract": "Accurate and efficient global ocean state estimation remains a grand challenge for Earth system science, hindered by the dual bottlenecks of computational scalability and degraded data fidelity in traditional data assimilation (DA) and deep learning (DL) approaches. Here we present an AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly assimilates multi-source and multi-scale observations, ranging from sparse in-situ measurements to 4 km satellite swaths, without any interpolation or data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous mapping from heterogeneous inputs to ocean states, preserving native data fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\\circ$ mesoscale dynamics from coarse 1$^\\circ$ fields, which ensures both efficiency and scalability, with just 3.7\\% more parameters than the 1$^\\circ$ configuration. When coupled with a DL forecasting system, ADAF-Ocean extends global forecast skill by up to 20 days compared to baselines without assimilation. This framework establishes a computationally viable and scientifically rigorous pathway toward real-time, high-resolution Earth system monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "Deep Learning-Driven Downscaling for Climate Risk Assessment of Projected Temperature Extremes in the Nordic Region",
    "url": "http://arxiv.org/abs/2511.03770v1",
    "authors": [
      "Parthiban Loganathan",
      "Elias Zea",
      "Ricardo Vinuesa",
      "Evelyn Otero"
    ],
    "published": "2025-11-05",
    "abstract": "Rapid changes and increasing climatic variability across the widely varied Koppen-Geiger regions of northern Europe generate significant needs for adaptation. Regional planning needs high-resolution projected temperatures. This work presents an integrative downscaling framework that incorporates Vision Transformer (ViT), Convolutional Long Short-Term Memory (ConvLSTM), and Geospatial Spatiotemporal Transformer with Attention and Imbalance-Aware Network (GeoStaNet) models. The framework is evaluated with a multicriteria decision system, Deep Learning-TOPSIS (DL-TOPSIS), for ten strategically chosen meteorological stations encompassing the temperate oceanic (Cfb), subpolar oceanic (Cfc), warm-summer continental (Dfb), and subarctic (Dfc) climate regions. Norwegian Earth System Model (NorESM2-LM) Coupled Model Intercomparison Project Phase 6 (CMIP6) outputs were bias-corrected during the 1951-2014 period and subsequently validated against earlier observations of day-to-day temperature metrics and diurnal range statistics. The ViT showed improved performance (Root Mean Squared Error (RMSE): 1.01 degrees C; R^2: 0.92), allowing for production of credible downscaled projections. Under the SSP5-8.5 scenario, the Dfc and Dfb climate zones are projected to warm by 4.8 degrees C and 3.9 degrees C, respectively, by 2100, with expansion in the diurnal temperature range by more than 1.5 degrees C. The Time of Emergence signal first appears in subarctic winter seasons (Dfc: approximately 2032), signifying an urgent need for adaptation measures. The presented framework offers station-based, high-resolution estimates of uncertainties and extremes, with direct uses for adaptation policy over high-latitude regions with fast environmental change.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Disentangling Internal Tides from Balanced Motions with Deep Learning and Surface Field Synergy",
    "url": "http://arxiv.org/abs/2511.03614v1",
    "authors": [
      "Han Wang",
      "Jeffrey Uncu",
      "Kaushik Srinivasan",
      "Nicolas Grisouard"
    ],
    "published": "2025-11-05",
    "abstract": "A fundamental challenge in ocean dynamics is the disentanglement of balanced motions and internal waves. Extracting internal tidal (IT) imprints on surface data is a central part of this challenge. For IT extraction, traditional harmonic analysis fails in the presence of strong incoherence and poor temporal sampling, as is common in global satellite observations. The advent of new wide-swath satellites, which provide two-dimensional spatial coverage, allows IT extraction to be reformulated as an image translation problem. Building on recent work where we developed a deep learning approach to extract IT signatures from sea surface height (SSH) in an idealized turbulent simulation, we show here that a simpler and computationally cheaper algorithm can perform equally well if the learning rate is annealed during training. Using this new, convenient algorithm, we experiment with different combinations of input surface fields -- SSH, surface temperature, and surface velocity. All fields contribute synergistically to disentanglement, with surface velocity by far the most informative. These findings underscore the value of coordinated multi-platform observational campaigns and highlight the critical importance of surface velocity observations for separating balanced motions and internal waves. Additional insights into the behavior of deep learning algorithm emerge: both wave signature and scattering medium aids IT extraction, and to exploit large-scale information in the scattering medium, the algorithm must be highly non-local. Residual errors of our algorithm concentrate at small spatial scales near mode-2 tidal wavelengths, likely arising from artifacts introduced during data preparation (e.g., Doppler shifts) as well as imperfections in the deep learning architecture.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets",
    "url": "http://arxiv.org/abs/2511.02887v1",
    "authors": [
      "Chaitanya Rele",
      "Aditya Rathod",
      "Kaustubh Natu",
      "Saurabh Kulkarni",
      "Ajay Koli",
      "Swapnali Makdey"
    ],
    "published": "2025-11-04",
    "abstract": "The North Indian Ocean, including the Arabian Sea and the Bay of Bengal, represents a vital source of livelihood for coastal communities, yet fishermen often face uncertainty in locating productive fishing grounds. To address this challenge, we present an AI-assisted framework for predicting Potential Fishing Zones (PFZs) using oceanographic parameters such as sea surface temperature and chlorophyll concentration. The approach is designed to enhance the accuracy of PFZ identification and provide region-specific insights for sustainable fishing practices. Preliminary results indicate that the framework can support fishermen by reducing search time, lowering fuel consumption, and promoting efficient resource utilization.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting",
    "url": "http://arxiv.org/abs/2510.25563v1",
    "authors": [
      "V\u00edctor Medina",
      "Giovanny A. Cuervo-Londo\u00f1o",
      "Javier S\u00e1nchez"
    ],
    "published": "2025-10-29",
    "abstract": "The accurate prediction of oceanographic variables is crucial for understanding climate change, managing marine resources, and optimizing maritime activities. Traditional ocean forecasting relies on numerical models; however, these approaches face limitations in terms of computational cost and scalability. In this study, we adapt Aurora, a foundational deep learning model originally designed for atmospheric forecasting, to predict sea surface temperature (SST) in the Canary Upwelling System. By fine-tuning this model with high-resolution oceanographic reanalysis data, we demonstrate its ability to capture complex spatiotemporal patterns while reducing computational demands. Our methodology involves a staged fine-tuning process, incorporating latitude-weighted error metrics and optimizing hyperparameters for efficient learning. The experimental results show that the model achieves a low RMSE of 0.119K, maintaining high anomaly correlation coefficients (ACC $\\approx 0.997$). The model successfully reproduces large-scale SST structures but faces challenges in capturing finer details in coastal regions. This work contributes to the field of data-driven ocean forecasting by demonstrating the feasibility of using deep learning models pre-trained in different domains for oceanic applications. Future improvements include integrating additional oceanographic variables, increasing spatial resolution, and exploring physics-informed neural networks to enhance interpretability and understanding. These advancements can improve climate modeling and ocean prediction accuracy, supporting decision-making in environmental and economic sectors.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Framework for Hybrid Physics-AI Coupled Ocean Models",
    "url": "http://arxiv.org/abs/2510.22676v1",
    "authors": [
      "Laure Zanna",
      "William Gregory",
      "Pavel Perezhogin",
      "Aakash Sane",
      "Cheng Zhang",
      "Alistair Adcroft",
      "Mitch Bushuk",
      "Carlos Fernandez-Granda",
      "Brandon Reichl",
      "Dhruv Balwada",
      "Julius Busecke",
      "William Chapman",
      "Alex Connolly",
      "Danni Du",
      "Kelsey Everard",
      "Fabrizio Falasca",
      "Renaud Falga",
      "David Kamm",
      "Etienne Meunier",
      "Qi Liu",
      "Antoine Nasser",
      "Matthew Pudig",
      "Andrew Shao",
      "Julia L. Simpson",
      "Linus Vogt",
      "Jiarong Wu"
    ],
    "published": "2025-10-26",
    "abstract": "Climate simulations, at all grid resolutions, rely on approximations that encapsulate the forcing due to unresolved processes on resolved variables, known as parameterizations. Parameterizations often lead to inaccuracies in climate models, with significant biases in the physics of key climate phenomena. Advances in artificial intelligence (AI) are now directly enabling the learning of unresolved processes from data to improve the physics of climate simulations. Here, we introduce a flexible framework for developing and implementing physics- and scale-aware machine learning parameterizations within climate models. We focus on the ocean and sea-ice components of a state-of-the-art climate model by implementing a spectrum of data-driven parameterizations, ranging from complex deep learning models to more interpretable equation-based models. Our results showcase the viability of AI-driven parameterizations in operational models, advancing the capabilities of a new generation of hybrid simulations, and include prototypes of fully coupled atmosphere-ocean-sea-ice hybrid simulations. The tools developed are open source, accessible, and available to all.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline",
    "url": "http://arxiv.org/abs/2511.00022v1",
    "authors": [
      "Jules Gerard",
      "Leandro Di Bella",
      "Filip Huyghe",
      "Marc Kochzius"
    ],
    "published": "2025-10-24",
    "abstract": "Coral reef monitoring in the Western Indian Ocean is limited by the labor demands of underwater visual censuses. This work evaluates a YOLOv8-based deep learning pipeline for automating family-level fish identification from video transects collected in Kenya and Tanzania. A curated dataset of 24 families was tested under different configurations, providing the first region-specific benchmark for automated reef fish monitoring in the Western Indian Ocean. The best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families but weaker detection of rare or complex taxa. Results demonstrate the potential of deep learning as a scalable complement to traditional monitoring methods.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets",
    "url": "http://arxiv.org/abs/2511.00021v1",
    "authors": [
      "Julio Jerison E. Macrohon",
      "Gordon Hung"
    ],
    "published": "2025-10-24",
    "abstract": "Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "ResNet",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters",
    "url": "http://arxiv.org/abs/2510.14250v1",
    "authors": [
      "Lianzi Jiang",
      "Jianxin Zhang",
      "Xinyu Han",
      "Huanhe Dong",
      "Xiangrong Wang"
    ],
    "published": "2025-10-16",
    "abstract": "Accurate motion response prediction for elastic Bragg breakwaters is critical for their structural safety and operational integrity in marine environments. However, conventional deep learning models often exhibit limited generalization capabilities when presented with unseen sea states. These deficiencies stem from the neglect of natural decay observed in marine systems and inadequate modeling of wave-structure interaction (WSI). To overcome these challenges, this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network (PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module incorporates a learnable temporal decay to assign higher weights to recent states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase differences guided bidirectional cross-attention (PDG-BCA) module explicitly captures the bidirectional interaction and phase relationship between waves and the structure using a cosine-based bias within a bidirectional cross-computation paradigm. These streams are synergistically integrated through a global context fusion (GCF) module. Finally, PhysAttnNet is trained with a hybrid time-frequency loss that jointly minimizes time-domain prediction errors and frequency-domain spectral discrepancies. Comprehensive experiments on wave flume datasets demonstrate that PhysAttnNet significantly outperforms mainstream models. Furthermore,cross-scenario generalization tests validate the model's robustness and adaptability to unseen environments, highlighting its potential as a framework to develop predictive models for complex systems in ocean engineering.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures",
    "url": "http://arxiv.org/abs/2509.17734v1",
    "authors": [
      "Pablo Rodr\u00edguez-Bocca",
      "Guillermo Pereira",
      "Diego Kiedanski",
      "Soledad Collazo",
      "Sebasti\u00e1n Basterrech",
      "Gerardo Rubino"
    ],
    "published": "2025-09-22",
    "abstract": "In recent years, great progress has been made in the field of forecasting meteorological variables. Recently, deep learning architectures have made a major breakthrough in forecasting the daily average temperature over a ten-day horizon. However, advances in forecasting events related to the maximum temperature over short horizons remain a challenge for the community. A problem that is even more complex consists in making predictions of the maximum daily temperatures in the short, medium, and long term. In this work, we focus on forecasting events related to the maximum daily temperature over medium-term periods (90 days). Therefore, instead of addressing the problem from a meteorological point of view, this article tackles it from a climatological point of view. Due to the complexity of this problem, a common approach is to frame the study as a temporal classification problem with the classes: maximum temperature \"above normal\", \"normal\" or \"below normal\". From a practical point of view, we created a large historical dataset (from 1981 to 2018) collecting information from weather stations located in South America. In addition, we also integrated exogenous information from the Pacific, Atlantic, and Indian Ocean basins. We applied the AutoGluonTS platform to solve the above-mentioned problem. This AutoML tool shows competitive forecasting performance with respect to large operational platforms dedicated to tackling this climatological problem; but with a \"relatively\" low computational cost in terms of time and resources.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations",
    "url": "http://arxiv.org/abs/2509.19384v1",
    "authors": [
      "Hongyuan Shi",
      "Yilin Zhai",
      "Ping Dong",
      "Zaijin You",
      "Chao Zhan",
      "Qing Wang"
    ],
    "published": "2025-09-21",
    "abstract": "Reconstructing high-resolution regional significant wave height fields from sparse and uneven buoy observations remains a core challenge for ocean monitoring and risk-aware operations. We introduce AUWave, a hybrid deep learning framework that fuses a station-wise sequence encoder (MLP) with a multi-scale U-Net enhanced by a bottleneck self-attention layer to recover 32$\\times$32 regional SWH fields. A systematic Bayesian hyperparameter search with Optuna identifies the learning rate as the dominant driver of generalization, followed by the scheduler decay and the latent dimension. Using NDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave attains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE distribution. Spatial errors are lowest near observation sites and increase with distance, reflecting identifiability limits under sparse sampling. Sensitivity experiments show that AUWave consistently outperforms a representative baseline in data-richer configurations, while the baseline is only marginally competitive in the most underdetermined single-buoy cases. The architecture's multi-scale and attention components translate into accuracy gains when minimal but non-trivial spatial anchoring is available. Error maps and buoy ablations reveal key anchor stations whose removal disproportionately degrades performance, offering actionable guidance for network design. AUWave provides a scalable pathway for gap filling, high-resolution priors for data assimilation, and contingency reconstruction.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "A Deep Learning Model of Lightning Stroke Density",
    "url": "http://arxiv.org/abs/2509.10399v1",
    "authors": [
      "Randall Jones",
      "Joel A. Thornton",
      "Chris J. Wright",
      "Robert Holzworth"
    ],
    "published": "2025-09-12",
    "abstract": "Lightning plays a crucial role in the Earth's climate system, yet existing parameterizations for use in forecasting and earth system models show room for improvement in capturing spatial and temporal variations in its frequency. This study develops deep learning-based parameterizations of lightning stroke density using meteorological variables from the ERA and IMERG datasets. Convolutional neural networks (CNNs) with U-Net architectures are trained using World Wide Lightning Location Network (WWLLN) data from 2010 to 2021 and evaluated on WWLLN lightning observations from 2022 and 2023. The CNNs reduce the average domain mean bias by an order of magnitude and produce significantly higher Fractions Skill Score (FSS) values across all lightning regimes compared to the multiplicative product of CAPE and precipitation. The CNNs show skill relative to previously published parameterizations over the oceans especially, with r2 values as high as 0.93 achieved between the best performing modeled and observed lightning stroke density climatologies. The CNNs are also able to accurately capture the 12-hourly evolution of lightning spatial patterns on an event-scale with high skill. These results show the potential for deep learning to improve on lightning parameterizations in weather and earth system models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction",
    "url": "http://arxiv.org/abs/2509.09128v1",
    "authors": [
      "Emam Hossain",
      "Md Osman Gani"
    ],
    "published": "2025-09-11",
    "abstract": "Conventional machine learning and deep learning models typically rely on correlation-based learning, which often fails to distinguish genuine causal relationships from spurious associations, limiting their robustness, interpretability, and ability to generalize. To overcome these limitations, we introduce a causality-aware deep learning framework that integrates Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily and monthly resolutions, the proposed method identifies causally influential predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary features, and enhances computational efficiency. Experimental results show that incorporating causal inputs leads to improved prediction accuracy and interpretability across varying lead times. While demonstrated on Arctic SIE forecasting, the framework is broadly applicable to other dynamic, high-dimensional domains, offering a scalable approach that advances both the theoretical foundations and practical performance of causality-informed predictive modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Finetuning AI Foundation Models to Develop Subgrid-Scale Parameterizations: A Case Study on Atmospheric Gravity Waves",
    "url": "http://arxiv.org/abs/2509.03816v1",
    "authors": [
      "Aman Gupta",
      "Aditi Sheshadri",
      "Sujit Roy",
      "Johannes Schmude",
      "Vishal Gaur",
      "Wei Ji Leong",
      "Manil Maskey",
      "Rahul Ramachandran"
    ],
    "published": "2025-09-04",
    "abstract": "Global climate models parameterize a range of atmospheric-oceanic processes like gravity waves, clouds, moist convection, and turbulence that cannot be sufficiently resolved. These subgrid-scale closures for unresolved processes are a leading source of model uncertainty. Here, we present a new approach to developing machine learning parameterizations of small-scale climate processes by fine-tuning a pre-trained AI foundation model (FM). FMs are largely unexplored in climate research. A pre-trained encoder-decoder from a 2.3 billion parameter FM (NASA and IBM Research's Prithvi WxC) -- which contains a latent probabilistic representation of atmospheric evolution -- is fine-tuned (or reused) to create a deep learning parameterization for atmospheric gravity waves (GWs). The parameterization captures GW effects for a coarse-resolution climate model by learning the fluxes from an atmospheric reanalysis with 10 times finer resolution. A comparison of monthly averages and instantaneous evolution with a machine learning model baseline (an Attention U-Net) reveals superior predictive performance of the FM parameterization throughout the atmosphere, even in regions excluded from pre-training. This performance boost is quantified using the Hellinger distance, which is 0.11 for the baseline and 0.06 for the fine-tuned model. Our findings emphasize the versatility and reusability of FMs, which could be used to accomplish a range of atmosphere- and climate-related applications, leading the way for the creation of observations-driven and physically accurate parameterizations for more earth-system processes.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "DeepSeasons: a Deep Learning scale-selecting approach to Seasonal Forecasts",
    "url": "http://arxiv.org/abs/2509.10494v1",
    "authors": [
      "A. Navarra",
      "G. G. Navarra"
    ],
    "published": "2025-08-31",
    "abstract": "Seasonal forecasting remains challenging due to the inherent chaotic nature of atmospheric dynamics. This paper introduces DeepSeasons, a novel deep learning approach designed to enhance the accuracy and reliability of seasonal forecasts. Leveraging advanced neural network architectures and extensive historical climatic datasets, DeepSeasons identifies complex, nonlinear patterns and dependencies in climate variables with similar or improved skill respcet GCM-based forecasting methods, at a significant lower cost. The framework also allow tailored application to specific regions or variables, rather than the overall problem of predicting the entire atmosphere/ocean system. The proposed methods also allow for direct predictions of anomalies and time-means, opening a new approach to long-term forecasting and highlighting its potential for operational deployment in climate-sensitive sectors. This innovative methodology promises substantial improvements in managing climate-related risks and decision-making processes.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Ensembles of Neural Surrogates for Parametric Sensitivity in Ocean Modeling",
    "url": "http://arxiv.org/abs/2508.16489v2",
    "authors": [
      "Yixuan Sun",
      "Romain Egele",
      "Sri Hari Krishna Narayanan",
      "Luke Van Roekel",
      "Carmelo Gonzales",
      "Steven Brus",
      "Balu Nadiga",
      "Sandeep Madireddy",
      "Prasanna Balaprakash"
    ],
    "published": "2025-08-22",
    "abstract": "Accurate simulations of the oceans are crucial in understanding the Earth system. Despite their efficiency, simulations at lower resolutions must rely on various uncertain parameterizations to account for unresolved processes. However, model sensitivity to parameterizations is difficult to quantify, making it challenging to tune these parameterizations to reproduce observations. Deep learning surrogates have shown promise for efficient computation of the parametric sensitivities in the form of partial derivatives, but their reliability is difficult to evaluate without ground truth derivatives. In this work, we leverage large-scale hyperparameter search and ensemble learning to improve both forward predictions, autoregressive rollout, and backward adjoint sensitivity estimation. Particularly, the ensemble method provides epistemic uncertainty of function value predictions and their derivatives, providing improved reliability of the neural surrogates in decision making.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MedFormer: a data-driven model for forecasting the Mediterranean Sea",
    "url": "http://arxiv.org/abs/2509.00015v1",
    "authors": [
      "Italo Epicoco",
      "Davide Donno",
      "Gabriele Accarino",
      "Simone Norberti",
      "Alessandro Grandi",
      "Michele Giurato",
      "Ronan McAdam",
      "Donatello Elia",
      "Emanuela Clementi",
      "Paola Nassisi",
      "Enrico Scoccimarro",
      "Giovanni Coppini",
      "Silvio Gualdi",
      "Giovanni Aloisio",
      "Simona Masina",
      "Giulio Boccaletti",
      "Antonio Navarra"
    ],
    "published": "2025-08-16",
    "abstract": "Accurate ocean forecasting is essential for supporting a wide range of marine applications. Recent advances in artificial intelligence have highlighted the potential of data-driven models to outperform traditional numerical approaches, particularly in atmospheric weather forecasting. However, extending these methods to ocean systems remains challenging due to their inherently slower dynamics and complex boundary conditions. In this work, we present MedFormer, a fully data-driven deep learning model specifically designed for medium-range ocean forecasting in the Mediterranean Sea. MedFormer is based on a U-Net architecture augmented with 3D attention mechanisms and operates at a high horizontal resolution of 1/24\u00b0. The model is trained on 20 years of daily ocean reanalysis data and fine-tuned with high-resolution operational analyses. It generates 9-day forecasts using an autoregressive strategy. The model leverages both historical ocean states and atmospheric forcings, making it well-suited for operational use. We benchmark MedFormer against the state-of-the-art Mediterranean Forecasting System (MedFS), developed at Euro-Mediterranean Center on Climate Change (CMCC), using both analysis data and independent observations. The forecast skills, evaluated with the Root Mean Squared Difference and the Anomaly Correlation Coefficient, indicate that MedFormer consistently outperforms MedFS across key 3D ocean variables. These findings underscore the potential of data-driven approaches like MedFormer to complement, or even surpass, traditional numerical ocean forecasting systems in both accuracy and computational efficiency.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Bridging ocean wave physics and deep learning: Physics-informed neural operators for nonlinear wavefield reconstruction in real-time",
    "url": "http://arxiv.org/abs/2508.03315v1",
    "authors": [
      "Svenja Ehlers",
      "Merten Stender",
      "Norbert Hoffmann"
    ],
    "published": "2025-08-05",
    "abstract": "Accurate real-time prediction of phase-resolved ocean wave fields remains a critical yet largely unsolved problem, primarily due to the absence of practical data assimilation methods for reconstructing initial conditions from sparse or indirect wave measurements. While recent advances in supervised deep learning have shown potential for this purpose, they require large labelled datasets of ground truth wave data, which are infeasible to obtain in real-world scenarios. To overcome this limitation, we propose a Physics-Informed Neural Operator (PINO) framework for reconstructing spatially and temporally phase-resolved, nonlinear ocean wave fields from sparse measurements, without the need for ground truth data during training. This is achieved by embedding residuals of the free surface boundary conditions of ocean gravity waves into the loss function of the PINO, constraining the solution space in a soft manner. After training, we validate our approach using highly realistic synthetic wave data and demonstrate the accurate reconstruction of nonlinear wave fields from both buoy time series and radar snapshots. Our results indicate that PINOs enable accurate, real-time reconstruction and generalize robustly across a wide range of wave conditions, thereby paving the way for operational, data-driven wave reconstruction and prediction in realistic marine environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Data-driven global ocean model resolving ocean-atmosphere coupling dynamics",
    "url": "http://arxiv.org/abs/2508.10908v1",
    "authors": [
      "Jeong-Hwan Kim",
      "Daehyun Kang",
      "Young-Min Yang",
      "Jae-Heung Park",
      "Yoo-Geun Ham"
    ],
    "published": "2025-07-31",
    "abstract": "Artificial intelligence has advanced global weather forecasting, outperforming traditional numerical models in both accuracy and computational efficiency. Nevertheless, extending predictions beyond subseasonal timescales requires the development of deep learning (DL)-based ocean-atmosphere coupled models that can realistically simulate complex oceanic responses to atmospheric forcing. This study presents KIST-Ocean, a DL-based global three-dimensional ocean general circulation model using a U-shaped visual attention adversarial network architecture. KIST-Ocean integrates partial convolution, adversarial training, and transfer learning to address coastal complexity and predictive distribution drift in auto-regressive models. Comprehensive evaluations confirmed the model's robust ocean predictive skill and efficiency. Moreover, it accurately captures realistic ocean response, such as Kelvin and Rossby wave propagation in the tropical Pacific, and vertical motions induced by cyclonic and anticyclonic wind stress, demonstrating its ability to represent key ocean-atmosphere coupling mechanisms underlying climate phenomena, including the El Nino-Southern Oscillation. These findings reinforce confidence in DL-based global weather and climate models and their extending DL-based approaches to broader Earth system modeling, offering potential for enhancing climate prediction capabilities.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents",
    "url": "http://arxiv.org/abs/2507.18067v2",
    "authors": [
      "Abdessamad El-Kabid",
      "Loubna Benabbou",
      "Redouane Lguensat",
      "Alex Hern\u00e1ndez-Garc\u00eda"
    ],
    "published": "2025-07-24",
    "abstract": "Accurate modeling of physical systems governed by partial differential equations is a central challenge in scientific computing. In oceanography, high-resolution current data are critical for coastal management, environmental monitoring, and maritime safety. However, available satellite products, such as Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and global ocean models, often lack the spatial granularity required for detailed local analyses. In this work, we (a) introduce a supervised deep learning framework based on neural operators for solving PDEs and providing arbitrary resolution solutions, and (b) propose downscaling models with an application to Copernicus ocean current data. Additionally, our method can model surrogate PDEs and predict solutions at arbitrary resolution, regardless of the input resolution. We evaluated our model on real-world Copernicus ocean current data and synthetic Navier-Stokes simulation datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Multimodal Data Fusion Generative Adversarial Network for Real Time Underwater Sound Speed Field Construction",
    "url": "http://arxiv.org/abs/2507.11812v1",
    "authors": [
      "Wei Huang",
      "Yuqiang Huang",
      "Yanan Wu",
      "Tianhe Xu",
      "Junting Wang",
      "Hao Zhang"
    ],
    "published": "2025-07-16",
    "abstract": "Sound speed profiles (SSPs) are essential parameters underwater that affects the propagation mode of underwater signals and has a critical impact on the energy efficiency of underwater acoustic communication and accuracy of underwater acoustic positioning. Traditionally, SSPs can be obtained by matching field processing (MFP), compressive sensing (CS), and deep learning (DL) methods. However, existing methods mainly rely on on-site underwater sonar observation data, which put forward strict requirements on the deployment of sonar observation systems. To achieve high-precision estimation of sound velocity distribution in a given sea area without on-site underwater data measurement, we propose a multi-modal data-fusion generative adversarial network model with residual attention block (MDF-RAGAN) for SSP construction. To improve the model's ability for capturing global spatial feature correlations, we embedded the attention mechanisms, and use residual modules for deeply capturing small disturbances in the deep ocean sound velocity distribution caused by changes of SST. Experimental results on real open dataset show that the proposed model outperforms other state-of-the-art methods, which achieves an accuracy with an error of less than 0.3m/s. Specifically, MDF-RAGAN not only outperforms convolutional neural network (CNN) and spatial interpolation (SITP) by nearly a factor of two, but also achieves about 65.8\\% root mean square error (RMSE) reduction compared to mean profile, which fully reflects the enhancement of overall profile matching by multi-source fusion and cross-modal attention.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "GAN"
    ],
    "applications": []
  },
  {
    "title": "Generative Lagrangian data assimilation for ocean dynamics under extreme sparsity",
    "url": "http://arxiv.org/abs/2507.06479v1",
    "authors": [
      "Niloofar Asefi",
      "Leonard Lupin-Jimenez",
      "Tianning Wu",
      "Ruoying He",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-07-09",
    "abstract": "Reconstructing ocean dynamics from observational data is fundamentally limited by the sparse, irregular, and Lagrangian nature of spatial sampling, particularly in subsurface and remote regions. This sparsity poses significant challenges for forecasting key phenomena such as eddy shedding and rogue waves. Traditional data assimilation methods and deep learning models often struggle to recover mesoscale turbulence under such constraints. We leverage a deep learning framework that combines neural operators with denoising diffusion probabilistic models (DDPMs) to reconstruct high-resolution ocean states from extremely sparse Lagrangian observations. By conditioning the generative model on neural operator outputs, the framework accurately captures small-scale, high-wavenumber dynamics even at $99\\%$ sparsity (for synthetic data) and $99.9\\%$ sparsity (for real satellite observations). We validate our method on benchmark systems, synthetic float observations, and real satellite data, demonstrating robust performance under severe spatial sampling limitations as compared to other deep learning baselines.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning in Acoustics: A Review and Open-Source Repository",
    "url": "http://arxiv.org/abs/2507.04419v1",
    "authors": [
      "Ryan A. McCarthy",
      "You Zhang",
      "Samuel A. Verburg",
      "William F. Jenkins",
      "Peter Gerstoft"
    ],
    "published": "2025-07-06",
    "abstract": "Acoustic data provide scientific and engineering insights in fields ranging from bioacoustics and communications to ocean and earth sciences. In this review, we survey recent advances and the transformative potential of machine learning (ML) in acoustics, including deep learning (DL). Using the Python high-level programming language, we demonstrate a broad collection of ML techniques to detect and find patterns for classification, regression, and generation in acoustics data automatically. We have ML examples including acoustic data classification, generative modeling for spatial audio, and physics-informed neural networks. This work includes AcousticsML, a set of practical Jupyter notebook examples on GitHub demonstrating ML benefits and encouraging researchers and practitioners to apply reproducible data-driven approaches to acoustic challenges.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Accurate Mediterranean Sea forecasting via graph-based deep learning",
    "url": "http://arxiv.org/abs/2506.23900v1",
    "authors": [
      "Daniel Holmberg",
      "Emanuela Clementi",
      "Italo Epicoco",
      "Teemu Roos"
    ],
    "published": "2025-06-30",
    "abstract": "Accurate ocean forecasting systems are essential for understanding marine dynamics, which play a crucial role in sectors such as shipping, aquaculture, environmental monitoring, and coastal risk management. Traditional numerical solvers, while effective, are computationally expensive and time-consuming. Recent advancements in machine learning have revolutionized weather forecasting, offering fast and energy-efficient alternatives. Building on these advancements, we introduce SeaCast, a neural network designed for high-resolution regional ocean forecasting. SeaCast employs a graph-based framework to effectively handle the complex geometry of ocean grids and integrates external forcing data tailored to the regional ocean context. Our approach is validated through experiments at a high horizontal resolution using the operational numerical forecasting system of the Mediterranean Sea, along with both numerical and data-driven atmospheric forcings. Results demonstrate that SeaCast consistently outperforms the operational model in forecast skill, marking a significant advancement in regional ocean prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research",
    "url": "http://arxiv.org/abs/2506.22174v1",
    "authors": [
      "Bavo Lesy",
      "Siemen Herremans",
      "Robin Kerstens",
      "Jan Steckel",
      "Walter Daems",
      "Siegfried Mercelis",
      "Ali Anwar"
    ],
    "published": "2025-06-27",
    "abstract": "The transport industry has recently shown significant interest in unmanned surface vehicles (USVs), specifically for port and inland waterway transport. These systems can improve operational efficiency and safety, which is especially relevant in the European Union, where initiatives such as the Green Deal are driving a shift towards increased use of inland waterways. At the same time, a shortage of qualified personnel is accelerating the adoption of autonomous solutions. However, there is a notable lack of open-source, high-fidelity simulation frameworks and datasets for developing and evaluating such solutions. To address these challenges, we introduce AirSim For Surface Vehicles (ASVSim), an open-source simulation framework specifically designed for autonomous shipping research in inland and port environments. The framework combines simulated vessel dynamics with marine sensor simulation capabilities, including radar and camera systems and supports the generation of synthetic datasets for training computer vision models and reinforcement learning agents. Built upon Cosys-AirSim, ASVSim provides a comprehensive platform for developing autonomous navigation algorithms and generating synthetic datasets. The simulator supports research of both traditional control methods and deep learning-based approaches. Through limited experiments, we demonstrate the potential of the simulator in these research areas. ASVSim is provided as an open-source project under the MIT license, making autonomous navigation research accessible to a larger part of the ocean engineering community.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting",
    "url": "http://arxiv.org/abs/2507.00036v1",
    "authors": [
      "Rohan Putatunda",
      "Sanjay Purushotham",
      "Ratnaksha Lele",
      "Vandana P. Janeja"
    ],
    "published": "2025-06-18",
    "abstract": "Drifting icebergs in the polar oceans play a key role in the Earth's climate system, impacting freshwater fluxes into the ocean and regional ecosystems while also posing a challenge to polar navigation. However, accurately forecasting iceberg trajectories remains a formidable challenge, primarily due to the scarcity of spatiotemporal data and the complex, nonlinear nature of iceberg motion, which is also impacted by environmental variables. The iceberg motion is influenced by multiple dynamic environmental factors, creating a highly variable system that makes trajectory identification complex. These limitations hinder the ability of deep learning models to effectively capture the underlying dynamics and provide reliable predictive outcomes. To address these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep learning model that combines an analytical formulation of iceberg drift physics, with an augmented residual learning model. The model learns the pattern of mismatch between the analytical solution and ground-truth observations, which is combined with a rotate-augmented spectral neural network that captures both global and local patterns from the data to forecast future iceberg drift positions. We compare IDRIFTNET model performance with state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings demonstrate that IDRIFTNET outperforms other models by achieving a lower Final Displacement Error (FDE) and Average Displacement Error (ADE) across a variety of time points. These results highlight IDRIFTNET's effectiveness in capturing the complex, nonlinear drift of icebergs for forecasting iceberg trajectories under limited data and dynamic environmental conditions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Forecast error diagnostics in neural weather models",
    "url": "http://arxiv.org/abs/2506.11987v2",
    "authors": [
      "Uros Perkan",
      "Ziga Zaplotnik",
      "Gregor Skok"
    ],
    "published": "2025-06-13",
    "abstract": "Deep-learning (DL) weather prediction models offer some notable advantages over traditional physics-based models, including auto-differentiability and low computational cost, enabling detailed diagnostics of forecast errors. Using our convolutional encoder-decoder model, ConvCastNet, we systematically relax selected subdomains of the forecast fields towards \"true\" weather states (ERA5 reanalyses) and monitor the forecast skill gain in other regions. Our results show that a medium-range mid-latitude forecast improves substantially when the stratosphere and boundary layer are relaxed, while relaxation of the tropical atmosphere has a negligible effect. This underscores the need for a more accurate representation of the stratosphere and the planetary boundary layer to improve medium-range weather predictability. Additionally, we investigate the relationship between the forecast error sensitivity to initial conditions and relaxation experiments. By utilising auto-differentiability, we identify overlapping regions of large error sensitivity and strong forecast skill improvement from relaxation. Average mid-latitude error sensitivity to initial conditions shows negligible influence from the tropics, corroborating the results of the tropical relaxation experiments. The error sensitivity shows a physically consistent influence of upstream weather dynamics and sea surface temperatures on forecast accuracy. The latter also highlights the importance of accurately representing the atmosphere--ocean coupling in numerical weather prediction models. This combined approach could provide valuable heuristics for diagnosing neural model errors and guiding targeted model improvements.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System",
    "url": "http://arxiv.org/abs/2505.24429v2",
    "authors": [
      "Giovanny A. Cuervo-Londo\u00f1o",
      "Javier S\u00e1nchez",
      "\u00c1ngel Rodr\u00edguez-Santana"
    ],
    "published": "2025-05-30",
    "abstract": "Oceanographic forecasting impacts various sectors of society by supporting environmental conservation and economic activities. Based on global circulation models, traditional forecasting methods are computationally expensive and slow, limiting their ability to provide rapid forecasts. Recent advances in deep learning offer faster and more accurate predictions, although these data-driven models are often trained with global data from numerical simulations, which may not reflect reality. The emergence of such models presents great potential for improving ocean prediction at a subregional domain. However, their ability to predict fine-scale ocean processes, like mesoscale structures, remains largely unknown. This work aims to adapt a graph neural network initially developed for global weather forecasting to improve subregional ocean prediction, specifically focusing on the Canary Current upwelling system. The model is trained with satellite data and compared to state-of-the-art physical ocean models to assess its performance in capturing ocean dynamics. Our results show that the deep learning model surpasses traditional methods in precision despite some challenges in upwelling areas. It demonstrated superior performance in reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis, particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape Bojador, and Cape Blanc. The model achieved improvements of up to 26.5% relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts compared to the GLORYS reanalysis at these critical locations, highlighting its enhanced capability to capture spatial variability and improve predictive accuracy in complex areas. These findings suggest the viability of adapting meteorological data-driven models for improving subregional medium-term ocean forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis",
    "url": "http://arxiv.org/abs/2505.14285v1",
    "authors": [
      "Eirini Panteli",
      "Paulo E. Santos",
      "Nabil Humphrey"
    ],
    "published": "2025-05-20",
    "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for preprocessing, denoising, classification, and novelty detection of underwater acoustic signals. Designed to operate effectively in noisy and dynamic marine environments, AquaSignal integrates state-of-the-art deep learning architectures to enhance the reliability and accuracy of acoustic signal analysis. The system is evaluated on a combined dataset from the Deepship and Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a ResNet18 convolutional neural network for classifying known acoustic events, and an AutoEncoder-based model for unsupervised detection of novel or anomalous signals. To our knowledge, this is the first comprehensive study to apply and evaluate this combination of techniques on maritime vessel acoustic data. Experimental results show that AquaSignal improves signal clarity and task performance, achieving 71% classification accuracy and 91% accuracy in novelty detection. Despite slightly lower classification performance compared to some state-of-the-art models, differences in data partitioning strategies limit direct comparisons. Overall, AquaSignal demonstrates strong potential for real-time underwater acoustic monitoring in scientific, environmental, and maritime domains.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting",
    "url": "http://arxiv.org/abs/2505.10894v1",
    "authors": [
      "Yishuo Wang",
      "Feng Zhou",
      "Muping Zhou",
      "Qicheng Meng",
      "Zhijun Hu",
      "Yi Wang"
    ],
    "published": "2025-05-16",
    "abstract": "This paper proposes CTP, a novel deep learning framework that integrates convolutional neural network(CNN), Transformer architectures, and physics-informed neural network(PINN) for ocean front prediction. Ocean fronts, as dynamic interfaces between distinct water masses, play critical roles in marine biogeochemical and physical processes. Existing methods such as LSTM, ConvLSTM, and AttentionConv often struggle to maintain spatial continuity and physical consistency over multi-step forecasts. CTP addresses these challenges by combining localized spatial encoding, long-range temporal attention, and physical constraint enforcement. Experimental results across south China sea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP achieves state-of-the-art(SOTA) performance in both single-step and multi-step predictions, significantly outperforming baseline models in accuracy, $F_1$ score, and temporal stability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer",
      "LSTM",
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting",
    "url": "http://arxiv.org/abs/2505.10191v1",
    "authors": [
      "Qingyu Zheng",
      "Qi Shao",
      "Guijun Han",
      "Wei Li",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2025-05-15",
    "abstract": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the ocean, and their impact on the energy cascade of the global ocean cannot be ignored. Eddy-resolving ocean forecasting is providing more reliable protection for fisheries and navigational safety, but also presents significant scientific challenges and high computational costs for traditional numerical models. Artificial intelligence (AI)-based weather and ocean forecasting systems are becoming powerful tools that balance forecast performance with computational efficiency. However, the complex multiscale features in the ocean dynamical system make AI models still face many challenges in mesoscale eddy forecasting (especially regional modelling). Here, we develop LanTu, a regional eddy-resolving ocean forecasting system based on dynamics-enhanced deep learning. We incorporate cross-scale interactions into LanTu and construct multiscale physical constraint for optimising LanTu guided by knowledge of eddy dynamics in order to improve the forecasting skill of LanTu for mesoscale evolution. The results show that LanTu outperforms the existing advanced operational numerical ocean forecasting system (NOFS) and AI-based ocean forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and current prediction, with a lead time of more than 10 days. Our study highlights that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for eddy-resolving ocean forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network",
    "url": "http://arxiv.org/abs/2505.00495v1",
    "authors": [
      "Nguyen Van Thanh",
      "Nguyen Dang Huynh",
      "Nguyen Ngoc Tan",
      "Nguyen Thai Minh",
      "Nguyen Nam Hoang"
    ],
    "published": "2025-05-01",
    "abstract": "A storm is a type of extreme weather. Therefore, forecasting the path of a storm is extremely important for protecting human life and property. However, storm forecasting is very challenging because storm trajectories frequently change. In this study, we propose an improved deep learning method using a Transformer network to predict the movement trajectory of a storm over the next 6 hours. The storm data used to train the model was obtained from the National Oceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that the proposed method is more accurate than traditional methods. Moreover, the proposed method is faster and more cost-effective",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal",
    "url": "http://arxiv.org/abs/2504.20620v1",
    "authors": [
      "Abhishek Pasula",
      "Deepak N. Subramani"
    ],
    "published": "2025-04-29",
    "abstract": "Climate change alters ocean conditions, notably temperature and sea level. In the Bay of Bengal, these changes influence monsoon precipitation and marine productivity, critical to the Indian economy. In Phase 6 of the Coupled Model Intercomparison Project (CMIP6), Global Climate Models (GCMs) use different shared socioeconomic pathways (SSPs) to obtain future climate projections. However, significant discrepancies are observed between these models and the reanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean square error (RMSE) between the climate model output and the Ocean Reanalysis System (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the dynamic sea level (DSL). We introduce a new data-driven deep learning model to correct for this bias. The deep neural model for each variable is trained using pairs of climatology-removed monthly climate projections as input and the corresponding month's ORAS5 as output. This model is trained with historical data (1950 to 2014), validated with future projection data from 2015 to 2020, and tested with future projections from 2021 to 2023. Compared to the conventional EquiDistant Cumulative Distribution Function (EDCDF) statistical method for bias correction in climate models, our approach decreases RMSE by 0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the projections for 2024-2100. A detailed analysis of the monthly, seasonal, and decadal means and variability is performed to underscore the implications of the novel dynamics uncovered in our corrected projections.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Global Climate Model Bias Correction Using Deep Learning",
    "url": "http://arxiv.org/abs/2504.19145v2",
    "authors": [
      "Abhishek Pasula",
      "Deepak N. Subramani"
    ],
    "published": "2025-04-27",
    "abstract": "Climate change affects ocean temperature, salinity and sea level, impacting monsoons and ocean productivity. Future projections by Global Climate Models based on shared socioeconomic pathways from the Coupled Model Intercomparison Project (CMIP) are widely used to understand the effects of climate change. However, CMIP models have significant bias compared to reanalysis in the Bay of Bengal for the time period when both projections and reanalysis are available. For example, there is a 1.5C root mean square error (RMSE) in the sea surface temperature (SST) projections of the climate model CNRM-CM6 compared to the Ocean Reanalysis System (ORAS5). We develop a suite of data-driven deep learning models for bias correction of climate model projections and apply it to correct SST projections of the Bay of Bengal. We propose the use of three different deep neural network architectures: convolutional encoder-decoder UNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression model and the Equi-Distant Cumulative Density Function (EDCDF) bias correction method for comparison and evaluating the impact of the new deep learning models. All bias correction models are trained using pairs of monthly CMIP6 projections and the corresponding month's ORAS5 as input and output. Historical data (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used for training and validation, including hyperparameter tuning. Testing is performed on future projection data from 2021 to 2024. Detailed analysis of the three deep neural models has been completed. We found that the UNet architecture trained using a climatology-removed CNRM-CM6 projection as input and climatology-removed ORAS5 as output gives the best bias-corrected projections. Our novel deep learning-based method for correcting CNRM-CM6 data has a 15% reduction in RMSE compared EDCDF.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "LSTM",
      "Deep Neural Network"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence",
    "url": "http://arxiv.org/abs/2504.19036v1",
    "authors": [
      "Henry Herzog",
      "Joshua Hansen",
      "Yawen Zhang",
      "Patrick Beukema"
    ],
    "published": "2025-04-26",
    "abstract": "Unsustainable exploitation of the oceans exacerbated by global warming is threatening coastal communities worldwide. Accurate and timely monitoring of maritime activity is an essential step to effective governance and to inform future policy. In support of this complex global-scale effort, we built Atlantes, a deep learning based system that provides the first-ever real-time view of vessel behavior at global scale. Atlantes leverages a series of bespoke transformers to distill a high volume, continuous stream of GPS messages emitted by hundreds of thousands of vessels into easily quantifiable behaviors. The combination of low latency and high performance enables operationally relevant decision-making and successful interventions on the high seas where illegal and exploitative activity is too common. Atlantes is already in use by hundreds of organizations worldwide. Here we provide an overview of the model and infrastructure that enables this system to function efficiently and cost-effectively at global-scale and in real-time.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Last-layer committee machines for uncertainty estimations of benthic imagery",
    "url": "http://arxiv.org/abs/2504.16952v1",
    "authors": [
      "H. Martin Gillis",
      "Isaac Xu",
      "Benjamin Misiuk",
      "Craig J. Brown",
      "Thomas Trappenberg"
    ],
    "published": "2025-04-22",
    "abstract": "Automating the annotation of benthic imagery (i.e., images of the seafloor and its associated organisms, habitats, and geological features) is critical for monitoring rapidly changing ocean ecosystems. Deep learning approaches have succeeded in this purpose; however, consistent annotation remains challenging due to ambiguous seafloor images, potential inter-user annotation disagreements, and out-of-distribution samples. Marine scientists implementing deep learning models often obtain predictions based on one-hot representations trained using a cross-entropy loss objective with softmax normalization, resulting with a single set of model parameters. While efficient, this approach may lead to overconfident predictions for context-challenging datasets, raising reliability concerns that present risks for downstream tasks such as benthic habitat mapping and marine spatial planning. In this study, we investigated classification uncertainty as a tool to improve the labeling of benthic habitat imagery. We developed a framework for two challenging sub-datasets of the recently publicly available BenthicNet dataset using Bayesian neural networks, Monte Carlo dropout inference sampling, and a proposed single last-layer committee machine. This approach resulted with a > 95% reduction of network parameters to obtain per-sample uncertainties while obtaining near-identical performance compared to computationally more expensive strategies such as Bayesian neural networks, Monte Carlo dropout, and deep ensembles. The method proposed in this research provides a strategy for obtaining prioritized lists of uncertain samples for human-in-the-loop interventions to identify ambiguous, mislabeled, out-of-distribution, and/or difficult images for enhancing existing annotation tools for benthic mapping and other applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping",
    "url": "http://arxiv.org/abs/2504.14372v1",
    "authors": [
      "Jose Marie Antonio Minoza"
    ],
    "published": "2025-04-19",
    "abstract": "Accurate ocean modeling and coastal hazard prediction depend on high-resolution bathymetric data; yet, current worldwide datasets are too coarse for exact numerical simulations. While recent deep learning advances have improved earth observation data resolution, existing methods struggle with the unique challenges of producing detailed ocean floor maps, especially in maintaining physical structure consistency and quantifying uncertainties. This work presents a novel uncertainty-aware mechanism using spatial blocks to efficiently capture local bathymetric complexity based on block-based conformal prediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE) architecture, the integration of this uncertainty quantification framework yields spatially adaptive confidence estimates while preserving topographical features via discrete latent representations. With smaller uncertainty widths in well-characterized areas and appropriately larger bounds in areas of complex seafloor structures, the block-based design adapts uncertainty estimates to local bathymetric complexity. Compared to conventional techniques, experimental results over several ocean regions show notable increases in both reconstruction quality and uncertainty estimation reliability. This framework increases the reliability of bathymetric reconstructions by preserving structural integrity while offering spatially adaptive uncertainty estimates, so opening the path for more solid climate modeling and coastal hazard assessment.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Joint Source-Environment Adaptation for Deep Learning-Based Underwater Acoustic Source Ranging",
    "url": "http://arxiv.org/abs/2503.23262v1",
    "authors": [
      "Dariush Kari",
      "Andrew C. Singer"
    ],
    "published": "2025-03-30",
    "abstract": "In this paper, we propose a method to adapt a pre-trained deep-learning-based model for underwater acoustic localization to a new environment. We use unsupervised domain adaptation to improve the generalization performance of the model, i.e., using an unsupervised loss, fine-tune the pre-trained network parameters without access to any labels of the target environment or any data used to pre-train the model. This method improves the pre-trained model prediction by coupling that with an almost independent estimation based on the received signal energy (that depends on the source). We show the effectiveness of this approach on Bellhop generated data in an environment similar to that of the SWellEx-96 experiment contaminated with real ocean noise from the KAM11 experiment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Joint Source-Environment Adaptation of Data-Driven Underwater Acoustic Source Ranging Based on Model Uncertainty",
    "url": "http://arxiv.org/abs/2503.23258v2",
    "authors": [
      "Dariush Kari",
      "Hari Vishnu",
      "Andrew C. Singer"
    ],
    "published": "2025-03-30",
    "abstract": "Adapting pre-trained deep learning models to new and unknown environments remains a major challenge in underwater acoustic localization. We show that although the performance of pre-trained models suffers from mismatch between the training and test data, they generally exhibit a higher uncertainty in environments where there is more mismatch. Additionally, in the presence of environmental mismatch, spurious peaks can appear in the output of classification-based localization approaches, which inspires us to define and use a method to quantify the \"implied uncertainty\" based on the number of model output peaks. Leveraging this notion of implied uncertainty, we partition the test samples into sets with more certain and less certain samples, and implement a method to adapt the model to new environments by using the certain samples to improve the labeling for uncertain samples, which helps to adapt the model. Thus, using this efficient method for model uncertainty quantification, we showcase an innovative approach to adapt a pre-trained model to unseen underwater environments at test time. This eliminates the need for labeled data from the target environment or the original training data. This adaptation is enhanced by integrating an independent estimate based on the received signal energy. We validate the approach extensively using real experimental data, as well as synthetic data consisting of model-generated signals with real ocean noise. The results demonstrate significant improvements in model prediction accuracy, underscoring the potential of the method to enhance underwater acoustic localization in diverse, noisy, and unknown environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Simulation-informed deep learning for enhanced SWOT observations of fine-scale ocean dynamics",
    "url": "http://arxiv.org/abs/2503.21303v1",
    "authors": [
      "Eugenio Cutolo",
      "Carlos Granero-Belinchon",
      "Ptashanna Thiraux",
      "Jinbo Wang",
      "Ronan Fablet"
    ],
    "published": "2025-03-27",
    "abstract": "Oceanic processes at fine scales are crucial yet difficult to observe accurately due to limitations in satellite and in-situ measurements. The Surface Water and Ocean Topography (SWOT) mission provides high-resolution Sea Surface Height (SSH) data, though noise patterns often obscure fine scale structures. Current methods struggle with noisy data or require extensive supervised training, limiting their effectiveness on real-world observations. We introduce SIMPGEN (Simulation-Informed Metric and Prior for Generative Ensemble Networks), an unsupervised adversarial learning framework combining real SWOT observations with simulated reference data. SIMPGEN leverages wavelet-informed neural metrics to distinguish noisy from clean fields, guiding realistic SSH reconstructions. Applied to SWOT data, SIMPGEN effectively removes noise, preserving fine-scale features better than existing neural methods. This robust, unsupervised approach not only improves SWOT SSH data interpretation but also demonstrates strong potential for broader oceanographic applications, including data assimilation and super-resolution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years",
    "url": "http://arxiv.org/abs/2503.21211v2",
    "authors": [
      "Rixu Hao",
      "Yuxin Zhao",
      "Shaoqing Zhang",
      "Guihua Wang",
      "Xiong Deng"
    ],
    "published": "2025-03-27",
    "abstract": "El Ni\u00f1o-Southern Oscillation (ENSO) exerts global climate and societal impacts, but real-time prediction with lead times beyond one year remains challenging. Dynamical models suffer from large biases and uncertainties, while deep learning struggles with interpretability and multi-scale dynamics. Here, we introduce PTSTnet, an interpretable model that unifies dynamical processes and cross-scale spatiotemporal learning in an innovative neural-network framework with physics-encoding learning. PTSTnet produces interpretable predictions significantly outperforming state-of-the-art benchmarks with lead times beyond 24 months, providing physical insights into error propagation in ocean-atmosphere interactions. PTSTnet learns feature representations with physical consistency from sparse data to tackle inherent multi-scale and multi-physics challenges underlying ocean-atmosphere processes, thereby inherently enhancing long-term prediction skill. Our successful realizations mark substantial steps forward in interpretable insights into innovative neural ocean modelling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
    "url": "http://arxiv.org/abs/2503.19502v1",
    "authors": [
      "Qi Chen",
      "Yinghao Cui",
      "Guobin Hong",
      "Karumuri Ashok",
      "Yuchun Pu",
      "Xiaogu Zheng",
      "Xuanze Zhang",
      "Wei Zhong",
      "Peng Zhan",
      "Zhonglei Wang"
    ],
    "published": "2025-03-25",
    "abstract": "El Ni\u00f1o-Southern Oscillation (ENSO) is a prominent mode of interannual climate variability with far-reaching global impacts. Its evolution is governed by intricate air-sea interactions, posing significant challenges for long-term prediction. In this study, we introduce CTEFNet, a multivariate deep learning model that synergizes convolutional neural networks and transformers to enhance ENSO forecasting. By integrating multiple oceanic and atmospheric predictors, CTEFNet extends the effective forecast lead time to 20 months while mitigating the impact of the spring predictability barrier, outperforming both dynamical models and state-of-the-art deep learning approaches. Furthermore, CTEFNet offers physically meaningful and statistically significant insights through gradient-based sensitivity analysis, revealing the key precursor signals that govern ENSO dynamics, which align with well-established theories and reveal new insights about inter-basin interactions among the Pacific, Atlantic, and Indian Oceans. The CTEFNet's superior predictive skill and interpretable sensitivity assessments underscore its potential for advancing climate prediction. Our findings highlight the importance of multivariate coupling in ENSO evolution and demonstrate the promise of deep learning in capturing complex climate dynamics with enhanced interpretability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Deep learning in the abyss: a stratified Physics Informed Neural Network for data assimilation",
    "url": "http://arxiv.org/abs/2503.19160v1",
    "authors": [
      "Vadim Limousin",
      "Nelly Pustelnik",
      "Bruno Deremble",
      "Antoine Venaille"
    ],
    "published": "2025-03-24",
    "abstract": "The reconstruction of deep ocean currents is a major challenge in data assimilation due to the scarcity of interior data. In this work, we present a proof of concept for deep ocean flow reconstruction using a Physics-Informed Neural Network (PINN), a machine learning approach that offers an alternative to traditional data assimilation methods. We introduce an efficient algorithm called StrAssPINN (for Stratified Assimilation PINNs), which assigns a separate network to each layer of the ocean model while allowing them to interact during training. The neural network takes spatiotemporal coordinates as input and predicts the velocity field at those points. Using a SIREN architecture (a multilayer perceptron with sine activation functions), which has proven effective in various contexts, the network is trained using both available observational data and dynamical priors enforced at several collocation points. We apply this method to pseudo-observed ocean data generated from a 3-layer quasi-geostrophic model, where the pseudo-observations include surface-level data akin to SWOT observations of sea surface height, interior data similar to ARGO floats, and a limited number of deep ARGO-like measurements in the lower layers. Our approach successfully reconstructs ocean flows in both the interior and surface layers, demonstrating a strong ability to resolve key ocean mesoscale features, including vortex rings, eastward jets associated with potential vorticity fronts, and smoother Rossby waves. This work serves as a prelude to applying StrAssPINN to real-world observational data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Observation-only learning of neural mapping schemes for gappy satellite-derived ocean colour parameters",
    "url": "http://arxiv.org/abs/2503.11532v1",
    "authors": [
      "Cl\u00e9ment Dorffer",
      "Fr\u00e9d\u00e9ric Jourdin",
      "Thi Thuy Nga Nguyen",
      "Rodolphe Devillers",
      "David Mouillot",
      "Ronan Fablet"
    ],
    "published": "2025-03-14",
    "abstract": "Monitoring optical properties of coastal and open ocean waters is crucial to assessing the health of marine ecosystems. Deep learning offers a promising approach to address these ecosystem dynamics, especially in scenarios where gap-free ground-truth data is lacking, which poses a challenge for designing effective training frameworks. Using an advanced neural variational data assimilation scheme (called 4DVarNet), we introduce a comprehensive training framework designed to effectively train directly on gappy data sets. Using the Mediterranean Sea as a case study, our experiments not only highlight the high performance of the chosen neural network in reconstructing gap-free images from gappy datasets but also demonstrate its superior performance over state-of-the-art algorithms such as DInEOF and Direct Inversion, whether using CNN or UNet architectures.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Open-Set Plankton Recognition",
    "url": "http://arxiv.org/abs/2503.11318v1",
    "authors": [
      "Joona Kareinen",
      "Annaliina Skytt\u00e4",
      "Tuomas Eerola",
      "Kaisa Kraft",
      "Lasse Lensu",
      "Sanna Suikkanen",
      "Maiju Lehtiniemi",
      "Heikki K\u00e4lvi\u00e4inen"
    ],
    "published": "2025-03-14",
    "abstract": "This paper considers open-set recognition (OSR) of plankton images. Plankton include a diverse range of microscopic aquatic organisms that have an important role in marine ecosystems as primary producers and as a base of food webs. Given their sensitivity to environmental changes, fluctuations in plankton populations offer valuable information about oceans' health and climate change motivating their monitoring. Modern automatic plankton imaging devices enable the collection of large-scale plankton image datasets, facilitating species-level analysis. Plankton species recognition can be seen as an image classification task and is typically solved using deep learning-based image recognition models. However, data collection in real aquatic environments results in imaging devices capturing a variety of non-plankton particles and plankton species not present in the training set. This creates a challenging fine-grained OSR problem, characterized by subtle differences between taxonomically close plankton species. We address this challenge by conducting extensive experiments on three OSR approaches using both phyto- and zooplankton images analyzing also on the effect of the rejection thresholds for OSR. The results demonstrate that high OSR accuracy can be obtained promoting the use of these methods in operational plankton research. We have made the data publicly available to the research community.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Correlation to Causation: A Causal Deep Learning Framework for Arctic Sea Ice Prediction",
    "url": "http://arxiv.org/abs/2503.02093v1",
    "authors": [
      "Emam Hossain",
      "Muhammad Hasan Ferdous",
      "Jianwu Wang",
      "Aneesh Subramanian",
      "Md Osman Gani"
    ],
    "published": "2025-03-03",
    "abstract": "Traditional machine learning and deep learning techniques rely on correlation-based learning, often failing to distinguish spurious associations from true causal relationships, which limits robustness, interpretability, and generalizability. To address these challenges, we propose a causality-driven deep learning framework that integrates Multivariate Granger Causality (MVGC) and PCMCI+ causal discovery algorithms with a hybrid deep learning architecture. Using 43 years (1979-2021) of daily and monthly Arctic Sea Ice Extent (SIE) and ocean-atmospheric datasets, our approach identifies causally significant factors, prioritizes features with direct influence, reduces feature overhead, and improves computational efficiency. Experiments demonstrate that integrating causal features enhances the deep learning model's predictive accuracy and interpretability across multiple lead times. Beyond SIE prediction, the proposed framework offers a scalable solution for dynamic, high-dimensional systems, advancing both theoretical understanding and practical applications in predictive modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "The Role, Trends, and Applications of Machine Learning in Undersea Communication: A Bangladesh Perspective",
    "url": "http://arxiv.org/abs/2503.00669v1",
    "authors": [
      "Yousuf Islam",
      "Sumon Chandra Das",
      "Md. Jalal Uddin Chowdhury"
    ],
    "published": "2025-03-01",
    "abstract": "The rapid evolution of machine learning (ML) has brought about groundbreaking developments in numerous industries, not the least of which is in the area of undersea communication. This domain is critical for applications like ocean exploration, environmental monitoring, resource management, and national security. Bangladesh, a maritime nation with abundant resources in the Bay of Bengal, can harness the immense potential of ML to tackle the unprecedented challenges associated with underwater communication. Beyond that, environmental conditions are unique to the region: in addition to signal attenuation, multipath propagation, noise interference, and limited bandwidth. In this study, we address the necessity to bring ML into communication via undersea; it investigates the latest technologies under the domain of ML in that respect, such as deep learning and reinforcement learning, especially concentrating on Bangladesh scenarios in the sense of implementation. This paper offers a contextualized regional perspective by incorporating region-specific needs, case studies, and recent research to propose a roadmap for deploying ML-driven solutions to improve safety at sea, promote sustainable resource use, and enhance disaster response systems. This research ultimately highlights the promise of ML-powered solutions for transforming undersea communication, leading to more efficient and cost-effective technologies that subsequently contribute to both economic growth and environmental sustainability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "CondensNet: Enabling stable long-term climate simulations via hybrid deep learning models with adaptive physical constraints",
    "url": "http://arxiv.org/abs/2502.13185v1",
    "authors": [
      "Xin Wang",
      "Juntao Yang",
      "Jeff Adie",
      "Simon See",
      "Kalli Furtado",
      "Chen Chen",
      "Troy Arcomano",
      "Romit Maulik",
      "Gianmarco Mengaldo"
    ],
    "published": "2025-02-18",
    "abstract": "Accurate and efficient climate simulations are crucial for understanding Earth's evolving climate. However, current general circulation models (GCMs) face challenges in capturing unresolved physical processes, such as cloud and convection. A common solution is to adopt cloud resolving models, that provide more accurate results than the standard subgrid parametrisation schemes typically used in GCMs. However, cloud resolving models, also referred to as super paramtetrizations, remain computationally prohibitive. Hybrid modeling, which integrates deep learning with equation-based GCMs, offers a promising alternative but often struggles with long-term stability and accuracy issues. In this work, we find that water vapor oversaturation during condensation is a key factor compromising the stability of hybrid models. To address this, we introduce CondensNet, a novel neural network architecture that embeds a self-adaptive physical constraint to correct unphysical condensation processes. CondensNet effectively mitigates water vapor oversaturation, enhancing simulation stability while maintaining accuracy and improving computational efficiency compared to super parameterization schemes.\n  We integrate CondensNet into a GCM to form PCNN-GCM (Physics-Constrained Neural Network GCM), a hybrid deep learning framework designed for long-term stable climate simulations in real-world conditions, including ocean and land. PCNN-GCM represents a significant milestone in hybrid climate modeling, as it shows a novel way to incorporate physical constraints adaptively, paving the way for accurate, lightweight, and stable long-term climate simulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Learning to generate physical ocean states: Towards hybrid climate modeling",
    "url": "http://arxiv.org/abs/2502.02499v1",
    "authors": [
      "Etienne Meunier",
      "David Kamm",
      "Guillaume Gachon",
      "Redouane Lguensat",
      "Julie Deshayes"
    ],
    "published": "2025-02-04",
    "abstract": "Ocean General Circulation Models require extensive computational resources to reach equilibrium states, while deep learning emulators, despite offering fast predictions, lack the physical interpretability and long-term stability necessary for climate scientists to understand climate sensitivity (to greenhouse gas emissions) and mechanisms of abrupt % variability such as tipping points. We propose to take the best from both worlds by leveraging deep generative models to produce physically consistent oceanic states that can serve as initial conditions for climate projections. We assess the viability of this hybrid approach through both physical metrics and numerical experiments, and highlight the benefits of enforcing physical constraints during generation. Although we train here on ocean variables from idealized numerical simulations, we claim that this hybrid approach, combining the computational efficiency of deep learning with the physical accuracy of numerical models, can effectively reduce the computational burden of running climate models to equilibrium, and reduce uncertainties in climate projections by minimizing drifts in baseline simulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Simultaneous emulation and downscaling with physically-consistent deep learning-based regional ocean emulators",
    "url": "http://arxiv.org/abs/2501.05058v1",
    "authors": [
      "Leonard Lupin-Jimenez",
      "Moein Darman",
      "Subhashis Hazarika",
      "Tianning Wu",
      "Michael Gray",
      "Ruyoing He",
      "Anthony Wong",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-01-09",
    "abstract": "Building on top of the success in AI-based atmospheric emulation, we propose an AI-based ocean emulation and downscaling framework focusing on the high-resolution regional ocean over Gulf of Mexico. Regional ocean emulation presents unique challenges owing to the complex bathymetry and lateral boundary conditions as well as from fundamental biases in deep learning-based frameworks, such as instability and hallucinations. In this paper, we develop a deep learning-based framework to autoregressively integrate ocean-surface variables over the Gulf of Mexico at $8$ Km spatial resolution without unphysical drifts over decadal time scales and simulataneously downscale and bias-correct it to $4$ Km resolution using a physics-constrained generative model. The framework shows both short-term skills as well as accurate long-term statistics in terms of mean and variability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Deep learning selection of analogues for Mars landing sites in the Qaidam Basin, Qinghai-Tibet Plateau",
    "url": "http://arxiv.org/abs/2501.08584v1",
    "authors": [
      "Fanwei Meng",
      "Xiaopeng Wang",
      "Andr\u00e9 Antunes",
      "Jie Zhao",
      "Guoliang Zhou",
      "Biqiong Wu",
      "Tianqi Hao"
    ],
    "published": "2024-12-31",
    "abstract": "Remote sensing observations and Mars rover missions have recorded the presence of beaches, salt lakes, and wind erosion landforms in Martian sediments. All these observations indicate that Mars was hydrated in its early history. There used to be oceans on Mars, but they have now dried up. Therefore, signs of previous life on Mars could be preserved in the evaporites formed during this process. The study of evaporite regions has thus become a priority area for Mars' life exploration. This study proposes a method for training similarity metrics from surface land image data of Earth and Mars, which can be used for recognition or validation applications. The method will be applied in simulating tasks to select Mars landing sites using a selecting small-scale area of the Mars analaogue the evaporite region of Qaidam Basin, Qinghai-Tibet Plateau. This learning process minimizes discriminative loss function, which makes the similarity measure smaller for images from the same location and larger for images from different locations. This study selected a Convolutional Neural Networks (CNN) based model, which has been trained to explain various changes in image appearance and identify different landforms in Mars. By identifying different landforms, priority landing sites on Mars can be selected.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Generating Unseen Nonlinear Evolution in Sea Surface Temperature Using a Deep Learning-Based Latent Space Data Assimilation Framework",
    "url": "http://arxiv.org/abs/2412.13477v1",
    "authors": [
      "Qingyu Zheng",
      "Guijun Han",
      "Wei Li",
      "Lige Cao",
      "Gongfu Zhou",
      "Haowen Wu",
      "Qi Shao",
      "Ru Wang",
      "Xiaobo Wu",
      "Xudong Cui",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2024-12-18",
    "abstract": "Advances in data assimilation (DA) methods have greatly improved the accuracy of Earth system predictions. To fuse multi-source data and reconstruct the nonlinear evolution missing from observations, geoscientists are developing future-oriented DA methods. In this paper, we redesign a purely data-driven latent space DA framework (DeepDA) that employs a generative artificial intelligence model to capture the nonlinear evolution in sea surface temperature. Under variational constraints, DeepDA embedded with nonlinear features can effectively fuse heterogeneous data. The results show that DeepDA remains highly stable in capturing and generating nonlinear evolutions even when a large amount of observational information is missing. It can be found that when only 10% of the observation information is available, the error increase of DeepDA does not exceed 40%. Furthermore, DeepDA has been shown to be robust in the fusion of real observations and ensemble simulations. In particular, this paper provides a mechanism analysis of the nonlinear evolution generated by DeepDA from the perspective of physical patterns, which reveals the inherent explainability of our DL model in capturing multi-scale ocean signals.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Advancing Marine Heatwave Forecasts: An Integrated Deep Learning Approach",
    "url": "http://arxiv.org/abs/2412.04475v1",
    "authors": [
      "Ding Ning",
      "Varvara Vetrova",
      "Yun Sing Koh",
      "Karin R. Bryan"
    ],
    "published": "2024-11-19",
    "abstract": "Marine heatwaves (MHWs), an extreme climate phenomenon, pose significant challenges to marine ecosystems and industries, with their frequency and intensity increasing due to climate change. This study introduces an integrated deep learning approach to forecast short-to-long-term MHWs on a global scale. The approach combines graph representation for modeling spatial properties in climate data, imbalanced regression to handle skewed data distributions, and temporal diffusion to enhance forecast accuracy across various lead times. To the best of our knowledge, this is the first study that synthesizes three spatiotemporal anomaly methodologies to predict MHWs. Additionally, we introduce a method for constructing graphs that avoids isolated nodes and provide a new publicly available sea surface temperature anomaly graph dataset. We examine the trade-offs in the selection of loss functions and evaluation metrics for MHWs. We analyze spatial patterns in global MHW predictability by focusing on historical hotspots, and our approach demonstrates better performance compared to traditional numerical models in regions such as the middle south Pacific, equatorial Atlantic near Africa, south Atlantic, and high-latitude Indian Ocean. We highlight the potential of temporal diffusion to replace the conventional sliding window approach for long-term forecasts, achieving improved prediction up to six months in advance. These insights not only establish benchmarks for machine learning applications in MHW forecasting but also enhance understanding of general climate forecasting methodologies.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "FengWu-W2S: A deep learning model for seamless weather-to-subseasonal forecast of global atmosphere",
    "url": "http://arxiv.org/abs/2411.10191v2",
    "authors": [
      "Fenghua Ling",
      "Kang Chen",
      "Jiye Wu",
      "Tao Han",
      "Jing-Jia Luo",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "published": "2024-11-15",
    "abstract": "Seamless forecasting that produces warning information at continuum timescales based on only one system is a long-standing pursuit for weather-climate service. While the rapid advancement of deep learning has induced revolutionary changes in classical forecasting field, current efforts are still focused on building separate AI models for weather and climate forecasts. To explore the seamless forecasting ability based on one AI model, we propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the FengWu global weather forecast model and incorporates an ocean-atmosphere-land coupling structure along with a diverse perturbation strategy. FengWu-W2S can generate 6-hourly atmosphere forecasts extending up to 42 days through an autoregressive and seamless manner. Our hindcast results demonstrate that FengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead, enhancing predictive capabilities for global surface air temperature, precipitation, geopotential height and intraseasonal signals such as the Madden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover, our ablation experiments on forecast error growth from daily to seasonal timescales reveal potential pathways for developing AI-based integrated system for seamless weather-climate forecasting in the future.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "An Analysis of Deep Learning Parameterizations for Ocean Subgrid Eddy Forcing",
    "url": "http://arxiv.org/abs/2411.06604v1",
    "authors": [
      "Cem Gultekin",
      "Adam Subel",
      "Cheng Zhang",
      "Matan Leibovich",
      "Pavel Perezhogin",
      "Alistair Adcroft",
      "Carlos Fernandez-Granda",
      "Laure Zanna"
    ],
    "published": "2024-11-10",
    "abstract": "Due to computational constraints, climate simulations cannot resolve a range of small-scale physical processes, which have a significant impact on the large-scale evolution of the climate system. Parameterization is an approach to capture the effect of these processes, without resolving them explicitly. In recent years, data-driven parameterizations based on convolutional neural networks have obtained promising results. In this work, we provide an in-depth analysis of these parameterizations developed using data from ocean simulations. The parametrizations account for the effect of mesoscale eddies toward improving simulations of momentum, heat, and mass exchange in the ocean. Our results provide several insights into the properties of data-driven parameterizations based on neural networks. First, their performance can be substantially improved by increasing the geographic extent of the training data. Second, they learn nonlinear structure, since they are able to outperform a linear baseline. Third, they generalize robustly across different CO2 forcings, but not necessarily across different ocean depths. Fourth, they exploit a relatively small region of their input to generate their output. Our results will guide the further development of ocean mesoscale eddy parameterizations, and multiscale modeling more generally.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark",
    "url": "http://arxiv.org/abs/2410.16695v2",
    "authors": [
      "Yang Yu",
      "Yuezun Li",
      "Xin Sun",
      "Junyu Dong"
    ],
    "published": "2024-10-22",
    "abstract": "Phytoplankton are a crucial component of aquatic ecosystems, and effective monitoring of them can provide valuable insights into ocean environments and ecosystem changes. Traditional phytoplankton monitoring methods are often complex and lack timely analysis. Therefore, deep learning algorithms offer a promising approach for automated phytoplankton monitoring. However, the lack of large-scale, high-quality training samples has become a major bottleneck in advancing phytoplankton tracking. In this paper, we propose a challenging benchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse background information and variations in motion during observation. The dataset includes 27 species of phytoplankton and zooplankton, 14 different backgrounds to simulate diverse and complex underwater environments, and a total of 140 videos. To enable accurate real-time observation of phytoplankton, we introduce a multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion Tracker(DSFT), which addresses issues such as focus shifts during tracking and the loss of small target information when computing frame-to-frame similarity. Specifically, we introduce an additional feature extractor to predict the residuals of the standard feature extractor's output, and compute multi-scale frame-to-frame similarity based on features from different layers of the extractor. Extensive experiments on the MPT have demonstrated the validity of the dataset and the superiority of DSFT in tracking phytoplankton, providing an effective solution for phytoplankton monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Foundation Models for Remote Sensing and Earth Observation: A Survey",
    "url": "http://arxiv.org/abs/2410.16602v3",
    "authors": [
      "Aoran Xiao",
      "Weihao Xuan",
      "Junjue Wang",
      "Jiaxing Huang",
      "Dacheng Tao",
      "Shijian Lu",
      "Naoto Yokoya"
    ],
    "published": "2024-10-22",
    "abstract": "Remote Sensing (RS) is a crucial technology for observing, monitoring, and interpreting our planet, with broad applications across geoscience, economics, humanitarian fields, etc. While artificial intelligence (AI), particularly deep learning, has achieved significant advances in RS, unique challenges persist in developing more intelligent RS systems, including the complexity of Earth's environments, diverse sensor modalities, distinctive feature patterns, varying spatial and spectral resolutions, and temporal dynamics. Meanwhile, recent breakthroughs in large Foundation Models (FMs) have expanded AI's potential across many domains due to their exceptional generalizability and zero-shot transfer capabilities. However, their success has largely been confined to natural data like images and video, with degraded performance and even failures for RS data of various non-optical modalities. This has inspired growing interest in developing Remote Sensing Foundation Models (RSFMs) to address the complex demands of Earth Observation (EO) tasks, spanning the surface, atmosphere, and oceans. This survey systematically reviews the emerging field of RSFMs. It begins with an outline of their motivation and background, followed by an introduction of their foundational concepts. It then categorizes and reviews existing RSFM studies including their datasets and technical contributions across Visual Foundation Models (VFMs), Visual-Language Models (VLMs), Large Language Models (LLMs), and beyond. In addition, we benchmark these models against publicly available datasets, discuss existing challenges, and propose future research directions in this rapidly evolving field. A project associated with this survey has been built at https://github.com/xiaoaoran/awesome-RSFMs .",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Accelerate Coastal Ocean Circulation Model with AI Surrogate",
    "url": "http://arxiv.org/abs/2410.14952v2",
    "authors": [
      "Zelin Xu",
      "Jie Ren",
      "Yupu Zhang",
      "Jose Maria Gonzalez Ondina",
      "Maitane Olabarrieta",
      "Tingsong Xiao",
      "Wenchong He",
      "Zibo Liu",
      "Shigang Chen",
      "Kaleb Smith",
      "Zhe Jiang"
    ],
    "published": "2024-10-19",
    "abstract": "Nearly 900 million people live in low-lying coastal zones around the world and bear the brunt of impacts from more frequent and severe hurricanes and storm surges. Oceanographers simulate ocean current circulation along the coasts to develop early warning systems that save lives and prevent loss and damage to property from coastal hazards. Traditionally, such simulations are conducted using coastal ocean circulation models such as the Regional Ocean Modeling System (ROMS), which usually runs on an HPC cluster with multiple CPU cores. However, the process is time-consuming and energy expensive. While coarse-grained ROMS simulations offer faster alternatives, they sacrifice detail and accuracy, particularly in complex coastal environments. Recent advances in deep learning and GPU architecture have enabled the development of faster AI (neural network) surrogates. This paper introduces an AI surrogate based on a 4D Swin Transformer to simulate coastal tidal wave propagation in an estuary for both hindcast and forecast (up to 12 days). Our approach not only accelerates simulations but also incorporates a physics-based constraint to detect and correct inaccurate results, ensuring reliability while minimizing manual intervention. We develop a fully GPU-accelerated workflow, optimizing the model training and inference pipeline on NVIDIA DGX-2 A100 GPUs. Our experiments demonstrate that our AI surrogate reduces the time cost of 12-day forecasting of traditional ROMS simulations from 9,908 seconds (on 512 CPU cores) to 22 seconds (on one A100 GPU), achieving over 450$\\times$ speedup while maintaining high-quality simulation results. This work contributes to oceanographic modeling by offering a fast, accurate, and physically consistent alternative to traditional simulation models, particularly for real-time forecasting in rapid disaster response.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MAX: Masked Autoencoder for X-ray Fluorescence in Geological Investigation",
    "url": "http://arxiv.org/abs/2410.12330v1",
    "authors": [
      "An-Sheng Lee",
      "Yu-Wen Pao",
      "Hsuan-Tien Lin",
      "Sofia Ya Hsuan Liou"
    ],
    "published": "2024-10-16",
    "abstract": "Pre-training foundation models has become the de-facto procedure for deep learning approaches, yet its application remains limited in the geological studies, where in needs of the model transferability to break the shackle of data scarcity. Here we target on the X-ray fluorescence (XRF) scanning data, a standard high-resolution measurement in extensive scientific drilling projects. We propose a scalable self-supervised learner, masked autoencoders on XRF spectra (MAX), to pre-train a foundation model covering geological records from multiple regions of the Pacific and Southern Ocean. In pre-training, we find that masking a high proportion of the input spectrum (50\\%) yields a nontrivial and meaningful self-supervisory task. For downstream tasks, we select the quantification of XRF spectra into two costly geochemical measurements, CaCO$_3$ and total organic carbon, due to their importance in understanding the paleo-oceanic carbon system. Our results show that MAX, requiring only one-third of the data, outperforms models without pre-training in terms of quantification accuracy. Additionally, the model's generalizability improves by more than 60\\% in zero-shot tests on new materials, with explainability further ensuring its robustness. Thus, our approach offers a promising pathway to overcome data scarcity in geological discovery by leveraging the self-supervised foundation model and fast-acquired XRF scanning data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "Improved deep learning of chaotic dynamical systems with multistep penalty losses",
    "url": "http://arxiv.org/abs/2410.05572v1",
    "authors": [
      "Dibyajyoti Chakraborty",
      "Seung Whan Chung",
      "Ashesh Chattopadhyay",
      "Romit Maulik"
    ],
    "published": "2024-10-08",
    "abstract": "Predicting the long-term behavior of chaotic systems remains a formidable challenge due to their extreme sensitivity to initial conditions and the inherent limitations of traditional data-driven modeling approaches. This paper introduces a novel framework that addresses these challenges by leveraging the recently proposed multi-step penalty (MP) optimization technique. Our approach extends the applicability of MP optimization to a wide range of deep learning architectures, including Fourier Neural Operators and UNETs. By introducing penalized local discontinuities in the forecast trajectory, we effectively handle the non-convexity of loss landscapes commonly encountered in training neural networks for chaotic systems. We demonstrate the effectiveness of our method through its application to two challenging use-cases: the prediction of flow velocity evolution in two-dimensional turbulence and ocean dynamics using reanalysis data. Our results highlight the potential of this approach for accurate and stable long-term prediction of chaotic dynamics, paving the way for new advancements in data-driven modeling of complex natural phenomena.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Anti-biofouling Lensless Camera System with Deep Learning based Image Reconstruction",
    "url": "http://arxiv.org/abs/2410.01365v1",
    "authors": [
      "Naoki Ide",
      "Tomohiro Kawahara",
      "Hiroshi Ueno",
      "Daiki Yanagidaira",
      "Susumu Takatsuka"
    ],
    "published": "2024-10-02",
    "abstract": "In recent years, there has been an increasing demand for underwater cameras that monitor the condition of offshore structures and check the number of individuals in aqua culture environments with long-period observation. One of the significant issues with this observation is that biofouling sticks to the aperture and lens densely and prevents cameras from capturing clear images. This study examines an underwater camera that applies material technologies with high inherent resistance to biofouling and computer vision technologies based on image reconstruction by deep learning to lens-less cameras. For this purpose, our prototype camera uses a coded aperture with 1k rectangular shape pinholes in a thin metal plate, such as copper, which hinder the growth of biofouling and keep the surface clean. Although images taken by lens-less cameras are usually not well formed due to lack of the traditional glass-based lens, a deep learning approach using ViT (Vision Transformer) has recently demonstrated reconstructing original photo images well and our study shows that using gated MLP (Multilayer Perceptron) also yields good results. On the other hand, a certain degree of thickness for bio-repellence materials is required to exhibit their effect the thickness of aperture is necessary to use apertures sufficiently thinner than the size of the pinholes to avoid unintentional reflection and absorption on the sidewalls. Therefore, we prepared a sufficiently thin plate for image reconstruction and now currently we conduct tests of the lens-less camera of the bio-repellence aperture with actual seawater environments to determine whether it can sufficiently demonstrate the biofouling effect compared with usual camera with only waterproof.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Multi-Sensor Deep Learning for Glacier Mapping",
    "url": "http://arxiv.org/abs/2409.12034v2",
    "authors": [
      "Codru\u0163-Andrei Diaconu",
      "Konrad Heidler",
      "Jonathan L. Bamber",
      "Harry Zekollari"
    ],
    "published": "2024-09-18",
    "abstract": "The more than 200,000 glaciers outside the ice sheets play a crucial role in our society by influencing sea-level rise, water resource management, natural hazards, biodiversity, and tourism. However, only a fraction of these glaciers benefit from consistent and detailed in-situ observations that allow for assessing their status and changes over time. This limitation can, in part, be overcome by relying on satellite-based Earth Observation techniques. Satellite-based glacier mapping applications have historically mainly relied on manual and semi-automatic detection methods, while recently, a fast and notable transition to deep learning techniques has started.\n  This chapter reviews how combining multi-sensor remote sensing data and deep learning allows us to better delineate (i.e. map) glaciers and detect their temporal changes. We explain how relying on deep learning multi-sensor frameworks to map glaciers benefits from the extensive availability of regional and global glacier inventories. We also analyse the rationale behind glacier mapping, the benefits of deep learning methodologies, and the inherent challenges in integrating multi-sensor earth observation data with deep learning algorithms.\n  While our review aims to provide a broad overview of glacier mapping efforts, we highlight a few setups where deep learning multi-sensor remote sensing applications have a considerable potential added value. This includes applications for debris-covered and rock glaciers that are visually difficult to distinguish from surroundings and for calving glaciers that are in contact with the ocean. These specific cases are illustrated through a series of visual imageries, highlighting some significant advantages and challenges when detecting glacier changes, including dealing with seasonal snow cover, changing debris coverage, and distinguishing glacier fronts from the surrounding sea ice.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling",
    "url": "http://arxiv.org/abs/2409.16313v2",
    "authors": [
      "Teerapong Panboonyuen"
    ],
    "published": "2024-09-14",
    "abstract": "Forecasting sea surface currents is essential for applications such as maritime navigation, environmental monitoring, and climate analysis, particularly in regions like the Gulf of Thailand and the Andaman Sea. This paper introduces SEA-ViT, an advanced deep learning model that integrates Vision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to capture spatio-temporal covariance for predicting sea surface currents (U, V) using high-frequency radar (HF) data. The name SEA-ViT is derived from ``Sea Surface Currents Forecasting using Vision Transformer,'' highlighting the model's emphasis on ocean dynamics and its use of the ViT architecture to enhance forecasting capabilities. SEA-ViT is designed to unravel complex dependencies by leveraging a rich dataset spanning over 30 years and incorporating ENSO indices (El Ni\u00f1o, La Ni\u00f1a, and neutral phases) to address the intricate relationship between geographic coordinates and climatic variations. This development enhances the predictive capabilities for sea surface currents, supporting the efforts of the Geo-Informatics and Space Technology Development Agency (GISTDA) in Thailand's maritime regions. The code and pretrained models are available at \\url{https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents}.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multi-scale decomposition of sea surface height snapshots using machine learning",
    "url": "http://arxiv.org/abs/2409.17354v1",
    "authors": [
      "Jingwen Lyu",
      "Yue Wang",
      "Christian Pedersen",
      "Spencer Jones",
      "Dhruv Balwada"
    ],
    "published": "2024-09-11",
    "abstract": "Knowledge of ocean circulation is important for understanding and predicting weather and climate, and managing the blue economy. This circulation can be estimated through Sea Surface Height (SSH) observations, but requires decomposing the SSH into contributions from balanced and unbalanced motions (BMs and UBMs). This decomposition is particularly pertinent for the novel SWOT satellite, which measures SSH at an unprecedented spatial resolution. Specifically, the requirement, and the goal of this work, is to decompose instantaneous SSH into BMs and UBMs. While a few studies using deep learning (DL) approaches have shown promise in framing this decomposition as an image-to-image translation task, these models struggle to work well across a wide range of spatial scales and require extensive training data, which is scarce in this domain. These challenges are not unique to our task, and pervade many problems requiring multi-scale fidelity. We show that these challenges can be addressed by using zero-phase component analysis (ZCA) whitening and data augmentation; making this a viable option for SSH decomposition across scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Sea ice floe segmentation in close-range optical imagery using active contour and foundation models",
    "url": "http://arxiv.org/abs/2409.06641v4",
    "authors": [
      "Giulio Passerotti",
      "Alberto Alberello",
      "Marcello Vichi",
      "Luke G. Bennetts",
      "James Bailey",
      "Alessandro Toffoli"
    ],
    "published": "2024-09-10",
    "abstract": "The size of sea ice floes in the marginal ice zone (MIZ) is a key factor influencing ice coverage, albedo, wave propagation through ice-covered waters, and ocean--atmosphere energy exchanges. Floe size can be observed by processing visual-range imagery from ships, aircraft, or satellites. However, autonomously capturing floe boundaries from imagery remains challenging, particularly due to the heterogeneity of sea ice, which impairs boundary definition and reduces image clarity. This study evaluates the accuracy of sea ice floe segmentation using the gradient vector flow (GVF) active contour method, the deep learning-based Segment Anything Model (SAM), and a hybrid approach combining GVF and SAM. These methods are evaluated on a representative subset of a large dataset of close-range, high-resolution sea ice imagery, collected from cameras aboard an icebreaker during an Antarctic winter expedition. Spanning a wide range of ice conditions and image clarity in the MIZ, the subset provides a rigorous test bed of segmentation approaches. Their performance is assessed in terms of floe detection accuracy, floe size distribution, and ice concentration, with results compared against a manually segmented benchmark. The findings indicate that SAM, when used in prompt-driven mode, offers the best balance between accuracy and computational efficiency. Its strong performance in estimating sea ice concentration and detecting floes, while maintaining close agreement with benchmark floe size distributions, makes it suitable for real-time applications and scalable analysis of large imagery datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "CAS-Canglong: A skillful 3D Transformer model for sub-seasonal to seasonal global sea surface temperature prediction",
    "url": "http://arxiv.org/abs/2409.05369v1",
    "authors": [
      "Longhao Wang",
      "Xuanze Zhang",
      "L. Ruby Leung",
      "Francis H. S. Chiew",
      "Amir AghaKouchak",
      "Kairan Ying",
      "Yongqiang Zhang"
    ],
    "published": "2024-09-09",
    "abstract": "Accurate prediction of global sea surface temperature at sub-seasonal to seasonal (S2S) timescale is critical for drought and flood forecasting, as well as for improving disaster preparedness in human society. Government departments or academic studies normally use physics-based numerical models to predict S2S sea surface temperature and corresponding climate indices, such as El Ni\u00f1o-Southern Oscillation. However, these models are hampered by computational inefficiencies, limited retention of ocean-atmosphere initial conditions, and significant uncertainty and biases. Here, we introduce a novel three-dimensional deep learning neural network to model the nonlinear and complex coupled atmosphere-ocean weather systems. This model incorporates climatic and temporal features and employs a self-attention mechanism to enhance the prediction of global S2S sea surface temperature pattern. Compared to the physics-based models, it shows significant computational efficiency and predictive capability, improving one to three months sea surface temperature predictive skill by 13.7% to 77.1% in seven ocean regions with dominant influence on S2S variability over land. This achievement underscores the significant potential of deep learning for largely improving forecasting skills at the S2S scale over land.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Generative Diffusion Model-based Downscaling of Observed Sea Surface Height over Kuroshio Extension since 2000",
    "url": "http://arxiv.org/abs/2408.12632v1",
    "authors": [
      "Qiuchang Han",
      "Xingliang Jiang",
      "Yang Zhao",
      "Xudong Wang",
      "Zhijin Li",
      "Renhe Zhang"
    ],
    "published": "2024-08-22",
    "abstract": "Satellite altimetry has been widely utilized to monitor global sea surface dynamics, enabling investigation of upper ocean variability from basin-scale to localized eddy ranges. However, the sparse spatial resolution of observational altimetry limits our understanding of oceanic submesoscale variability, prevalent at horizontal scales below 0.25o resolution. Here, we introduce a state-of-the-art generative diffusion model to train high-resolution sea surface height (SSH) reanalysis data and demonstrate its advantage in observational SSH downscaling over the eddy-rich Kuroshio Extension region. The diffusion-based model effectively downscales raw satellite-interpolated data from 0.25o resolution to 1/16o, corresponding to approximately 12-km wavelength. This model outperforms other high-resolution reanalysis datasets and neural network-based methods. Also, it successfully reproduces the spatial patterns and power spectra of satellite along-track observations. Our diffusion-based results indicate that eddy kinetic energy at horizontal scales less than 250 km has intensified significantly since 2004 in the Kuroshio Extension region. These findings underscore the great potential of deep learning in reconstructing satellite altimetry and enhancing our understanding of ocean dynamics at eddy scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Long-Range Vision-Based UAV-assisted Localization for Unmanned Surface Vehicles",
    "url": "http://arxiv.org/abs/2408.11429v1",
    "authors": [
      "Waseem Akram",
      "Siyuan Yang",
      "Hailiang Kuang",
      "Xiaoyu He",
      "Muhayy Ud Din",
      "Yihao Dong",
      "Defu Lin",
      "Lakmal Seneviratne",
      "Shaoming He",
      "Irfan Hussain"
    ],
    "published": "2024-08-21",
    "abstract": "The global positioning system (GPS) has become an indispensable navigation method for field operations with unmanned surface vehicles (USVs) in marine environments. However, GPS may not always be available outdoors because it is vulnerable to natural interference and malicious jamming attacks. Thus, an alternative navigation system is required when the use of GPS is restricted or prohibited. To this end, we present a novel method that utilizes an Unmanned Aerial Vehicle (UAV) to assist in localizing USVs in GNSS-restricted marine environments. In our approach, the UAV flies along the shoreline at a consistent altitude, continuously tracking and detecting the USV using a deep learning-based approach on camera images. Subsequently, triangulation techniques are applied to estimate the USV's position relative to the UAV, utilizing geometric information and datalink range from the UAV. We propose adjusting the UAV's camera angle based on the pixel error between the USV and the image center throughout the localization process to enhance accuracy. Additionally, visual measurements are integrated into an Extended Kalman Filter (EKF) for robust state estimation. To validate our proposed method, we utilize a USV equipped with onboard sensors and a UAV equipped with a camera. A heterogeneous robotic interface is established to facilitate communication between the USV and UAV. We demonstrate the efficacy of our approach through a series of experiments conducted during the ``Muhammad Bin Zayed International Robotic Challenge (MBZIRC-2024)'' in real marine environments, incorporating noisy measurements and ocean disturbances. The successful outcomes indicate the potential of our method to complement GPS for USV navigation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Forecasting seasonal rainfall in SE Australia using Empirical Orthogonal Functions and Neural Networks",
    "url": "http://arxiv.org/abs/2408.10550v1",
    "authors": [
      "Stjepan Marcelja"
    ],
    "published": "2024-08-20",
    "abstract": "Quantitative forecasting of average rainfall into the next season remains highly challenging, but in some favourable isolated cases may be possible with a series of relatively simple steps. We chose to explore predictions of austral springtime rainfall in SE Australia regions based on the surrounding ocean surface temperatures during the winter. In the first stage, we search for correlations between the target rainfall and both the standard ocean climate indicators as well as the time series of surface temperature data expanded in terms of Empirical Orthogonal Functions (EOFs). In the case of the Indian Ocean, during the winter the dominant EOF shows stronger correlation with the future rainfall than the commonly used Indian Ocean Dipole. Information sources with the strongest correlation to the historical rainfall data are then used as inputs into deep learning artificial neural networks. The resulting hindcasts appear accurate for September and October and less reliable for November. We also attempt to forecast the rainfall in several regions for the coming austral spring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "DUNE: A Machine Learning Deep UNet++ based Ensemble Approach to Monthly, Seasonal and Annual Climate Forecasting",
    "url": "http://arxiv.org/abs/2408.06262v1",
    "authors": [
      "Pratik Shukla",
      "Milton Halem"
    ],
    "published": "2024-08-12",
    "abstract": "Capitalizing on the recent availability of ERA5 monthly averaged long-term data records of mean atmospheric and climate fields based on high-resolution reanalysis, deep-learning architectures offer an alternative to physics-based daily numerical weather predictions for subseasonal to seasonal (S2S) and annual means. A novel Deep UNet++-based Ensemble (DUNE) neural architecture is introduced, employing multi-encoder-decoder structures with residual blocks. When initialized from a prior month or year, this architecture produced the first AI-based global monthly, seasonal, or annual mean forecast of 2-meter temperatures (T2m) and sea surface temperatures (SST). ERA5 monthly mean data is used as input for T2m over land, SST over oceans, and solar radiation at the top of the atmosphere for each month of 40 years to train the model. Validation forecasts are performed for an additional two years, followed by five years of forecast evaluations to account for natural annual variability. AI-trained inference forecast weights generate forecasts in seconds, enabling ensemble seasonal forecasts. Root Mean Squared Error (RMSE), Anomaly Correlation Coefficient (ACC), and Heidke Skill Score (HSS) statistics are presented globally and over specific regions. These forecasts outperform persistence, climatology, and multiple linear regression for all domains. DUNE forecasts demonstrate comparable statistical accuracy to NOAA's operational monthly and seasonal probabilistic outlook forecasts over the US but at significantly higher resolutions. RMSE and ACC error statistics for other recent AI-based daily forecasts also show superior performance for DUNE-based forecasts. The DUNE model's application to an ensemble data assimilation cycle shows comparable forecast accuracy with a single high-resolution model, potentially eliminating the need for retraining on extrapolated datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Riverbed litter monitoring using consumer-grade aerial-aquatic speedy scanner (AASS) and deep learning based super-resolution reconstruction and detection network",
    "url": "http://arxiv.org/abs/2408.03564v3",
    "authors": [
      "Fan Zhao",
      "Yongying Liu",
      "Jiaqi Wang",
      "Yijia Chen",
      "Dianhan Xi",
      "Xinlei Shao",
      "Shigeru Tabeta",
      "Katsunori Mizuno"
    ],
    "published": "2024-08-07",
    "abstract": "Underwater litter is widely spread across aquatic environments such as lakes, rivers, and oceans, significantly impacting natural ecosystems. Current monitoring technologies for detecting underwater litter face limitations in survey efficiency, cost, and environmental conditions, highlighting the need for efficient, consumer-grade technologies for automatic detection. This research introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with Super-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network. AASS enhances data acquisition efficiency over traditional methods, capturing high-quality images that accurately identify underwater waste. SRR improves image-resolution by mitigating motion blur and insufficient resolution, thereby enhancing detection tasks. Specifically, the RCAN model achieved the highest mean average precision (mAP) of 78.6% for detection accuracy on reconstructed images among the tested SRR models. With a magnification factor of 4, the SRR test set shows an improved mAP compared to the conventional bicubic set. These results demonstrate the effectiveness of the proposed method in detecting underwater litter.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Super-Resolution"
    ]
  },
  {
    "title": "Introducing VaDA: Novel Image Segmentation Model for Maritime Object Segmentation Using New Dataset",
    "url": "http://arxiv.org/abs/2407.09005v1",
    "authors": [
      "Yongjin Kim",
      "Jinbum Park",
      "Sanha Kang",
      "Hanguen Kim"
    ],
    "published": "2024-07-12",
    "abstract": "The maritime shipping industry is undergoing rapid evolution driven by advancements in computer vision artificial intelligence (AI). Consequently, research on AI-based object recognition models for maritime transportation is steadily growing, leveraging advancements in sensor technology and computing performance. However, object recognition in maritime environments faces challenges such as light reflection, interference, intense lighting, and various weather conditions. To address these challenges, high-performance deep learning algorithms tailored to maritime imagery and high-quality datasets specialized for maritime scenes are essential. Existing AI recognition models and datasets have limited suitability for composing autonomous navigation systems. Therefore, in this paper, we propose a Vertical and Detail Attention (VaDA) model for maritime object segmentation and a new model evaluation method, the Integrated Figure of Calculation Performance (IFCP), to verify its suitability for the system in real-time. Additionally, we introduce a benchmark maritime dataset, OASIs (Ocean AI Segmentation Initiatives) to standardize model performance evaluation across diverse maritime environments. OASIs dataset and details are available at our website: https://www.navlue.com/dataset",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Recognition"
    ]
  },
  {
    "title": "A Computer Vision Approach to Estimate the Localized Sea State",
    "url": "http://arxiv.org/abs/2407.03755v2",
    "authors": [
      "Aleksandar Vorkapic",
      "Miran Pobar",
      "Marina Ivasic-Kos"
    ],
    "published": "2024-07-04",
    "abstract": "This research presents a novel application of computer vision (CV) and deep learning methods for real-time sea state recognition, aiming to contribute to improving the operational safety and energy efficiency of seagoing vessels, key factors in meeting the legislative carbon reduction targets. Our work focuses on utilizing sea images in operational envelopes captured by a single stationary camera mounted on the ship bridge. The collected images are used to train a deep learning model to automatically recognize the state of the sea based on the Beaufort scale. To recognize the sea state, we used 4 state-of-the-art deep neural networks with different characteristics that proved useful in various computer vision tasks: Resnet-101, NASNet, MobileNet_v2, and Transformer ViT-b32. Furthermore, we have defined a unique large-scale dataset, collected over a broad range of sea conditions from an ocean-going vessel prepared for machine learning. We used the transfer learning approach to fine-tune the models on our dataset. The obtained results demonstrate the potential for this approach to complement traditional methods, particularly where in-situ measurements are unfeasible or interpolated weather buoy data is insufficiently accurate. This study sets the groundwork for further development of sea state classification models to address recognized gaps in maritime research and enable safer and more efficient maritime operations.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet",
      "Transformer",
      "Deep Neural Network"
    ],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Advanced Framework for Animal Sound Classification With Features Optimization",
    "url": "http://arxiv.org/abs/2407.03440v1",
    "authors": [
      "Qiang Yang",
      "Xiuying Chen",
      "Changsheng Ma",
      "Carlos M. Duarte",
      "Xiangliang Zhang"
    ],
    "published": "2024-07-03",
    "abstract": "The automatic classification of animal sounds presents an enduring challenge in bioacoustics, owing to the diverse statistical properties of sound signals, variations in recording equipment, and prevalent low Signal-to-Noise Ratio (SNR) conditions. Deep learning models like Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) have excelled in human speech recognition but have not been effectively tailored to the intricate nature of animal sounds, which exhibit substantial diversity even within the same domain. We propose an automated classification framework applicable to general animal sound classification. Our approach first optimizes audio features from Mel-frequency cepstral coefficients (MFCC) including feature rearrangement and feature reduction. It then uses the optimized features for the deep learning model, i.e., an attention-based Bidirectional LSTM (Bi-LSTM), to extract deep semantic features for sound classification. We also contribute an animal sound benchmark dataset encompassing oceanic animals and birds1. Extensive experimentation with real-world datasets demonstrates that our approach consistently outperforms baseline methods by over 25% in precision, recall, and accuracy, promising advancements in animal sound classification.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Scale-Translation Equivariant Network for Oceanic Internal Solitary Wave Localization",
    "url": "http://arxiv.org/abs/2406.13060v1",
    "authors": [
      "Zhang Wan",
      "Shuo Wang",
      "Xudong Zhang"
    ],
    "published": "2024-06-18",
    "abstract": "Internal solitary waves (ISWs) are gravity waves that are often observed in the interior ocean rather than the surface. They hold significant importance due to their capacity to carry substantial energy, thus influence pollutant transport, oil platform operations, submarine navigation, etc. Researchers have studied ISWs through optical images, synthetic aperture radar (SAR) images, and altimeter data from remote sensing instruments. However, cloud cover in optical remote sensing images variably obscures ground information, leading to blurred or missing surface observations. As such, this paper aims at altimeter-based machine learning solutions to automatically locate ISWs. The challenges, however, lie in the following two aspects: 1) the altimeter data has low resolution, which requires a strong machine learner; 2) labeling data is extremely labor-intensive, leading to very limited data for training. In recent years, the grand progress of deep learning demonstrates strong learning capacity given abundant data. Besides, more recent studies on efficient learning and self-supervised learning laid solid foundations to tackle the aforementioned challenges. In this paper, we propose to inject prior knowledge to achieve a strong and efficient learner. Specifically, intrinsic patterns in altimetry data are efficiently captured using a scale-translation equivariant convolutional neural network (ST-ECNN). By considering inherent symmetries in neural network design, ST-ECNN achieves higher efficiency and better performance than baseline models. Furthermore, we also introduce prior knowledge from massive unsupervised data to enhance our solution using the SimCLR framework for pre-training. Our final solution achieves an overall better performance than baselines on our handcrafted altimetry dataset. Data and codes are available at https://github.com/ZhangWan-byte/Internal_Solitary_Wave_Localization .",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "Reconstructing the Tropical Pacific Upper Ocean using Online Data Assimilation with a Deep Learning model",
    "url": "http://arxiv.org/abs/2406.07063v1",
    "authors": [
      "Zilu Meng",
      "Gregory J. Hakim"
    ],
    "published": "2024-06-11",
    "abstract": "A deep learning (DL) model, based on a transformer architecture, is trained on a climate-model dataset and compared with a standard linear inverse model (LIM) in the tropical Pacific. We show that the DL model produces more accurate forecasts compared to the LIM when tested on a reanalysis dataset. We then assess the ability of an ensemble Kalman filter to reconstruct the monthly-averaged upper ocean from a noisy set of 24 sea-surface temperature observations designed to mimic existing coral proxy measurements, and compare results for the DL model and LIM. Due to signal damping in the DL model, we implement a novel inflation technique by adding noise from hindcast experiments. Results show that assimilating observations with the DL model yields better reconstructions than the LIM for observation averaging times ranging from one month to one year. The improved reconstruction is due to the enhanced predictive capabilities of the DL model, which map the memory of past observations to future assimilation times.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Ocean Wave Forecasting with Deep Learning as Alternative to Conventional Models",
    "url": "http://arxiv.org/abs/2406.03848v4",
    "authors": [
      "Ziliang Zhang",
      "Huaming Yu",
      "Danqin Ren",
      "Chenyu Zhang",
      "Minghua Sun",
      "Xin Qi"
    ],
    "published": "2024-06-06",
    "abstract": "This study presents OceanCastNet (OCN), a machine learning approach for wave forecasting that incorporates wind and wave fields to predict significant wave height, mean wave period, and mean wave direction.We evaluate OCN's performance against the operational ECWAM model using two independent datasets: NDBC buoy and Jason-3 satellite observations. NDBC station validation indicates OCN performs better at 24 stations compared to ECWAM's 10 stations, and Jason-3 satellite validation confirms similar accuracy across 228-hour forecasts. OCN successfully captures wave patterns during extreme weather conditions, demonstrated through Typhoon Goni with prediction errors typically within $\\pm$0.5 m. The approach also offers computational efficiency advantages. The results suggest that machine learning approaches can achieve performance comparable to conventional wave forecasting systems for operational wave prediction applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Applications of Deep Learning parameterization of Ocean Momentum Forcing",
    "url": "http://arxiv.org/abs/2406.03659v1",
    "authors": [
      "Guosong Wang",
      "Min Hou",
      "Xinrong Wu",
      "Xidong Wang",
      "Zhigang Gao",
      "Hongli Fu",
      "Bo Dan",
      "Chunjian Sun",
      "Xiaoshuang Zhang"
    ],
    "published": "2024-06-06",
    "abstract": "Mesoscale eddies are of utmost importance in understanding ocean dynamics and the transport of heat, salt, and nutrients. Accurate representation of these eddies in ocean models is essential for improving model predictions. However, accurately representing these mesoscale features in numerical models is challenging due to their relatively small size. In this study, we propose a convolutional neural network (CNN) that combines data-driven techniques with physical principles to develop a robust and interpretable parameterization scheme for mesoscale eddies in ocean modeling. We first analyze a high-resolution reanalysis dataset to extract subgrid eddy momentum and use machine learning algorithms to identify patterns and correlations. To ensure physical consistency, we have introduced conservation of momentum constraints in our CNN parameterization scheme through soft and hard constraints. The interpretability analysis illustrate that the pre-trained CNN parameterization shows promising results in accurately solving the resolved mean velocity at the local scale and effectively capturing the representation of unresolved subgrid turbulence processes at the global scale. Furthermore, to validate the CNN parameterization scheme offline, we conduct simulations using the MITgcm ocean model. A series of experiments is conducted to compare the performance of the model with the CNN parameterization scheme and high-resolution simulations. The offline validation using MITgcm simulations demonstrates the effectiveness of the CNN parameterization scheme in improving the representation of mesoscale eddies in the ocean model. Incorporating the CNN parameterization scheme leads to better agreement with high-resolution simulations and a more accurate representation of the kinetic energy spectra.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Data-driven Global Ocean Modeling for Seasonal to Decadal Prediction",
    "url": "http://arxiv.org/abs/2405.15412v2",
    "authors": [
      "Zijie Guo",
      "Pumeng Lyu",
      "Fenghua Ling",
      "Lei Bai",
      "Jing-Jia Luo",
      "Niklas Boers",
      "Toshio Yamagata",
      "Takeshi Izumo",
      "Sophie Cravatte",
      "Antonietta Capotondi",
      "Wanli Ouyang"
    ],
    "published": "2024-05-24",
    "abstract": "Accurate ocean dynamics modeling is crucial for enhancing understanding of ocean circulation, predicting climate variability, and tackling challenges posed by climate change. Despite improvements in traditional numerical models, predicting global ocean variability over multi-year scales remains challenging. Here, we propose ORCA-DL (Oceanic Reliable foreCAst via Deep Learning), the first data-driven 3D ocean model for seasonal to decadal prediction of global ocean circulation. ORCA-DL accurately simulates three-dimensional ocean dynamics and outperforms state-of-the-art dynamical models in capturing extreme events, including El Ni\u00f1o-Southern Oscillation and upper ocean heatwaves. This demonstrates the high potential of data-driven models for efficient and accurate global ocean forecasting. Moreover, ORCA-DL stably emulates ocean dynamics at decadal timescales, demonstrating its potential even for skillful decadal predictions and climate projections.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Deep learning-based hyperspectral image reconstruction for quality assessment of agro-product",
    "url": "http://arxiv.org/abs/2405.12313v1",
    "authors": [
      "Md. Toukir Ahmed",
      "Ocean Monjur",
      "Mohammed Kamruzzaman"
    ],
    "published": "2024-05-20",
    "abstract": "Hyperspectral imaging (HSI) has recently emerged as a promising tool for many agricultural applications; however, the technology cannot be directly used in a real-time system due to the extensive time needed to process large volumes of data. Consequently, the development of a simple, compact, and cost-effective imaging system is not possible with the current HSI systems. Therefore, the overall goal of this study was to reconstruct hyperspectral images from RGB images through deep learning for agricultural applications. Specifically, this study used Hyperspectral Convolutional Neural Network - Dense (HSCNN-D) to reconstruct hyperspectral images from RGB images for predicting soluble solid content (SSC) in sweet potatoes. The algorithm accurately reconstructed the hyperspectral images from RGB images, with the resulting spectra closely matching the ground-truth. The partial least squares regression (PLSR) model based on reconstructed spectra outperformed the model using the full spectral range, demonstrating its potential for SSC prediction in sweet potatoes. These findings highlight the potential of deep learning-based hyperspectral image reconstruction as a low-cost, efficient tool for various agricultural uses.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "BharatBench: Dataset for data-driven weather forecasting over India",
    "url": "http://arxiv.org/abs/2405.07534v1",
    "authors": [
      "Animesh Choudhury",
      "Jagabandhu Panda",
      "Asmita Mukherjee"
    ],
    "published": "2024-05-13",
    "abstract": "Advanced weather and climate models use numerical techniques on grided meshes to simulate atmospheric and ocean dynamics, which are computationally expensive. Data-driven approaches are gaining popularity in weather and climate modeling, with a broad scope of applications. Although Machine Learning (ML) has been employed in this domain, significant progress has occurred in the past decade, leading to ML applications that are now competitive with traditional numerical methods. This study presents a user-friendly dataset for data-driven medium-range weather forecasting focused on India. The dataset is derived from IMDAA reanalysis datasets and optimized for ML applications. The study provides clear evaluation metrics and a few baseline scores from simple linear regression techniques and deep learning models. The dataset can be found at https://www.kaggle.com/datasets/maslab/bharatbench, while the codes are available at https://github.com/MASLABnitrkl/BharatBench. We hope this dataset will boost data-driven weather forecasting over India. We also address limitations in the current evaluation process and future challenges in data-driven weather forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "OXYGENERATOR: Reconstructing Global Ocean Deoxygenation Over a Century with Deep Learning",
    "url": "http://arxiv.org/abs/2405.07233v1",
    "authors": [
      "Bin Lu",
      "Ze Zhao",
      "Luyu Han",
      "Xiaoying Gan",
      "Yuntao Zhou",
      "Lei Zhou",
      "Luoyi Fu",
      "Xinbing Wang",
      "Chenghu Zhou",
      "Jing Zhang"
    ],
    "published": "2024-05-12",
    "abstract": "Accurately reconstructing the global ocean deoxygenation over a century is crucial for assessing and protecting marine ecosystem. Existing expert-dominated numerical simulations fail to catch up with the dynamic variation caused by global warming and human activities. Besides, due to the high-cost data collection, the historical observations are severely sparse, leading to big challenge for precise reconstruction. In this work, we propose OxyGenerator, the first deep learning based model, to reconstruct the global ocean deoxygenation from 1920 to 2023. Specifically, to address the heterogeneity across large temporal and spatial scales, we propose zoning-varying graph message-passing to capture the complex oceanographic correlations between missing values and sparse observations. Additionally, to further calibrate the uncertainty, we incorporate inductive bias from dissolved oxygen (DO) variations and chemical effects. Compared with in-situ DO observations, OxyGenerator significantly outperforms CMIP6 numerical simulations, reducing MAPE by 38.77%, demonstrating a promising potential to understand the \"breathless ocean\" in data-driven manner.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Continual Learning of Range-Dependent Transmission Loss for Underwater Acoustic using Conditional Convolutional Neural Net",
    "url": "http://arxiv.org/abs/2404.08091v1",
    "authors": [
      "Indu Kant Deo",
      "Akash Venkateshwaran",
      "Rajeev K. Jaiman"
    ],
    "published": "2024-04-11",
    "abstract": "There is a significant need for precise and reliable forecasting of the far-field noise emanating from shipping vessels. Conventional full-order models based on the Navier-Stokes equations are unsuitable, and sophisticated model reduction methods may be ineffective for accurately predicting far-field noise in environments with seamounts and significant variations in bathymetry. Recent advances in reduced-order models, particularly those based on convolutional and recurrent neural networks, offer a faster and more accurate alternative. These models use convolutional neural networks to reduce data dimensions effectively. However, current deep-learning models face challenges in predicting wave propagation over long periods and for remote locations, often relying on auto-regressive prediction and lacking far-field bathymetry information. This research aims to improve the accuracy of deep-learning models for predicting underwater radiated noise in far-field scenarios. We propose a novel range-conditional convolutional neural network that incorporates ocean bathymetry data into the input. By integrating this architecture into a continual learning framework, we aim to generalize the model for varying bathymetry worldwide. To demonstrate the effectiveness of our approach, we analyze our model on several test cases and a benchmark scenario involving far-field prediction over Dickin's seamount in the Northeast Pacific. Our proposed architecture effectively captures transmission loss over a range-dependent, varying bathymetry profile. This architecture can be integrated into an adaptive management system for underwater radiated noise, providing real-time end-to-end mapping between near-field ship noise sources and received noise at the marine mammal's location.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks",
    "url": "http://arxiv.org/abs/2404.06437v1",
    "authors": [
      "Dimitrios Michail",
      "Lefki-Ioanna Panagiotou",
      "Charalampos Davalas",
      "Ioannis Prapas",
      "Spyros Kondylatos",
      "Nikolaos Ioannis Bountos",
      "Ioannis Papoutsis"
    ],
    "published": "2024-04-09",
    "abstract": "With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we train deep learning models with different architectures that capture the spatio-temporal context leading to wildfires. Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models. Our findings demonstrate the great potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations",
    "url": "http://arxiv.org/abs/2404.06400v2",
    "authors": [
      "Maximilian Witte",
      "Fabricio Rodrigues Lapolli",
      "Philip Freese",
      "Sebastian G\u00f6tschel",
      "Daniel Ruprecht",
      "Peter Korn",
      "Christopher Kadow"
    ],
    "published": "2024-04-09",
    "abstract": "Using the nonlinear shallow water equations as benchmark, we demonstrate that a simulation with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a simulation with 10km resolution. The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h. Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow. We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution simulation. After 8 day of simulation, the $L_2$-error of the corrected run is similar to a simulation run on the finer mesh. While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Downscaling GRACE-derived ocean bottom pressure anomalies using self-supervised data fusion",
    "url": "http://arxiv.org/abs/2404.05818v1",
    "authors": [
      "Junyang Gou",
      "Lara B\u00f6rger",
      "Michael Schindelegger",
      "Benedikt Soja"
    ],
    "published": "2024-04-08",
    "abstract": "The gravimetry measurements from the Gravity Recovery and Climate Experiment (GRACE) and its follow-on (GRACE-FO) satellite mission provide an essential way to monitor changes in ocean bottom pressure ($p_b$), which is a critical variable in understanding ocean circulation. However, the coarse spatial resolution of the GRACE(-FO) fields blurs important spatial details, such as $p_b$ gradients. In this study, we employ a self-supervised deep learning algorithm to downscale global monthly $p_b$ anomalies derived from GRACE(-FO) observations to an equal-angle $0.25^\\circ$ grid in the absence of high-resolution ground truth. The optimization process is realized by constraining the outputs to follow the large-scale mass conservation contained in the gravity field estimates while learning the spatial details from two ocean reanalysis products. The downscaled product agrees with GRACE(-FO) solutions over large ocean basins at the millimeter level in terms of equivalent water height and shows signs of outperforming them when evaluating short spatial scale variability. In particular, the downscaled $p_b$ product has more realistic signal content near the coast and exhibits better agreement with tide gauge measurements at around 80% of 465 globally distributed stations. Our method presents a novel way of combining the advantages of satellite measurements and ocean models at the product level, with potential downstream applications for studies of the large-scale ocean circulation, coastal sea level variability, and changes in global geodetic parameters.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A Multiobjective Hyperparameter and Architecture Optimization Approach",
    "url": "http://arxiv.org/abs/2404.05768v2",
    "authors": [
      "Yixuan Sun",
      "Ololade Sowunmi",
      "Romain Egele",
      "Sri Hari Krishna Narayanan",
      "Luke Van Roekel",
      "Prasanna Balaprakash"
    ],
    "published": "2024-04-07",
    "abstract": "Training an effective deep learning model to learn ocean processes involves careful choices of various hyperparameters. We leverage the advanced search algorithms for multiobjective optimization in DeepHyper, a scalable hyperparameter optimization software, to streamline the development of neural networks tailored for ocean modeling. The focus is on optimizing Fourier neural operators (FNOs), a data-driven model capable of simulating complex ocean behaviors. Selecting the correct model and tuning the hyperparameters are challenging tasks, requiring much effort to ensure model accuracy. DeepHyper allows efficient exploration of hyperparameters associated with data preprocessing, FNO architecture-related hyperparameters, and various model training strategies. We aim to obtain an optimal set of hyperparameters leading to the most performant model. Moreover, on top of the commonly used mean squared error for model training, we propose adopting the negative anomaly correlation coefficient as the additional loss term to improve model performance and investigate the potential trade-off between the two terms. The experimental results show that the optimal set of hyperparameters enhanced model performance in single timestepping forecasting and greatly exceeded the baseline configuration in the autoregressive rollout for long-horizon forecasting up to 30 days. Utilizing DeepHyper, we demonstrate an approach to enhance the use of FNOs in ocean dynamics forecasting, offering a scalable solution with improved precision.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Difference Learning for Air Quality Forecasting Transport Emulation",
    "url": "http://arxiv.org/abs/2402.14806v1",
    "authors": [
      "Reed River Chen",
      "Christopher Ribaudo",
      "Jennifer Sleeman",
      "Chace Ashcraft",
      "Collin Kofroth",
      "Marisa Hughes",
      "Ivanka Stajner",
      "Kevin Viner",
      "Kai Wang"
    ],
    "published": "2024-02-22",
    "abstract": "Human health is negatively impacted by poor air quality including increased risk for respiratory and cardiovascular disease. Due to a recent increase in extreme air quality events, both globally and locally in the United States, finer resolution air quality forecasting guidance is needed to effectively adapt to these events. The National Oceanic and Atmospheric Administration provides air quality forecasting guidance for the Continental United States. Their air quality forecasting model is based on a 15 km spatial resolution; however, the goal is to reach a three km spatial resolution. This is currently not feasible due in part to prohibitive computational requirements for modeling the transport of chemical species. In this work, we describe a deep learning transport emulator that is able to reduce computations while maintaining skill comparable with the existing numerical model. We show how this method maintains skill in the presence of extreme air quality events, making it a potential candidate for operational use. We also explore evaluating how well this model maintains the physical properties of the modeled transport for a given set of species.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Quantitative causality, causality-guided scientific discovery, and causal machine learning",
    "url": "http://arxiv.org/abs/2402.13427v1",
    "authors": [
      "X. San Liang",
      "Dake Chen",
      "Renhe Zhang"
    ],
    "published": "2024-02-20",
    "abstract": "It has been said, arguably, that causality analysis should pave a promising way to interpretable deep learning and generalization. Incorporation of causality into artificial intelligence (AI) algorithms, however, is challenged with its vagueness, non-quantitiveness, computational inefficiency, etc. During the past 18 years, these challenges have been essentially resolved, with the establishment of a rigorous formalism of causality analysis initially motivated from atmospheric predictability. This not only opens a new field in the atmosphere-ocean science, namely, information flow, but also has led to scientific discoveries in other disciplines, such as quantum mechanics, neuroscience, financial economics, etc., through various applications. This note provides a brief review of the decade-long effort, including a list of major theoretical results, a sketch of the causal deep learning framework, and some representative real-world applications in geoscience pertaining to this journal, such as those on the anthropogenic cause of global warming, the decadal prediction of El Ni\u00f1o Modoki, the forecasting of an extreme drought in China, among others.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Guiding the underwater acoustic target recognition with interpretable contrastive learning",
    "url": "http://arxiv.org/abs/2402.12658v1",
    "authors": [
      "Yuan Xie",
      "Jiawei Ren",
      "Ji Xu"
    ],
    "published": "2024-02-20",
    "abstract": "Recognizing underwater targets from acoustic signals is a challenging task owing to the intricate ocean environments and variable underwater channels. While deep learning-based systems have become the mainstream approach for underwater acoustic target recognition, they have faced criticism for their lack of interpretability and weak generalization performance in practical applications. In this work, we apply the class activation mapping (CAM) to generate visual explanations for the predictions of a spectrogram-based recognition system. CAM can help to understand the behavior of recognition models by highlighting the regions of the input features that contribute the most to the prediction. Our explorations reveal that recognition models tend to focus on the low-frequency line spectrum and high-frequency periodic modulation information of underwater signals. Based on the observation, we propose an interpretable contrastive learning (ICL) strategy that employs two encoders to learn from acoustic features with different emphases (line spectrum and modulation information). By imposing constraints between encoders, the proposed strategy can enhance the generalization performance of the recognition system. Our experiments demonstrate that the proposed contrastive learning approach can improve the recognition accuracy and bring significant improvements across various underwater databases.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Recognition",
      "Forecast"
    ]
  },
  {
    "title": "Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NWP forecasts",
    "url": "http://arxiv.org/abs/2402.07851v2",
    "authors": [
      "Apoorva Narula",
      "Aastha Jain",
      "Jatin Batra",
      "MN Rajeevan",
      "Sandeep Juneja"
    ],
    "published": "2024-02-12",
    "abstract": "The Indian summer monsoon is a highly complex and critical weather system that directly affects the livelihoods of over a billion people across the Indian subcontinent. Accurate short-term forecasting remains a major scientific challenge due to the monsoon's intrinsic nonlinearity and its sensitivity to multi-scale drivers, including local land-atmosphere interactions and large-scale ocean-atmosphere phenomena. In this study, we address the problem of forecasting daily rainfall across India during the summer months, focusing on both one-day and three-day lead times. We use Autoformers - deep learning transformer-based architectures designed for time series forecasting. These are trained on historical gridded precipitation data from the Indian Meteorological Department (1901--2023) at spatial resolutions of $0.25^\\circ \\times 0.25^\\circ$, as well as $1^\\circ \\times 1^\\circ$. The models also incorporate auxiliary meteorological variables from ECMWFs reanalysis datasets, namely, cloud cover, humidity, temperature, soil moisture, vorticity, and wind speed. Forecasts at $0.25^\\circ \\times 0.25^\\circ$ are benchmarked against ECMWFs High-Resolution Ensemble System (HRES), widely regarded as the most accurate numerical weather predictor, and at $1^\\circ \\times 1^\\circ $ with those from National Centre for Environmental Prediction (NCEP). We conduct both nationwide evaluations and localized analyses for major Indian cities. Our results indicate that transformer-based deep learning models consistently outperform both HRES and NCEP, as well as other climatological baselines. Specifically, compared to our model, forecasts from HRES and NCEP model have about 22\\% and 43\\% higher error, respectively, for a single day prediction, and over 27\\% and 66\\% higher error respectively, for a three day prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Climate Trends of Tropical Cyclone Intensity and Energy Extremes Revealed by Deep Learning",
    "url": "http://arxiv.org/abs/2402.00362v1",
    "authors": [
      "Buo-Fu Chen",
      "Boyo Chen",
      "Chun-Min Hsiao",
      "Hsu-Feng Teng",
      "Cheng-Shang Lee",
      "Hung-Chi Kuo"
    ],
    "published": "2024-02-01",
    "abstract": "Anthropogenic influences have been linked to tropical cyclone (TC) poleward migration, TC extreme precipitation, and an increased proportion of major hurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is critical for projecting future TC impacts on human society considering the changing climate [5]. However, past trends of TC structure/energy remain uncertain due to limited observations; subjective-analyzed and spatiotemporal-heterogeneous \"best-track\" datasets lead to reduced confidence in the assessed TC repose to climate change [6, 7]. Here, we use deep learning to reconstruct past \"observations\" and yield an objective global TC wind profile dataset during 1981 to 2020, facilitating a comprehensive examination of TC structure/energy. By training with uniquely labeled data integrating best tracks and numerical model analysis of 2004 to 2018 TCs, our model converts multichannel satellite imagery to a 0-750-km wind profile of axisymmetric surface winds. The model performance is verified to be sufficient for climate studies by comparing it to independent satellite-radar surface winds. Based on the new homogenized dataset, the major TC proportion has increased by ~13% in the past four decades. Moreover, the proportion of extremely high-energy TCs has increased by ~25%, along with an increasing trend (> one standard deviation of the 40-y variability) of the mean total energy of high-energy TCs. Although the warming ocean favors TC intensification, the TC track migration to higher latitudes and altered environments further affect TC structure/energy. This new deep learning method/dataset reveals novel trends regarding TC structure extremes and may help verify simulations/studies regarding TCs in the changing climate.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Current effects on wind generated waves near an Ocean Eddy Dipole",
    "url": "http://arxiv.org/abs/2511.12711v1",
    "authors": [
      "Nelson Violante-Carvalho",
      "Thiago de Paula",
      "Leandro Calado",
      "Felipe Marques dos Santos",
      "Luiz Mariano Carvalho",
      "Andre Luiz Cordeiro dos Santos",
      "Wilton Z. Arruda",
      "Leandro Farina"
    ],
    "published": "2025-11-16",
    "abstract": "Ocean eddy dipoles are among the most common mesoscale features and may be ubiquitous across the global oceans. However, wave-current interactions in their proximity have not been extensively studied. Here we examine the impact of surface currents on the wave field near an ocean eddy dipole. Using the WW3 wave model, we conducted idealized numerical simulations to assess the influence of different configurations on the spatial variability of Significant Wave Height ($H_s$). Additionally, a two-month hindcast of a strong dipole event in the Southwestern Atlantic Ocean was performed using three distinct surface current products: SSalto/Duacs, HYCOM NCODA and GlobCurrent. Among these, HYCOM, which incorporates ageostrophic effects, provided a more detailed representation of oceanic energy compared to GlobCurrent and SSalto/Duacs, which primarily reflect geostrophic components. The hindcast assessment employed denoised altimeter-derived $H_s$ data, with a spatial resolution of approximately 6~km. The greatest increase in wave energy occurs in the region between the peak values of positive and negative vorticity, where the opposing surface currents reach their maximum intensity. Therefore, dipoles act as converging lenses for surface waves, channeling their refraction towards the central jet. Despite its poorer spatial and temporal resolutions, SSalto-Duacs surface current data provides more reliable $H_s$ fields, in the study region where geostrophic dynamics are expected to be significant or even dominant.\n  HYCOM captures a broader range of dynamical processes, essential for accurately representing the total energy, though discrepancies with SSalto/Duacs data may arise from assimilation inaccuracies and model limitations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Uncoupled high-latitude wave models in COAMPS",
    "url": "http://arxiv.org/abs/2508.16755v1",
    "authors": [
      "W. E. Rogers",
      "T. J. Campbell",
      "J. Yu",
      "R. A. Allard"
    ],
    "published": "2025-08-22",
    "abstract": "This report describes six demonstration cases of two numerical ocean wave models in high latitude regions where waves interact with sea ice. The two wave models are SWAN (Simulating WAves Nearshore) and WW3 (WAVEWATCH III), run in uncoupled mode within the Navy's coupled regional modeling system, COAMPS(R) (Coupled Ocean Atmosphere Mesoscale Prediction System). The COAMPS software handles a large majority of the tasks associated with the setup, running, and post-processing of the wave models. All six cases are cycling runs with 12-hour increments, each providing a continuous hindcast of four to 26 days duration. SWAN is applied in a Bering Strait case and two Gulf of Bothnia cases. WW3 is applied in a Sea of Okhotsk case and two Barents Sea cases. Verification is performed by visual inspection of model output fields, and by comparing model runs with alternative settings. In the standard configuration, forcing comes from archived global model output, including information on surface wind vectors, sea ice concentration and thickness, and surface current vectors, and the wave model uses a new empirical formula for dissipation of wave energy by sea ice that is dependent on ice thickness. The Barents Sea cases are compared to spectral wave data from satellite (SWIM instrument on CFOSAT) and from motion sensors deployed on the ice by the Norwegian Meteorological Institute. Experiments are performed with non-standard settings, 1) disabling the dissipation by sea ice, 2) using an older formula for dissipation by sea ice which does not depend on ice thickness, 3) using higher resolution wind and sea ice concentration forcing fields, and 4) omitting surface currents. The impact of these settings on model skill is quantified by comparison to the observations. The skill is also compared to that from a global (thus, lower resolution) ocean wave model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A validated coupled three-dimensional hydrodynamic and spectral wind-wave model for the western north Atlantic Ocean",
    "url": "http://arxiv.org/abs/2505.18803v1",
    "authors": [
      "Maria Venolia",
      "Reza Marsooli",
      "Jaime R. Calzada"
    ],
    "published": "2025-05-24",
    "abstract": "Wind-wave and ocean current interactions affect critical coastal and oceanic processes, yet modeling these interactions presents significant challenges. The western North Atlantic Ocean provides an ideal test environment for coupled hydrodynamics and wind wave models, thanks to its energetic surface currents such as the Gulf Stream. This study evaluates a high-resolution coupled SCHISM WWM III model, utilizing NOAA's 'STOFS-3D-Atlantic' computational mesh, while incorporating three-dimensional baroclinic dynamics to account for density stratification effects. We evaluate the model's calculated water level and tidal predictions against NOAA tide gauge measurements during December 2016. The coupled model demonstrates robust skills in reproducing tidal constituents, non-tidal components, and total water level predictions along the U.S. East and Gulf of Mexico Coasts. In addition, we systematically evaluate three wave physics parameterizations (Ardhuin, Makin and Stam, and Cycle Three) in the spectral wave model to quantify their effects on the modeled wave characteristics. This validated modeling framework enhances our ability to understand and predict complex coastal and oceanic processes, offering significant applications for coastal management, maritime operations, and climate adaptation planning throughout the western North Atlantic region.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "ORCAst: Operational High-Resolution Current Forecasts",
    "url": "http://arxiv.org/abs/2501.12054v1",
    "authors": [
      "Pierre Garcia",
      "In\u00e8s Larroche",
      "Am\u00e9lie Pesnec",
      "Hannah Bull",
      "Th\u00e9o Archambault",
      "Evangelos Moschos",
      "Alexandre Stegner",
      "Anastase Charantonis",
      "Dominique B\u00e9r\u00e9ziat"
    ],
    "published": "2025-01-21",
    "abstract": "We present ORCAst, a multi-stage, multi-arm network for Operational high-Resolution Current forecAsts over one week. Producing real-time nowcasts and forecasts of ocean surface currents is a challenging problem due to indirect or incomplete information from satellite remote sensing data. Entirely trained on real satellite data and in situ measurements from drifters, our model learns to forecast global ocean surface currents using various sources of ground truth observations in a multi-stage learning procedure. Our multi-arm encoder-decoder model architecture allows us to first predict sea surface height and geostrophic currents from larger quantities of nadir and SWOT altimetry data, before learning to predict ocean surface currents from much more sparse in situ measurements from drifters. Training our model on specific regions improves performance. Our model achieves stronger nowcast and forecast performance in predicting ocean surface currents than various state-of-the-art methods.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Nowcast"
    ]
  },
  {
    "title": "An analysis on OpenMetBuoy-v2021 drifter in-situ data and Lagrangian trajectory simulations in the Agulhas Current System",
    "url": "http://arxiv.org/abs/2409.20096v1",
    "authors": [
      "Bente Moerman",
      "\u00d8yvind Breivik",
      "Lars R. Hole",
      "Gaute Hope",
      "Johnny A. Johannessen",
      "Jean Rabault"
    ],
    "published": "2024-09-30",
    "abstract": "In order to perform a sensitivity analysis of Lagrangian trajectory models, Lagrangian trajectory simulations have been compared to six OpenMetBuoy-v2021 drifter trajectories in the Agulhas Current System (Jan-Mar 2023). Three different Lagrangian trajectory simulations have been assessed: (1) two offline Lagrangian tracking tools, OpenDrift and Parcels, (2) three Eulerian ocean surface current products, HYCOM, Mercator and Globcurrent, and (3) the addition of wind and/or wave forcing parameterizations. The latter has also been evaluated by strong ocean current, high wind speed and Stokes drift regimes.\n  Firstly, using the same time stepping scheme and linear interpolation methods, the different Lagrangian simulators OpenDrift and Parcels, performed identically. Secondly, the Globcurrent product showed the highest mean skill of the three ocean current products, although it underestimated the speed for strong ocean currents due to its spatial resolution. The HYCOM and Mercator model simulations showed, respectively, 40\\% and 15\\% lower skill than the Globcurrent simulations. Finally, the addition of the Stokes drift and a wind drift factor (WDF), improved the Lagrangian simulation performance in skill and speed, especially in high wind (>10 m/s) and/or Stokes drift regimes (>0.15 m/s). The optimal WDF for the OpenMetBuoy-v2021 is found to be ~1.8\\% and ~2.3\\% for simulations including and excluding Stokes drift forcing respectively. To further improve the incorporation of Stokes drift and direct wind drag on the trajectory simulations, a more physically based solution is advised as there are still numerous wind and wave related processes that remain unresolved, like wave-current interactions and vertical shear.\n  To statistically strengthen the conclusions from this research, incorporating additional observed drifter trajectories would be highly favourable.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "High Frequency Radar Observing System Simulation Experiment in the Western Mediterranean Sea: a Lagrangian assessment approach",
    "url": "http://arxiv.org/abs/2406.03579v2",
    "authors": [
      "Jaime Hernandez Lasheras",
      "Alejandro Orfila",
      "Alex Santana",
      "Ismael Hernandez Carrasco",
      "Baptiste Mourre"
    ],
    "published": "2024-06-05",
    "abstract": "The impact of the expansion of a high-frequency radar (HFR) system in a dynamic coastal area (the Ibiza Channel in the Western Mediterranean Sea) is evaluated through an Observing System Simulation Experiment (OSSE). The installation of two new antennas in the Iberian Peninsula would complement the existing ones in the islands of Ibiza and Formentera, providing surface currents observations of the full channel. Two different configurations of the same model, validated to give realistic simulations, are used: i) a Nature Run (NR) which is considered as the real ocean state and that is used to generate pseudo-observations, and ii) a Control Run (CR) in which the pseudo-observations are assimilated. The OSSE is first validated by comparison against a previous Observing System Experiment (OSE). The impact of the new antennas for forecasting surface currents is evaluated in two different periods with different levels of agreement between NR and CR. The HFR expansion is found to contribute to significantly correct the circulation patterns in the Channel, leading to surface merdional velocity error reductions up to 19%. The effects on the transport in the area are also analyzed from a Lagrangian perspective, showing that DA can help to better represent the Lagrangian Coherent Structures present in the NR and constrain the ocean dynamics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Synthetic RAW data generator for ESA HARMONY mission",
    "url": "http://arxiv.org/abs/2405.09938v1",
    "authors": [
      "Goulven Monnier",
      "Benjamin Camus",
      "Yann-Herv\u00e9 Hellouvry",
      "Pierre Dubois",
      "Erik de Witte"
    ],
    "published": "2024-05-16",
    "abstract": "In this paper, we introduce HEEPS/MARE, the end-to-end simulator developed for the SAR oceanographic products of ESA Earth Explorer 10 mission, Harmony, expected to launch in Decembre 2029. Harmony is primarily dedicated to the observation of small-scale motion and deformation fields of the Earth surface (oceans, glaciers and ice sheets, solid Earth), thanks to passive SAR/ATI receivers carried by two companion satellites for Sentinel-1. The paper focuses on the raw data generator designed to efficiently simulate large, heterogeneous, moving oceanic areas and produce the acquired SAR/ATI bistatic IQ signals. The heterogeneous sea-surface model, bistatic scattering model, multi-GPU implementation and achieved performance are emphasized. Finally, sample results are presented, to illustrate the ability of Harmony to map wind and surface current vectors at kilometric scale.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Scattering of surface waves by ocean currents: the U2H map",
    "url": "http://arxiv.org/abs/2402.05652v3",
    "authors": [
      "Han Wang",
      "Ana B. Villas B\u00f4as",
      "Jacques Vanneste",
      "William R. Young"
    ],
    "published": "2024-02-08",
    "abstract": "Ocean turbulence at meso- and submesocales affects the propagation of surface waves through refraction and scattering, inducing spatial modulations in significant wave height (SWH). We develop a theoretical framework that relates these modulations to the current that induces them. We exploit the asymptotic smallness of the ratio of typical current speed to wave group speed to derive a linear map -- the U2H map -- between surface current velocity and SWH anomaly. The U2H map is a convolution, non-local in space, expressible as a product in Fourier space by a factor independent of the magnitude of the wavenumber vector. Analytic expressions of the U2H map show how the SWH responds differently to the vortical and divergent parts of the current, and how the anisotropy of the wave spectrum is key to large current-induced SWH anomalies. We implement the U2H map numerically and test its predictions against WAVEWATCH III numerical simulations for both idealised and realistic current configurations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
    "url": "http://arxiv.org/abs/2406.02770v1",
    "authors": [
      "Kathrin Donandt",
      "Karim B\u00f6ttger",
      "Dirk S\u00f6ffker"
    ],
    "published": "2024-06-04",
    "abstract": "Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept",
    "url": "http://arxiv.org/abs/2510.23994v1",
    "authors": [
      "Geoffery Agorku",
      "Sarah Hernandez",
      "Hayley Hames",
      "Cade Wagner"
    ],
    "published": "2025-10-28",
    "abstract": "Accurate, real-time estimation of barge quantity on inland waterways remains a critical challenge due to the non-self-propelled nature of barges and the limitations of existing monitoring systems. This study introduces a novel method to use Automatic Identification System (AIS) vessel tracking data to predict the number of barges in tow using Machine Learning (ML). To train and test the model, barge instances were manually annotated from satellite scenes across the Lower Mississippi River. Labeled images were matched to AIS vessel tracks using a spatiotemporal matching procedure. A comprehensive set of 30 AIS-derived features capturing vessel geometry, dynamic movement, and trajectory patterns were created and evaluated using Recursive Feature Elimination (RFE) to identify the most predictive variables. Six regression models, including ensemble, kernel-based, and generalized linear approaches, were trained and evaluated. The Poisson Regressor model yielded the best performance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of the 30 features. The feature importance analysis revealed that metrics capturing vessel maneuverability such as course entropy, speed variability and trip length were most predictive of barge count. The proposed approach provides a scalable, readily implementable method for enhancing Maritime Domain Awareness (MDA), with strong potential applications in lock scheduling, port management, and freight planning. Future work will expand the proof of concept presented here to explore model transferability to other inland rivers with differing operational and environmental conditions.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Tracking"
    ]
  },
  {
    "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models",
    "url": "http://arxiv.org/abs/2509.13047v1",
    "authors": [
      "Nolan Platt",
      "Pragyansmita Nayak"
    ],
    "published": "2025-09-16",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventing over- fitting and ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves 75% accuracy on maritime tasks, while being substantially cheaper than using a larger model for inference. We show that smaller, cheaper models - when fine tuned properly - can provide similar accuracy compared to larger models that are prohibitively expensive. Our work contributes to the growing field of synthetic dataset generation for specialized AI applications and presents a highly reproducible framework for domains where manual annotation is infeasible. Beyond expand- ing research in the growing field of specialized small language models, our approach has immediate applications in maritime safety, security operations, and vessel traffic management systems in various industries.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Vessel Detection and Localization Using Distributed Acoustic Sensing in Submarine Optical Fiber Cables",
    "url": "http://arxiv.org/abs/2509.11614v3",
    "authors": [
      "Erick Eduardo Ramirez-Torres",
      "Javier Macias-Guarasa",
      "Daniel Pizarro-Perez",
      "Javier Tejedor",
      "Sira Elena Palazuelos-Cagigas",
      "Pedro J. Vidal-Moreno",
      "Sonia Martin-Lopez",
      "Miguel Gonzalez-Herraez",
      "Roel Vanthillo"
    ],
    "published": "2025-09-15",
    "abstract": "Submarine cables play a critical role in global internet connectivity, energy transmission, and communication but remain vulnerable to accidental damage and sabotage. Recent incidents in the Baltic Sea highlighted the need for enhanced monitoring to protect this vital infrastructure. Traditional vessel detection methods, such as synthetic aperture radar, video surveillance, and multispectral satellite imagery, face limitations in real-time processing, adverse weather conditions, and coverage range. This paper explores Distributed Acoustic Sensing (DAS) as an alternative by repurposing submarine telecommunication cables as large-scale acoustic sensor arrays. DAS offers continuous real-time monitoring, operates independently of cooperative systems like the \"Automatic Identification System\" (AIS), being largely unaffected by lighting or weather conditions. However, existing research on DAS for vessel tracking is limited in scale and lacks validation under real-world conditions. To address these gaps, a general and systematic methodology is presented for vessel detection and distance estimation using DAS. Advanced machine learning models are applied to improve detection and localization accuracy in dynamic maritime environments. The approach is evaluated over a continuous ten-day period, covering diverse ship and operational conditions, representing one of the largest-scale DAS-based vessel monitoring studies to date, and for which we release the full evaluation dataset. Results demonstrate DAS as a practical tool for maritime surveillance, with an overall F1-score of over 90% in vessel detection, and a mean average error of 141 m for vessel distance estimation, bridging the gap between experimental research and real-world deployment.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction",
    "url": "http://arxiv.org/abs/2503.21834v1",
    "authors": [
      "Haomin Yu",
      "Tianyi Li",
      "Kristian Torp",
      "Christian S. Jensen"
    ],
    "published": "2025-03-27",
    "abstract": "Accurate vessel trajectory prediction facilitates improved navigational safety, routing, and environmental protection. However, existing prediction methods are challenged by the irregular sampling time intervals of the vessel tracking data from the global AIS system and the complexity of vessel movement. These aspects render model learning and generalization difficult. To address these challenges and improve vessel trajectory prediction, we propose the multi-modal knowledge-enhanced framework (MAKER) for vessel trajectory prediction. To contend better with the irregular sampling time intervals, MAKER features a Large language model-guided Knowledge Transfer (LKT) module that leverages pre-trained language models to transfer trajectory-specific contextual knowledge effectively. To enhance the ability to learn complex trajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning (KSL) module. This module employs kinematic knowledge to progressively integrate complex patterns during training, allowing for adaptive learning and enhanced generalization. Experimental results on two vessel trajectory datasets show that MAKER can improve the prediction accuracy of state-of-the-art methods by 12.08%-17.86%.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "Predicting Barge Presence and Quantity on Inland Waterways using Vessel Tracking Data: A Machine Learning Approach",
    "url": "http://arxiv.org/abs/2501.00615v2",
    "authors": [
      "Geoffery Agorku",
      "Sarah Hernandez",
      "Maria Falquez",
      "Subhadipto Poddar",
      "Shihao Pang"
    ],
    "published": "2024-12-31",
    "abstract": "This study presents a machine learning approach to predict the number of barges transported by vessels on inland waterways using tracking data from the Automatic Identification System (AIS). While AIS tracks the location of tug and tow vessels, it does not monitor the presence or number of barges transported by those vessels. Understanding the number and types of barges conveyed along river segments, between ports, and at ports is crucial for estimating the quantities of freight transported on the nation's waterways. This insight is also valuable for waterway management and infrastructure operations impacting areas such as targeted dredging operations, and data-driven resource allocation. Labeled sample data was generated using observations from traffic cameras located along key river segments and matched to AIS data records. A sample of 164 vessels representing up to 42 barge convoys per vessel was used for model development. The methodology involved first predicting barge presence and then predicting barge quantity. Features derived from the AIS data included speed measures, vessel characteristics, turning measures, and interaction terms. For predicting barge presence, the AdaBoost model achieved an F1 score of 0.932. For predicting barge quantity, the Random Forest combined with an AdaBoost ensemble model achieved an F1 score of 0.886. Bayesian optimization was used for hyperparameter tuning. By advancing predictive modeling for inland waterways, this study offers valuable insights for transportation planners and organizations, which require detailed knowledge of traffic volumes, including the flow of commodities, their destinations, and the tonnage moving in and out of ports.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "DisBeaNet: A Deep Neural Network to augment Unmanned Surface Vessels for maritime situational awareness",
    "url": "http://arxiv.org/abs/2405.06149v2",
    "authors": [
      "Srikanth Vemula",
      "Eulises Franco",
      "Michael Frye"
    ],
    "published": "2024-05-10",
    "abstract": "Intelligent detection and tracking of the vessels on the sea play a significant role in conducting traffic avoidance in unmanned surface vessels(USV). Current traffic avoidance software relies mainly on Automated Identification System (AIS) and radar to track other vessels to avoid collisions and acts as a typical perception system to detect targets. However, in a contested environment, emitting radar energy also presents the vulnerability to detection by adversaries. Deactivating these Radiofrequency transmitting sources will increase the threat of detection and degrade the USV's ability to monitor shipping traffic in the vicinity. Therefore, an intelligent visual perception system based on an onboard camera with passive sensing capabilities that aims to assist USV in addressing this problem is presented in this paper. This paper will present a novel low-cost vision perception system for detecting and tracking vessels in the maritime environment. This novel low-cost vision perception system is introduced using the deep learning framework. A neural network, DisBeaNet, can detect vessels, track, and estimate the vessel's distance and bearing from the monocular camera. The outputs obtained from this neural network are used to determine the latitude and longitude of the identified vessel.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent and Independent Data under Preferential Sampling",
    "url": "http://arxiv.org/abs/2509.09336v1",
    "authors": [
      "Daniela Silva",
      "Raquel Menezes",
      "Gon\u00e7alo Ara\u00fajo",
      "Ana Machado",
      "Renato Rosa",
      "Ana Moreno",
      "Alexandra Silva",
      "Susana Garrido"
    ],
    "published": "2025-09-11",
    "abstract": "Sustainable management of marine ecosystems is vital for maintaining healthy fishery resources, and benefits from advanced scientific tools to accurately assess species distribution patterns. In fisheries science, two primary data sources are used: fishery-independent data (FID), collected through systematic surveys, and fishery-dependent data (FDD), obtained from commercial fishing activities. While these sources provide complementary information, their distinct sampling schemes - systematic for FID and preferential for FDD - pose significant integration challenges. This study introduces a novel spatio-temporal model that integrates FID and FDD, addressing challenges associated with zero-inflation and preferential sampling (PS) common in ecological data. The model employs a six-layer structure to differentiate between presence-absence and biomass observations, offering a robust framework for ecological studies affected by PS biases. Simulation results demonstrate the model's accuracy in parameter estimation across diverse PS scenarios and its ability to detect preferential signals. Application to the study of the distribution patterns of the European sardine populations along the southern Portuguese continental shelf illustrates the model's effectiveness in integrating diverse data sources and incorporating environmental and vessel-specific covariates. The model reveals spatio-temporal variability in sardine presence and biomass, providing actionable insights for fisheries management. Beyond ecology, this framework offers broad applicability to data integration challenges in other disciplines.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks",
    "url": "http://arxiv.org/abs/2509.18109v1",
    "authors": [
      "Jonatan Katz Nielsen"
    ],
    "published": "2025-09-09",
    "abstract": "Accurate recognition of vessel types from Automatic Identification System (AIS) tracks is essential for safety oversight and combating illegal, unreported, and unregulated (IUU) activity. This paper presents a strait-scale, machine-learning pipeline that classifies moving vessels using only AIS data. We analyze eight days of historical AIS from the Danish Maritime Authority covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After forward/backward filling voyage records, removing kinematic and geospatial outliers, and segmenting per-MMSI tracks while excluding stationary periods ($\\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g., SOG statistics), temporal, geospatial (Haversine distances, spans), and ship-shape attributes computed from AIS A/B/C/D reference points (length, width, aspect ratio, bridge-position ratio). To avoid leakage, we perform grouped train/test splits by MMSI and use stratified 5-fold cross-validation. Across five classes (cargo, tanker, passenger, high-speed craft, fishing; N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall 92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the bridge-position ratio and maximum SOG as the most discriminative signals; principal errors occur between cargo and tanker, reflecting similar transit behavior. We demonstrate operational value by backfilling missing ship types on unseen data and discuss improvements such as DBSCAN based trip segmentation and gradient-boosted ensembles to handle frequent-stop ferries and further lift performance. The results show that lightweight features over AIS trajectories enable real-time vessel type classification in straits.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "Integrating Activity Predictions in Knowledge Graphs",
    "url": "http://arxiv.org/abs/2507.19733v3",
    "authors": [
      "Forrest Hare",
      "Alec Sculley",
      "Cameron Stockton"
    ],
    "published": "2025-07-26",
    "abstract": "We argue that ontology-structured knowledge graphs can play a crucial role in generating predictions about future events. By leveraging the semantic framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies (CCO), we demonstrate how data such as the movements of a fishing vessel can be organized in and retrieved from a knowledge graph. These query results are then used to create Markov chain models, allowing us to predict future states based on the vessel's history. To fully support this process, we introduce the term `spatiotemporal instant' to complete the necessary structural semantics. Additionally, we critique the prevailing ontological model of probability, according to which probabilities are about the future. We propose an alternative view, where at least some probabilities are treated as being about actual process profiles, which better captures the dynamics of real-world phenomena. Finally, we demonstrate how our Markov chain-based probability calculations can be seamlessly integrated back into the knowledge graph, enabling further analysis and decision-making.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Modeling Maritime Transportation Behavior Using AIS Trajectories and Markovian Processes in the Gulf of St. Lawrence",
    "url": "http://arxiv.org/abs/2506.00025v3",
    "authors": [
      "Gabriel Spadon",
      "Ruixin Song",
      "Vaishnav Vaidheeswaran",
      "Md Mahbub Alam",
      "Floris Goerlandt",
      "Ronald Pelot"
    ],
    "published": "2025-05-22",
    "abstract": "Maritime transportation is central to the global economy, and analyzing its large-scale behavioral data is critical for operational planning, environmental stewardship, and governance. This work presents a spatio-temporal analytical framework based on discrete-time Markov chains to model vessel movement patterns in the Gulf of St. Lawrence, with particular emphasis on disruptions induced by the COVID-19 pandemic. We discretize the maritime domain into hexagonal cells and construct mobility signatures for distinct vessel types using cell transition frequencies and dwell times. These features are used to build origin-destination matrices and spatial transition probability models that characterize maritime dynamics across multiple temporal resolutions. Focusing on commercial, fishing, and passenger vessels, we analyze the temporal evolution of mobility behaviors during the pandemic, highlighting significant yet transient disruptions to recurring transport patterns. The methodology we contribute to this paper allows for an extensive behavioral analytics key for transportation planning. Accordingly, our findings reveal vessel-specific mobility signatures that persist across spatially disjoint regions, suggesting behaviors invariant to time. In contrast, we observe temporal deviations among passenger and fishing vessels during the pandemic, reflecting the influence of social isolation measures and operational constraints on non-essential maritime transport in this region.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Spatio-temporal characterisation of underwater noise through semantic trajectories",
    "url": "http://arxiv.org/abs/2501.11131v1",
    "authors": [
      "Giulia Rovinelli",
      "Davide Rocchesso",
      "Marta Simeoni",
      "Esteban Zim\u00e1nyi",
      "Alessandra Raffaet\u00e0"
    ],
    "published": "2025-01-19",
    "abstract": "Underwater noise pollution from human activities, particularly shipping, has been recognised as a serious threat to marine life. The sound generated by vessels can have various adverse effects on fish and aquatic ecosystems in general. In this setting, the estimation and analysis of the underwater noise produced by vessels is an important challenge for the preservation of the marine environment. In this paper we propose a model for the spatio-temporal characterisation of the underwater noise generated by vessels. The approach is based on the reconstruction of the vessels' trajectories from Automatic Identification System (AIS) data and on their deployment in a spatio-temporal database. Trajectories are enriched with semantic information like the acoustic characteristics of the vessels' engines or the activity performed by the vessels. We define a model for underwater noise propagation and use the trajectories' information to infer how noise propagates in the area of interest. We develop our approach for the case study of the fishery activities in the Northern Adriatic sea, an area of the Mediterranean sea which is well known to be highly exploited. We implement our approach using MobilityDB, an open source geospatial trajectory data management and analysis platform, which offers spatio-temporal operators and indexes improving the efficiency of our system. We use this platform to conduct various analyses of the underwater noise generated in the Northern Adriatic Sea, aiming at estimating the impact of fishing activities on underwater noise pollution and at demonstrating the flexibility and expressiveness of our approach.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Architecture for Trajectory-Based Fishing Ship Classification with AIS Data",
    "url": "http://arxiv.org/abs/2501.02038v1",
    "authors": [
      "David S\u00e1nchez Pedroche",
      "Daniel Amigo",
      "Jes\u00fas Garc\u00eda",
      "Jose M. Molina"
    ],
    "published": "2025-01-03",
    "abstract": "This paper proposes a data preparation process for managing real-world kinematic data and detecting fishing vessels. The solution is a binary classification that classifies ship trajectories into either fishing or non-fishing ships. The data used are characterized by the typical problems found in classic data mining applications using real-world data, such as noise and inconsistencies. The two classes are also clearly unbalanced in the data, a problem which is addressed using algorithms that resample the instances. For classification, a series of features are extracted from spatiotemporal data that represent the trajectories of the ships, available from sequences of Automatic Identification System (AIS) reports. These features are proposed for the modelling of ship behavior but, because they do not contain context-related information, the classification can be applied in other scenarios. Experimentation shows that the proposed data preparation process is useful for the presented classification problem. In addition, positive results are obtained using minimal information.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Composing Open-domain Vision with RAG for Ocean Monitoring and Conservation",
    "url": "http://arxiv.org/abs/2412.02262v1",
    "authors": [
      "Sepand Dyanatkar",
      "Angran Li",
      "Alexander Dungate"
    ],
    "published": "2024-12-03",
    "abstract": "Climate change's destruction of marine biodiversity is threatening communities and economies around the world which rely on healthy oceans for their livelihoods. The challenge of applying computer vision to niche, real-world domains such as ocean conservation lies in the dynamic and diverse environments where traditional top-down learning struggle with long-tailed distributions, generalization, and domain transfer. Scalable species identification for ocean monitoring is particularly difficult due to the need to adapt models to new environments and identify rare or unseen species. To overcome these limitations, we propose leveraging bottom-up, open-domain learning frameworks as a resilient, scalable solution for image and video analysis in marine applications. Our preliminary demonstration uses pretrained vision-language models (VLMs) combined with retrieval-augmented generation (RAG) as grounding, leaving the door open for numerous architectural, training and engineering optimizations. We validate this approach through a preliminary application in classifying fish from video onboard fishing vessels, demonstrating impressive emergent retrieval and prediction capabilities without domain-specific training or knowledge of the task itself.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Identifying companies and financial actors exposed to marine tipping points",
    "url": "http://arxiv.org/abs/2411.10307v2",
    "authors": [
      "Juan C. Rocha",
      "Jean-Baptiste Jouffray",
      "Frida Bengtsson",
      "Bianca-Ioana Voicu",
      "Paula A. S\u00e1nchez",
      "Victor Galaz"
    ],
    "published": "2024-11-15",
    "abstract": "Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Here we used satellite data to track fishing vessels operating in areas prone to marine regime shifts, as identified by their loss of resilience and vulnerability to marine heatwaves, and uncovered their corporate beneficial owners and shareholders. Despite some data gaps, we identified key countries, companies, and shareholders exposed to tipping risk. We also outline the potential challenges and opportunities that these actors may face if marine ecosystems shift to less productive states.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Vessel Re-identification and Activity Detection in Thermal Domain for Maritime Surveillance",
    "url": "http://arxiv.org/abs/2406.08294v1",
    "authors": [
      "Yasod Ginige",
      "Ransika Gunasekara",
      "Darsha Hewavitharana",
      "Manjula Ariyarathne",
      "Ranga Rodrigo",
      "Peshala Jayasekara"
    ],
    "published": "2024-06-12",
    "abstract": "Maritime surveillance is vital to mitigate illegal activities such as drug smuggling, illegal fishing, and human trafficking. Vision-based maritime surveillance is challenging mainly due to visibility issues at night, which results in failures in re-identifying vessels and detecting suspicious activities. In this paper, we introduce a thermal, vision-based approach for maritime surveillance with object tracking, vessel re-identification, and suspicious activity detection capabilities. For vessel re-identification, we propose a novel viewpoint-independent algorithm which compares features of the sides of the vessel separately (separate side-spaces) leveraging shape information in the absence of color features. We propose techniques to adapt tracking and activity detection algorithms for the thermal domain and train them using a thermal dataset we created. This dataset will be the first publicly available benchmark dataset for thermal maritime surveillance. Our system is capable of re-identifying vessels with an 81.8% Top1 score and identifying suspicious activities with a 72.4\\% frame mAP score; a new benchmark for each task in the thermal domain.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General Optimization Problems",
    "url": "http://arxiv.org/abs/2405.02850v7",
    "authors": [
      "Jiawen Li",
      "Anwar PP Abdul Majeed",
      "Pascal Lefevre"
    ],
    "published": "2024-05-05",
    "abstract": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a quantum-inspired metaheuristic designed to address general optimization problems. The HEO mimics the effects between quantum such as tunneling, entanglement. After the introduction to the HEO mechansims, the study presents a comprehensive evaluation of HEO's performance against extensively-used optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary analysis encompasses 14 benchmark functions with dimension 30, demonstrating HEO's effectiveness and adaptability in navigating general optimization problems. The test of HEO in Pressure Vessel Design and Tubular Column Design also infers its feasibility and potential in real-time applications. Further validation of HEO in Osmancik-97 and Cammeo Rice Classification achieves a higher accuracy record.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "FAD-SAR: A Novel Fishing Activity Detection System via Synthetic Aperture Radar Images Based on Deep Learning Method",
    "url": "http://arxiv.org/abs/2404.18245v2",
    "authors": [
      "Yanbing Bai",
      "Siao Li",
      "Rui-Yang Ju",
      "Zihao Yang",
      "Jinze Yu",
      "Jen-Shiun Chiang"
    ],
    "published": "2024-04-28",
    "abstract": "Illegal, unreported, and unregulated (IUU) fishing activities seriously affect various aspects of human life. However, traditional methods for detecting and monitoring IUU fishing activities at sea have limitations. Although synthetic aperture radar (SAR) can complement existing vessel detection systems, extracting useful information from SAR images using traditional methods remains a challenge, especially in IUU fishing. This paper proposes a deep learning based fishing activity detection system, which is implemented on the xView3 dataset using six classical object detection models: SSD, RetinaNet, FSAF, FCOS, Faster R-CNN, and Cascade R-CNN. In addition, this work employs different enhancement techniques to improve the performance of the Faster R-CNN model. The experimental results demonstrate that training the Faster R-CNN model using the Online Hard Example Mining (OHEM) strategy increases the Avg-F1 value from 0.212 to 0.216.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Shipping traffic through the Arctic Ocean: spatial distribution, temporal evolution and its dependence on the sea ice extent",
    "url": "http://arxiv.org/abs/2403.01856v1",
    "authors": [
      "Jorge P. Rodr\u00edguez",
      "Konstantin Klemm",
      "Carlos M. Duarte",
      "V\u00edctor M. Egu\u00edluz"
    ],
    "published": "2024-03-04",
    "abstract": "The reduction in sea ice cover with Arctic warming facilitates the transit of ships through routes that are remarkably shorter than the traditional shipping routes. Automatic Identification System (AIS), ideally designed to avoid vessel collisions, transmits on vessel navigation information (currently 27 types of messages) such as name, position or speed, is a powerful data source to monitor the progress of Arctic shipping as the ice cover decreases. Based on the analysis of an online platform collecting shipping AIS data, we quantified the spatial distribution of shipping through the Arctic Ocean, its intensity and the temporal evolution, in relation to the area released by the sea ice area. Shipping through the Arctic Ocean is distributed spatially following a heavy-tailed distribution, implying heavy traffic through a limited Arctic area, with an exponent that depends on the vessel category. Fishing is the category with the largest spatial spread, with the width of shipping routes correlated with the proximal sea ice area. The time evolution of these routes is characterized by increasing extended periods of shipping activity through the year. AIS data offers valuable information on the activity of the international fleet worldwide. In the context of the new international agreements, it is a valuable source to monitor shipping, fishing and the potential impact in marine life among other aspects. Here we have focused on the Arctic shipping in recent years, which is rapidly growing, particularly around the Northeastern and Northwest Passage coastal routes, providing an opportunity for the design of shorter shipping routes and reduced greenhouse gas emissions from transport of goods, but at a risk of impacts on the Arctic ecosystem.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Prediction of the Economic Behavior of Fishery Biotechnology Companies Based on Machine Learning-Based Deep Metacellular Automata",
    "url": "http://arxiv.org/abs/2402.13509v2",
    "authors": [
      "Liguo Chen",
      "Hongyang Hua",
      "Xinyue Luo",
      "Guoli Xu",
      "Xu Yan"
    ],
    "published": "2024-02-21",
    "abstract": "Ocean warming significantly affects the fishing industry, with species like Scottish herring and mackerel migrating northwards. Our research, a fusion of artificial intelligence, data science, and operations research, addresses this crisis. Using Long Short Term Memory networks, we forecast sea surface temperatures (SST) and model fish migratory patterns with Enhanced Cellular Automata. A corrective factor within our model adjusts for human impact on SST, guiding diverse mitigation scenarios. We apply operational research to strategize responses, including the modernization of fishing vessels as a less costly alternative to relocation. Our data-driven approach, suggesting fleet modernization, strategic relocation, and product diversification, offers an effective approach to mitigating the threats to the ocean warming phenomenon.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Experimental investigations of underwater and airborne noises produced by a large hovercraft in Ural River estuary",
    "url": "http://arxiv.org/abs/2401.14204v1",
    "authors": [
      "A. I. Vedenev",
      "O. Yu. Kochetov",
      "A. A. Lunkov",
      "A. S. Shurup",
      "S. S. Kassymbekova"
    ],
    "published": "2024-01-25",
    "abstract": "Simultaneous measurements of underwater and airborne noises produced by Griffon Hoverwork BHT130 hovercraft were carried out in environmentally sensitive area - wildlife preserve in the area of the Ural River estuary near the Caspian Sea shelf. Measurements were organized to assess the possible negative impact of noise from hovercraft on fish and birds in wildlife preserve. The particle velocity of underwater noise was estimated by using a gradient-type vector receiver. That was a distinctive aspect of the underwater noise studies since the majority of fish perceives the sound in terms of vibration of particles, and only a few as the pressure. Using synchronous recording of underwater and airborne noises, the mutual correlation of these data was investigated. The obtained correlation levels between underwater and airborne noises produced by hovercraft can be used for simplified estimation of the upper boundary of underwater noise level by measuring levels of airborne noise. The measured and estimated maximal levels of underwater noises of hovercraft are considerably lower than noises from conventional vessels with underwater engines, that makes hovercraft attractive alternative for use in locations with high underwater noise requirements, such as Ural River estuary and Caspian Sea shelf.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case",
    "url": "http://arxiv.org/abs/2401.02278v1",
    "authors": [
      "Febrian Kurniawan",
      "Gandeva Bayu Satrya",
      "Firuz Kamalov"
    ],
    "published": "2024-01-04",
    "abstract": "The enormous demand for seafood products has led to exploitation of marine resources and near-extinction of some species. In particular, overfishing is one the main issues in sustainable marine development. In alignment with the protection of marine resources and sustainable fishing, this study proposes to advance fish classification techniques that support identifying protected fish species using state-of-the-art machine learning. We use a custom modification of the MobileNet model to design a lightweight classifier called M-MobileNet that is capable of running on limited hardware. As part of the study, we compiled a labeled dataset of 37,462 images of fish found in the waters of the Indonesian archipelago. The proposed model is trained on the dataset to classify images of the captured fish into their species and give recommendations on whether they are consumable or not. Our modified MobileNet model uses only 50\\% of the top layer parameters with about 42% GTX 860M utility and achieves up to 97% accuracy in fish classification and determining its consumability. Given the limited computing capacity available on many fishing vessels, the proposed model provides a practical solution to on-site fish classification. In addition, synchronized implementation of the proposed model on multiple vessels can supply valuable information about the movement and location of different species of fish.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery",
    "url": "http://arxiv.org/abs/2511.10387v3",
    "authors": [
      "Prince Mensah",
      "Pelumi Victor Aderinto",
      "Ibrahim Salihu Yusuf",
      "Arnu Pretorius"
    ],
    "published": "2025-11-13",
    "abstract": "Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder",
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Chlorophyll-a Mapping and Prediction in the Mar Menor Lagoon Using C2RCC-Processed Sentinel 2 Imagery",
    "url": "http://arxiv.org/abs/2510.09736v1",
    "authors": [
      "Antonio Mart\u00ednez-Ibarra",
      "Aurora Gonz\u00e1lez-Vidal",
      "Adri\u00e1n C\u00e1novas-Rodr\u00edguez",
      "Antonio F. Skarmeta"
    ],
    "published": "2025-10-10",
    "abstract": "The Mar Menor, Europe's largest coastal lagoon, located in Spain, has undergone severe eutrophication crises. Monitoring chlorophyll-a (Chl-a) is essential to anticipate harmful algal blooms and guide mitigation. Traditional in situ measurements are spatially and temporally limited. Satellite-based approaches provide a more comprehensive view, enabling scalable, long-term, and transferable monitoring. This study aims to overcome limitations of chlorophyll monitoring, often restricted to surface estimates or limited temporal coverage, by developing a reliable methodology to predict and map Chl-a across the water column of the Mar Menor. The work integrates Sentinel 2 imagery with buoy-based ground truth to create models capable of high-resolution, depth-specific monitoring, enhancing early-warning capabilities for eutrophication. Nearly a decade of Sentinel 2 images was atmospherically corrected using C2RCC processors. Buoy data were aggregated by depth (0-1 m, 1-2 m, 2-3 m, 3-4 m). Multiple ML and DL algorithms-including RF, XGBoost, CatBoost, Multilater Perceptron Networks, and ensembles-were trained and validated using cross-validation. Systematic band-combination experiments and spatial aggregation strategies were tested to optimize prediction. Results show depth-dependent performance. At the surface, C2X-Complex with XGBoost and ensemble models achieved R2 = 0.89; at 1-2 m, CatBoost and ensemble models reached R2 = 0.87; at 2-3 m, TOA reflectances with KNN performed best (R2 = 0.81); while at 3-4 m, RF achieved R2 = 0.66. Generated maps successfully reproduced known eutrophication events (e.g., 2016 crisis, 2025 surge), confirming robustness. The study delivers an end-to-end, validated methodology for depth-specific Chl-amapping. Its integration of multispectral band combinations, buoy calibration, and ML/DL modeling offers a transferable framework for other turbid coastal systems.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Branched Broomrape Detection in Tomato Farms Using Satellite Imagery and Time-Series Analysis",
    "url": "http://arxiv.org/abs/2509.10804v1",
    "authors": [
      "Mohammadreza Narimani",
      "Alireza Pourreza",
      "Ali Moghimi",
      "Parastoo Farajpoor",
      "Hamid Jafarbiglu",
      "Mohsen Mesgaran"
    ],
    "published": "2025-09-13",
    "abstract": "Branched broomrape (Phelipanche ramosa (L.) Pomel) is a chlorophyll-deficient parasitic plant that threatens tomato production by extracting nutrients from the host, with reported yield losses up to 80 percent. Its mostly subterranean life cycle and prolific seed production (more than 200,000 seeds per plant, viable for up to 20 years) make early detection essential. We present an end-to-end pipeline that uses Sentinel-2 imagery and time-series analysis to identify broomrape-infested tomato fields in California. Regions of interest were defined from farmer-reported infestations, and images with less than 10 percent cloud cover were retained. We processed 12 spectral bands and sun-sensor geometry, computed 20 vegetation indices (e.g., NDVI, NDMI), and derived five plant traits (Leaf Area Index, Leaf Chlorophyll Content, Canopy Chlorophyll Content, Fraction of Absorbed Photosynthetically Active Radiation, and Fractional Vegetation Cover) using a neural network calibrated with ground-truth and synthetic data. Trends in Canopy Chlorophyll Content delineated transplanting-to-harvest periods, and phenology was aligned using growing degree days. Vegetation pixels were segmented and used to train a Long Short-Term Memory (LSTM) network on 18,874 pixels across 48 growing-degree-day time points. The model achieved 88 percent training accuracy and 87 percent test accuracy, with precision 0.86, recall 0.92, and F1 0.89. Permutation feature importance ranked NDMI, Canopy Chlorophyll Content, FAPAR, and a chlorophyll red-edge index as most informative, consistent with the physiological effects of infestation. Results show the promise of satellite-driven time-series modeling for scalable detection of parasitic stress in tomato farms.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions",
    "url": "http://arxiv.org/abs/2504.13476v1",
    "authors": [
      "Jiadong Lou",
      "Bingqing Liu",
      "Yuanheng Xiong",
      "Xiaodong Zhang",
      "Xu Yuan"
    ],
    "published": "2025-04-18",
    "abstract": "Phytoplankton absorb and scatter light in unique ways, subtly altering the color of water, changes that are often minor for human eyes to detect but can be captured by sensitive ocean color instruments onboard satellites from space. Hyperspectral sensors, paired with advanced algorithms, are expected to significantly enhance the characterization of phytoplankton community composition, especially in coastal waters where ocean color remote sensing applications have historically encountered significant challenges. This study presents novel machine learning-based solutions for NASA's hyperspectral missions, including EMIT and PACE, tackling high-fidelity retrievals of phytoplankton absorption coefficient and chlorophyll a from their hyperspectral remote sensing reflectance. Given that a single Rrs spectrum may correspond to varied combinations of inherent optical properties and associated concentrations, the Variational Autoencoder (VAE) is used as a backbone in this study to handle such multi-distribution prediction problems. We first time tailor the VAE model with innovative designs to achieve hyperspectral retrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex estuarine-coastal waters. Validation with extensive experimental observation demonstrates superior performance of the VAE models with high precision and low bias. The in-depth analysis of VAE's advanced model structures and learning designs highlights the improvement and advantages of VAE-based solutions over the mixture density network (MDN) approach, particularly on high-dimensional data, such as PACE. Our study provides strong evidence that current EMIT and PACE hyperspectral data as well as the upcoming Surface Biology Geology mission will open new pathways toward a better understanding of phytoplankton community dynamics in aquatic ecosystems when integrated with AI technologies.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Generalization performance of neural mapping schemes for the space-time interpolation of satellite-derived ocean colour datasets",
    "url": "http://arxiv.org/abs/2503.11588v1",
    "authors": [
      "Thi Thuy Nga Nguyen",
      "Cl\u00e9ment Dorffer",
      "Fr\u00e9d\u00e9ric Jourdin",
      "Ronan Fablet"
    ],
    "published": "2025-03-14",
    "abstract": "Neural mapping schemes have become appealing approaches to deliver gap-free satellite-derived products for sea surface tracers. The generalization performance of these learning-based approaches naturally arises as a key challenge. This is particularly true for satellite-derived ocean colour products given the variety of bio-optical variables of interest, as well as the diversity of processes and scales involved. Considering region-specific and parameter-specific neural mapping schemes will result in substantial training costs. This study addresses generalization performance of neural mapping schemes to deliver gap-free satellite-derived ocean colour products. We develop a comprehensive experimental framework using real multi-sensor ocean colour datasets for two regions (the Mediterranean Sea and the North Sea) and a representative set of bio-optical parameters (Chlorophyll-a concentration, suspended particulate matter concentration, particulate backscattering coefficient). We consider several neural mapping schemes, and we report excellent generalization performance across regions and bio-optical parameters without any fine-tuning using appropriate dataset-specific normalization procedures. We discuss further how these results provide new insights towards the large-scale deployment of neural schemes for the processing of satellite-derived ocean colour datasets beyond case-study-specific demonstrations.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Bayesian hierarchical framework for fusion of remote sensing data: An example with solar-induced fluorescence",
    "url": "http://arxiv.org/abs/2503.03901v1",
    "authors": [
      "Manju Johny",
      "Jonathan Hobbs",
      "Vineet Yadav",
      "Margaret Johnson",
      "Nicholas Parazoo",
      "Hai Nguyen",
      "Amy Braverman"
    ],
    "published": "2025-03-05",
    "abstract": "Solar-induced chlorophyll fluorescence (SIF) has emerged as an effective indicator of vegetation productivity and plant health. The global quantification of SIF and its associated uncertainties yields many important capabilities, including improving carbon flux estimation, improving the identification of carbon sources and sinks, monitoring a variety of ecosystems, and evaluating carbon sequestration efforts. Long-term, regional-to-global scale monitoring is now feasible with the availability of SIF estimates from multiple Earth-observing satellites. These efforts can be aided by a rigorous accounting of the sources of uncertainty present in satellite SIF data products. In this paper, we introduce a Bayesian Hierarchical Model (BHM) for the estimation of SIF and associated uncertainties from Orbiting Carbon Observatory-2 (OCO-2) satellite observations at one-degree resolution with global coverage. The hierarchical structure of our modeling framework allows for convenient model specification, quantification of various sources of variation, and the incorporation of seasonal SIF information through Fourier terms in the regression model. The modeling framework leverages the predictable seasonality of SIF in most temperate land areas. The resulting data product complements existing atmospheric carbon dioxide estimates at the same spatio-temporal resolution.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Improving Water Quality Time-Series Prediction in Hong Kong using Sentinel-2 MSI Data and Google Earth Engine Cloud Computing",
    "url": "http://arxiv.org/abs/2408.14010v2",
    "authors": [
      "Rohin Sood",
      "Kevin Zhu"
    ],
    "published": "2024-08-26",
    "abstract": "Effective water quality monitoring in coastal regions is crucial due to the progressive deterioration caused by pollution and human activities. To address this, this study develops time-series models to predict chlorophyll-a (Chl-a), suspended solids (SS), and turbidity using Sentinel-2 satellite data and Google Earth Engine (GEE) in the coastal regions of Hong Kong. Leveraging Long Short-Term Memory (LSTM) Recurrent Neural Networks, the study incorporates extensive temporal datasets to enhance prediction accuracy. The models utilize spectral data from Sentinel-2, focusing on optically active components, and demonstrate that selected variables closely align with the spectral characteristics of Chl-a and SS. The results indicate improved predictive performance over previous methods, highlighting the potential for remote sensing technology in continuous and comprehensive water quality assessment.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Advancing Applications of Satellite Photogrammetry: Novel Approaches for Built-up Area Modeling and Natural Environment Monitoring using Stereo/Multi-view Satellite Image-derived 3D Data",
    "url": "http://arxiv.org/abs/2404.12487v1",
    "authors": [
      "Shengxi Gui"
    ],
    "published": "2024-04-18",
    "abstract": "With the development of remote sensing technology in recent decades, spaceborne sensors with sub-meter and meter spatial resolution (Worldview and PlanetScope) have achieved a considerable image quality to generate 3D geospatial data via a stereo matching pipeline. These achievements have significantly increased the data accessibility in 3D, necessitating adapting these 3D geospatial data to analyze human and natural environments. This dissertation explores several novel approaches based on stereo and multi-view satellite image-derived 3D geospatial data, to deal with remote sensing application issues for built-up area modeling and natural environment monitoring, including building model 3D reconstruction, glacier dynamics tracking, and lake algae monitoring. Specifically, the dissertation introduces four parts of novel approaches that deal with the spatial and temporal challenges with satellite-derived 3D data. The first study advances LoD-2 building modeling from satellite-derived Orthophoto and DSMs with a novel approach employing a model-driven workflow that generates building rectangular 3D geometry models. Secondly, we further enhanced our building reconstruction framework for dense urban areas and non-rectangular purposes, we implemented deep learning for unit-level segmentation and introduced a gradient-based circle reconstruction for circular buildings to develop a polygon composition technique for advanced building LoD2 reconstruction. Our third study utilizes high-spatiotemporal resolution PlanetScope satellite imagery for glacier tracking at 3D level in mid-latitude regions. Finally, we proposed a term as \"Algal Behavior Function\" to refine the quantification of chlorophyll-a concentrations from satellite imagery in water quality monitoring, addressing algae fluctuations and timing discrepancies between satellite observations and field measurements, thus enhancing the precision of underwater algae volume estimates. Overall, this dissertation demonstrates the extensive potential of satellite photogrammetry applications in addressing urban and environmental challenges. It further showcases innovative analytical methodologies that enhance the applicability of adapting stereo and multi-view very high-resolution satellite-derived 3D data. (See full abstract in the document)",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Tracking"
    ]
  },
  {
    "title": "Seasonality of primary productivity affects coastal species more than its magnitude",
    "url": "http://arxiv.org/abs/2401.11289v1",
    "authors": [
      "Carlota Muniz",
      "Christopher McQuaid",
      "Nicolas Weidberg"
    ],
    "published": "2024-01-20",
    "abstract": "While the importance of extreme conditions is recognised, patterns in species abundances are often interpreted through average environmental conditions within their distributional range. For marine species with pelagic larvae, temperature and phytoplankton concentration are key variables. Along the south coast of South Africa, conspicuous spatial patterns in recruitment rates and the abundances of different mussel species exist, with focal areas characterized by large populations. We studied 15 years of sea surface temperature (SST) and chlorophyll-a (chl-a) satellite data, using spectral analyses to partition their temporal variability over ecologically relevant time periods, including seasonal (101 to 365 days) and intra-seasonal cycles (20 to 100 days). Adult cover and mussel recruitment were measured at 10 sites along the south coast and regression models showed that about 70 percent of the variability in recruitment and adult cover was explained by seasonal variability in chl-a, while mean annual chl-a and SST only explained 30 percent of the recruitment, with no significant effect for adult cover. SST and chl-a at two upwelling centres showed less predictable seasonal cycles during the second half of the study period with a significant cooling trend during austral autumn, coinciding with one of the mussel reproductive peaks. This likely reflects recent changes in the Agulhas Current, the world largest western boundary current, which affects coastal ecosystems by driving upwelling.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Fine scale depth regulation of invertebrate larvae around coastal fronts",
    "url": "http://arxiv.org/abs/2401.10303v1",
    "authors": [
      "Nicolas Weidberg",
      "Wayne Goschen",
      "Jennifer M. Jackson",
      "Paula Pattrick",
      "Christopher D. McQuaid",
      "Francesca Porri"
    ],
    "published": "2024-01-18",
    "abstract": "Vertical migrations of zooplankters have been widely described, but their active movements through shallow, highly dynamic water columns within the inner shelf may be more complex and difficult to characterize. In this study, invertebrate larvae, currents, and hydrographic variables were sampled at different depths during and after the presence of fronts on three different cruises off the southern coast of South Africa. Internal wave dynamics were observed in the hydrographic data set but also through satellite imagery, although strong surface convergent currents were absent and thermal stratification was weak. During the first two cruises, fronts were more conspicuous and they preceded strong onshore currents at depth which developed with the rising tide. Vertical distributions of larvae changed accordingly, with higher abundances at these deep layers once the front disappeared. The third cruise was carried out during slack tides, the front was not conspicuous, deep strong onshore currents did not occur afterward and larval distributions did not change consistently through time. Overall, the vertical distributions of many larval taxa matched the vertical profiles of shoreward currents and multivariate analyses revealed that these flows structured the larval community, which was neither influenced by temperature nor chlorophyll. Thus, the ability to regulate active vertical positioning may enhance shoreward advection and determine nearshore larval distributions.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "The Future Orchid Diversity of Great Britain and Ireland using an SDM Approach",
    "url": "http://arxiv.org/abs/2511.01122v1",
    "authors": [
      "Sofoklis Mouratidis",
      "Konstantinos Kougioumoutzis",
      "Martha Charitonidou",
      "John M. Halley"
    ],
    "published": "2025-11-03",
    "abstract": "In this paper we use Species Distribution Models (SDMs) to forecast the future diversity and distribution of orchids in Great Britain and Ireland under scenarios of climate and land-use change. The study analyzes occurrence data for native orchid taxa in the BSBI database at a fine spatial resolution (1 km^2, monads) and incorporates multiple environmental variables including climate, land use, topography, and soil. These SDMs project significant losses in orchid species richness by 2050 and 2070, especially under severe climate and land-use scenarios, with declines expected across most species and regions, including Ireland where historical data previously indicated gains. The models reveal vulnerable species likely to face extinction by 2070, emphasizing the impact of both climate warming and habitat modifications. This approach differs from previous trend-based analyses by integrating future projections, high-resolution spatial data, and dynamic land-use scenarios, thereby providing higher-resolution estimates of orchid range contractions and diversity losses. While current observed orchid trends show some regional increases, particularly in Ireland, the SDM forecasts indicate substantial future risks. The study also discusses uncertainties due to niche truncation from geographic data limits and highlights the need for broader-scale modeling for more robust predictions. Overall, the paper anticipates conservation challenges for orchid biodiversity in response to ongoing environmental changes.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models",
    "url": "http://arxiv.org/abs/2510.19749v2",
    "authors": [
      "Catherine Villeneuve",
      "Benjamin Akera",
      "M\u00e9lisande Teng",
      "David Rolnick"
    ],
    "published": "2025-10-22",
    "abstract": "Species distribution models (SDMs), which aim to predict species occurrence based on environmental variables, are widely used to monitor and respond to biodiversity change. Recent deep learning advances for SDMs have been shown to perform well on complex and heterogeneous datasets, but their effectiveness remains limited by spatial biases in the data. In this paper, we revisit deep SDMs from a Bayesian perspective and introduce BATIS, a novel and practical framework wherein prior predictions are updated iteratively using limited observational data. Models must appropriately capture both aleatoric and epistemic uncertainty to effectively combine fine-grained local insights with broader ecological patterns. We benchmark an extensive set of uncertainty quantification approaches on a novel dataset including citizen science observations from the eBird platform. Our empirical study shows how Bayesian deep learning approaches can greatly improve the reliability of SDMs in data-scarce locations, which can contribute to ecological understanding and conservation efforts.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation",
    "url": "http://arxiv.org/abs/2510.19305v1",
    "authors": [
      "Chirag Padubidri",
      "Pranesh Velmurugan",
      "Andreas Lanitis",
      "Andreas Kamilaris"
    ],
    "published": "2025-10-22",
    "abstract": "Monitoring species distribution is vital for conservation efforts, enabling the assessment of environmental impacts and the development of effective preservation strategies. Traditional data collection methods, including citizen science, offer valuable insights but remain limited in coverage and completeness. Species Distribution Modelling (SDM) helps address these gaps by using occurrence data and environmental variables to predict species presence across large regions. In this study, we enhance SDM accuracy for frogs (Anura) by applying deep learning and data imputation techniques using data from the \"EY - 2022 Biodiversity Challenge.\" Our experiments show that data balancing significantly improved model performance, reducing the Mean Absolute Error (MAE) from 189 to 29 in frog counting tasks. Feature selection identified key environmental factors influencing occurrence, optimizing inputs while maintaining predictive accuracy. The multimodal ensemble model, integrating land cover, NDVI, and other environmental inputs, outperformed individual models and showed robust generalization across unseen regions. The fusion of image and tabular data improved both frog counting and habitat classification, achieving 84.9% accuracy with an AUC of 0.90. This study highlights the potential of multimodal learning and data preprocessing techniques such as balancing and imputation to improve predictive ecological modeling when data are sparse or incomplete, contributing to more precise and scalable biodiversity monitoring.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory",
    "url": "http://arxiv.org/abs/2510.16676v1",
    "authors": [
      "Anindya Sarkar",
      "Binglin Ji",
      "Yevgeniy Vorobeychik"
    ],
    "published": "2025-10-19",
    "abstract": "In many scientific and engineering fields, where acquiring high-quality data is expensive--such as medical imaging, environmental monitoring, and remote sensing--strategic sampling of unobserved regions based on prior observations is crucial for maximizing discovery rates within a constrained budget. The rise of powerful generative models, such as diffusion models, has enabled active target discovery in partially observable environments by leveraging learned priors--probabilistic representations that capture underlying structure from data. With guidance from sequentially gathered task-specific observations, these models can progressively refine exploration and efficiently direct queries toward promising regions. However, in domains where learning a strong prior is infeasible due to extremely limited data or high sampling cost (such as rare species discovery, diagnostics for emerging diseases, etc.), these methods struggle to generalize. To overcome this limitation, we propose a novel approach that enables effective active target discovery even in settings with uninformative priors, ensuring robust exploration and adaptability in complex real-world scenarios. Our framework is theoretically principled and draws inspiration from neuroscience to guide its design. Unlike black-box policies, our approach is inherently interpretable, providing clear insights into decision-making. Furthermore, it guarantees a strong, monotonic improvement in prior estimates with each new observation, leading to increasingly accurate sampling and reinforcing both reliability and adaptability in dynamic settings. Through comprehensive experiments and ablation studies across various domains, including species distribution modeling and remote sensing, we demonstrate that our method substantially outperforms baseline approaches.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Diffusion Models",
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "A Single Index Approach to Integrated Species Distribution Modeling for Fisheries Abundance Data",
    "url": "http://arxiv.org/abs/2509.15379v1",
    "authors": [
      "Quan Vu",
      "Francis K. C. Hui",
      "A. H. Welsh",
      "Samuel Muller",
      "Eva Cantoni",
      "Christopher R. Haak"
    ],
    "published": "2025-09-18",
    "abstract": "In fisheries ecology, species abundance data are often collected by multiple surveys, each with unique characteristics. This article focuses on Atlantic sea scallop abundance data along the northeast coast of the United States, collected from two bottom trawl surveys which cover a larger spatial domain but have low catch efficiency, and a dredge survey which is more efficient but limited to domains where the species are believed to be present. To model such data, integrated species distribution models (ISDMs) have been proposed to incorporate information from multiple surveys, by including common environmental effects along with correlated survey-specific spatial fields. However, while flexible, these ISDMs can be susceptible to overfitting, which can complicate interpretability of the shared environmental effects and potentially lead to poor predictive performance. To overcome these drawbacks, we introduce a novel single index ISDM, built from a single index (with spatial random effects) that represents a latent measure of the true species distribution, and survey-specific catch efficiency functions which map the single index to the survey-specific expected catch. Our results show that the single index ISDM offers more meaningful interpretations of the environmental effects and survey catch efficiency differences, while potentially achieving better predictive performance than existing ISDMs.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Modelling species distributions using remote sensing predictors: Comparing Dynamic Habitat Index and LULC",
    "url": "http://arxiv.org/abs/2509.14862v1",
    "authors": [
      "Ma\u00efri Souza Oliveira",
      "Cl\u00e9mentine Pr\u00e9au",
      "Samuel Alleaume",
      "Maxime Lenormand",
      "Sandra Luque"
    ],
    "published": "2025-09-18",
    "abstract": "This study compares the predictive capacity of the Dynamic Habitat Index (DHI) - a remote sensing (RS)-based measure of habitat productivity and variability - against traditional land-use/land-cover (LULC) metrics in species distribution modelling (SDM) applications. RS and LULC-based SDMs were built using distribution data for eleven bird, amphibian, and mammal species in \u00cele-de-France. Predictor variables were derived from Sentinel-2 RS data and LULC classifications, with the latter incorporating Euclidean distance to habitat types. Ensemble SDMs were built using nine algorithms and evaluated with the Continuous Boyce Index (CBI) and a calibrated AUC. Habitat suitability scores and their binary transformations were assessed using niche overlap indices (Schoener, Warren, and Spearman rank correlation coefficient). Both RS and LULC approaches exhibited similar predictive accuracy overall. After binarisation however, the resulting niche maps diverged significantly. While LULC-based models exhibited spatial constraints (habitat suitability decreased as distance from recorded occurrences increased), RS-based models, which used continuous data, were not affected by geographic bias or distance effects. These results underscore the need to account for spatial biases in LULC-based SDMs. The DHI may offer a more spatially neutral alternative, making it a promising predictor for modelling species niches at regional scales.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Investigating Different Geo Priors for Image Classification",
    "url": "http://arxiv.org/abs/2508.15946v1",
    "authors": [
      "Angela Zhu",
      "Christian Lange",
      "Max Hamilton"
    ],
    "published": "2025-08-21",
    "abstract": "Species distribution models encode spatial patterns of species occurrence making them effective priors for vision-based species classification when location information is available. In this study, we evaluate various SINR (Spatial Implicit Neural Representations) models as a geographical prior for visual classification of species from iNaturalist observations. We explore the impact of different model configurations and adjust how we handle predictions for species not included in Geo Prior training. Our analysis reveals factors that contribute to the effectiveness of these models as Geo Priors, factors that may differ from making accurate range maps.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations",
    "url": "http://arxiv.org/abs/2508.06704v1",
    "authors": [
      "Hager Radi Abdelwahed",
      "M\u00e9lisande Teng",
      "Robin Zbinden",
      "Laura Pollock",
      "Hugo Larochelle",
      "Devis Tuia",
      "David Rolnick"
    ],
    "published": "2025-08-08",
    "abstract": "Species distribution models (SDMs) are widely used to predict species' geographic distributions, serving as critical tools for ecological research and conservation planning. Typically, SDMs relate species occurrences to environmental variables representing abiotic factors, such as temperature, precipitation, and soil properties. However, species distributions are also strongly influenced by biotic interactions with other species, which are often overlooked. While some methods partially address this limitation by incorporating biotic interactions, they often assume symmetrical pairwise relationships between species and require consistent co-occurrence data. In practice, species observations are sparse, and the availability of information about the presence or absence of other species varies significantly across locations. To address these challenges, we propose CISO, a deep learning-based method for species distribution modeling Conditioned on Incomplete Species Observations. CISO enables predictions to be conditioned on a flexible number of species observations alongside environmental variables, accommodating the variability and incompleteness of available biotic data. We demonstrate our approach using three datasets representing different species groups: sPlotOpen for plants, SatBird for birds, and a new dataset, SatButterfly, for butterflies. Our results show that including partial biotic information improves predictive performance on spatially separate test sets. When conditioned on a subset of species within the same dataset, CISO outperforms alternative methods in predicting the distribution of the remaining species. Furthermore, we show that combining observations from multiple datasets can improve performance. CISO is a promising ecological tool, capable of incorporating incomplete biotic information and identifying potential interactions between species from disparate taxa.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Uncovering symmetric and asymmetric species associations from community and environmental data",
    "url": "http://arxiv.org/abs/2507.09317v1",
    "authors": [
      "Sara Si-Moussi",
      "Esther Galbrun",
      "Mickael Hedde",
      "Giovanni Poggiato",
      "Matthias Rohr",
      "Wilfried Thuiller"
    ],
    "published": "2025-07-12",
    "abstract": "There is no much doubt that biotic interactions shape community assembly and ultimately the spatial co-variations between species. There is a hope that the signal of these biotic interactions can be observed and retrieved by investigating the spatial associations between species while accounting for the direct effects of the environment. By definition, biotic interactions can be both symmetric and asymmetric. Yet, most models that attempt to retrieve species associations from co-occurrence or co-abundance data internally assume symmetric relationships between species. Here, we propose and validate a machine-learning framework able to retrieve bidirectional associations by analyzing species community and environmental data.\n  Our framework (1) models pairwise species associations as directed influences from a source to a target species, parameterized with two species-specific latent embeddings: the effect of the source species on the community, and the response of the target species to the community; and (2) jointly fits these associations within a multi-species conditional generative model with different modes of interactions between environmental drivers and biotic associations. Using both simulated and empirical data, we demonstrate the ability of our framework to recover known asymmetric and symmetric associations and highlight the properties of the learned association networks. By comparing our approach to other existing models such as joint species distribution models and probabilistic graphical models, we show its superior capacity at retrieving symmetric and asymmetric interactions. The framework is intuitive, modular and broadly applicable across various taxonomic groups.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Digging deeper: deep joint species distribution modeling reveals environmental drivers of Earthworm Communities",
    "url": "http://arxiv.org/abs/2506.13568v1",
    "authors": [
      "Sara Si-moussi",
      "Wilfried Thuiller",
      "Esther Galbrun",
      "Thibaud Deca\u00ebns",
      "Sylvain G\u00e9rard",
      "Daniel F. March\u00e1n",
      "Claire Marsden",
      "Yvan Capowiez",
      "Micka\u00ebl Hedde"
    ],
    "published": "2025-06-16",
    "abstract": "Earthworms are key drivers of soil function, influencing organic matter turnover, nutrient cycling, and soil structure. Understanding the environmental controls on their distribution is essential for predicting the impacts of land use and climate change on soil ecosystems. While local studies have identified abiotic drivers of earthworm communities, broad-scale spatial patterns remain underexplored.\n  We developed a multi-species, multi-task deep learning model to jointly predict the distribution of 77 earthworm species across metropolitan France, using historical (1960-1970) and contemporary (1990-2020) records. The model integrates climate, soil, and land cover variables to estimate habitat suitability. We applied SHapley Additive exPlanations (SHAP) to identify key environmental drivers and used species clustering to reveal ecological response groups.\n  The joint model achieved high predictive performance (TSS >= 0.7) and improved predictions for rare species compared to traditional species distribution models. Shared feature extraction across species allowed for more robust identification of common and contrasting environmental responses. Precipitation variability, temperature seasonality, and land cover emerged as dominant predictors of earthworm distribution. Species clustering revealed distinct ecological strategies tied to climatic and land use gradients.\n  Our study advances both the methodological and ecological understanding of soil biodiversity. We demonstrate the utility of interpretable deep learning approaches for large-scale soil fauna modeling and provide new insights into earthworm habitat specialization. These findings support improved soil biodiversity monitoring and conservation planning in the face of global environmental change.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "ROC Curves for Spatial Point Patterns and Presence-Absence Data",
    "url": "http://arxiv.org/abs/2506.03414v1",
    "authors": [
      "Adrian Baddeley",
      "Ege Rubak",
      "Suman Rakshit",
      "Gopalan Nair"
    ],
    "published": "2025-06-03",
    "abstract": "Receiver Operating Characteristic (ROC) curves have recently been used to evaluate the performance of models for spatial presence-absence or presence-only data. Applications include species distribution modelling and mineral prospectivity analysis. We clarify the interpretation of the ROC curve in this context. Contrary to statements in the literature, ROC does not measure goodness-of-fit of a spatial model, and its interpretation as a measure of predictive ability is weak; it is a measure of ranking ability, insensitive to the precise form of the model. To gain insight we draw connections between ROC and existing statistical techniques for spatial point pattern data. The area under the ROC curve (AUC) is related to hypothesis tests of the null hypothesis that the explanatory variables have no effect. The shape of the ROC curve has a diagnostic interpretation. This suggests several new techniques, which extend the scope of application of ROC curves for spatial data, to support variable selection and model selection, analysis of segregation between different types of points, adjustment for a baseline, and analysis of spatial case-control data. The new techniques are illustrated with several real example datasets. Open source R code implementing the techniques is available in the development version of our package spatstat [Baddeley and Turner, 2005, Baddeley et al., 2015] and will be included in the next public release.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Modelling benthic animals in space and time using Bayesian Point Process with cross validation: the case of Holoturians",
    "url": "http://arxiv.org/abs/2506.01763v2",
    "authors": [
      "Daniele Poggio",
      "Gian Mario Sangiovanni",
      "Gianluca Mastrantonio",
      "Giovanna Jona Lasinio",
      "Edoardo Casoli",
      "Stefano Moro",
      "Daniele Ventura"
    ],
    "published": "2025-06-02",
    "abstract": "Understanding the spatial distribution of Holothurians is an essential task for ecosystem monitoring and sustainable management, particularly in the Mediterranean habitats. However, species distribution modeling is often complicated by the presence-only nature of the data and heterogeneous sampling designs. This study develops a spatio-temporal framework based on Log-Gaussian Cox Processes to analyze Holothurians' positions collected across nine survey campaigns conducted from 2022 to 2024 near Giglio Island, Italy. The surveys combined high-resolution photogrammetry with diver-based visual censuses, leading to varying detection probabilities across habitats, especially within Posidonia oceanica meadows. We adopt a model with a shared spatial Gaussian process component to accommodate this complexity, accounting for habitat structure, environmental covariates, and temporal variability. Model estimation is performed using Integrated Nested Laplace Approximation. We evaluate the predictive performances of alternative model specifications through a novel k-fold cross-validation strategy for point processes, using the Continuous Ranked Probability Score. Our approach provides a flexible and computationally efficient framework for integrating heterogeneous presence-only data in marine ecology and comparing the predictive ability of alternative models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "GeoThinneR: An R Package for Efficient Spatial Thinning of Species Occurrences and Point Data",
    "url": "http://arxiv.org/abs/2505.07867v1",
    "authors": [
      "J. Mestre-Tom\u00e1s"
    ],
    "published": "2025-05-09",
    "abstract": "In this paper we present GeoThinneR, an R package for efficient and flexible spatial thinning of species occurrence data. Spatial thinning is a widely used preprocessing step in species distribution modeling (SDM) that can help reduce sampling bias, but existing R implementations rely on brute-force algorithms that scale poorly with large datasets. GeoThinneR implements multiple thinning approaches, including ensuring a minimum distance between points, subsampling points on a grid, and filtering based on decimal precision. To handle large datasets, it introduces two optimized algorithms based on local kd-trees and adaptive neighbor estimation, which greatly reduce memory usage and execution time. Additional functionalities such as group-wise thinning and point prioritization are included to facilitate its use in SDM workflows. We here provide performance benchmarks using both simulated and real-world data to demonstrate substantial performance improvements over existing tools.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "GLOSSA: a user-friendly R Shiny application for Bayesian machine learning analysis of marine species distribution",
    "url": "http://arxiv.org/abs/2505.05862v1",
    "authors": [
      "J. Mestre-Tom\u00e1s",
      "A. Fuster-Alonso",
      "J. M. Bellido",
      "M. Coll"
    ],
    "published": "2025-05-09",
    "abstract": "Species distribution models (SDMs) are one of the most common statistical methods to assess species occupancy and geographic distribution patterns. With the increasing complexity of ecological data, many methodological approaches have been developed, often accessible through command-line interfaces or graphical user interfaces (GUIs). However, few species distribution modeling tools are designed to be well-documented, user-friendly, flexible, and reproducible.\n  Here we introduce GLOSSA, an open-source R package and Shiny app designed for species distribution modeling using species occurrence and environmental data. GLOSSA's user-friendly interface guides users through steps including data uploading, processing, model fitting, spatial and temporal projections, and interactive visualization of results. The app also calculates variable importance, generates response curves with environmental variables, and performs cross-validation. At its core, GLOSSA modeling approach is based on Bayesian Additive Regression Trees (BART), an innovative machine learning method.\n  We present the functionality and versatility of GLOSSA through three case studies, addressing a range of ecological scenarios at regional and global scales. Along with comprehensive documentation, examples, and tutorials, these case studies illustrate how an intuitive graphical interface can make species distribution modeling accessible to a broad audience.\n  GLOSSA stands out as an easy-to-use tool for species distribution modeling, providing an intuitive interface, detailed documentation, flexible modeling, and interactive result exploration and export options. Additionally, its outputs can be used directly to inform marine ecosystem models (MEMs), enhancing its utility in ecological research and applications.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Mapping biodiversity at very-high resolution in Europe",
    "url": "http://arxiv.org/abs/2504.05231v1",
    "authors": [
      "C\u00e9sar Leblanc",
      "Lukas Picek",
      "Benjamin Deneu",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "R\u00e9mi Palard",
      "Alexis Joly"
    ],
    "published": "2025-04-07",
    "abstract": "This paper describes a cascading multimodal pipeline for high-resolution biodiversity mapping across Europe, integrating species distribution modeling, biodiversity indicators, and habitat classification. The proposed pipeline first predicts species compositions using a deep-SDM, a multimodal model trained on remote sensing, climate time series, and species occurrence data at 50x50m resolution. These predictions are then used to generate biodiversity indicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM designed for species-to-habitat mapping. With this approach, continental-scale species distribution maps, biodiversity indicator maps, and habitat maps are produced, providing fine-grained ecological insights. Unlike traditional methods, this framework enables joint modeling of interspecies dependencies, bias-aware training with heterogeneous presence-absence data, and large-scale inference from multi-source remote sensing inputs.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "LLM",
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Climplicit: Climatic Implicit Embeddings for Global Ecological Tasks",
    "url": "http://arxiv.org/abs/2504.05089v2",
    "authors": [
      "Johannes Dollinger",
      "Damien Robert",
      "Elena Plekhanova",
      "Lukas Drees",
      "Jan Dirk Wegner"
    ],
    "published": "2025-04-07",
    "abstract": "Deep learning on climatic data holds potential for macroecological applications. However, its adoption remains limited among scientists outside the deep learning community due to storage, compute, and technical expertise barriers. To address this, we introduce Climplicit, a spatio-temporal geolocation encoder pretrained to generate implicit climatic representations anywhere on Earth. By bypassing the need to download raw climatic rasters and train feature extractors, our model uses x3500 less disk space and significantly reduces computational needs for downstream tasks. We evaluate our Climplicit embeddings on biomes classification, species distribution modeling, and plant trait regression. We find that single-layer probing our Climplicit embeddings consistently performs better or on par with training a model from scratch on downstream tasks and overall better than alternative geolocation encoding models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Bayesian Deep Latent Class Regression",
    "url": "http://arxiv.org/abs/2503.17531v2",
    "authors": [
      "Yuren Zhou",
      "Yuqi Gu",
      "David B. Dunson"
    ],
    "published": "2025-03-21",
    "abstract": "High-dimensional categorical data arise in diverse scientific domains and are often accompanied by covariates. Latent class regression models are routinely used in such settings, reducing dimensionality by assuming conditional independence of the categorical variables given a single latent class that depends on covariates through a logistic regression model. However, such methods become unreliable as the dimensionality increases. To address this, we propose a flexible family of deep latent class models. Our model satisfies key theoretical properties, including identifiability and posterior consistency, and we establish a Bayes oracle clustering property that ensures robustness against the curse of dimensionality. We develop efficient posterior computation methods, validate them through simulation studies, and apply our model to joint species distribution modeling in ecology.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling",
    "url": "http://arxiv.org/abs/2503.13057v2",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Gencer Sumbul",
      "Chiara Vanalli",
      "Benjamin Kellenberger",
      "Devis Tuia"
    ],
    "published": "2025-03-17",
    "abstract": "Species Distribution Models (SDMs) play a vital role in biodiversity research, conservation planning, and ecological niche modeling by predicting species distributions based on environmental conditions. The selection of predictors is crucial, strongly impacting both model accuracy and how well the predictions reflect ecological patterns. To ensure meaningful insights, input variables must be carefully chosen to match the study objectives and the ecological requirements of the target species. However, existing SDMs, including both traditional and deep learning-based approaches, often lack key capabilities for variable selection: (i) flexibility to choose relevant predictors at inference without retraining; (ii) robustness to handle missing predictor values without compromising accuracy; and (iii) explainability to interpret and accurately quantify each predictor's contribution. To overcome these limitations, we introduce MaskSDM, a novel deep learning-based SDM that enables flexible predictor selection by employing a masked training strategy. This approach allows the model to make predictions with arbitrary subsets of input variables while remaining robust to missing data. It also provides a clearer understanding of how adding or removing a given predictor affects model performance and predictions. Additionally, MaskSDM leverages Shapley values for precise predictor contribution assessments, improving upon traditional approximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling the distributions of 12,738 plant species. Our results show that MaskSDM outperforms imputation-based methods and approximates models trained on specific subsets of variables. These findings underscore MaskSDM's potential to increase the applicability and adoption of SDMs, laying the groundwork for developing foundation models in SDMs that can be readily applied to diverse ecological applications.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Heterogeneous graph neural networks for species distribution modeling",
    "url": "http://arxiv.org/abs/2503.11900v3",
    "authors": [
      "Lauren Harrell",
      "Christine Kaeser-Chen",
      "Burcu Karagol Ayan",
      "Keith Anderson",
      "Michelangelo Conserva",
      "Elise Kleeman",
      "Maxim Neumann",
      "Matt Overlan",
      "Melissa Chapman",
      "Drew Purves"
    ],
    "published": "2025-03-14",
    "abstract": "Species distribution models (SDMs) are necessary for measuring and predicting occurrences and habitat suitability of species and their relationship with environmental factors. We introduce a novel presence-only SDM with graph neural networks (GNN). In our model, species and locations are treated as two distinct node sets, and the learning task is predicting detection records as the edges that connect locations to species. Using GNN for SDM allows us to model fine-grained interactions between species and the environment. We evaluate the potential of this methodology on the six-region dataset compiled by National Center for Ecological Analysis and Synthesis (NCEAS) for benchmarking SDMs. For each of the regions, the heterogeneous GNN model is comparable to or outperforms previously-benchmarked single-species SDMs as well as a feed-forward neural network baseline model.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "GNN",
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling",
    "url": "http://arxiv.org/abs/2502.03480v1",
    "authors": [
      "Diana Koldasbayeva",
      "Alexey Zaytsev"
    ],
    "published": "2025-01-27",
    "abstract": "Species Distribution Models (SDMs) often suffer from spatial autocorrelation (SAC), leading to biased performance estimates. We tested cross-validation (CV) strategies - random splits, spatial blocking with varied distances, environmental (ENV) clustering, and a novel spatio-temporal method - under two proposed training schemes: LAST FOLD, widely used in spatial CV at the cost of data loss, and RETRAIN, which maximizes data usage but risks reintroducing SAC. LAST FOLD consistently yielded lower errors and stronger correlations. Spatial blocking at an optimal distance (SP 422) and ENV performed best, achieving Spearman and Pearson correlations of 0.485 and 0.548, respectively, although ENV may be unsuitable for long-term forecasts involving major environmental shifts. A spatio-temporal approach yielded modest benefits in our moderately variable dataset, but may excel with stronger temporal changes. These findings highlight the need to align CV approaches with the spatial and temporal structure of SDM data, ensuring rigorous validation and reliable predictive outcomes.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Applying the maximum entropy principle to neural networks enhances multi-species distribution models",
    "url": "http://arxiv.org/abs/2412.19217v3",
    "authors": [
      "Maxime Ryckewaert",
      "Diego Marcos",
      "Christophe Botella",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2024-12-26",
    "abstract": "The rapid expansion of citizen science initiatives has led to a significant growth of biodiversity databases, and particularly presence-only (PO) observations. PO data are invaluable for understanding species distributions and their dynamics, but their use in a Species Distribution Model (SDM) is curtailed by sampling biases and the lack of information on absences. Poisson point processes are widely used for SDMs, with Maxent being one of the most popular methods. Maxent maximises the entropy of a probability distribution across sites as a function of predefined transformations of variables, called features. In contrast, neural networks and deep learning have emerged as a promising technique for automatic feature extraction from complex input variables. Arbitrarily complex transformations of input variables can be learned from the data efficiently through backpropagation and stochastic gradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses neural networks to automatically learn shared features among species, using the maximum entropy principle. To do so, it employs a normalised Poisson loss where for each species, presence probabilities across sites are modelled by a neural network. We evaluate DeepMaxent on a benchmark dataset known for its spatial sampling biases, using PO data for calibration and presence-absence (PA) data for validation across six regions with different biological groups and covariates. Our results indicate that DeepMaxent performs better than Maxent and other leading SDMs across all regions and taxonomic groups. The method performs particularly well in regions of uneven sampling, demonstrating substantial potential to increase SDM performances. In particular, our approach yields more accurate predictions than traditional single-species models, which opens up new possibilities for methodological enhancement.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MiTREE: Multi-input Transformer Ecoregion Encoder for Species Distribution Modelling",
    "url": "http://arxiv.org/abs/2412.18995v1",
    "authors": [
      "Theresa Chen",
      "Yao-Yi Chiang"
    ],
    "published": "2024-12-25",
    "abstract": "Climate change poses an extreme threat to biodiversity, making it imperative to efficiently model the geographical range of different species. The availability of large-scale remote sensing images and environmental data has facilitated the use of machine learning in Species Distribution Models (SDMs), which aim to predict the presence of a species at any given location. Traditional SDMs, reliant on expert observation, are labor-intensive, but advancements in remote sensing and citizen science data have facilitated machine learning approaches to SDM development. However, these models often struggle with leveraging spatial relationships between different inputs -- for instance, learning how climate data should inform the data present in satellite imagery -- without upsampling or distorting the original inputs. Additionally, location information and ecological characteristics at a location play a crucial role in predicting species distribution models, but these aspects have not yet been incorporated into state-of-the-art approaches. In this work, we introduce MiTREE: a multi-input Vision-Transformer-based model with an ecoregion encoder. MiTREE computes spatial cross-modal relationships without upsampling as well as integrates location and ecological context. We evaluate our model on the SatBird Summer and Winter datasets, the goal of which is to predict bird species encounter rates, and we find that our approach improves upon state-of-the-art baselines.",
    "categories": [
      "fish_plankton",
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer",
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Spatial Clustering of Citizen Science Data Improves Downstream Species Distribution Models",
    "url": "http://arxiv.org/abs/2412.15559v3",
    "authors": [
      "Nahian Ahmed",
      "Mark Roth",
      "Tyler A. Hallman",
      "W. Douglas Robinson",
      "Rebecca A. Hutchinson"
    ],
    "published": "2024-12-20",
    "abstract": "Citizen science biodiversity data present great opportunities for ecology and conservation across vast spatial and temporal scales. However, the opportunistic nature of these data lacks the sampling structure required by modeling methodologies that address a pervasive challenge in ecological data collection: imperfect detection, i.e., the likelihood of under-observing species on field surveys. Occupancy modeling is an example of an approach that accounts for imperfect detection by explicitly modeling the observation process separately from the biological process of habitat selection. This produces species distribution models that speak to the pattern of the species on a landscape after accounting for imperfect detection in the data, rather than the pattern of species observations corrupted by errors. To achieve this benefit, occupancy models require multiple surveys of a site across which the site's status (i.e., occupied or not) is assumed constant. Since citizen science data are not collected under the required repeated-visit protocol, observations may be grouped into sites post hoc. Existing approaches for constructing sites discard some observations and/or consider only geographic distance and not environmental similarity. In this study, we compare ten approaches for site construction in terms of their impact on downstream species distribution models for 31 bird species in Oregon, using observations recorded in the eBird database. We find that occupancy models built on sites constructed by spatial clustering algorithms perform better than existing alternatives.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Joint species distribution modeling of abundance data through latent variable barcodes",
    "url": "http://arxiv.org/abs/2412.08793v2",
    "authors": [
      "Braden Scherting",
      "Otso Ovaskainen",
      "David B. Dunson"
    ],
    "published": "2024-12-11",
    "abstract": "Accelerating global biodiversity loss has highlighted the role of complex relationships and shared patterns among species in determining their responses to environmental changes. The structure of an ecological community, represented by patterns of dependence among constituent species, signals its robustness more than individual species distributions. We focus on obtaining community-level insights based on underlying patterns in abundances of bird species in Finland. We propose \\texttt{barcode}, a modeling framework to infer latent binary and continuous features of samples and species, expanding the class of concurrent ordinations. This approach introduces covariates and spatial autocorrelation hierarchically to facilitate ecological interpretations of the learned features. By analyzing 132 bird species counts, we infer the dominant environmental drivers of the community, species clusters and regions of common profile. Three of the learned drivers correspond to distinct climactic regions with different dominant forest types. Three further drivers are spatially heterogeneous and signal urban, agricultural, and wetland areas, respectively.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "FathomVerse: A community science dataset for ocean animal discovery",
    "url": "http://arxiv.org/abs/2412.01701v1",
    "authors": [
      "Genevieve Patterson",
      "Joost Daniels",
      "Benjamin Woodward",
      "Kevin Barnard",
      "Giovanna Sainz",
      "Lonny Lundsten",
      "Kakani Katija"
    ],
    "published": "2024-12-02",
    "abstract": "Can computer vision help us explore the ocean? The ultimate challenge for computer vision is to recognize any visual phenomena, more than only the objects and animals humans encounter in their terrestrial lives. Previous datasets have explored everyday objects and fine-grained categories humans see frequently. We present the FathomVerse v0 detection dataset to push the limits of our field by exploring animals that rarely come in contact with people in the deep sea. These animals present a novel vision challenge.\n  The FathomVerse v0 dataset consists of 3843 images with 8092 bounding boxes from 12 distinct morphological groups recorded at two locations on the deep seafloor that are new to computer vision. It features visually perplexing scenarios such as an octopus intertwined with a sea star, and confounding categories like vampire squids and sea spiders. This dataset can push forward research on topics like fine-grained transfer learning, novel category discovery, species distribution modeling, and carbon cycle analysis, all of which are important to the care and husbandry of our planet.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Joint Spatiotemporal Modeling of Zooplankton and Whale Abundance in a Dynamic Marine Environment",
    "url": "http://arxiv.org/abs/2411.06001v1",
    "authors": [
      "Bokgyeong Kang",
      "Erin M. Schliep",
      "Alan E. Gelfand",
      "Christopher W. Clark",
      "Christine A. Hudak",
      "Charles A. Mayo",
      "Ryan Schosberg",
      "Tina M. Yack",
      "Robert S. Schick"
    ],
    "published": "2024-11-08",
    "abstract": "North Atlantic right whales are an endangered species; their entire population numbers approximately 372 individuals, and they are subject to major anthropogenic threats. They feed on zooplankton species whose distribution shifts in a dynamic and warming oceanic environment. Because right whales in turn follow their shifting food resource, it is necessary to jointly study the distribution of whales and their prey. The innovative joint species distribution modeling (JSDM) contribution here is different from anything in the large JDSM literature, reflecting the processes and data we have to work with. Specifically, our JSDM supplies a geostatistical model for expected amount of zooplankton collected at a site. We require a point pattern model for the intensity of right whale abundance. The two process models are joined through a latent conditional-marginal specification. Further, each species has two data sources to inform their respective distributions and these sources require novel data fusion. What emerges is a complex multi-level model. Through simulation we demonstrate the ability of our joint specification to identify model unknowns and learn better about the species distributions than modeling them individually. We then apply our modeling to real data from Cape Cod Bay, Massachusetts in the U.S.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Multi-Scale and Multimodal Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2411.04016v1",
    "authors": [
      "Nina van Tiel",
      "Robin Zbinden",
      "Emanuele Dalsasso",
      "Benjamin Kellenberger",
      "Lo\u00efc Pellissier",
      "Devis Tuia"
    ],
    "published": "2024-11-06",
    "abstract": "Species distribution models (SDMs) aim to predict the distribution of species by relating occurrence data with environmental variables. Recent applications of deep learning to SDMs have enabled new avenues, specifically the inclusion of spatial data (environmental rasters, satellite images) as model predictors, allowing the model to consider the spatial context around each species' observations. However, the appropriate spatial extent of the images is not straightforward to determine and may affect the performance of the model, as scale is recognized as an important factor in SDMs. We develop a modular structure for SDMs that allows us to test the effect of scale in both single- and multi-scale settings. Furthermore, our model enables different scales to be considered for different modalities, using a late fusion approach. Results on the GeoLifeCLEF 2023 benchmark indicate that considering multimodal data and learning multi-scale representations leads to more accurate models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Hybrid Spatial Representations for Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2410.10937v2",
    "authors": [
      "Shiran Yuan",
      "Hao Zhao"
    ],
    "published": "2024-10-14",
    "abstract": "We address an important problem in ecology called Species Distribution Modeling (SDM), whose goal is to predict whether a species exists at a certain position on Earth. In particular, we tackle a challenging version of this task, where we learn from presence-only data in a community-sourced dataset, model a large number of species simultaneously, and do not use any additional environmental information. Previous work has used neural implicit representations to construct models that achieve promising results. However, implicit representations often generate predictions of limited spatial precision. We attribute this limitation to their inherently global formulation and inability to effectively capture local feature variations. This issue is especially pronounced with presence-only data and a large number of species. To address this, we propose a hybrid embedding scheme that combines both implicit and explicit embeddings. Specifically, the explicit embedding is implemented with a multiresolution hashgrid, enabling our models to better capture local information. Experiments demonstrate that our results exceed other works by a large margin on various standard benchmarks, and that the hybrid representation is better than both purely implicit and explicit ones. Qualitative visualizations and comprehensive ablation studies reveal that our hybrid representation successfully addresses the two main challenges. Our code is open-sourced at https://github.com/Shiran-Yuan/HSR-SDM.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MALPOLON: A Framework for Deep Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2409.18102v1",
    "authors": [
      "Theo Larcher",
      "Lukas Picek",
      "Benjamin Deneu",
      "Titouan Lorieul",
      "Maximilien Servajean",
      "Alexis Joly"
    ],
    "published": "2024-09-26",
    "abstract": "This paper describes a deep-SDM framework, MALPOLON. Written in Python and built upon the PyTorch library, this framework aims to facilitate training and inferences of deep species distribution models (deep-SDM) and sharing for users with only general Python language skills (e.g., modeling ecologists) who are interested in testing deep learning approaches to build new SDMs. More advanced users can also benefit from the framework's modularity to run more specific experiments by overriding existing classes while taking advantage of press-button examples to train neural networks on multiple classification tasks using custom or provided raw and pre-processed datasets. The framework is open-sourced on GitHub and PyPi along with extensive documentation and examples of use in various scenarios. MALPOLON offers straightforward installation, YAML-based configuration, parallel computing, multi-GPU utilization, baseline and foundational models for benchmarking, and extensive tutorials/documentation, aiming to enhance accessibility and performance scalability for ecologists and researchers.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Integrating systematic surveys with historical data to model the distribution of Ornithodoros turicata americanus, a vector of epidemiological concern in North America",
    "url": "http://arxiv.org/abs/2409.12761v1",
    "authors": [
      "Sebastian Botero-Canola",
      "Carson Torhorst",
      "Nicholas Canino",
      "Lorenza Beati",
      "Kathleen C. O Hara",
      "Angela M. James",
      "Samantha M. Wisely"
    ],
    "published": "2024-09-19",
    "abstract": "Globally, vector-borne diseases are increasing in distribution and frequency, affecting humans, domestic animals and livestock, and wildlife. Science-based management and prevention of these diseases requires a sound understanding of the distribution and environmental requirements of the vectors and hosts involved in disease transmission. Integrated Species Distribution Models (ISDM) account for diverse data types through hierarchical modeling and represent a significant advancement in species distribution modeling that have not yet been leveraged in disease ecology. We used this approach, as implemented in the recently developed R package RISDM, to assess the distribution of the soft tick subspecies Ornithodoros turicata americanus. We created an ISDM for O. t. americanus, using systematically collected field data and historical records of this tick species in the southeastern US, to predict its distribution and assess potential correlations with environmental variables. Given the novelty of this method, we compared the results to a conventional Maxent SDM and validated the results through data partitioning using true skills statistics (TSS), sensitivity, and area under the ROC curve (AUC) metrics. We found that a combination of climatic variables describing seasonality and temperature extremes, along with the amount of sand in the soil, determined the predicted intensity of occurrence of this tick species. When projected in geographic space, this distribution model predicted 62% of Florida as suitable habitat for this tick species. The ISDM presented a higher TSS and AUC than the Maxent conventional model, while sensitivity was similar between both models. Our case example shows the utility of ISDMs in disease ecology studies and highlights the broad range of geographic suitability for this important disease vector.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Trophic Cascades and Habitat Suitability in Udanti Sitnadi Tiger Reserve: Impacts of Prey Depletion and Climate Change on Predator Prey Dynamics",
    "url": "http://arxiv.org/abs/2409.00193v1",
    "authors": [
      "Krishnendu Basak",
      "Chiranjib Chaudhuri",
      "M Suraj",
      "Moiz Ahmed"
    ],
    "published": "2024-08-30",
    "abstract": "This study investigates the trophic cascades and habitat suitability in Udanti Sitnadi Tiger Reserve (USTR), highlighting the roles of apex predators, subordinate predators, and prey species in maintaining ecosystem balance. Using the Trophic Species Distribution Model (SDM), we explored prey-predator interactions and habitat suitability, revealing that tigers, due to prey depletion, increasingly rely on cattle, while leopards adapt by preying on smaller species. The study emphasizes the need for prey augmentation and habitat restoration to support apex predators. Additionally, climate change projections for 2021-2040 and 2081-2100 under CMIP6 scenarios SSP245 and SSP585 indicate significant regional habitat shifts, necessitating adaptive management strategies. Kuladighat is projected to face habitat contraction, while Sitanadi may experience habitat expansion. Effective conservation efforts such as habitat restoration, prey augmentation and predator recovery are the most important steps needed to maintain the purpose of a Tiger reserve and conservation potential of Udanti-Sonabeda Tiger Conservation Unit (TCU). To achieve these dynamics, focusing on community participation, anti-poaching measures, and scientific recommendations are the most crucial components to focus on. This comprehensive analysis underscores the critical role of targeted conservation activities in prey-depleted landscapes to ensure the long-term survival of tigers and the overall health of forest ecosystems, enhancing biodiversity and mitigating human-wildlife conflicts in USTR.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Dogs on forest trails; Understanding ecology of Striped Hyena and wild Canids in the presence of free-ranging dogs in Udanti-Sitanadi Tiger Reserve, Central India using Joint Distribution and Deep Neural Networks",
    "url": "http://arxiv.org/abs/2409.00185v1",
    "authors": [
      "Chiranjib Chaudhuri",
      "Krishnendu Basak",
      "M Suraj",
      "Moiz Ahmed",
      "Amit Kumar"
    ],
    "published": "2024-08-30",
    "abstract": "This study uses Joint Species Distribution Models (JSDMs) and Deep Neural Networks (DNNs) to explore how wild carnivores and free-ranging dogs interact in the Udanti-Sitanadi Tiger Reserve (USTR) in Central India. The research focuses on key species like the Striped Hyena, Grey Wolf, Golden Jackal, and Indian Fox, revealing significant overlaps in habitat with free-ranging dogs, especially in densely populated areas like the Sitanadi region of the tiger reserve. These overlaps pose serious risks to wildlife through competition for resources, predation, and the spread of diseases. The study shows that the Striped Hyena prefers gentle slopes and forested areas, while the Grey Wolf tends to avoid cropland and thrives in regions with higher rainfall that supports a stable prey base. The Golden Jackal, more adaptable than the others, favors west-facing slopes and stable temperatures, whereas the Indian Fox is mainly found in the less disturbed, mountainous Kuladighat region. Additionally, the study highlights the potential impacts of climate change, predicting that the Grey Wolf could face habitat extinction under more severe scenarios. These findings underscore the urgent need for conservation strategies tailored to address both dog wild carnivore interactions and the growing challenges posed by climate change, focusing on protecting the critical habitats of vulnerable species like the Striped Hyena and Grey Wolf.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Deep Neural Network",
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Generating Binary Species Range Maps",
    "url": "http://arxiv.org/abs/2408.15956v1",
    "authors": [
      "Filip Dorm",
      "Christian Lange",
      "Scott Loarie",
      "Oisin Mac Aodha"
    ],
    "published": "2024-08-28",
    "abstract": "Accurately predicting the geographic ranges of species is crucial for assisting conservation efforts. Traditionally, range maps were manually created by experts. However, species distribution models (SDMs) and, more recently, deep learning-based variants offer a potential automated alternative. Deep learning-based SDMs generate a continuous probability representing the predicted presence of a species at a given location, which must be binarized by setting per-species thresholds to obtain binary range maps. However, selecting appropriate per-species thresholds to binarize these predictions is non-trivial as different species can require distinct thresholds. In this work, we evaluate different approaches for automatically identifying the best thresholds for binarizing range maps using presence-only data. This includes approaches that require the generation of additional pseudo-absence data, along with ones that only require presence data. We also propose an extension of an existing presence-only technique that is more robust to outliers. We perform a detailed evaluation of different thresholding techniques on the tasks of binary range estimation and large-scale fine-grained visual classification, and we demonstrate improved performance over existing pseudo-absence free approaches using our method.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "GeoPlant: Spatial Plant Species Prediction Dataset",
    "url": "http://arxiv.org/abs/2408.13928v2",
    "authors": [
      "Lukas Picek",
      "Christophe Botella",
      "Maximilien Servajean",
      "C\u00e9sar Leblanc",
      "R\u00e9mi Palard",
      "Th\u00e9o Larcher",
      "Benjamin Deneu",
      "Diego Marcos",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2024-08-25",
    "abstract": "The difficulty of monitoring biodiversity at fine scales and over large areas limits ecological knowledge and conservation efforts. To fill this gap, Species Distribution Models (SDMs) predict species across space from spatially explicit features. Yet, they face the challenge of integrating the rich but heterogeneous data made available over the past decade, notably millions of opportunistic species observations and standardized surveys, as well as multimodal remote sensing data. In light of that, we have designed and developed a new European-scale dataset for SDMs at high spatial resolution (10--50m), including more than 10k species (i.e., most of the European flora). The dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive Presence-Absence survey records, all accompanied by diverse environmental rasters (e.g., elevation, human footprint, and soil) traditionally used in SDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10 m resolution, a 20-year time series of climatic variables, and satellite time series from the Landsat program. In addition to the data, we provide an openly accessible SDM benchmark (hosted on Kaggle), which has already attracted an active community and a set of strong baselines for single predictor/modality and multimodal approaches. All resources, e.g., the dataset, pre-trained models, and baseline methods (in the form of notebooks), are available on Kaggle, allowing one to start with our dataset literally with two mouse clicks.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Fast fitting of phylogenetic mixed-effects models",
    "url": "http://arxiv.org/abs/2408.05333v2",
    "authors": [
      "Bert van der Veen",
      "Robert Brian O'Hara"
    ],
    "published": "2024-08-09",
    "abstract": "Mixed-effects models are among the most commonly used statistical methods for the exploration of multispecies data. In recent years, also Joint Species Distribution Models and Generalized Linear Latent Variale Models have gained in popularity when the goal is to incorporate residual covariation between species that cannot be explained due to measured environmental covariates. Few software implementations of such models exist that can additionally incorporate phylogenetic information, and those that exist tend to utilize Markov chain Monte Carlo methods for estimation, so that model fitting takes a long time. In this article we develop new methods for quickly and flexibly fitting phylogenetic mixed-effects models, potentially incorporating residual covariation between species using latent variables, with the possibility to estimate the strength of phylogenetic structuring in species responses per environmental covariate, and while incorporating correlation between different covariate effects. By combining Variational approximations, a sparse approximation to the phylogenetic precision matrix, and parallel computation, phylogenetic mixed-effects models can be fitted much more quickly than the current state-of-the-art. Two simulation studies demonstrate that the proposed combination of approximations is fast and enjoys high accuracy. We explore sensitivity of the approximation to the ordering of species with a real world dataset of wood-decaying fungi.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Implication of modelling choices on connectivity estimation: A comparative analysis",
    "url": "http://arxiv.org/abs/2407.09564v1",
    "authors": [
      "Marie Soret",
      "Sylvain Moulherat",
      "Maxime Lenormand",
      "Sandra Luque"
    ],
    "published": "2024-07-05",
    "abstract": "We focus on connectivity methods used to understand and predict how landscapes and habitats facilitate or impede the movement and dispersal of species. Our objective is to compare the implication of methodological choices at three stages of the modelling framework: landscape characterisation, connectivity estimation, and connectivity assessment. What are the convergences and divergences of different modelling approaches? What are the implications of their combined results for landscape planning? We implemented two landscape characterisation approaches: expert opinion and species distribution model (SDM); four connectivity estimation models: Euclidean distance, least-cost paths (LCP), circuit theory, and stochastic movement simulation (SMS); and two connectivity indices: flux and area-weighted flux (dPCflux). We compared outcomes such as movement maps and habitat prioritisation for a rural landscape in southwestern France. Landscape characterisation is the main factor influencing connectivity assessment. The movement maps reflect the models' assumptions: LCP produced narrow beams reflecting the optimal pathways; whereas circuit theory and SMS produced wider estimation reflecting movement stochasticity, with SMS integrating behavioural drivers. The indices highlighted different aspects: dPCflux the surface of suitable habitats and flux their proximity. We recommend focusing on landscape characterisation before engaging further in the modelling framework. We emphasise the importance of stochasticity and behavioural drivers in connectivity, which can be reflected using circuit theory, SMS or other stochastic individual-based models. We stress the importance of using multiple indices to capture the multi-factorial aspect of connectivity.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning",
    "url": "http://arxiv.org/abs/2406.15658v3",
    "authors": [
      "Nemin Wu",
      "Qian Cao",
      "Zhangyu Wang",
      "Zeping Liu",
      "Yanlin Qi",
      "Jielu Zhang",
      "Joshua Ni",
      "Xiaobai Yao",
      "Hongxu Ma",
      "Lan Mu",
      "Stefano Ermon",
      "Tanuja Ganu",
      "Akshay Nambi",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published": "2024-06-21",
    "abstract": "Spatial representation learning (SRL) aims at learning general-purpose neural network representations from various types of spatial data (e.g., points, polylines, polygons, networks, images, etc.) in their native formats. Learning good spatial representations is a fundamental problem for various downstream applications such as species distribution modeling, weather forecasting, trajectory generation, geographic question answering, etc. Even though SRL has become the foundation of almost all geospatial artificial intelligence (GeoAI) research, we have not yet seen significant efforts to develop an extensive deep learning framework and benchmark to support SRL model development and evaluation. To fill this gap, we propose TorchSpatial, a learning framework and benchmark for location (point) encoding, which is one of the most fundamental data types of spatial representation learning. TorchSpatial contains three key components: 1) a unified location encoding framework that consolidates 15 commonly recognized location encoders, ensuring scalability and reproducibility of the implementations; 2) the LocBench benchmark tasks encompassing 7 geo-aware image classification and 10 geo-aware image regression datasets; 3) a comprehensive suite of evaluation metrics to quantify geo-aware model's overall performance as well as their geographic bias, with a novel Geo-Bias Score metric. Finally, we provide a detailed analysis and insights into the model performance and geographic bias of different location encoders. We believe TorchSpatial will foster future advancement of spatial representation learning and spatial fairness in GeoAI research. The TorchSpatial model framework and LocBench benchmark are available at https://github.com/seai-lab/TorchSpatial, and the Geo-Bias Score evaluation framework is available at https://github.com/seai-lab/PyGBS.",
    "categories": [
      "fish_plankton",
      "geo_reasoning"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Cumulant-based approximation for fast and efficient prediction for species distribution",
    "url": "http://arxiv.org/abs/2405.14456v1",
    "authors": [
      "Osamu Komori",
      "Yusuke Saigusa",
      "Shinto Eguchi",
      "Yasuhiro Kubota"
    ],
    "published": "2024-05-23",
    "abstract": "Species distribution modeling plays an important role in estimating the habitat suitability of species using environmental variables. For this purpose, Maxent and the Poisson point process are popular and powerful methods extensively employed across various ecological and biological sciences. However, the computational speed becomes prohibitively slow when using huge background datasets, which is often the case with fine-resolution data or global-scale estimations. To address this problem, we propose a computationally efficient species distribution model using a cumulant-based approximation (CBA) applied to the loss function of $\u03b3$-divergence. Additionally, we introduce a sequential estimating algorithm with an $L_1$ penalty to select important environmental variables closely associated with species distribution. The regularized geometric-mean method, derived from the CBA, demonstrates high computational efficiency and estimation accuracy. Moreover, by applying CBA to Maxent, we establish that Maxent and Fisher linear discriminant analysis are equivalent under a normality assumption. This equivalence leads to an highly efficient computational method for estimating species distribution. The effectiveness of our proposed methods is illustrated through simulation studies and by analyzing data on 226 species from the National Centre for Ecological Analysis and Synthesis and 709 Japanese vascular plant species. The computational efficiency of the proposed methods is significantly improved compared to Maxent, while maintaining comparable estimation accuracy. A R package {\\tt CBA} is also prepared to provide all programming codes used in simulation studies and real data analysis.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "A Comparison of Joint Species Distribution Models for Percent Cover Data",
    "url": "http://arxiv.org/abs/2403.11562v1",
    "authors": [
      "Pekka Korhonen",
      "Francis K. C. Hui",
      "Jenni Niku",
      "Sara Taskinen",
      "Bert van der Veen"
    ],
    "published": "2024-03-18",
    "abstract": "1. Joint species distribution models (JSDMs) have gained considerable traction among ecologists over the past decade, due to their capacity to answer a wide range of questions at both the species- and the community-level. The family of generalized linear latent variable models in particular has proven popular for building JSDMs, being able to handle many response types including presence-absence data, biomass, overdispersed and/or zero-inflated counts.\n  2. We extend latent variable models to handle percent cover data, with vegetation, sessile invertebrate, and macroalgal cover data representing the prime examples of such data arising in community ecology.\n  3. Sparsity is a commonly encountered challenge with percent cover data. Responses are typically recorded as percentages covered per plot, though some species may be completely absent or present, i.e., have 0% or 100% cover respectively, rendering the use of beta distribution inadequate.\n  4. We propose two JSDMs suitable for percent cover data, namely a hurdle beta model and an ordered beta model. We compare the two proposed approaches to a beta distribution for shifted responses, transformed presence-absence data, and an ordinal model for percent cover classes. Results demonstrate the hurdle beta JSDM was generally the most accurate at retrieving the latent variables and predicting ecological percent cover data.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Imbalance-aware Presence-only Loss Function for Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2403.07472v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Marc Ru\u00dfwurm",
      "Devis Tuia"
    ],
    "published": "2024-03-12",
    "abstract": "In the face of significant biodiversity decline, species distribution models (SDMs) are essential for understanding the impact of climate change on species habitats by connecting environmental conditions to species occurrences. Traditionally limited by a scarcity of species observations, these models have significantly improved in performance through the integration of larger datasets provided by citizen science initiatives. However, they still suffer from the strong class imbalance between species within these datasets, often resulting in the penalization of rare species--those most critical for conservation efforts. To tackle this issue, this study assesses the effectiveness of training deep learning models using a balanced presence-only loss function on large citizen science-based datasets. We demonstrate that this imbalance-aware loss function outperforms traditional loss functions across various datasets and tasks, particularly in accurately modeling rare species with limited observations.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Infinite joint species distribution models",
    "url": "http://arxiv.org/abs/2402.13384v3",
    "authors": [
      "Federica Stolf",
      "David B. Dunson"
    ],
    "published": "2024-02-20",
    "abstract": "Joint species distribution models are popular in ecology for modeling covariate effects on species occurrence, while characterizing cross-species dependence. Data consist of multivariate binary indicators of the occurrences of different species in each sample, along with sample-specific covariates. A key problem is that current models implicitly assume that the list of species under consideration is predefined and finite, while for highly diverse groups of organisms, it is impossible to anticipate which species will be observed in a study and discovery of unknown species is common. This article proposes a new modeling paradigm for statistical ecology, which generalizes traditional multivariate probit models to accommodate large numbers of rare species and new species discovery. We discuss theoretical properties of the proposed modeling paradigm and implement efficient algorithms for posterior computation. Simulation studies and applications to fungal biodiversity data provide compelling support for the new modeling class.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Novel community data in ecology -- properties and prospects",
    "url": "http://arxiv.org/abs/2401.10860v1",
    "authors": [
      "Florian Hartig",
      "Nerea Abrego",
      "Alex Bush",
      "Jonathan M. Chase",
      "Gurutzeta Guillera-Arroita",
      "Mathew A. Leibold",
      "Otso Ovaskainen",
      "Lo\u00efc Pellissier",
      "Maximilian Pichler",
      "Giovanni Poggiato",
      "Laura Pollock",
      "Sara Si-Moussi",
      "Wilfried Thuiller",
      "Duarte S. Viana",
      "David I. Warton",
      "Damaris Zurell",
      "Douglas W. Yu"
    ],
    "published": "2024-01-19",
    "abstract": "New technologies for acquiring biological information such as eDNA, acoustic or optical sensors, make it possible to generate spatial community observations at unprecedented scales. The potential of these novel community data to standardize community observations at high spatial, temporal, and taxonomic resolution and at large spatial scale ('many rows and many columns') has been widely discussed, but so far, there has been little integration of these data with ecological models and theory. Here, we review these developments and highlight emerging solutions, focusing on statistical methods for analyzing novel community data, in particular joint species distribution models; the new ecological questions that can be answered with these data; and the potential implications of these developments for policy and conservation.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "A sequential Monte Carlo algorithm for data assimilation problems in ecology",
    "url": "http://arxiv.org/abs/2401.06515v1",
    "authors": [
      "Kwaku Peprah Adjei",
      "Rob Cooke",
      "Nick Isaac",
      "Robert B. O'Hara"
    ],
    "published": "2024-01-12",
    "abstract": "1. Temporal trends in species distributions are necessary for monitoring changes in biodiversity, which aids policymakers and conservationists in making informed decisions. Dynamic species distribution models are often fitted to ecological time series data using Markov Chain Monte Carlo algorithms to produce these temporal trends. However, the fitted models can be time-consuming to produce and run, making it inefficient to refit them as new observations become available.\n  2. We propose an algorithm that updates model parameters and the latent state distribution (e.g. true occupancy) using the saved information from a previously fitted model. This algorithm capitalises on the strength of importance sampling to generate new posterior samples of interest by updating the model output. The algorithm was validated with simulation studies on linear Gaussian state space models and occupancy models, and we applied the framework to Crested Tits in Switzerland and Yellow Meadow Ants in the UK.\n  3. We found that models updated with the proposed algorithm captured the true model parameters and latent state values as good as the models refitted to the expanded dataset. Moreover, the updated models were much faster to run and preserved the trajectory of the derived quantities.\n  4. The proposed approach serves as an alternative to conventional methods for updating state-space models (SSMs), and it is most beneficial when the fitted SSMs have a long run time. Overall, we provide a Monte Carlo algorithm to efficiently update complex models, a key issue in developing biodiversity models and indicators.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts",
    "url": "http://arxiv.org/abs/2401.05470v1",
    "authors": [
      "Joaquim Estopinan",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "Fran\u00e7ois Munoz",
      "Alexis Joly"
    ],
    "published": "2024-01-10",
    "abstract": "The post-2020 global biodiversity framework needs ambitious, research-based targets. Estimating the accelerated extinction risk due to climate change is critical. The International Union for Conservation of Nature (IUCN) measures the extinction risk of species. Automatic methods have been developed to provide information on the IUCN status of under-assessed taxa. However, these compensatory methods are based on current species characteristics, mainly geographical, which precludes their use in future projections. Here, we evaluate a novel method for classifying the IUCN status of species benefiting from the generalisation power of species distribution models based on deep learning. Our method matches state-of-the-art classification performance while relying on flexible SDM-based features that capture species' environmental preferences. Cross-validation yields average accuracies of 0.61 for status classification and 0.78 for binary classification. Climate change will reshape future species distributions. Under the species-environment equilibrium hypothesis, SDM projections approximate plausible future outcomes. Two extremes of species dispersal capacity are considered: unlimited or null. The projected species distributions are translated into features feeding our IUCN classification method. Finally, trends in threatened species are analysed over time and i) by continent and as a function of average ii) latitude or iii) altitude. The proportion of threatened species is increasing globally, with critical rates in Africa, Asia and South America. Furthermore, the proportion of threatened species is predicted to peak around the two Tropics, at the Equator, in the lowlands and at altitudes of 800-1,500 m.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale",
    "url": "http://arxiv.org/abs/2401.04691v1",
    "authors": [
      "Joaquim Estopinan",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly",
      "Fran\u00e7ois Munoz"
    ],
    "published": "2024-01-09",
    "abstract": "Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution. We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island. Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. The highest level of threat is found at Madagascar and the neighbouring islands. In Sumatra, we found good correspondence of protected areas with our indicators, but supplementing current IUCN assessments with status predictions results in alarming levels of species threat across the island. Recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale. As an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning",
    "url": "http://arxiv.org/abs/2401.02989v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Benjamin Kellenberger",
      "Lloyd Hughes",
      "Devis Tuia"
    ],
    "published": "2024-01-03",
    "abstract": "Species distribution modeling is a highly versatile tool for understanding the intricate relationship between environmental conditions and species occurrences. However, the available data often lacks information on confirmed species absence and is limited to opportunistically sampled, presence-only observations. To overcome this limitation, a common approach is to employ pseudo-absences, which are specific geographic locations designated as negative samples. While pseudo-absences are well-established for single-species distribution models, their application in the context of multi-species neural networks remains underexplored. Notably, the significant class imbalance between species presences and pseudo-absences is often left unaddressed. Moreover, the existence of different types of pseudo-absences (e.g., random and target-group background points) adds complexity to the selection process. Determining the optimal combination of pseudo-absences types is difficult and depends on the characteristics of the data, particularly considering that certain types of pseudo-absences can be used to mitigate geographic biases. In this paper, we demonstrate that these challenges can be effectively tackled by integrating pseudo-absences in the training of multi-species neural networks through modifications to the loss function. This adjustment involves assigning different weights to the distinct terms of the loss function, thereby addressing both the class imbalance and the choice of pseudo-absence types. Additionally, we propose a strategy to set these loss weights using spatial block cross-validation with presence-only data. We evaluate our approach using a benchmark dataset containing independent presence-absence data from six different regions and report improved results when compared to competing approaches.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook",
    "url": "http://arxiv.org/abs/2505.00630v1",
    "authors": [
      "Muyi Bao",
      "Shuchang Lyu",
      "Zhaoyang Xu",
      "Huiyu Zhou",
      "Jinchang Ren",
      "Shiming Xiang",
      "Xiangtai Li",
      "Guangliang Cheng"
    ],
    "published": "2025-05-01",
    "abstract": "Deep learning has profoundly transformed remote sensing, yet prevailing\narchitectures like Convolutional Neural Networks (CNNs) and Vision Transformers\n(ViTs) remain constrained by critical trade-offs: CNNs suffer from limited\nreceptive fields, while ViTs grapple with quadratic computational complexity,\nhindering their scalability for high-resolution remote sensing data. State\nSpace Models (SSMs), particularly the recently proposed Mamba architecture,\nhave emerged as a paradigm-shifting solution, combining linear computational\nscaling with global context modeling. This survey presents a comprehensive\nreview of Mamba-based methodologies in remote sensing, systematically analyzing\nabout 120 studies to construct a holistic taxonomy of innovations and\napplications. Our contributions are structured across five dimensions: (i)\nfoundational principles of vision Mamba architectures, (ii) micro-architectural\nadvancements such as adaptive scan strategies and hybrid SSM formulations,\n(iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids\nand frequency-domain adaptations, (iv) rigorous benchmarking against\nstate-of-the-art methods in multiple application tasks, such as object\ndetection, semantic segmentation, change detection, etc. and (v) critical\nanalysis of unresolved challenges with actionable future directions. By\nbridging the gap between SSM theory and remote sensing practice, this survey\nestablishes Mamba as a transformative framework for remote sensing analysis. To\nour knowledge, this paper is the first systematic review of Mamba architectures\nin remote sensing. Our work provides a structured foundation for advancing\nresearch in remote sensing systems through SSM-based methods. We curate an\nopen-source repository\n(https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster\ncommunity-driven advancements.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Field-scale soil moisture estimated from Sentinel-1 SAR data using a knowledge-guided deep learning approach",
    "url": "http://arxiv.org/abs/2505.00265v1",
    "authors": [
      "Yi Yu",
      "Patrick Filippi",
      "Thomas F. A. Bishop"
    ],
    "published": "2025-05-01",
    "abstract": "Soil moisture (SM) estimation from active microwave data remains challenging\ndue to the complex interactions between radar backscatter and surface\ncharacteristics. While the water cloud model (WCM) provides a semi-physical\napproach for understanding these interactions, its empirical component often\nlimits performance across diverse agricultural landscapes. This research\npresents preliminary efforts for developing a knowledge-guided deep learning\napproach, which integrates WCM principles into a long short-term memory (LSTM)\nmodel, to estimate field SM using Sentinel-1 Synthetic Aperture Radar (SAR)\ndata. Our proposed approach leverages LSTM's capacity to capture spatiotemporal\ndependencies while maintaining physical consistency through a modified\ndual-component loss function, including a WCM-based semi-physical component and\na boundary condition regularisation. The proposed approach is built upon the\nsoil backscatter coefficients isolated from the total backscatter, together\nwith Landsat-resolution vegetation information and surface characteristics. A\nfour-fold spatial cross-validation was performed against in-situ SM data to\nassess the model performance. Results showed the proposed approach reduced SM\nretrieval uncertainties by 0.02 m$^3$/m$^3$ and achieved correlation\ncoefficients (R) of up to 0.64 in areas with varying vegetation cover and\nsurface conditions, demonstrating the potential to address the\nover-simplification in WCM.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LSTM"
=======
    "title": "Robust Geospatial Coordination of Multi-Agent Communications Networks Under Attrition",
    "url": "http://arxiv.org/abs/2512.02079v1",
    "authors": [
      "Jonathan S. Kent",
      "Eliana Stefani",
      "Brian K. Plancher"
    ],
    "published": "2025-11-30",
    "abstract": "Fast, efficient, robust communication during wildfire and other emergency responses is critical. One way to achieve this is by coordinating swarms of autonomous aerial vehicles carrying communications equipment to form an ad-hoc network connecting emergency response personnel to both each other and central command. However, operating in such extreme environments may lead to individual networking agents being damaged or rendered inoperable, which could bring down the network and interrupt communications.\n  To overcome this challenge and enable multi-agent UAV networking in difficult environments, this paper introduces and formalizes the problem of Robust Task Networking Under Attrition (RTNUA), which extends connectivity maintenance in multi-robot systems to explicitly address proactive redundancy and attrition recovery. We introduce Physics-Informed Robust Employment of Multi-Agent Networks ($\u03a6$IREMAN), a topological algorithm leveraging physics-inspired potential fields to solve this problem. Through simulation across 25 problem configurations, $\u03a6$IREMAN consistently outperforms the DCCRS baseline, and on large-scale problems with up to 100 tasks and 500 drones, maintains $>99.9\\%$ task uptime despite substantial attrition, demonstrating both effectiveness and scalability.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "PINN"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2504.21491v1",
    "authors": [
      "Qinfeng Zhu",
      "Yunxi Jiang",
      "Lei Fan"
    ],
    "published": "2025-04-30",
    "abstract": "We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping",
    "url": "http://arxiv.org/abs/2504.21194v1",
    "authors": [
      "Vedika Srivastava",
      "Hemant Kumar Singh",
      "Jaisal Singh"
    ],
    "published": "2025-04-29",
    "abstract": "This paper presents a novel approach to geolocating images captured from the\nInternational Space Station (ISS) using advanced machine learning algorithms.\nDespite having precise ISS coordinates, the specific Earth locations depicted\nin astronaut-taken photographs often remain unidentified. Our research\naddresses this gap by employing three distinct image processing pipelines: a\nNeural Network based approach, a SIFT based method, and GPT-4 model. Each\npipeline is tailored to process high-resolution ISS imagery, identifying both\nnatural and man-made geographical features. Through extensive evaluation on a\ndiverse dataset of over 140 ISS images, our methods demonstrate significant\npromise in automated geolocation with varied levels of success. The NN approach\nshowed a high success rate in accurately matching geographical features, while\nthe SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided\nenriched geographical descriptions alongside location predictions. This\nresearch contributes to the fields of remote sensing and Earth observation by\nenhancing the accuracy and efficiency of geolocating space-based imagery,\nthereby aiding environmental monitoring and global mapping efforts.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery",
    "url": "http://arxiv.org/abs/2504.19996v1",
    "authors": [
      "Andreas Kalogeras",
      "Dimitrios Bormpoudakis",
      "Iason Tsardanidis",
      "Dimitra A. Loka",
      "Charalampos Kontoes"
    ],
    "published": "2025-04-28",
    "abstract": "The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data",
    "url": "http://arxiv.org/abs/2504.19991v1",
    "authors": [
      "Ioannis Kontogiorgakis",
      "Iason Tsardanidis",
      "Dimitrios Bormpoudakis",
      "Ilias Tsoumas",
      "Dimitra A. Loka",
      "Christos Noulas",
      "Alexandros Tsitouras",
      "Charalampos Kontoes"
    ],
    "published": "2025-04-28",
    "abstract": "Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as commonly rely on on-ground\nfield surveys, which are often costly, time-consuming and subject to delays. In\norder to tackle this problem, we leverage Earth Observation (EO) data and\nMachine Learning (ML). Specifically, we developed an ML approach for mapping\nfour distinct weed management methods (Mowing, Tillage, Chemical-spraying, and\nNo practice) in orchards using satellite image time series (SITS) data from two\ndifferent sources: Sentinel-2 (S2) and PlanetScope (PS). The findings\ndemonstrate the potential of ML-driven remote sensing to enhance the efficiency\nand accuracy of weed management mapping in orchards.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype",
    "url": "http://arxiv.org/abs/2504.19074v1",
    "authors": [
      "Anyong Qin",
      "Chaoqi Yuan",
      "Qiang Li",
      "Feng Yang",
      "Tiecheng Song",
      "Chenqiang Gao"
    ],
    "published": "2025-04-27",
    "abstract": "Convolutional neural networks (CNNs) are effective for hyperspectral image\n(HSI) classification, but their 3D convolutional structures introduce high\ncomputational costs and limited generalization in few-shot scenarios. Domain\nshifts caused by sensor differences and environmental variations further hinder\ncross-dataset adaptability. Metric-based few-shot learning (FSL) prototype\nnetworks mitigate this problem, yet their performance is sensitive to prototype\nquality, especially with limited samples. To overcome these challenges, a\ndual-branch residual network that integrates spatial and spectral features via\nparallel branches is proposed in this letter. Additionally, more robust refined\nprototypes are obtained through a regulation term. Furthermore, a kernel\nprobability matching strategy aligns source and target domain features,\nalleviating domain shift. Experiments on four publicly available HSI datasets\nillustrate that the proposal achieves superior performance compared to other\nmethods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Evaluating AI-Driven Automated Map Digitization in QGIS",
    "url": "http://arxiv.org/abs/2504.18777v1",
    "authors": [
      "Diana Febrita"
    ],
    "published": "2025-04-26",
    "abstract": "Map digitization is an important process that converts maps into digital\nformats that can be used for further analysis. This process typically requires\na deep human involvement because of the need for interpretation and\ndecision-making when translating complex features. With the advancement of\nartificial intelligence, there is an alternative to conducting map digitization\nwith the help of machine learning techniques. Deepness, or Deep Neural Remote\nSensing, is an advanced AI-driven tool designed and integrated as a plugin in\nQGIS application. This research focuses on assessing the effectiveness of\nDeepness in automated digitization. This study analyses AI-generated\ndigitization results from Google Earth imagery and compares them with digitized\noutputs from OpenStreetMap (OSM) to evaluate performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network",
    "url": "http://arxiv.org/abs/2505.00495v1",
    "authors": [
      "Nguyen Van Thanh",
      "Nguyen Dang Huynh",
      "Nguyen Ngoc Tan",
      "Nguyen Thai Minh",
      "Nguyen Nam Hoang"
    ],
    "published": "2025-05-01",
    "abstract": "A storm is a type of extreme weather. Therefore, forecasting the path of a\nstorm is extremely important for protecting human life and property. However,\nstorm forecasting is very challenging because storm trajectories frequently\nchange. In this study, we propose an improved deep learning method using a\nTransformer network to predict the movement trajectory of a storm over the next\n6 hours. The storm data used to train the model was obtained from the National\nOceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that\nthe proposed method is more accurate than traditional methods. Moreover, the\nproposed method is faster and more cost-effective",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AI-Enhanced Automatic Design of Efficient Underwater Gliders",
    "url": "http://arxiv.org/abs/2505.00222v1",
    "authors": [
      "Peter Yichen Chen",
      "Pingchuan Ma",
      "Niklas Hagemann",
      "John Romanishin",
      "Wei Wang",
      "Daniela Rus",
      "Wojciech Matusik"
    ],
    "published": "2025-04-30",
    "abstract": "The development of novel autonomous underwater gliders has been hindered by\nlimited shape diversity, primarily due to the reliance on traditional design\ntools that depend heavily on manual trial and error. Building an automated\ndesign framework is challenging due to the complexities of representing glider\nshapes and the high computational costs associated with modeling complex\nsolid-fluid interactions. In this work, we introduce an AI-enhanced automated\ncomputational framework designed to overcome these limitations by enabling the\ncreation of underwater robots with non-trivial hull shapes. Our approach\ninvolves an algorithm that co-optimizes both shape and control signals,\nutilizing a reduced-order geometry representation and a differentiable\nneural-network-based fluid surrogate model. This end-to-end design workflow\nfacilitates rapid iteration and evaluation of hydrodynamic performance, leading\nto the discovery of optimal and complex hull shapes across various control\nsettings. We validate our method through wind tunnel experiments and swimming\npool gliding tests, demonstrating that our computationally designed gliders\nsurpass manually designed counterparts in terms of energy efficiency. By\naddressing challenges in efficient shape representation and neural fluid\nsurrogate models, our work paves the way for the development of highly\nefficient underwater gliders, with implications for long-range ocean\nexploration and environmental monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves",
    "url": "http://arxiv.org/abs/2504.21195v1",
    "authors": [
      "Kelsey E. Ennis",
      "Elizabeth A. Barnes",
      "Marybeth C. Arcodia",
      "Martin A. Fernandez",
      "Eric D. Maloney"
    ],
    "published": "2025-04-29",
    "abstract": "Extreme heat is the deadliest weather-related hazard in the United States.\nFurthermore, it is increasing in intensity, frequency, and duration, making\nskillful forecasts vital to protecting life and property. Traditional numerical\nweather prediction (NWP) models struggle with extreme heat for medium-range and\nsubseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial\nintelligence-based weather prediction (AIWP) models are progressing rapidly.\nHowever, it is largely unknown how well AIWP models forecast extremes,\nespecially for medium-range and S2S timescales. This study investigates 2-m\ntemperature forecasts for 60 heat waves across the four boreal seasons and over\nfour CONUS regions at lead times up to 20 days, using two AIWP models (Google\nGraphCast and Pangu-Weather) and one traditional NWP model (NOAA United\nForecast System Global Ensemble Forecast System (UFS GEFS)). First, case study\nanalyses show that both AIWP models and the UFS GEFS exhibit consistent cold\nbiases on regional scales in the 5-10 days of lead time before heat wave onset.\nGraphCast is the more skillful AIWP model, outperforming UFS GEFS and\nPangu-Weather in most locations. Next, the two AIWP models are isolated and\nanalyzed across all heat waves and seasons, with events split among the model's\ntesting (2018-2023) and training (1979-2017) periods. There are cold biases\nbefore and during the heat waves in both models and all seasons, except\nPangu-Weather in winter, which exhibits a mean warm bias before heat wave\nonset. Overall, results offer encouragement that AIWP models may be useful for\nmedium-range and S2S predictability of extreme heat.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning (ML)-Physics Fusion Model Outperforms Both Physics-Only and ML-Only Models in Typhoon Predictions",
    "url": "http://arxiv.org/abs/2504.20852v1",
    "authors": [
      "Zeyi Niu",
      "Wei Huang",
      "Hao Li",
      "Xuliang Fan",
      "Yuhua Yang",
      "Mengqi Yang",
      "Bo Qin"
    ],
    "published": "2025-04-29",
    "abstract": "Data-driven machine learning (ML) models, such as FuXi, exhibit notable\nlimitations in forecasting typhoon intensity and structure. This study presents\na comprehensive evaluation of FuXi-SHTM, a hybrid ML-physics model, using all\n2024 western North Pacific typhoon cases. The FuXi-SHTM hybrid demonstrates\nclear improvements in both track and intensity forecasts compared to the\nstandalone SHTM, FuXi, and ECMWF HRES models. Compared to FuXi alone, FuXi-SHTM\nreduces typhoon track forecast errors by 16.5% and 5.2% at lead times of 72 h\nand 120 h, respectively, and reduces intensity forecast errors by 59.7% and\n47.6%. Furthermore, FuXi-SHTM simulates cloud structures more realistically\ncompared to SHTM, and achieves superior representation of the 10-m wind fields\nin both intensity and spatial structure compared to FuXi and SHTM. Increasing\nthe resolution of FuXi-SHTM from 9 km to 3 km further enhances intensity\nforecasts, highlighting the critical role of the resolution of the physical\nmodel in advancing hybrid forecasting capabilities.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Quantifying the Influence of Climate on Storm Activity Using Machine Learning",
    "url": "http://arxiv.org/abs/2504.20521v1",
    "authors": [
      "Or Hadas",
      "Yohai Kaspi"
    ],
    "published": "2025-04-29",
    "abstract": "Extratropical storms shape midlatitude weather and vary due to both the\nslowly evolving climate and the rapid changes in synoptic conditions. While the\ninfluence of each factor has been studied extensively, their relative\nimportance remains unclear. Here, we quantify the climate's relative importance\nin both mean storm activity and individual storm development using 84 years of\nERA-5 data and Convolutional Neural Networks (CNN). We find that the\nconstructed CNN model predicts more than 90% of the variability in the mean\nstorm activity. However, a similar model predicts up to a third of the\nvariability in individual storm features, such as intensity, growth time, and\nstorm trajectory, showing their variability is dominated by synoptic\nconditions. Isolating present-day climate change yields a signal-to-noise ratio\n(SNR) of about 0.16% for storm-intensity attribution, whereas the SNR for heat\nanomalies is ~50 times higher, highlighting that focusing on variables more\ndirectly tied to warming provides a clearer attribution pathway.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": []
  },
  {
    "title": "Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model",
    "url": "http://arxiv.org/abs/2504.20238v1",
    "authors": [
      "P. Trent Vonich",
      "Gregory J. Hakim"
    ],
    "published": "2025-04-28",
    "abstract": "Atmospheric predictability research has long held that the limit of skillful\ndeterministic weather forecasts is about 14 days. We challenge this limit using\nGraphCast, a machine-learning weather model, by optimizing forecast initial\nconditions using gradient-based techniques for twice-daily forecasts spanning\n2020. This approach yields an average error reduction of 86% at 10 days, with\nskill lasting beyond 30 days. Mean optimal initial-condition perturbations\nreveal large-scale, spatially coherent corrections to ERA5, primarily\nreflecting an intensification of the Hadley circulation. Forecasts using\nGraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21%\nerror reduction, peaking at 4 days, indicating that analysis corrections\nreflect a combination of both model bias and a reduction in analysis error.\nThese results demonstrate that, given accurate initial conditions, skillful\ndeterministic forecasts are consistently achievable far beyond two weeks,\nchallenging long-standing assumptions about the limits of atmospheric\npredictability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States",
    "url": "http://arxiv.org/abs/2504.20129v1",
    "authors": [
      "Arun M. Saranathan",
      "Mahmoud Saeedimoghaddam",
      "Brandon Smith",
      "Deepthi Raghunandan",
      "Grey Nearing",
      "Craig Pelissier"
    ],
    "published": "2025-04-28",
    "abstract": "Snow is an essential input for various land surface models. Seasonal snow\nestimates are available as snow water equivalent (SWE) from process-based\nreanalysis products or locally from in situ measurements. While the reanalysis\nproducts are computationally expensive and available at only fixed spatial and\ntemporal resolutions, the in situ measurements are highly localized and sparse.\nTo address these issues and enable the analysis of the effect of a large suite\nof physical, morphological, and geological conditions on the presence and\namount of snow, we build a Long Short-Term Memory (LSTM) network, which is able\nto estimate the SWE based on time series input of the various\nphysical/meteorological factors as well static spatial/morphological factors.\nSpecifically, this model breaks down the SWE estimation into two separate\ntasks: (i) a classification task that indicates the presence/absence of snow on\na specific day and (ii) a regression task that indicates the height of the SWE\non a specific day in the case of snow presence. The model is trained using\nphysical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows\nin the western United States. We will show that trained LSTM models have a\nclassification accuracy of $\\geq 93\\%$ for the presence of snow and a\ncoefficient of correlation of $\\sim 0.9$ concerning their SWE estimates. We\nwill also demonstrate that the models can generalize both spatially and\ntemporally to previously unseen data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Digital Twin-based Out-of-Distribution Detection in Autonomous Vessels",
    "url": "http://arxiv.org/abs/2504.19816v1",
    "authors": [
      "Erblin Isaku",
      "Hassan Sartaj",
      "Shaukat Ali"
    ],
    "published": "2025-04-28",
    "abstract": "An autonomous vessel (AV) is a complex cyber-physical system (CPS) with\nsoftware enabling many key functionalities, e.g., navigation software enables\nan AV to autonomously or semi-autonomously follow a path to its destination.\nDigital twins of such AVs enable advanced functionalities such as running\nwhat-if scenarios, performing predictive maintenance, and enabling fault\ndiagnosis. Due to technological improvements, real-time analyses using\ncontinuous data from vessels' real-time operations have become increasingly\npossible. However, the literature has little explored developing advanced\nanalyses in real-time data in AVs with digital twins built with machine\nlearning techniques. To this end, we present a novel digital twin-based\napproach (ODDIT) to detect future out-of-distribution (OOD) states of an AV\nbefore reaching them, enabling proactive intervention. Such states may indicate\nanomalies requiring attention (e.g., manual correction by the ship master) and\nassist testers in scenario-centered testing. The digital twin consists of two\nmachine-learning models predicting future vessel states and whether the\npredicted state will be OOD. We evaluated ODDIT with five vessels across\nwaypoint and zigzag maneuvering under simulated conditions, including sensor\nand actuator noise and environmental disturbances i.e., ocean current. ODDIT\nachieved high accuracy in detecting OOD states, with AUROC and TNR@TPR95 scores\nreaching 99\\% across multiple vessels.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics",
    "url": "http://arxiv.org/abs/2504.19066v1",
    "authors": [
      "Deeksha Varshney",
      "Keane Ong",
      "Rui Mao",
      "Erik Cambria",
      "Gianmarco Mengaldo"
    ],
    "published": "2025-04-27",
    "abstract": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Exploring the Potential of Latent Embeddings for Sea Ice Characterization using ICESat-2 Data",
    "url": "http://arxiv.org/abs/2504.18668v1",
    "authors": [
      "Daehyeon Han",
      "Morteza Karimzadeh"
    ],
    "published": "2025-04-25",
    "abstract": "The Ice, Cloud, and Elevation Satellite-2 (ICESat-2) provides high-resolution\nmeasurements of sea ice height. Recent studies have developed machine learning\nmethods on ICESat-2 data, primarily focusing on surface type classification.\nHowever, the heavy reliance on manually collected labels requires significant\ntime and effort for supervised learning, as it involves cross-referencing track\nmeasurements with overlapping background optical imagery. Additionally, the\ncoincidence of ICESat-2 tracks with background images is relatively rare due to\nthe different overpass patterns and atmospheric conditions. To address these\nlimitations, this study explores the potential of unsupervised autoencoder on\nunlabeled data to derive latent embeddings. We develop autoencoder models based\non Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) to\nreconstruct topographic sequences from ICESat-2 and derive embeddings. We then\napply Uniform Manifold Approximation and Projection (UMAP) to reduce dimensions\nand visualize the embeddings. Our results show that embeddings from\nautoencoders preserve the overall structure but generate relatively more\ncompact clusters compared to the original ICESat-2 data, indicating the\npotential of embeddings to lessen the number of required labels samples.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM",
      "Autoencoder"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation",
    "url": "http://arxiv.org/abs/2504.18181v1",
    "authors": [
      "Yvonne Jenniges",
      "Maike Sonnewald",
      "Sebastian Maneth",
      "Are Olsen",
      "Boris P. Koch"
    ],
    "published": "2025-04-25",
    "abstract": "Defining ocean regions and water masses helps to understand marine processes\nand can serve downstream-tasks such as defining marine protected areas.\nHowever, such definitions are often a result of subjective decisions\npotentially producing misleading, unreproducible results. Here, the aim was to\nobjectively define regions of the North Atlantic. For this, a data-driven,\nsystematic machine learning approach was applied to generate and validate ocean\nclusters employing external, internal and relative validation techniques. About\n300 million measured salinity, temperature, and oxygen, nitrate, phosphate and\nsilicate concentration values served as input for various clustering methods\n(KMeans, agglomerative Ward, and Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN)). Uniform Manifold Approximation and\nProjection (UMAP) emphasised (dis-)similarities in the data while reducing\ndimensionality. Based on a systematic validation of the considered clustering\nmethods and their hyperparameters, the results showed that UMAP-DBSCAN best\nrepresented the data. To address stochastic variability, 100 UMAP-DBSCAN\nclustering runs were conducted and aggregated using Native Emergent Manifold\nInterrogation (NEMI), producing a final set of 321 clusters. Reproducibility\nwas evaluated by calculating the ensemble overlap (88.81 +- 1.8%) and the mean\ngrid cell-wise uncertainty estimated by NEMI (15.49 +- 20%). The presented\nclustering results agreed very well with common water mass definitions. This\nstudy revealed a more detailed regionalization compared to previous concepts\nsuch as the Longhurst provinces. The applied method is objective, efficient and\nreproducible and will support future research focusing on biogeochemical\ndifferences and changes in oceanic regions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes",
    "url": "http://arxiv.org/abs/2504.20303v1",
    "authors": [
      "Junlin Guo",
      "James R. Zimmer-Dauphinee",
      "Jordan M. Nieusma",
      "Siqi Lu",
      "Quan Liu",
      "Ruining Deng",
      "Can Cui",
      "Jialin Yue",
      "Yizhe Lin",
      "Tianyuan Yao",
      "Juming Xiong",
      "Junchao Zhu",
      "Chongyu Qu",
      "Yuechen Yang",
      "Mitchell Wilkes",
      "Xiao Wang",
      "Parker VanValkenburgh",
      "Steven A. Wernke",
      "Yuankai Huo"
    ],
    "published": "2025-04-28",
    "abstract": "By mapping sites at large scales using remotely sensed data, archaeologists\ncan generate unique insights into long-term demographic trends, inter-regional\nsocial networks, and past adaptations to climate change. Remote sensing surveys\ncomplement field-based approaches, and their reach can be especially great when\ncombined with deep learning and computer vision techniques. However,\nconventional supervised deep learning methods face challenges in annotating\nfine-grained archaeological features at scale. While recent vision foundation\nmodels have shown remarkable success in learning large-scale remote sensing\ndata with minimal annotations, most off-the-shelf solutions are designed for\nRGB images rather than multi-spectral satellite imagery, such as the 8-band\ndata used in our study. In this paper, we introduce DeepAndes, a\ntransformer-based vision foundation model trained on three million\nmulti-spectral satellite images, specifically tailored for Andean archaeology.\nDeepAndes incorporates a customized DINOv2 self-supervised learning algorithm\noptimized for 8-band multi-spectral imagery, marking the first foundation model\ndesigned explicitly for the Andes region. We evaluate its image understanding\nperformance through imbalanced image classification, image instance retrieval,\nand pixel-level semantic segmentation tasks. Our experiments show that\nDeepAndes achieves superior F1 scores, mean average precision, and Dice scores\nin few-shot learning scenarios, significantly outperforming models trained from\nscratch or pre-trained on smaller datasets. This underscores the effectiveness\nof large-scale self-supervised pre-training in archaeological remote sensing.\nCodes will be available on https://github.com/geopacha/DeepAndes.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach",
    "url": "http://arxiv.org/abs/2505.03299v1",
    "authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "St\u00e9phane May",
      "S\u00e9bastien Lef\u00e8vre"
    ],
    "published": "2025-05-06",
    "abstract": "Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery",
    "url": "http://arxiv.org/abs/2505.02829v1",
    "authors": [
      "Jerome Quenum",
      "Wen-Han Hsieh",
      "Tsung-Han Wu",
      "Ritwik Gupta",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published": "2025-05-05",
    "abstract": "Segmentation models can recognize a pre-defined set of objects in images.\nHowever, models that can reason over complex user queries that implicitly refer\nto multiple objects of interest are still in their infancy. Recent advances in\nreasoning segmentation--generating segmentation masks from complex, implicit\nquery text--demonstrate that vision-language models can operate across an open\ndomain and produce reasonable outputs. However, our experiments show that such\nmodels struggle with complex remote-sensing imagery. In this work, we introduce\nLISAt, a vision-language model designed to describe complex remote-sensing\nscenes, answer questions about them, and segment objects of interest. We\ntrained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,\nwith 27,615 annotations over 9,205 images, and a multimodal pretraining\ndataset, PreGRES, containing over 1 million question-answer pairs. LISAt\noutperforms existing geospatial foundation models such as RS-GPT4V by over\n10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses\nstate-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %\n(gIoU). Our model, datasets, and code are available at\nhttps://lisat-bair.github.io/LISAt/",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation",
    "url": "http://arxiv.org/abs/2505.03844v1",
    "authors": [
      "Sol\u00e8ne Debuys\u00e8re",
      "Nicolas Trouv\u00e9",
      "Nathan Letheule",
      "Olivier L\u00e9v\u00eaque",
      "Elise Colin"
    ],
    "published": "2025-05-05",
    "abstract": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has\nincreased considerably in recent years, with datasets commercially available.\nHowever, the acquisition of high-resolution SAR images in airborne\nconfigurations, remains costly and limited. Thus, the lack of open source,\nwell-labeled, or easily exploitable SAR text-image datasets is a barrier to the\nuse of existing foundation models in remote sensing applications. In this\ncontext, synthetic image generation is a promising solution to augment this\nscarce data, enabling a broader range of applications. Leveraging over 15 years\nof ONERA's extensive archival airborn data from acquisition campaigns, we\ncreated a comprehensive training dataset of 110 thousands SAR images to exploit\na 3.5 billion parameters pre-trained latent diffusion model. In this work, we\npresent a novel approach utilizing spatial conditioning techniques within a\nfoundation model to transform satellite SAR imagery into airborne SAR\nrepresentations. Additionally, we demonstrate that our pipeline is effective\nfor bridging the realism of simulated images generated by ONERA's physics-based\nsimulator EMPRISE. Our method explores a key application of AI in advancing SAR\nimaging technology. To the best of our knowledge, we are the first to introduce\nthis approach in the literature.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "url": "http://arxiv.org/abs/2505.03368v1",
    "authors": [
      "Stef De Sabbata",
      "Stefano Mizzaro",
      "Kevin Roitero"
    ],
    "published": "2025-05-06",
    "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder",
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling",
    "url": "http://arxiv.org/abs/2505.04802v1",
    "authors": [
      "Xiao Wang",
      "Jong-Youl Choi",
      "Takuya Kurihaya",
      "Isaac Lyngaas",
      "Hong-Jun Yoon",
      "Ming Fan",
      "Nasik Muhammad Nafi",
      "Aristeidis Tsaris",
      "Ashwin M. Aji",
      "Maliha Hossain",
      "Mohamed Wahib",
      "Dali Wang",
      "Peter Thornton",
      "Prasanna Balaprakash",
      "Moetasim Ashfaq",
      "Dan Lu"
    ],
    "published": "2025-05-07",
    "abstract": "Sparse observations and coarse-resolution climate models limit effective\nregional decision-making, underscoring the need for robust downscaling.\nHowever, existing AI methods struggle with generalization across variables and\ngeographies and are constrained by the quadratic complexity of Vision\nTransformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation\nmodel for global, hyper-resolution climate downscaling. ORBIT-2 incorporates\ntwo key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture\nwith residual learning and Bayesian regularization for efficient, robust\nprediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces\nself-attention complexity from quadratic to linear, enabling long-sequence\nprocessing and massive parallelism. ORBIT-2 scales to 10 billion parameters\nacross 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and\n92-98% strong scaling efficiency. It supports downscaling to 0.9 km global\nresolution and processes sequences up to 4.2 billion tokens. On 7 km resolution\nbenchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98\nto 0.99 against observation data.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery",
    "url": "http://arxiv.org/abs/2505.05321v1",
    "authors": [
      "Chintan B. Maniyar",
      "Minakshi Kumar",
      "Gengchen Mai"
    ],
    "published": "2025-05-08",
    "abstract": "Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks",
    "url": "http://arxiv.org/abs/2505.03522v1",
    "authors": [
      "Haotong Cheng",
      "Zhiqi Zhang",
      "Hao Li",
      "Xinshang Zhang"
    ],
    "published": "2025-05-06",
    "abstract": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning",
    "url": "http://arxiv.org/abs/2505.03327v1",
    "authors": [
      "Jos\u00e9-Luis Bueso-Bello",
      "Benjamin Chauvel",
      "Daniel Carcereri",
      "Philipp Posovszky",
      "Pietro Milillo",
      "Jennifer Ruiz",
      "Juan-Carlos Fern\u00e1ndez-Diaz",
      "Carolina Gonz\u00e1lez",
      "Michele Martone",
      "Ronny H\u00e4nsch",
      "Paola Rizzoli"
    ],
    "published": "2025-05-06",
    "abstract": "Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Sampling Kantorovich operators for speckle noise reduction using a Down-Up scaling approach and gap filling in remote sensing images",
    "url": "http://arxiv.org/abs/2505.02422v1",
    "authors": [
      "Danilo Costarelli",
      "Mariarosaria Natale"
    ],
    "published": "2025-05-05",
    "abstract": "In the literature, several approaches have been proposed for restoring and\nenhancing remote sensing images, including methods based on interpolation,\nfiltering, and deep learning. In this paper, we investigate the application of\nmultivariate sampling Kantorovich (SK) operators for image reconstruction, with\na particular focus on gap filling and speckle noise reduction. To understand\nthe accuracy performances of the proposed algorithms, we first derive a\nquantitative estimate in $C(\\R^n)$ for the error of approximation using the\nEuler-Maclaurin summation formula, which provides sharper error bounds under\nminimal regularity conditions. We also establish a convergence result and a\nquantitative estimate with respect to the dissimilarity index measured by the\ncontinuous SSIM for functions in Lebesgue spaces. Additionally, we prove a\nmultidimensional linear prediction result, which is used to design a new\nSK-based reconstruction algorithm to handle missing data, that we call LP-SK\nalgorithm. To address speckle noise, we integrate SK operators into a newly\nproposed Down-Up scaling approach. Numerical tests are presented on synthetic\nand real SAR images to validate the proposed methods. Performance is assessed\nusing similarity metrics such as SSIM and PSNR, along with speckle-specific\nindexes. Comparative analysis with state-of-the-art techniques highlights the\neffectiveness of the proposed approaches.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks",
    "url": "http://arxiv.org/abs/2505.03034v1",
    "authors": [
      "Sweta Rai",
      "Douglas W. Nychka",
      "Soutir Bandyopadhyay"
    ],
    "published": "2025-05-05",
    "abstract": "Data derived from remote sensing or numerical simulations often have a\nregular gridded structure and are large in volume, making it challenging to\nfind accurate spatial models that can fill in missing grid cells or simulate\nthe process effectively, especially in the presence of spatial heterogeneity\nand heavy-tailed marginal distributions. To overcome this issue, we present a\nspatial autoregressive modeling framework, which maps observations at a\nlocation and its neighbors to independent random variables. This is a highly\nflexible modeling approach and well-suited for non-Gaussian fields, providing\nsimpler interpretability. In particular, we consider the SAR model with\nGeneralized Extreme Value distribution innovations to combine the observation\nat a central grid location with its neighbors, capturing extreme spatial\nbehavior based on the heavy-tailed innovations. While these models are fast to\nsimulate by exploiting the sparsity of the key matrices in the computations,\nthe maximum likelihood estimation of the parameters is prohibitive due to the\nintractability of the likelihood, making optimization challenging. To overcome\nthis, we train a convolutional neural network on a large training set that\ncovers a useful parameter space, and then use the trained network for fast\nparameter estimation. Finally, we apply this model to analyze annual maximum\nprecipitation data from ERA-Interim-driven Weather Research and Forecasting\n(WRF) simulations, allowing us to explore its spatial extreme behavior across\nNorth America.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Reduced Cloud Cover Errors in a Hybrid AI-Climate Model Through Equation Discovery And Automatic Tuning",
    "url": "http://arxiv.org/abs/2505.04358v1",
    "authors": [
      "Arthur Grundner",
      "Tom Beucler",
      "Julien Savre",
      "Axel Lauer",
      "Manuel Schlund",
      "Veronika Eyring"
    ],
    "published": "2025-05-07",
    "abstract": "Climate models rely on parameterizations that account for the effects of\nsmall-scale processes on large-scale dynamics. Particularly cloud-related\nparameterizations remain a major source of uncertainty in climate projections.\nWhile hybrid Earth system models (ESMs) with machine learning-based\nparameterizations could improve current ESMs, deep learning approaches often\nlack interpretability, physical consistency, and computational efficiency.\nFurthermore, most data-driven parameterizations are trained in a stand-alone\nfashion and fail within ESMs, partly due to the difficulty of tuning the ESM to\naccommodate new, non-traditional schemes. In this study, we introduce a novel\ntwo-step pipeline for improving a climate model with data-driven\nparameterizations. First, we incorporate a physically consistent, data-driven\ncloud cover parameterization into the ICON global atmospheric model. The\nparameterization, a diagnostic equation derived from storm-resolving\nsimulations via symbolic regression, retains the interpretability and\nefficiency of traditional parameterizations while improving fidelity. Second,\nwe introduce an automated, gradient-free tuning procedure to recalibrate the\nnew climate model with Earth observations. We employ the Nelder-Mead algorithm\nand progressively increase simulation length, making our approach simple,\ncomputationally efficient, and easily extendable to other ESMs. The tuned\nhybrid model significantly reduces some long-standing biases in cloud cover and\nradiative budgets, particularly over regions such as the Southern Ocean and the\nsubtropical stratocumulus regions. Moreover, it remains robust under +4K\nsurface warming. Our results highlight the potential of data-driven\nparameterizations when combined with model tuning. This framework offers an\nautomatic, efficient and practical approach to enhancing climate projections\nwithout losing performance or interpretability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Big Data Architecture for Large Organizations",
    "url": "http://arxiv.org/abs/2505.04717v1",
    "authors": [
      "Fathima Nuzla Ismail",
      "Abira Sengupta",
      "Shanika Amarasoma"
    ],
    "published": "2025-05-07",
    "abstract": "The exponential growth of big data has transformed how large organisations\nleverage information to drive innovation, optimise processes, and maintain\ncompetitive advantages. However, managing and extracting insights from vast,\nheterogeneous data sources requires a scalable, secure, and well-integrated big\ndata architecture. This paper proposes a comprehensive big data framework that\naligns with organisational objectives while ensuring flexibility, scalability,\nand governance. The architecture encompasses multiple layers, including data\ningestion, transformation, storage, analytics, machine learning, and security,\nincorporating emerging technologies such as Generative AI (GenAI) and low-code\nmachine learning. Cloud-based implementations across Google Cloud, AWS, and\nMicrosoft Azure are analysed, highlighting their tools and capabilities.\nAdditionally, this study explores advancements in big data architecture,\nincluding AI-driven automation, data mesh, and Data Ocean paradigms. By\nestablishing a structured, adaptable framework, this research provides a\nfoundational blueprint for large organisations to harness big data as a\nstrategic asset effectively.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast",
    "url": "http://arxiv.org/abs/2505.04396v1",
    "authors": [
      "Jingnan Wang",
      "Jie Chao",
      "Shangshang Yang",
      "Congyi Nai",
      "Kaijun Ren",
      "Kefeng Deng",
      "Xi Chen",
      "Yaxin Liu",
      "Hanqiuzi Wen",
      "Ziniu Xiao",
      "Lifeng Zhang",
      "Xiaodong Wang",
      "Jiping Guan",
      "Baoxiang Pan"
    ],
    "published": "2025-05-07",
    "abstract": "The planning and operation of renewable energy, especially wind power, depend\ncrucially on accurate, timely, and high-resolution weather information.\nCoarse-grid global numerical weather forecasts are typically downscaled to meet\nthese requirements, introducing challenges of scale inconsistency, process\nrepresentation error, computation cost, and entanglement of distinct\nuncertainty sources from chaoticity, model bias, and large-scale forcing. We\naddress these challenges by learning the climatological distribution of a\ntarget wind farm using its high-resolution numerical weather simulations. An\noptimal combination of this learned high-resolution climatological prior with\ncoarse-grid large scale forecasts yields highly accurate, fine-grained,\nfull-variable, large ensemble of weather pattern forecasts. Using observed\nmeteorological records and wind turbine power outputs as references, the\nproposed methodology verifies advantageously compared to existing\nnumerical/statistical forecasting-downscaling pipelines, regarding either\ndeterministic/probabilistic skills or economic gains. Moreover, a 100-member,\n10-day forecast with spatial resolution of 1 km and output frequency of 15 min\ntakes < 1 hour on a moderate-end GPU, as contrast to $\\mathcal{O}(10^3)$ CPU\nhours for conventional numerical simulation. By drastically reducing\ncomputational costs while maintaining accuracy, our method paves the way for\nmore efficient and reliable renewable energy planning and operation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Parameter estimation for land-surface models using machine learning libraries",
    "url": "http://arxiv.org/abs/2505.02979v1",
    "authors": [
      "Ruiyue Huang",
      "Claire E. Heaney",
      "Maarten van Reeuwijk"
    ],
    "published": "2025-05-05",
    "abstract": "The Neural Networks for Partial Differential Equations (NN4PDEs) approach is\nused to determine the parameters of a simple land-surface model using PyTorch's\nbackpropagation engine. In order to test the inverse model, a synthetic dataset\nis created by running the model in forward mode with known parameter values to\ncreate soil temperature time series that can be used as observations for the\ninverse model. We show that it is not possible to obtain a reliable parameter\nestimation using a single observed soil temperature time series. Using\nmeasurements at two depths, reliable parameter estimates can be obtained\nalthough it is not possible to differentiate between latent and sensible heat\nfluxes. We apply the inverse model to urban flux tower data in Phoenix, United\nStates, and show that the thermal conductivity, volumetric heat capacity, and\nthe combined sensible-latent heat transfer coefficient can be reliably\nestimated using an observed value for the effective surface albedo. The\nresulting model accurately predicts the outgoing longwave radiation, conductive\nsoil fluxes and the combined sensible-latent heat fluxes.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2505.08723v1",
    "authors": [
      "Xiaolei Qin",
      "Di Wang",
      "Jing Zhang",
      "Fengxiang Wang",
      "Xin Su",
      "Bo Du",
      "Liangpei Zhang"
    ],
    "published": "2025-05-13",
    "abstract": "Satellite image time series (SITS) provide continuous observations of the\nEarth's surface, making them essential for applications such as environmental\nmanagement and disaster assessment. However, existing spatiotemporal foundation\nmodels rely on plain vision transformers, which encode entire temporal\nsequences without explicitly capturing multiscale spatiotemporal relationships\nbetween land objects. This limitation hinders their effectiveness in downstream\ntasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision\ntransformer foundation model tailored for SITS analysis. At its core, we\nintroduce a spatiotemporal gyroscope attention mechanism that dynamically\ncaptures evolving multiscale patterns across both time and space. For\npre-training, we curate MillionST, a large-scale dataset of one million images\nfrom 100,000 geographic locations, each captured across 10 temporal phases over\nfive years, encompassing diverse geospatial changes and seasonal variations.\nLeveraging this dataset, we adapt masked image modeling to pre-train TiMo,\nenabling it to effectively learn and encode generalizable spatiotemporal\nrepresentations.Extensive experiments across multiple spatiotemporal\ntasks-including deforestation monitoring, land cover segmentation, crop type\nclassification, and flood detection-demonstrate TiMo's superiority over\nstate-of-the-art methods. Code, model, and dataset will be released at\nhttps://github.com/MiliLab/TiMo.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Climate in a Bottle: Towards a Generative Foundation Model for the Kilometer-Scale Global Atmosphere",
    "url": "http://arxiv.org/abs/2505.06474v1",
    "authors": [
      "Noah D. Brenowitz",
      "Tao Ge",
      "Akshay Subramaniam",
      "Aayush Gupta",
      "David M. Hall",
      "Morteza Mardani",
      "Arash Vahdat",
      "Karthik Kashinath",
      "Michael S. Pritchard"
    ],
    "published": "2025-05-10",
    "abstract": "AI emulators offer a path to compressing, boosting limited ensembles, and\nimproving the latency of interacting with petabyte-scale climate prediction\ndata. However, prevailing auto-regressive paradigms offer limited flexibility,\nand are challenging to train on climate time horizons due to drifts,\ninstabilities and component-coupling challenges. Conditionally generative\nmodels offer an appealing alternative. In this context we demonstrate a\ngenerative diffusion-based framework -- Climate in a Bottle (cBottle) -- for\nemulating global km-scale climate simulations and reanalysis on the equal-area\nHEALPix grid. cBottle consists of two model stages: a globally-trained\ncoarse-resolution image generator that generates 100km (50k-pixel) fields given\nmonthly average sea surface temperatures and solar conditioning, followed by a\nlocally-trained 16x super-resolution stage that generates 5km (12.5M-pixel)\nfields; global super-resolution is made affordable using an overlapping\npatch-based multi-diffusion. Overall, cBottle shows promise as an emulator\nacross a battery of climate model diagnostics, including diurnal-to-seasonal\nscale variability, large-scale modes of variability, tropical cyclone\nstatistics, and trends of climate change and weather extremes. Moreover,\ncBottle is a step towards a foundation model, by bridging multiple data\nmodalities (reanalysis and simulation) with corresponding utility beyond\nemulation to tasks such as zero-shot bias correction, climate downscaling, and\nchannel in-filling.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction",
    "url": "http://arxiv.org/abs/2505.10027v1",
    "authors": [
      "Shijie Lyu"
    ],
    "published": "2025-05-15",
    "abstract": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing",
    "url": "http://arxiv.org/abs/2505.08302v1",
    "authors": [
      "Oishee Bintey Hoque",
      "Nibir Chandra Mandal",
      "Abhijin Adiga",
      "Samarth Swarup",
      "Sayjro Kossi Nouwakpo",
      "Amanda Wilson",
      "Madhav Marathe"
    ],
    "published": "2025-05-13",
    "abstract": "Accurate mapping of irrigation methods is crucial for sustainable\nagricultural practices and food systems. However, existing models that rely\nsolely on spectral features from satellite imagery are ineffective due to the\ncomplexity of agricultural landscapes and limited training data, making this a\nchallenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a\nnovel Swin-Transformer based approach that uses (i) a specialized projection\nmatrix to encode crop to irrigation probability, (ii) a spatial attention map\nto identify agricultural lands from non-agricultural lands, (iii)\nbi-directional cross-attention to focus complementary information from\ndifferent modalities, and (iv) a weighted ensemble for combining predictions\nfrom images and crop information. Our experimentation on five states in the US\nshows up to 22.9\\% (IoU) improvement over baseline with a 71.4% (IoU)\nimprovement for hard-to-classify drip irrigation. In addition, we propose a\ntwo-phase transfer learning approach to enhance cross-state irrigation mapping,\nachieving a 51% IoU boost in a state with limited labeled data. The ability to\nachieve baseline performance with only 40% of the training data highlights its\nefficiency, reducing the dependency on extensive manual labeling efforts and\nmaking large-scale, automated irrigation mapping more feasible and\ncost-effective.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction",
    "url": "http://arxiv.org/abs/2505.06905v1",
    "authors": [
      "Jian Song",
      "Hongruixuan Chen",
      "Naoto Yokoya"
    ],
    "published": "2025-05-11",
    "abstract": "Monocular height estimation (MHE) from very-high-resolution (VHR) remote\nsensing imagery via deep learning is notoriously challenging due to the lack of\nsufficient structural information. Conventional digital elevation models\n(DEMs), typically derived from airborne LiDAR or multi-view stereo, remain\ncostly and geographically limited. Recently, models trained on synthetic data\nand refined through domain adaptation have shown remarkable performance in MHE,\nyet it remains unclear how these models make predictions or how reliable they\ntruly are. In this paper, we investigate a state-of-the-art MHE model trained\npurely on synthetic data to explore where the model looks when making height\npredictions. Through systematic analyses, we find that the model relies heavily\non shadow cues, a factor that can lead to overestimation or underestimation of\nheights when shadows deviate from expected norms. Furthermore, the inherent\ndifficulty of evaluating regression tasks with the human eye underscores\nadditional limitations of purely synthetic training. To address these issues,\nwe propose a novel correction pipeline that integrates sparse, imperfect global\nLiDAR measurements (ICESat-2) with deep-learning outputs to improve local\naccuracy and achieve spatially consistent corrections. Our method comprises two\nstages: pre-processing raw ICESat-2 data, followed by a random forest-based\napproach to densely refine height estimates. Experiments in three\nrepresentative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal\nsubstantial error reductions, with mean absolute error (MAE) decreased by\n22.8\\%, 6.9\\%, and 4.9\\%, respectively. These findings highlight the critical\nrole of shadow awareness in synthetic data-driven models and demonstrate how\nfusing imperfect real-world LiDAR data can bolster the robustness of MHE,\npaving the way for more reliable and scalable 3D mapping solutions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Predicting butterfly species presence from satellite imagery using soft contrastive regularisation",
    "url": "http://arxiv.org/abs/2505.09306v1",
    "authors": [
      "Thijs L van der Plas",
      "Stephen Law",
      "Michael JO Pocock"
    ],
    "published": "2025-05-14",
    "abstract": "The growing demand for scalable biodiversity monitoring methods has fuelled\ninterest in remote sensing data, due to its widespread availability and\nextensive coverage. Traditionally, the application of remote sensing to\nbiodiversity research has focused on mapping and monitoring habitats, but with\nincreasing availability of large-scale citizen-science wildlife observation\ndata, recent methods have started to explore predicting multi-species presence\ndirectly from satellite images. This paper presents a new data set for\npredicting butterfly species presence from satellite data in the United\nKingdom. We experimentally optimise a Resnet-based model to predict\nmulti-species presence from 4-band satellite images, and find that this model\nespecially outperforms the mean rate baseline for locations with high species\nbiodiversity. To improve performance, we develop a soft, supervised contrastive\nregularisation loss that is tailored to probabilistic labels (such as\nspecies-presence data), and demonstrate that this improves prediction accuracy.\nIn summary, our new data set and contrastive regularisation method contribute\nto the open challenge of accurately predicting species biodiversity from remote\nsensing data, which is key for efficient biodiversity monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FedRS-Bench: Realistic Federated Learning Datasets and Benchmarks in Remote Sensing",
    "url": "http://arxiv.org/abs/2505.08325v1",
    "authors": [
      "Haodong Zhao",
      "Peng Peng",
      "Chiyu Chen",
      "Linqing Huang",
      "Gongshen Liu"
    ],
    "published": "2025-05-13",
    "abstract": "Remote sensing (RS) images are usually produced at an unprecedented scale,\nyet they are geographically and institutionally distributed, making centralized\nmodel training challenging due to data-sharing restrictions and privacy\nconcerns. Federated learning (FL) offers a solution by enabling collaborative\nmodel training across decentralized RS data sources without exposing raw data.\nHowever, there lacks a realistic federated dataset and benchmark in RS. Prior\nworks typically rely on manually partitioned single dataset, which fail to\ncapture the heterogeneity and scale of real-world RS data, and often use\ninconsistent experimental setups, hindering fair comparison. To address this\ngap, we propose a realistic federated RS dataset, termed FedRS. FedRS consists\nof eight datasets that cover various sensors and resolutions and builds 135\nclients, which is representative of realistic operational scenarios. Data for\neach client come from the same source, exhibiting authentic federated\nproperties such as skewed label distributions, imbalanced client data volumes,\nand domain heterogeneity across clients. These characteristics reflect\npractical challenges in federated RS and support evaluation of FL methods at\nscale. Based on FedRS, we implement 10 baseline FL algorithms and evaluation\nmetrics to construct the comprehensive FedRS-Bench. The experimental results\ndemonstrate that FL can consistently improve model performance over training on\nisolated data silos, while revealing performance trade-offs of different\nmethods under varying client heterogeneity and availability conditions. We hope\nFedRS-Bench will accelerate research on large-scale, realistic FL in RS by\nproviding a standardized, rich testbed and facilitating fair comparisons across\nfuture works. The source codes and dataset are available at\nhttps://fedrs-bench.github.io/.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset",
    "url": "http://arxiv.org/abs/2505.07396v2",
    "authors": [
      "Olaf Wysocki",
      "Benedikt Schwab",
      "Manoj Kumar Biswanath",
      "Michael Greza",
      "Qilin Zhang",
      "Jingwei Zhu",
      "Thomas Froech",
      "Medhini Heeramaglore",
      "Ihab Hijazi",
      "Khaoula Kanna",
      "Mathias Pechinger",
      "Zhaiyu Chen",
      "Yao Sun",
      "Alejandro Rueda Segura",
      "Ziyang Xu",
      "Omar AbdelGafar",
      "Mansour Mehranfar",
      "Chandan Yeshwanth",
      "Yueh-Cheng Liu",
      "Hadi Yazdi",
      "Jiapan Wang",
      "Stefan Auer",
      "Katharina Anders",
      "Klaus Bogenberger",
      "Andre Borrmann",
      "Angela Dai",
      "Ludwig Hoegner",
      "Christoph Holst",
      "Thomas H. Kolbe",
      "Ferdinand Ludwig",
      "Matthias Nie\u00dfner",
      "Frank Petzold",
      "Xiao Xiang Zhu",
      "Boris Jutzi"
    ],
    "published": "2025-05-12",
    "abstract": "Urban Digital Twins (UDTs) have become essential for managing cities and\nintegrating complex, heterogeneous data from diverse sources. Creating UDTs\ninvolves challenges at multiple process stages, including acquiring accurate 3D\nsource data, reconstructing high-fidelity 3D models, maintaining models'\nupdates, and ensuring seamless interoperability to downstream tasks. Current\ndatasets are usually limited to one part of the processing chain, hampering\ncomprehensive UDTs validation. To address these challenges, we introduce the\nfirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.\nThis dataset includes georeferenced, semantically aligned 3D models and\nnetworks along with various terrestrial, mobile, aerial, and satellite\nobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently\n767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high\naccuracy, and multimodal data integration, the benchmark supports robust\nanalysis of sensors and the development of advanced reconstruction methods.\nAdditionally, we explore downstream tasks demonstrating the potential of\nTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar\npotential analysis, point cloud semantic segmentation, and LoD3 building\nreconstruction. We are convinced this contribution lays a foundation for\novercoming current limitations in UDT creation, fostering new research\ndirections and practical solutions for smarter, data-driven urban environments.\nThe project is available under: https://tum2t.win",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Online Feedback Efficient Active Target Discovery in Partially Observable Environments",
    "url": "http://arxiv.org/abs/2505.06535v1",
    "authors": [
      "Anindya Sarkar",
      "Binglin Ji",
      "Yevgeniy Vorobeychik"
    ],
    "published": "2025-05-10",
    "abstract": "In various scientific and engineering domains, where data acquisition is\ncostly, such as in medical imaging, environmental monitoring, or remote\nsensing, strategic sampling from unobserved regions, guided by prior\nobservations, is essential to maximize target discovery within a limited\nsampling budget. In this work, we introduce Diffusion-guided Active Target\nDiscovery (DiffATD), a novel method that leverages diffusion dynamics for\nactive target discovery. DiffATD maintains a belief distribution over each\nunobserved state in the environment, using this distribution to dynamically\nbalance exploration-exploitation. Exploration reduces uncertainty by sampling\nregions with the highest expected entropy, while exploitation targets areas\nwith the highest likelihood of discovering the target, indicated by the belief\ndistribution and an incrementally trained reward model designed to learn the\ncharacteristics of the target. DiffATD enables efficient target discovery in a\npartially observable environment within a fixed sampling budget, all without\nrelying on any prior supervised training. Furthermore, DiffATD offers\ninterpretability, unlike existing black-box policies that require extensive\nsupervised training. Through extensive experiments and ablation studies across\ndiverse domains, including medical imaging and remote sensing, we show that\nDiffATD performs significantly better than baselines and competitively with\nsupervised methods that operate under full environmental observability.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting",
    "url": "http://arxiv.org/abs/2505.10191v1",
    "authors": [
      "Qingyu Zheng",
      "Qi Shao",
      "Guijun Han",
      "Wei Li",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2025-05-15",
    "abstract": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the\nocean, and their impact on the energy cascade of the global ocean cannot be\nignored. Eddy-resolving ocean forecasting is providing more reliable protection\nfor fisheries and navigational safety, but also presents significant scientific\nchallenges and high computational costs for traditional numerical models.\nArtificial intelligence (AI)-based weather and ocean forecasting systems are\nbecoming powerful tools that balance forecast performance with computational\nefficiency. However, the complex multiscale features in the ocean dynamical\nsystem make AI models still face many challenges in mesoscale eddy forecasting\n(especially regional modelling). Here, we develop LanTu, a regional\neddy-resolving ocean forecasting system based on dynamics-enhanced deep\nlearning. We incorporate cross-scale interactions into LanTu and construct\nmultiscale physical constraint for optimising LanTu guided by knowledge of eddy\ndynamics in order to improve the forecasting skill of LanTu for mesoscale\nevolution. The results show that LanTu outperforms the existing advanced\noperational numerical ocean forecasting system (NOFS) and AI-based ocean\nforecasting system (AI-OFS) in temperature, salinity, sea level anomaly and\ncurrent prediction, with a lead time of more than 10 days. Our study highlights\nthat dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for\neddy-resolving ocean forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "An AI-driven framework for the prediction of personalised health response to air pollution",
    "url": "http://arxiv.org/abs/2505.10556v1",
    "authors": [
      "Nazanin Zounemat Kermani",
      "Sadjad Naderi",
      "Claire H. Dilliway",
      "Claire E. Heaney",
      "Shrreya Behll",
      "Boyang Chen",
      "Hisham Abubakar-Waziri",
      "Alexandra E. Porter",
      "Marc Chadeau-Hyam",
      "Fangxin Fang",
      "Ian M. Adcock",
      "Kian Fan Chung",
      "Christopher C. Pain"
    ],
    "published": "2025-05-15",
    "abstract": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Score-based diffusion nowcasting of GOES imagery",
    "url": "http://arxiv.org/abs/2505.10432v1",
    "authors": [
      "Randy J. Chase",
      "Katherine Haynes",
      "Lander Ver Hoef",
      "Imme Ebert-Uphoff"
    ],
    "published": "2025-05-15",
    "abstract": "Clouds and precipitation are important for understanding weather and climate.\nSimulating clouds and precipitation with traditional numerical weather\nprediction is challenging because of the sub-grid parameterizations required.\nMachine learning has been explored for forecasting clouds and precipitation,\nbut early machine learning methods often created blurry forecasts. In this\npaper we explore a newer method, named score-based diffusion, to nowcast (zero\nto three hour forecast) clouds and precipitation. We discuss the background and\nintuition of score-based diffusion models - thus providing a starting point for\nthe community - while exploring the methodology's use for nowcasting\ngeostationary infrared imagery. We experiment with three main types of\ndiffusion models: a standard score-based diffusion model (Diff); a residual\ncorrection diffusion model (CorrDiff); and a latent diffusion model (LDM). Our\nresults show that the diffusion models are able to not only advect existing\nclouds, but also generate and decay clouds, including convective initiation.\nThese results are surprising because the forecasts are initiated with only the\npast 20 mins of infrared satellite imagery. A case study qualitatively shows\nthe preservation of high resolution features longer into the forecast than a\nconventional mean-squared error trained U-Net. The best of the three diffusion\nmodels tested was the CorrDiff approach, outperforming all other diffusion\nmodels, the traditional U-Net, and a persistence forecast by one to two kelvin\non root mean squared error. The diffusion models also enable out-of-the-box\nensemble generation, which shows skillful calibration, with the spread of the\nensemble correlating well to the error.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "Diffusion Models"
    ],
    "applications": [
      "Forecast",
      "Nowcast"
    ]
  },
  {
    "title": "Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations",
    "url": "http://arxiv.org/abs/2505.09284v1",
    "authors": [
      "Panqi Chen",
      "Yifan Sun",
      "Lei Cheng",
      "Yang Yang",
      "Weichang Li",
      "Yang Liu",
      "Weiqing Liu",
      "Jiang Bian",
      "Shikai Fang"
    ],
    "published": "2025-05-14",
    "abstract": "Modeling and reconstructing multidimensional physical dynamics from sparse\nand off-grid observations presents a fundamental challenge in scientific\nresearch. Recently, diffusion-based generative modeling shows promising\npotential for physical simulation. However, current approaches typically\noperate on on-grid data with preset spatiotemporal resolution, but struggle\nwith the sparsely observed and continuous nature of real-world physical\ndynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in\nFunctional Tucker space, a novel framework that generates full-field evolution\nof physical dynamics from irregular sparse observations. SDIFT leverages the\nfunctional Tucker model as the latent space representer with proven universal\napproximation property, and represents observations as latent functions and\nTucker core sequences. We then construct a sequential diffusion model with\ntemporally augmented UNet in the functional Tucker space, denoising noise drawn\nfrom a Gaussian process to generate the sequence of core tensors.\n  At the posterior sampling stage, we propose a Message-Passing Posterior\nSampling mechanism, enabling conditional generation of the entire sequence\nguided by observations at limited time steps. We validate SDIFT on three\nphysical systems spanning astronomical (supernova explosions, light-year\nscale), environmental (ocean sound speed fields, kilometer scale), and\nmolecular (organic liquid, millimeter scale) domains, demonstrating significant\nimprovements in both reconstruction accuracy and computational efficiency\ncompared to state-of-the-art approaches.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control",
    "url": "http://arxiv.org/abs/2505.07045v1",
    "authors": [
      "Junjie Yu",
      "John S. Schreck",
      "David John Gagne",
      "Keith W. Oleson",
      "Jie Li",
      "Yongtu Liang",
      "Qi Liao",
      "Mingfei Sun",
      "David O. Topping",
      "Zhonghua Zheng"
    ],
    "published": "2025-05-11",
    "abstract": "Reinforcement learning (RL)-based heating, ventilation, and air conditioning\n(HVAC) control has emerged as a promising technology for reducing building\nenergy consumption while maintaining indoor thermal comfort. However, the\nefficacy of such strategies is influenced by the background climate and their\nimplementation may potentially alter both the indoor climate and local urban\nclimate. This study proposes an integrated framework combining RL with an urban\nclimate model that incorporates a building energy model, aiming to evaluate the\nefficacy of RL-based HVAC control across different background climates, impacts\nof RL strategies on indoor climate and local urban climate, and the\ntransferability of RL strategies across cities. Our findings reveal that the\nreward (defined as a weighted combination of energy consumption and thermal\ncomfort) and the impacts of RL strategies on indoor climate and local urban\nclimate exhibit marked variability across cities with different background\nclimates. The sensitivity of reward weights and the transferability of RL\nstrategies are also strongly influenced by the background climate. Cities in\nhot climates tend to achieve higher rewards across most reward weight\nconfigurations that balance energy consumption and thermal comfort, and those\ncities with more varying atmospheric temperatures demonstrate greater RL\nstrategy transferability. These findings underscore the importance of\nthoroughly evaluating RL-based HVAC control strategies in diverse climatic\ncontexts. This study also provides a new insight that city-to-city learning\nwill potentially aid the deployment of RL-based HVAC control.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models",
    "url": "http://arxiv.org/abs/2505.16793v1",
    "authors": [
      "Xiang Li",
      "Yong Tao",
      "Siyuan Zhang",
      "Siwei Liu",
      "Zhitong Xiong",
      "Chunbo Luo",
      "Lu Liu",
      "Mykola Pechenizkiy",
      "Xiao Xiang Zhu",
      "Tianjin Huang"
    ],
    "published": "2025-05-22",
    "abstract": "Earth observation foundation models have shown strong generalization across\nmultiple Earth observation tasks, but their robustness under real-world\nperturbations remains underexplored. To bridge this gap, we introduce REOBench,\nthe first comprehensive benchmark for evaluating the robustness of Earth\nobservation foundation models across six tasks and twelve types of image\ncorruptions, including both appearance-based and geometric perturbations. To\nensure realistic and fine-grained evaluation, our benchmark focuses on\nhigh-resolution optical remote sensing images, which are widely used in\ncritical applications such as urban planning and disaster response. We conduct\na systematic evaluation of a broad range of models trained using masked image\nmodeling, contrastive learning, and vision-language pre-training paradigms. Our\nresults reveal that (1) existing Earth observation foundation models experience\nsignificant performance degradation when exposed to input corruptions. (2) The\nseverity of degradation varies across tasks, model architectures, backbone\nsizes, and types of corruption, with performance drop varying from less than 1%\nto over 20%. (3) Vision-language models show enhanced robustness, particularly\nin multimodal tasks. REOBench underscores the vulnerability of current Earth\nobservation foundation models to real-world corruptions and provides actionable\ninsights for developing more robust and reliable models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation",
    "url": "http://arxiv.org/abs/2505.16540v1",
    "authors": [
      "Inbal Cohen",
      "Boaz Meivar",
      "Peihan Tu",
      "Shai Avidan",
      "Gal Oren"
    ],
    "published": "2025-05-22",
    "abstract": "Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2505.15334v1",
    "authors": [
      "Bernardin Ligan",
      "Khalide Jbilou",
      "Fahd Kalloubi",
      "Ahmed Ratnani"
    ],
    "published": "2025-05-21",
    "abstract": "Foundation models have achieved great success across diverse domains,\nincluding remote sensing (RS), thanks to their versatility and strong\ngeneralization abilities. However, most RS foundation models are designed for\nmultispectral data, while hyperspectral imagery (HSI) - with its hundreds of\nspectral bands - remains less explored. Fine-tuning such models for downstream\ntasks is also challenging, often demanding considerable memory and storage. In\nthis paper, we propose an efficient framework to fine-tune SpectralGPT, a\nmultispectral foundation model, for hyperspectral image classification (HSIC).\nWe explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including\nLow-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank\nKronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for\nlow-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce\nKronA+, which applies a similar mechanism to the Kronecker matrices. We\nevaluate our approach on five datasets from different sensors, showing\ncompetitive performance with state-of-the-art HSI models. Our full fine-tuning\n(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral\nfoundation model on some datasets while requiring only a quarter of the\ntraining epochs. Under the same number of epochs, KronA+ reaches similar\nperformance with far fewer trainable parameters - just 0.056 percent - and adds\nonly approximately 0.2 megabytes of storage, making it the most effective PEFT\nmethod tested.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation",
    "url": "http://arxiv.org/abs/2505.15147v1",
    "authors": [
      "Quanwei Liu",
      "Tao Huang",
      "Yanni Dong",
      "Jiaqi Yang",
      "Wei Xiang"
    ],
    "published": "2025-05-21",
    "abstract": "Remote sensing images (RSIs) capture both natural and human-induced changes\non the Earth's surface, serving as essential data for environmental monitoring,\nurban planning, and resource management. Semantic segmentation (SS) of RSIs\nenables the fine-grained interpretation of surface features, making it a\ncritical task in remote sensing analysis. With the increasing diversity and\nvolume of RSIs collected by sensors on various platforms, traditional\nprocessing methods struggle to maintain efficiency and accuracy. In response,\ndeep learning (DL) has emerged as a transformative approach, enabling\nsubstantial advances in remote sensing image semantic segmentation (RSISS) by\nautomating feature extraction and improving segmentation accuracy across\ndiverse modalities. This paper revisits the evolution of DL-based RSISS by\ncategorizing existing approaches into four stages: the early pixel-based\nmethods, the prevailing patch-based and tile-based techniques, and the emerging\nimage-based strategies enabled by foundation models. We analyze these\ndevelopments from the perspective of feature extraction and learning\nstrategies, revealing the field's progression from pixel-level to tile-level\nand from unimodal to multimodal segmentation. Furthermore, we conduct a\ncomprehensive evaluation of nearly 40 advanced techniques on a unified dataset\nto quantitatively characterize their performance and applicability. This review\noffers a holistic view of DL-based SS for RS, highlighting key advancements,\ncomparative insights, and open challenges to guide future research.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives",
    "url": "http://arxiv.org/abs/2505.14361v1",
    "authors": [
      "Xingxing Weng",
      "Chao Pang",
      "Gui-Song Xia"
    ],
    "published": "2025-05-20",
    "abstract": "Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts",
    "url": "http://arxiv.org/abs/2505.14088v1",
    "authors": [
      "Xi Chen",
      "Shen Yan",
      "Juelin Zhu",
      "Chen Chen",
      "Yu Liu",
      "Maojun Zhang"
    ],
    "published": "2025-05-20",
    "abstract": "We introduce Land-MoE, a novel approach for multispectral land cover\nclassification (MLCC). Spectral shift, which emerges from disparities in\nsensors and geospatial conditions, poses a significant challenge in this\ndomain. Existing methods predominantly rely on domain adaptation and\ngeneralization strategies, often utilizing small-scale models that exhibit\nlimited performance. In contrast, Land-MoE addresses these issues by\nhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,\nto fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.\nSpecifically, Land-MoE comprises two key modules: the mixture of low-rank token\nexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages\nrank-differentiated tokens to generate diverse feature adjustments for\nindividual instances within multispectral images. By dynamically combining\nlearnable low-rank token experts of varying ranks, it enhances the robustness\nagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on\nthe refined features. This process enables the model to effectively capture\nfrequency band information that is strongly correlated with semantic essence,\nwhile simultaneously suppressing frequency noise irrelevant to the task.\nComprehensive experiments on MLCC tasks involving cross-sensor and\ncross-geospatial setups demonstrate that Land-MoE outperforms existing methods\nby a large margin. Additionally, the proposed approach has also achieved\nstate-of-the-art performance in domain generalization semantic segmentation\ntasks of RGB remote sensing images.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models",
    "url": "http://arxiv.org/abs/2505.12108v1",
    "authors": [
      "Jiancheng Pan",
      "Shiye Lei",
      "Yuqian Fu",
      "Jiahao Li",
      "Yanxing Liu",
      "Yuze Sun",
      "Xiao He",
      "Long Peng",
      "Xiaomeng Huang",
      "Bo Zhao"
    ],
    "published": "2025-05-17",
    "abstract": "Remote sensing image (RSI) interpretation typically faces challenges due to\nthe scarcity of labeled data, which limits the performance of RSI\ninterpretation tasks. To tackle this challenge, we propose EarthSynth, a\ndiffusion-based generative foundation model that enables synthesizing\nmulti-category, cross-satellite labeled Earth observation for downstream RSI\ninterpretation tasks. To the best of our knowledge, EarthSynth is the first to\nexplore multi-task generation for remote sensing. EarthSynth, trained on the\nEarthSynth-180K dataset, employs the Counterfactual Composition training\nstrategy to improve training data diversity and enhance category control.\nFurthermore, a rule-based method of R-Filter is proposed to filter more\ninformative synthetic data for downstream tasks. We evaluate our EarthSynth on\nscene classification, object detection, and semantic segmentation in open-world\nscenarios, offering a practical solution for advancing RSI interpretation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation",
    "url": "http://arxiv.org/abs/2505.15077v1",
    "authors": [
      "Alessandro dos Santos Ferreira",
      "Ana Paula Marques Ramos",
      "Jos\u00e9 Marcato Junior",
      "Wesley Nunes Gon\u00e7alves"
    ],
    "published": "2025-05-21",
    "abstract": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN",
      "Diffusion Models"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry",
    "url": "http://arxiv.org/abs/2505.13088v1",
    "authors": [
      "Zhaoyi Wang",
      "Shengyu Huang",
      "Jemil Avers Butt",
      "Yuanzhou Cai",
      "Matej Varga",
      "Andreas Wieser"
    ],
    "published": "2025-05-19",
    "abstract": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction",
    "url": "http://arxiv.org/abs/2505.12280v1",
    "authors": [
      "Sijie Zhao",
      "Feng Liu",
      "Xueliang Zhang",
      "Hao Chen",
      "Pengfeng Xiao",
      "Lei Bai"
    ],
    "published": "2025-05-18",
    "abstract": "The proliferation of diverse remote sensing data has spurred advancements in\ndense prediction tasks, yet significant challenges remain in handling data\nheterogeneity. Remote sensing imagery exhibits substantial variability across\ntemporal, spectral, and spatial (TSS) dimensions, complicating unified data\nprocessing. Current deep learning models for dense prediction tasks, such as\nsemantic segmentation and change detection, are typically tailored to specific\ninput-output configurations. Consequently, variations in data dimensionality or\ntask requirements often lead to significant performance degradation or model\nincompatibility, necessitating costly retraining or fine-tuning efforts for\ndifferent application scenarios. This paper introduces the\nTemporal-Spectral-Spatial Unified Network (TSSUN), a novel architecture\ndesigned for unified representation and modeling of remote sensing data across\ndiverse TSS characteristics and task types. TSSUN employs a\nTemporal-Spectral-Spatial Unified Strategy that leverages meta-information to\ndecouple and standardize input representations from varied temporal, spectral,\nand spatial configurations, and similarly unifies output structures for\ndifferent dense prediction tasks and class numbers. Furthermore, a Local-Global\nWindow Attention mechanism is proposed to efficiently capture both local\ncontextual details and global dependencies, enhancing the model's adaptability\nand feature extraction capabilities. Extensive experiments on multiple datasets\ndemonstrate that a single TSSUN model effectively adapts to heterogeneous\ninputs and unifies various dense prediction tasks. The proposed approach\nconsistently achieves or surpasses state-of-the-art performance, highlighting\nits robustness and generalizability for complex remote sensing applications\nwithout requiring task-specific modifications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples",
    "url": "http://arxiv.org/abs/2505.12069v1",
    "authors": [
      "Shenzhou Liu",
      "Di Wang",
      "Haonan Guo",
      "Chengxi Han",
      "Wenzhi Zeng"
    ],
    "published": "2025-05-17",
    "abstract": "Accurate and fine-grained crop yield prediction plays a crucial role in\nadvancing global agriculture. However, the accuracy of pixel-level yield\nestimation based on satellite remote sensing data has been constrained by the\nscarcity of ground truth data. To address this challenge, we propose a novel\napproach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This\nframework introduces an effective multi-task feature-sharing strategy, where\nfeatures extracted from a shared backbone network are simultaneously utilized\nby both crop yield prediction decoders and crop classification decoders with\nthe ability to fuse information between them. This design allows MT-CYP-Net to\nbe trained with extremely sparse crop yield point labels and crop type labels,\nwhile still generating detailed pixel-level crop yield maps. Concretely, we\ncollected 1,859 yield point labels along with corresponding crop type labels\nand satellite images from eight farms in Heilongjiang Province, China, in 2023,\ncovering soybean, maize, and rice crops, and constructed a sparse crop yield\nlabel dataset. MT-CYP-Net is compared with three classical machine learning and\ndeep learning benchmark methods in this dataset. Experimental results not only\nindicate the superiority of MT-CYP-Net compared to previous methods on multiple\ntypes of crops but also demonstrate the potential of deep networks on precise\npixel-level crop yield prediction, especially with limited data labels.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Beluga Whale Detection from Satellite Imagery with Point Labels",
    "url": "http://arxiv.org/abs/2505.12066v1",
    "authors": [
      "Yijie Zheng",
      "Jinxuan Yang",
      "Yu Chen",
      "Yaxuan Wang",
      "Yihang Lu",
      "Guoqing Li"
    ],
    "published": "2025-05-17",
    "abstract": "Very high-resolution (VHR) satellite imagery has emerged as a powerful tool\nfor monitoring marine animals on a large scale. However, existing deep\nlearning-based whale detection methods usually require manually created,\nhigh-quality bounding box annotations, which are labor-intensive to produce.\nMoreover, existing studies often exclude ``uncertain whales'', individuals that\nhave ambiguous appearances in satellite imagery, limiting the applicability of\nthese models in real-world scenarios. To address these limitations, this study\nintroduces an automated pipeline for detecting beluga whales and harp seals in\nVHR satellite imagery. The pipeline leverages point annotations and the Segment\nAnything Model (SAM) to generate precise bounding box annotations, which are\nused to train YOLOv8 for multiclass detection of certain whales, uncertain\nwhales, and harp seals. Experimental results demonstrated that SAM-generated\nannotations significantly improved detection performance, achieving higher\n$\\text{F}_\\text{1}$-scores compared to traditional buffer-based annotations.\nYOLOv8 trained on SAM-labeled boxes achieved an overall\n$\\text{F}_\\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,\nwith superior performance in dense scenes. The proposed approach not only\nreduces the manual effort required for annotation but also enhances the\ndetection of uncertain whales, offering a more comprehensive solution for\nmarine animal monitoring. This method holds great potential for extending to\nother species, habitats, and remote sensing platforms, as well as for\nestimating whale biometrics, thereby advancing ecological monitoring and\nconservation efforts. The codes for our label and detection pipeline are\npublicly available at http://github.com/voyagerxvoyagerx/beluga-seeker .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection",
    "url": "http://arxiv.org/abs/2505.11793v1",
    "authors": [
      "Jianing Wang",
      "Siying Guo",
      "Zheng Hua",
      "Runhu Huang",
      "Jinyu Hu",
      "Maoguo Gong"
    ],
    "published": "2025-05-17",
    "abstract": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ]
  },
  {
    "title": "Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors",
    "url": "http://arxiv.org/abs/2505.14122v1",
    "authors": [
      "Ehsan Masoudian",
      "Ali Mirzaei",
      "Hossein Bagheri"
    ],
    "published": "2025-05-20",
    "abstract": "This study investigates the multifaceted factors influencing wildfire risk in\nIran, focusing on the interplay between climatic conditions and human\nactivities. Utilizing advanced remote sensing, geospatial information system\n(GIS) processing techniques such as cloud computing, and machine learning\nalgorithms, this research analyzed the impact of climatic parameters,\ntopographic features, and human-related factors on wildfire susceptibility\nassessment and prediction in Iran. Multiple scenarios were developed for this\npurpose based on the data sampling strategy. The findings revealed that\nclimatic elements such as soil moisture, temperature, and humidity\nsignificantly contribute to wildfire susceptibility, while human\nactivities-particularly population density and proximity to powerlines-also\nplayed a crucial role. Furthermore, the seasonal impact of each parameter was\nseparately assessed during warm and cold seasons. The results indicated that\nhuman-related factors, rather than climatic variables, had a more prominent\ninfluence during the seasonal analyses. This research provided new insights\ninto wildfire dynamics in Iran by generating high-resolution wildfire\nsusceptibility maps using advanced machine learning classifiers. The generated\nmaps identified high risk areas, particularly in the central Zagros region, the\nnortheastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting\nthe urgent need for effective fire management strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction",
    "url": "http://arxiv.org/abs/2505.14747v1",
    "authors": [
      "Fatemeh Chajaei",
      "Hossein Bagheri"
    ],
    "published": "2025-05-20",
    "abstract": "Three-dimensional reconstruction of buildings, particularly at Level of\nDetail 1 (LOD1), plays a crucial role in various applications such as urban\nplanning, urban environmental studies, and designing optimized transportation\nnetworks. This study focuses on assessing the potential of LiDAR data for\naccurate 3D building reconstruction at LOD1 and extracting morphological\nfeatures from these models. Four deep semantic segmentation models, U-Net,\nAttention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning\nto extract building footprints from LiDAR data. The results showed that U-Net3+\nand Attention U-Net outperformed the others, achieving IoU scores of 0.833 and\n0.814, respectively. Various statistical measures, including maximum, range,\nmode, median, and the 90th percentile, were used to estimate building heights,\nresulting in the generation of 3D models at LOD1. As the main contribution of\nthe research, the impact of segmentation accuracy on the quality of 3D building\nmodeling and the accuracy of morphological features like building area and\nexternal wall surface area was investigated. The results showed that the\naccuracy of building identification (segmentation performance) significantly\naffects the 3D model quality and the estimation of morphological features,\ndepending on the height calculation method. Overall, the UNet3+ method,\nutilizing the 90th percentile and median measures, leads to accurate height\nestimation of buildings and the extraction of morphological features.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Continuous Domain Generalization",
    "url": "http://arxiv.org/abs/2505.13519v1",
    "authors": [
      "Zekun Cai",
      "Yiheng Yao",
      "Guangji Bai",
      "Renhe Jiang",
      "Xuan Song",
      "Ryosuke Shibasaki",
      "Liang Zhao"
    ],
    "published": "2025-05-17",
    "abstract": "Real-world data distributions often shift continuously across multiple latent\nfactors such as time, geography, and socioeconomic context. However, existing\ndomain generalization approaches typically treat domains as discrete or\nevolving along a single axis (e.g., time), which fails to capture the complex,\nmulti-dimensional nature of real-world variation. This paper introduces the\ntask of Continuous Domain Generalization (CDG), which aims to generalize\npredictive models to unseen domains defined by arbitrary combinations of\ncontinuous variation descriptors. We present a principled framework grounded in\ngeometric and algebraic theory, showing that optimal model parameters across\ndomains lie on a low-dimensional manifold. To model this structure, we propose\na Neural Lie Transport Operator (NeuralLTO), which enables structured parameter\ntransitions by enforcing geometric continuity and algebraic consistency. To\nhandle noisy or incomplete domain descriptors, we introduce a gating mechanism\nto suppress irrelevant dimensions and a local chart-based strategy for robust\ngeneralization. Extensive experiments on synthetic and real-world\ndatasets-including remote sensing, scientific documents, and traffic\nforecasting-demonstrate that our method significantly outperforms existing\nbaselines in generalization accuracy and robustness under descriptor\nimperfections.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation",
    "url": "http://arxiv.org/abs/2505.15802v1",
    "authors": [
      "Sarah E. Wessinger",
      "Leslie N. Smith",
      "Jacob Gull",
      "Jonathan Gehman",
      "Zachary Beever",
      "Andrew J. Kammerer"
    ],
    "published": "2025-05-21",
    "abstract": "Accurately estimating the refractive environment over multiple frequencies\nwithin the marine atmospheric boundary layer is crucial for the effective\ndeployment of radar technologies. Traditional parabolic equation simulations,\nwhile effective, can be computationally expensive and time-intensive, limiting\ntheir practical application. This communication explores a novel approach using\ndeep neural networks to estimate the pattern propagation factor, a critical\nparameter for characterizing environmental impacts on signal propagation.\nImage-to-image translation generators designed to ingest modified refractivity\ndata and generate predictions of pattern propagation factors over the same\ndomain were developed. Findings demonstrate that deep neural networks can be\ntrained to analyze multiple frequencies and reasonably predict the pattern\npropagation factor, offering an alternative to traditional methods.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis",
    "url": "http://arxiv.org/abs/2505.14285v1",
    "authors": [
      "Eirini Panteli",
      "Paulo E. Santos",
      "Nabil Humphrey"
    ],
    "published": "2025-05-20",
    "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Terrain-aware Deep Learning for Wind Energy Applications: From Kilometer-scale Forecasts to Fine Wind Fields",
    "url": "http://arxiv.org/abs/2505.12732v1",
    "authors": [
      "Chensen Lin",
      "Ruian Tie",
      "Shihong Yi",
      "Xiaohui Zhong",
      "Hao Li"
    ],
    "published": "2025-05-19",
    "abstract": "High-resolution wind information is essential for wind energy planning and\npower forecasting, particularly in regions with complex terrain. However, most\nAI-based weather forecasting models operate at kilometer-scale resolution,\nconstrained by the reanalysis datasets they are trained on. Here we introduce\nFuXi-CFD, an AI-based downscaling framework designed to generate detailed\nthree-dimensional wind fields at 30-meter horizontal resolution, using only\ncoarse-resolution atmospheric inputs. The model is trained on a large-scale\ndataset generated via computational fluid dynamics (CFD), encompassing a wide\nrange of terrain types, surface roughness, and inflow conditions. Remarkably,\nFuXi-CFD predicts full 3D wind structures -- including vertical wind and\nturbulent kinetic energy -- based solely on horizontal wind input at 10 meters\nabove ground, the typical output of AI-based forecast systems. It achieves\nCFD-comparable accuracy while reducing inference time from hours to seconds. By\nbridging the resolution gap between regional forecasts and site-specific wind\ndynamics, FuXi-CFD offers a scalable and operationally efficient solution to\nsupport the growing demands of renewable energy deployment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Improving the Predictability of the Madden-Julian Oscillation at Subseasonal Scales with Gaussian Process Models",
    "url": "http://arxiv.org/abs/2505.15934v1",
    "authors": [
      "Haoyuan Chen",
      "Emil Constantinescu",
      "Vishwas Rao",
      "Cristiana Stan"
    ],
    "published": "2025-05-21",
    "abstract": "The Madden--Julian Oscillation (MJO) is an influential climate phenomenon\nthat plays a vital role in modulating global weather patterns. In spite of the\nimprovement in MJO predictions made by machine learning algorithms, such as\nneural networks, most of them cannot provide the uncertainty levels in the MJO\nforecasts directly. To address this problem, we develop a nonparametric\nstrategy based on Gaussian process (GP) models. We calibrate GPs using\nempirical correlations and we propose a posteriori covariance correction.\nNumerical experiments demonstrate that our model has better prediction skills\nthan the ANN models for the first five lead days. Additionally, our posteriori\ncovariance correction extends the probabilistic coverage by more than three\nweeks.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Multi-Tiered Bayesian Network Coastal Compound Flood Analysis Framework",
    "url": "http://arxiv.org/abs/2505.15520v1",
    "authors": [
      "Ziyue Liu",
      "Meredith L. Carr",
      "Norberto C. Nadal-Caraballo",
      "Luke A. Aucoin",
      "Madison C. Yawn",
      "Michelle T. Bensi"
    ],
    "published": "2025-05-21",
    "abstract": "Coastal compound floods (CCFs) are triggered by the interaction of multiple\nmechanisms, such as storm surges, storm rainfall, tides, and river flow. These\nevents can bring significant damage to communities, and there is an increasing\ndemand for accurate and efficient probabilistic analyses of CCFs to support\nrisk assessments and decision-making. In this study, a multi-tiered Bayesian\nnetwork (BN) CCF analysis framework is established. In this framework, multiple\ntiers of BN models with different complexities are designed for application\nwith varying levels of data availability and computational resources. A case\nstudy is conducted in New Orleans, LA, to demonstrate this framework. In the\nTier-1 BN model, storm surges and river flow are incorporated based on\nhydrodynamic simulations. A seasonality node is used to capture the dependence\nbetween concurrent river flow and tropical cyclone (TC) parameters. In the\nTier-2 BN model, joint distribution models of TC parameters are built for\nseparate TC intensity categories. TC-induced rainfall is modeled as input to\nhydraulic simulations. In the Tier-3 BN model, potential variations of\nmeteorological conditions are incorporated by quantifying their effects on TC\nactivity and coastal water level. Flood antecedent conditions are also\nincorporated to more completely represent the conditions contributing to flood\nseverity. In this case study, a series of joint distribution, numerical,\nmachine learning, and experimental models are used to compute conditional\nprobability tables needed for BNs. A series of probabilistic analyses is\nperformed based on these BN models, including CCF hazard curve construction and\nCCF deaggregation. The results of the analysis demonstrate the promise of this\nframework in performing CCF hazard analysis under varying levels of resource\navailability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning",
    "url": "http://arxiv.org/abs/2505.12904v1",
    "authors": [
      "Hilde I. Hummel",
      "Arwin Gansekoele",
      "Sandjai Bhulai",
      "Rob van der Mei"
    ],
    "published": "2025-05-19",
    "abstract": "The increasing level of sound pollution in marine environments poses an\nincreased threat to ocean health, making it crucial to monitor underwater\nnoise. By monitoring this noise, the sources responsible for this pollution can\nbe mapped. Monitoring is performed by passively listening to these sounds. This\ngenerates a large amount of data records, capturing a mix of sound sources such\nas ship activities and marine mammal vocalizations. Although machine learning\noffers a promising solution for automatic sound classification, current\nstate-of-the-art methods implement supervised learning. This requires a large\namount of high-quality labeled data that is not publicly available. In\ncontrast, a massive amount of lower-quality unlabeled data is publicly\navailable, offering the opportunity to explore unsupervised learning\ntechniques. This research explores this possibility by implementing an\nunsupervised Contrastive Learning approach. Here, a Conformer-based encoder is\noptimized by the so-called Variance-Invariance-Covariance Regularization loss\nfunction on these lower-quality unlabeled data and the translation to the\nlabeled data is made. Through classification tasks involving recognizing ship\ntypes and marine mammal vocalizations, our method demonstrates to produce\nrobust and generalized embeddings. This shows to potential of unsupervised\nmethods for various automatic underwater acoustic analysis tasks.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
    "url": "http://arxiv.org/abs/2505.21375v1",
    "authors": [
      "Fengxiang Wang",
      "Mingshuo Chen",
      "Yueying Li",
      "Di Wang",
      "Haotian Wang",
      "Zonghao Guo",
      "Zefan Wang",
      "Boqi Shan",
      "Long Lan",
      "Yulin Wang",
      "Hongzhen Wang",
      "Wenjing Yang",
      "Bo Du",
      "Jing Zhang"
    ],
    "published": "2025-05-27",
    "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data\nfor Earth observation but pose challenges for existing multimodal foundation\nmodels due to two key bottlenecks: (1) limited availability of UHR training\ndata, and (2) token explosion caused by the large image size. To address data\nscarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA\n(avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in\nRS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,\nour pilot studies reveal significant redundancy in RS images: crucial\ninformation is concentrated in a small subset of object-centric tokens, while\npruning background tokens (e.g., ocean or forest) can even improve performance.\nMotivated by these findings, we propose two strategies: Background Token\nPruning and Anchored Token Selection, to reduce the memory footprint while\npreserving key semantics.Integrating these techniques, we introduce\nGeoLLaVA-8K, the first RS-focused multimodal large language model capable of\nhandling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework.\nTrained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art\non the XLRS-Bench.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping",
    "url": "http://arxiv.org/abs/2505.21357v2",
    "authors": [
      "Wenyuan Li",
      "Shunlin Liang",
      "Keyan Chen",
      "Yongzhe Chen",
      "Han Ma",
      "Jianglei Xu",
      "Yichuan Ma",
      "Shikang Guan",
      "Husheng Fang",
      "Zhenwei Shi"
    ],
    "published": "2025-05-27",
    "abstract": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nhttps://github.com/flyakon/AgriFM.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2505.19447v1",
    "authors": [
      "Hengtong Shen",
      "Haiyan Gu",
      "Haitao Li",
      "Yi Yang",
      "Agen qiu"
    ],
    "published": "2025-05-26",
    "abstract": "Self-Supervised Learning (SSL) enables us to pre-train foundation models\nwithout costly labeled data. Among SSL methods, Contrastive Learning (CL)\nmethods are better at obtaining accurate semantic representations in noise\ninterference. However, due to the significant domain gap, while CL methods have\nachieved great success in many computer vision tasks, they still require\nspecific adaptation for Remote Sensing (RS) images. To this end, we present a\nnovel self-supervised method called PerA, which produces all-purpose RS\nfeatures through semantically Perfectly Aligned sample pairs. Specifically,\nPerA obtains features from sampled views by applying spatially disjoint masks\nto augmented images rather than random cropping. With disjoint masks, we divide\npatches from different views into different parts that are semantically aligned\nbut inconsistent in appearance. Our framework provides high-quality features by\nensuring consistency between teacher and student and predicting learnable mask\ntokens. Compared to previous contrastive methods, our method demonstrates\nhigher memory efficiency and can be trained with larger batches due to its\nsparse inputs. We also collect an unlabeled pre-training dataset, which\ncontains about 5 million RS images. We conducted experiments on multiple\ndownstream task datasets and achieved performance comparable to previous\nstate-of-the-art methods with a limited model scale, which verified the\nsuperiority of our method. We hope this work will contribute to practical\nremote sensing interpretation works.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation",
    "url": "http://arxiv.org/abs/2505.23597v1",
    "authors": [
      "Georgios Voulgaris"
    ],
    "published": "2025-05-29",
    "abstract": "The accurate semantic segmentation of tree crowns within remotely sensed data\nis crucial for scientific endeavours such as forest management, biodiversity\nstudies, and carbon sequestration quantification. However, precise segmentation\nremains challenging due to complexities in the forest canopy, including\nshadows, intricate backgrounds, scale variations, and subtle spectral\ndifferences among tree species. Compared to the traditional methods, Deep\nLearning models improve accuracy by extracting informative and discriminative\nfeatures, but often fall short in capturing the aforementioned complexities.\n  To address these challenges, we propose PerceptiveNet, a novel model\nincorporating a Logarithmic Gabor-parameterised convolutional layer with\ntrainable filter parameters, alongside a backbone that extracts salient\nfeatures while capturing extensive context and spatial information through a\nwider receptive field. We investigate the impact of Log-Gabor, Gabor, and\nstandard convolutional layers on semantic segmentation performance through\nextensive experimentation. Additionally, we conduct an ablation study to assess\nthe contributions of individual layers and their combinations to overall model\nperformance, and we evaluate PerceptiveNet as a backbone within a novel hybrid\nCNN-Transformer model. Our results outperform state-of-the-art models,\ndemonstrating significant performance improvements on a tree crown dataset\nwhile generalising across domains, including two benchmark aerial scene\nsemantic segmentation datasets with varying complexities.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Collaborative Learning for Unsupervised Multimodal Remote Sensing Image Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based Image Translation",
    "url": "http://arxiv.org/abs/2505.22000v1",
    "authors": [
      "Xiaochen Wei",
      "Weiwei Guo",
      "Wenxian Yu"
    ],
    "published": "2025-05-28",
    "abstract": "The substantial modality-induced variations in radiometric, texture, and\nstructural characteristics pose significant challenges for the accurate\nregistration of multimodal images. While supervised deep learning methods have\ndemonstrated strong performance, they often rely on large-scale annotated\ndatasets, limiting their practical application. Traditional unsupervised\nmethods usually optimize registration by minimizing differences in feature\nrepresentations, yet often fail to robustly capture geometric discrepancies,\nparticularly under substantial spatial and radiometric variations, thus\nhindering convergence stability. To address these challenges, we propose a\nCollaborative Learning framework for Unsupervised Multimodal Image\nRegistration, named CoLReg, which reformulates unsupervised registration\nlearning into a collaborative training paradigm comprising three components:\n(1) a cross-modal image translation network, MIMGCD, which employs a learnable\nMaximum Index Map (MIM) guided conditional diffusion model to synthesize\nmodality-consistent image pairs; (2) a self-supervised intermediate\nregistration network which learns to estimate geometric transformations using\naccurate displacement labels derived from MIMGCD outputs; (3) a distilled\ncross-modal registration network trained with pseudo-label predicted by the\nintermediate network. The three networks are jointly optimized through an\nalternating training strategy wherein each network enhances the performance of\nthe others. This mutual collaboration progressively reduces modality\ndiscrepancies, enhances the quality of pseudo-labels, and improves registration\naccuracy. Extensive experimental results on multiple datasets demonstrate that\nour ColReg achieves competitive or superior performance compared to\nstate-of-the-art unsupervised approaches and even surpasses several supervised\nbaselines.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Kernel Space Diffusion Model for Efficient Remote Sensing Pansharpening",
    "url": "http://arxiv.org/abs/2505.18991v1",
    "authors": [
      "Hancong Jin",
      "Zihan Cao",
      "Liangjian Deng"
    ],
    "published": "2025-05-25",
    "abstract": "Pansharpening is a fundamental task in remote sensing that integrates\nhigh-resolution panchromatic imagery (PAN) with low-resolution multispectral\nimagery (LRMS) to produce an enhanced image with both high spatial and spectral\nresolution. Despite significant progress in deep learning-based approaches,\nexisting methods often fail to capture the global priors inherent in remote\nsensing data distributions. Diffusion-based models have recently emerged as\npromising solutions due to their powerful distribution mapping capabilities;\nhowever, they suffer from significant inference latency, which limits their\npractical applicability. In this work, we propose the Kernel Space Diffusion\nModel (KSDiff), a novel approach that leverages diffusion processes in a latent\nspace to generate convolutional kernels enriched with global contextual\ninformation, thereby improving pansharpening quality while enabling faster\ninference. Specifically, KSDiff constructs these kernels through the\nintegration of a low-rank core tensor generator and a unified factor generator,\norchestrated by a structure-aware multi-head attention mechanism. We further\nintroduce a two-stage training strategy tailored for pansharpening, enabling\nKSDiff to serve as a framework for enhancing existing pansharpening\narchitectures. Experiments on three widely used datasets, including\nWorldView-3, GaoFen-2, and QuickBird, demonstrate the superior performance of\nKSDiff both qualitatively and quantitatively. Code will be released upon\npossible acceptance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation",
    "url": "http://arxiv.org/abs/2505.18461v1",
    "authors": [
      "Morteza Karimzadeh",
      "Zhongying Wang",
      "James L. Crooks"
    ],
    "published": "2025-05-24",
    "abstract": "Deep learning models have demonstrated success in geospatial applications,\nyet quantifying the role of geolocation information in enhancing model\nperformance and geographic generalizability remains underexplored. A new\ngeneration of location encoders have emerged with the goal of capturing\nattributes present at any given location for downstream use in predictive\nmodeling. Being a nascent area of research, their evaluation has remained\nlargely limited to static tasks such as species distributions or average\ntemperature mapping. In this paper, we discuss and quantify the impact of\nincorporating geolocation into deep learning for a real-world application\ndomain that is characteristically dynamic (with fast temporal change) and\nspatially heterogeneous at high resolutions: estimating surface-level daily\nPM2.5 levels using remotely sensed and ground-level data. We build on a\nrecently published deep learning-based PM2.5 estimation model that achieves\nstate-of-the-art performance on data observed in the contiguous United States.\nWe examine three approaches for incorporating geolocation: excluding\ngeolocation as a baseline, using raw geographic coordinates, and leveraging\npretrained location encoders. We evaluate each approach under within-region\n(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance\nmetrics indicate that while na\\\"ive incorporation of raw geographic coordinates\nimproves within-region performance by retaining the interpolative value of\ngeographic location, it can hinder generalizability across regions. In\ncontrast, pretrained location encoders like GeoCLIP enhance predictive\nperformance and geographic generalizability for both WR and OoR scenarios.\nHowever, qualitative analysis reveals artifact patterns caused by high-degree\nbasis functions and sparse upstream samples in certain areas, and ablation\nresults indicate varying performance among location encoders...",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences",
    "url": "http://arxiv.org/abs/2505.22008v1",
    "authors": [
      "Jing-An Sun",
      "Hang Fan",
      "Junchao Gong",
      "Ben Fei",
      "Kun Chen",
      "Fenghua Ling",
      "Wenlong Zhang",
      "Wanghan Xu",
      "Li Yan",
      "Pierre Gentine",
      "Lei Bai"
    ],
    "published": "2025-05-28",
    "abstract": "Data assimilation (DA) aims to estimate the full state of a dynamical system\nby combining partial and noisy observations with a prior model forecast,\ncommonly referred to as the background. In atmospheric applications, this\nproblem is fundamentally ill-posed due to the sparsity of observations relative\nto the high-dimensional state space. Traditional methods address this challenge\nby simplifying background priors to regularize the solution, which are\nempirical and require continual tuning for application. Inspired by alignment\ntechniques in text-to-image diffusion models, we propose Align-DA, which\nformulates DA as a generative process and uses reward signals to guide\nbackground priors, replacing manual tuning with data-driven alignment.\nSpecifically, we train a score-based model in the latent space to approximate\nthe background-conditioned prior, and align it using three complementary reward\nsignals for DA: (1) assimilation accuracy, (2) forecast skill initialized from\nthe assimilated state, and (3) physical adherence of the analysis fields.\nExperiments with multiple reward signals demonstrate consistent improvements in\nanalysis quality across different evaluation metrics and observation-guidance\nstrategies. These results show that preference alignment, implemented as a soft\nconstraint, can automatically adapt complex background priors tailored to DA,\noffering a promising new direction for advancing the field.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs",
    "url": "http://arxiv.org/abs/2505.22686v1",
    "authors": [
      "Ange-Clement Akazan",
      "Verlon Roel Mbingui",
      "Gnankan Landry Regis N'guessan",
      "Issa Karambal"
    ],
    "published": "2025-05-27",
    "abstract": "Weather forecasting is crucial for managing risks and economic planning,\nparticularly in tropical Africa, where extreme events severely impact\nlivelihoods. Yet, existing forecasting methods often struggle with the region's\ncomplex, non-linear weather patterns. This study benchmarks deep recurrent\nneural networks such as $\\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and\nKolmogorov-Arnold-based models $(\\texttt{KAN} and \\texttt{TKAN})$ for daily\nforecasting of temperature, precipitation, and pressure in two tropical cities:\nAbidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce\ntwo customized variants of $ \\texttt{TKAN}$ that replace its original\n$\\texttt{SiLU}$ activation function with $ \\texttt{GeLU}$ and \\texttt{MiSH},\nrespectively. Using station-level meteorological data spanning from 2010 to\n2024, we evaluate all the models on standard regression metrics. $\\texttt{KAN}$\nachieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,\n$\\texttt{MSE} < 0.0014~^\\circ C ^2$), while $\\texttt{TKAN}$ variants minimize\nabsolute errors for precipitation forecasting in low-rainfall regimes. The\ncustomized $\\texttt{TKAN}$ models demonstrate improvements over the standard\n$\\texttt{TKAN}$ across both datasets. Classical \\texttt{RNNs} remain highly\ncompetitive for atmospheric pressure ($R^2 \\approx 0.83{-}0.86$), outperforming\n$\\texttt{KAN}$-based models in this task. These results highlight the potential\nof spline-based neural architectures for efficient and data-efficient\nforecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM",
      "RNN"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation",
    "url": "http://arxiv.org/abs/2505.21020v1",
    "authors": [
      "Yuan Gao",
      "Ruiqi Shu",
      "Hao Wu",
      "Fan Xu",
      "Yanfei Xiang",
      "Ruijian Gou",
      "Qingsong Wen",
      "Xian Wu",
      "Xiaomeng Huang"
    ],
    "published": "2025-05-27",
    "abstract": "Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically\nimportant for marine research, yet remains challenging due to its substantial\nthermal inertia and extended time delay. Machine learning (ML)-based models\nhave demonstrated significant advancements in simulation accuracy and\ncomputational efficiency compared to traditional numerical methods.\nNevertheless, a significant limitation of current ML models for S2S ocean\nsimulation is their inadequate incorporation of physical consistency and the\nslow-changing properties of the ocean system. In this work, we propose a neural\nocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive\ngraph neural network to emulate diverse physical phenomena associated with\nocean systems effectively. Specifically, we propose a multi-stage framework\ntailored to model the ocean's slowly changing nature. Additionally, we\nintroduce a multi-scale interactive messaging module to capture complex\ndynamical behaviors, such as gradient changes and multiplicative coupling\nrelationships inherent in ocean dynamics. Extensive experimental evaluations\nconfirm that our proposed NeuralOM outperforms state-of-the-art models in S2S\nand extreme event simulation. The codes are available at\nhttps://github.com/YuanGao-YG/NeuralOM.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Automated data curation for self-supervised learning in underwater acoustic analysis",
    "url": "http://arxiv.org/abs/2505.20066v1",
    "authors": [
      "Hilde I Hummel",
      "Sandjai Bhulai",
      "Burooj Ghani",
      "Rob van der Mei"
    ],
    "published": "2025-05-26",
    "abstract": "The sustainability of the ocean ecosystem is threatened by increased levels\nof sound pollution, making monitoring crucial to understand its variability and\nimpact. Passive acoustic monitoring (PAM) systems collect a large amount of\nunderwater sound recordings, but the large volume of data makes manual analysis\nimpossible, creating the need for automation. Although machine learning offers\na potential solution, most underwater acoustic recordings are unlabeled.\nSelf-supervised learning models have demonstrated success in learning from\nlarge-scale unlabeled data in various domains like computer vision, Natural\nLanguage Processing, and audio. However, these models require large, diverse,\nand balanced datasets for training in order to generalize well. To address\nthis, a fully automated self-supervised data curation pipeline is proposed to\ncreate a diverse and balanced dataset from raw PAM data. It integrates\nAutomatic Identification System (AIS) data with recordings from various\nhydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw\naudio data is sampled and then combined with AIS samples to create a balanced\nand diverse dataset. The resulting curated dataset enables the development of\nself-supervised learning models, facilitating various tasks such as monitoring\nmarine mammals and assessing sound pollution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Accelerated Bayesian calibration and uncertainty quantification of RANS turbulence model parameters for stratified atmospheric boundary layer flows",
    "url": "http://arxiv.org/abs/2505.18756v1",
    "authors": [
      "E. Y. Shin",
      "M. F. Howland"
    ],
    "published": "2025-05-24",
    "abstract": "In operational weather models, the effects of turbulence in the atmospheric\nboundary layer (ABL) on the resolved flow are modeled using turbulence\nparameterizations. These parameterizations typically use a predetermined set of\nmodel parameters that are tuned to limited data from canonical flows. Using\nthese fixed parameters results in deterministic predictions that neglect\nuncertainty in the unresolved turbulence processes. In this study, we perform a\nmachine learning-accelerated Bayesian inversion of a single-column model of the\nABL. This approach is used to calibrate and quantify uncertainty in model\nparameters of Reynolds-averaged Navier-Stokes turbulence models. Following\nverification of the uncertainty quantification methodology, we learn the\nparameters and their uncertainties in two different turbulence models\nconditioned on scale-resolving large-eddy simulation data over a range of ABL\nstabilities. We show how Bayesian inversion of a numerical model improves flow\npredictions by investigating the underlying mean momentum budgets. Further, we\nshow that uncertainty quantification based on neutral surface layer data\nrecovers the relationships between parameters derived from theoretical\nmodeling, but that learning the parameters based on stable ABL data or data\nfrom outside the surface layer can lead to different relationships than neutral\nsurface layer theory. Systematic uncertainty reduction methods reveal that (1)\nsampling wind speed up to the ABL height can reduce uncertainty in key model\nparameters by up to 84%, and (2) assimilating fluid flow quantities beyond\nfirst-order moment statistics can further reduce uncertainty in ways that wind\nspeed assimilation alone cannot achieve. The parameters learned using Bayesian\nuncertainty quantification generally yield lower error than standard\ndeterministic parameters in out-of-sample tests and provide uncertainty\nintervals on predictions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A validated coupled three-dimensional hydrodynamic and spectral wind-wave model for the western north Atlantic Ocean",
    "url": "http://arxiv.org/abs/2505.18803v1",
    "authors": [
      "Maria Venolia",
      "Reza Marsooli",
      "Jaime R. Calzada"
    ],
    "published": "2025-05-24",
    "abstract": "Wind-wave and ocean current interactions affect critical coastal and oceanic\nprocesses, yet modeling these interactions presents significant challenges. The\nwestern North Atlantic Ocean provides an ideal test environment for coupled\nhydrodynamics and wind wave models, thanks to its energetic surface currents\nsuch as the Gulf Stream. This study evaluates a high-resolution coupled SCHISM\nWWM III model, utilizing NOAA's 'STOFS-3D-Atlantic' computational mesh, while\nincorporating three-dimensional baroclinic dynamics to account for density\nstratification effects. We evaluate the model's calculated water level and\ntidal predictions against NOAA tide gauge measurements during December 2016.\nThe coupled model demonstrates robust skills in reproducing tidal constituents,\nnon-tidal components, and total water level predictions along the U.S. East and\nGulf of Mexico Coasts. In addition, we systematically evaluate three wave\nphysics parameterizations (Ardhuin, Makin and Stam, and Cycle Three) in the\nspectral wave model to quantify their effects on the modeled wave\ncharacteristics. This validated modeling framework enhances our ability to\nunderstand and predict complex coastal and oceanic processes, offering\nsignificant applications for coastal management, maritime operations, and\nclimate adaptation planning throughout the western North Atlantic region.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery",
    "url": "http://arxiv.org/abs/2506.03388v1",
    "authors": [
      "Pengyu Chen",
      "Xiao Huang",
      "Teng Fei",
      "Sicheng Wang"
    ],
    "published": "2025-06-03",
    "abstract": "Environmental soundscapes convey substantial ecological and social\ninformation regarding urban environments; however, their potential remains\nlargely untapped in large-scale geographic analysis. In this study, we\ninvestigate the extent to which urban sounds correspond with visual scenes by\ncomparing various visual representation strategies in capturing acoustic\nsemantics. We employ a multimodal approach that integrates geo-referenced sound\nrecordings with both street-level and remote sensing imagery across three major\nglobal cities: London, New York, and Tokyo. Utilizing the AST model for audio,\nalong with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV\nfor semantic segmentation, we extract embeddings and class-level features to\nevaluate cross-modal similarity. The results indicate that street view\nembeddings demonstrate stronger alignment with environmental sounds compared to\nsegmentation outputs, whereas remote sensing segmentation is more effective in\ninterpreting ecological categories through a Biophony--Geophony--Anthrophony\n(BGA) framework. These findings imply that embedding-based models offer\nsuperior semantic alignment, while segmentation-based methods provide\ninterpretable links between visual structure and acoustic ecology. This work\nadvances the burgeoning field of multimodal urban sensing by offering novel\nperspectives for incorporating sound into geospatial analysis.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings",
    "url": "http://arxiv.org/abs/2506.02868v1",
    "authors": [
      "Amal S. Perera",
      "David Fernandez",
      "Chandi Witharana",
      "Elias Manos",
      "Michael Pimenta",
      "Anna K. Liljedahl",
      "Ingmar Nitze",
      "Yili Yang",
      "Todd Nicholson",
      "Chia-Yu Hsu",
      "Wenwen Li",
      "Guido Grosse"
    ],
    "published": "2025-06-03",
    "abstract": "Accurate mapping of permafrost landforms, thaw disturbances, and human-built\ninfrastructure at pan-Arctic scale using sub-meter satellite imagery is\nincreasingly critical. Handling petabyte-scale image data requires\nhigh-performance computing and robust feature detection models. While\nconvolutional neural network (CNN)-based deep learning approaches are widely\nused for remote sensing (RS),similar to the success in transformer based large\nlanguage models, Vision Transformers (ViTs) offer advantages in capturing\nlong-range dependencies and global context via attention mechanisms. ViTs\nsupport pretraining via self-supervised learning-addressing the common\nlimitation of labeled data in Arctic feature detection and outperform CNNs on\nbenchmark datasets. Arctic also poses challenges for model generalization,\nespecially when features with the same semantic class exhibit diverse spectral\ncharacteristics. To address these issues for Arctic feature detection, we\nintegrate geospatial location embeddings into ViTs to improve adaptation across\nregions. This work investigates: (1) the suitability of pre-trained ViTs as\nfeature extractors for high-resolution Arctic remote sensing tasks, and (2) the\nbenefit of combining image and location embeddings. Using previously published\ndatasets for Arctic feature detection, we evaluate our models on three\ntasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and\nhuman-built infrastructure. We empirically explore multiple configurations to\nfuse image embeddings and location embeddings. Results show that ViTs with\nlocation embeddings outperform prior CNN-based models on two of the three tasks\nincluding F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating\nthe potential of transformer-based models with spatial awareness for Arctic RS\napplications.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale",
    "url": "http://arxiv.org/abs/2506.01297v3",
    "authors": [
      "Ya Wen",
      "Jixuan Cai",
      "Qiyao Ma",
      "Linyan Li",
      "Xinhua Chen",
      "Chris Webster",
      "Yulun Zhou"
    ],
    "published": "2025-06-02",
    "abstract": "Representation learning of geospatial locations remains a core challenge in\nachieving general geospatial intelligence. Current embedding methods often lack\nversatility, limiting their utility across diverse tasks in both human and\nnatural domains. We present MobCLIP, the first nationwide general-purpose\nlocation encoder, integrating an unprecedented diversity of data modalities\nthrough effective and scalable multimodal fusion. Adopting a novel CLIP-based\narchitecture, our framework aligns 100M+ POIs, nationwide remote sensing\nimagery, and structured demographic statistics with a billion-edge mobility\ngraph. By tokenizing spatial locations into grid cells inspired by Vision\nTransformers, we establish a unified representation space bridging mobility\npatterns and multimodal features. To rigorously evaluate the general-purpose\neffectiveness of MobCLIP, we construct a benchmark dataset composed of 11\ndownstream prediction tasks across social, economic, and natural domains.\nExperiments show that MobCLIP, with four input modalities and a compact\n128-dimensional representation space, achieves significantly superior\ngeneral-purpose predictive performances than state-of-the-art models by an\naverage of 35%. Thanks to the effective integration of human-centric\nmodalities, the performance gain is particularly profound in human-centric\ntasks, such as energy consumption (+260%), offline retail consumption amount\n(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we\nfurther demonstrate the scaling behavior in geospatial representation learning.\nWe open-source code and pretrained models at:\nhttps://github.com/ylzhouchris/MobCLIP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "LLM",
      "CLIP"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels",
    "url": "http://arxiv.org/abs/2506.02534v1",
    "authors": [
      "Sining Chen",
      "Yilei Shi",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-06-03",
    "abstract": "Monocular height estimation is considered the most efficient and\ncost-effective means of 3D perception in remote sensing, and it has attracted\nmuch attention since the emergence of deep learning. While training neural\nnetworks requires a large amount of data, data with perfect labels are scarce\nand only available within developed regions. The trained models therefore lack\ngeneralizability, which limits the potential for large-scale application of\nexisting methods. We tackle this problem for the first time, by introducing\ndata with imperfect labels into training pixel-wise height estimation networks,\nincluding labels that are incomplete, inexact, and inaccurate compared to\nhigh-quality labels. We propose an ensemble-based pipeline compatible with any\nmonocular height estimation network. Taking the challenges of noisy labels,\ndomain shift, and long-tailed distribution of height values into consideration,\nwe carefully design the architecture and loss functions to leverage the\ninformation concealed in imperfect labels using weak supervision through\nbalanced soft losses and ordinal constraints. We conduct extensive experiments\non two datasets with different resolutions, DFC23 (0.5 to 1 m) and GBH (3 m).\nThe results indicate that the proposed pipeline outperforms baselines by\nachieving more balanced performance across various domains, leading to\nimprovements of average root mean square errors up to 22.94 %, and 18.62 % on\nDFC23 and GBH, respectively. The efficacy of each design component is validated\nthrough ablation studies. Code is available at\nhttps://github.com/zhu-xlab/weakim2h.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Image Restoration Learning via Noisy Supervision in the Fourier Domain",
    "url": "http://arxiv.org/abs/2506.00564v1",
    "authors": [
      "Haosen Liu",
      "Jiahao Liu",
      "Shan Tan",
      "Edmund Y. Lam"
    ],
    "published": "2025-05-31",
    "abstract": "Noisy supervision refers to supervising image restoration learning with noisy\ntargets. It can alleviate the data collection burden and enhance the practical\napplicability of deep learning techniques. However, existing methods suffer\nfrom two key drawbacks. Firstly, they are ineffective in handling spatially\ncorrelated noise commonly observed in practical applications such as low-light\nimaging and remote sensing. Secondly, they rely on pixel-wise loss functions\nthat only provide limited supervision information. This work addresses these\nchallenges by leveraging the Fourier domain. We highlight that the Fourier\ncoefficients of spatially correlated noise exhibit sparsity and independence,\nmaking them easier to handle. Additionally, Fourier coefficients contain global\ninformation, enabling more significant supervision. Motivated by these\ninsights, we propose to establish noisy supervision in the Fourier domain. We\nfirst prove that Fourier coefficients of a wide range of noise converge in\ndistribution to the Gaussian distribution. Exploiting this statistical\nproperty, we establish the equivalence between using noisy targets and clean\ntargets in the Fourier domain. This leads to a unified learning framework\napplicable to various image restoration tasks, diverse network architectures,\nand different noise models. Extensive experiments validate the outstanding\nperformance of this framework in terms of both quantitative indices and\nperceptual quality.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review",
    "url": "http://arxiv.org/abs/2506.03938v1",
    "authors": [
      "C\u00e9dric L\u00e9onard",
      "Dirk Stober",
      "Martin Schulz"
    ],
    "published": "2025-06-04",
    "abstract": "New UAV technologies and the NewSpace era are transforming Earth Observation\nmissions and data acquisition. Numerous small platforms generate large data\nvolume, straining bandwidth and requiring onboard decision-making to transmit\nhigh-quality information in time. While Machine Learning allows real-time\nautonomous processing, FPGAs balance performance with adaptability to\nmission-specific requirements, enabling onboard deployment. This review\nsystematically analyzes 66 experiments deploying ML models on FPGAs for Remote\nSensing applications. We introduce two distinct taxonomies to capture both\nefficient model architectures and FPGA implementation strategies. For\ntransparency and reproducibility, we follow PRISMA 2020 guidelines and share\nall data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Validating remotely sensed biomass estimates with forest inventory data in the western US",
    "url": "http://arxiv.org/abs/2506.03120v1",
    "authors": [
      "Xiuyu Cao",
      "Joseph O. Sexton",
      "Panshi Wang",
      "Dimitrios Gounaridis",
      "Neil H. Carter",
      "Kai Zhu"
    ],
    "published": "2025-06-03",
    "abstract": "Monitoring aboveground biomass (AGB) and its density (AGBD) at high\nresolution is essential for carbon accounting and ecosystem management. While\nNASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission\nprovides globally distributed reference measurements for AGBD estimation, the\nmajority of commercial remote sensing products based on GEDI remain without\nrigorous or independent validation. Here, we present an independent regional\nvalidation of an AGBD dataset offered by terraPulse, Inc., based on independent\nreference data from the US Forest Service Forest Inventory and Analysis (FIA)\nprogram. Aggregated to 64,000-hectare hexagons and US counties across the US\nstates of Utah, Nevada, and Washington, we found very strong agreement between\nterraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE =\n26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale,\nagreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95.\nSpatial and statistical analyses indicated that terraPulse AGBD values tended\nto exceed FIA estimates in non-forest areas, likely due to FIA's limited\nsampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited\nlower values in high-biomass forests, likely due to saturation effects in its\noptical remote-sensing covariates. This study advances operational carbon\nmonitoring by delivering a scalable framework for comprehensive AGBD validation\nusing independent FIA data, as well as a benchmark validation of a new\ncommercial dataset for global biomass monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery",
    "url": "http://arxiv.org/abs/2506.03114v1",
    "authors": [
      "Michelle Chen",
      "David Russell",
      "Amritha Pallavoor",
      "Derek Young",
      "Jane Wu"
    ],
    "published": "2025-06-03",
    "abstract": "Large-scale delineation of individual trees from remote sensing imagery is\ncrucial to the advancement of ecological research, particularly as climate\nchange and other environmental factors rapidly transform forest landscapes\nacross the world. Current RGB tree segmentation methods rely on training\nspecialized machine learning models with labeled tree datasets. While these\nlearning-based approaches can outperform manual data collection when accurate,\nthe existing models still depend on training data that's hard to scale. In this\npaper, we investigate the efficacy of using a state-of-the-art image\nsegmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for\nindividual tree detection and segmentation. We evaluate a pretrained SAM2 model\non two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot\ntransfer by using predictions from an existing tree detection model as prompts.\nOur results suggest that SAM2 not only has impressive generalization\ncapabilities, but also can form a natural synergy with specialized methods\ntrained on in-domain labeled data. We find that applying large pretrained\nmodels to problems in remote sensing is a promising avenue for future progress.\nWe make our code available at:\nhttps://github.com/open-forest-observatory/tree-detection-framework.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Improving Post-Processing for Quantitative Precipitation Forecasting Using Deep Learning: Learning Precipitation Physics from High-Resolution Observations",
    "url": "http://arxiv.org/abs/2506.03842v1",
    "authors": [
      "ChangJae Lee",
      "Heecheol Yang",
      "Byeonggwon Kim"
    ],
    "published": "2025-06-04",
    "abstract": "Accurate quantitative precipitation forecasting (QPF) remains one of the main\nchallenges in numerical weather prediction (NWP), primarily due to the\ndifficulty of representing the full complexity of atmospheric microphysics\nthrough parameterization schemes. This study introduces a deep learning-based\npost-processing model, DL-QPF, which diagnoses precipitation fields from\nmeteorological forecasts by learning directly from high-resolution radar\nestimates precipitation. The DL-QPF model is constructed using a\nPatch-conditional Generative Adversarial Network (Patch-cGAN) architecture\ncombined with a U-Net generator and a discriminator. The generator learns\nmeteorological features relevant to precipitation, while the adversarial loss\nfrom the discriminator encourages the generation of realistic rainfall patterns\nand distributions. Training is performed on three years of warm-season data\nover the Korean Peninsula, with input variables derived from ECMWF's Integrated\nForecasting System High-Resolution forecast (IFS-HRES). Model verification is\nconducted against multiple reference models, including global (IFS-HRES, KIM),\nregional (KIM-Regional, KIM-LENS), and AI-based (GraphCast) forecasts.\nVerification across multiple rainfall thresholds shows that DL-QPF achieves a\nfrequency bias near one and superior success ratios. Particularly for heavy and\nintense rainfall events, DL-QPF outperforms both conventional NWP and an AI\nmodel, demonstrating improved skill in capturing high-intensity precipitation.\nThis study highlights the potential of observational data-driven deep learning\napproaches in post-processing QPF. By directly learning from observations,\nDL-QPF reduces systematic biases and enhances the realism of forecasted\nrainfall distributions. These results demonstrate the model's potential to\nenhance QPF realism.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "GAN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Probabilistic measures afford fair comparisons of AIWP and NWP model output",
    "url": "http://arxiv.org/abs/2506.03744v1",
    "authors": [
      "Tilmann Gneiting",
      "Tobias Biegert",
      "Kristof Kraus",
      "Eva-Maria Walz",
      "Alexander I. Jordan",
      "Sebastian Lerch"
    ],
    "published": "2025-06-04",
    "abstract": "We introduce a new measure for fair and meaningful comparisons of\nsingle-valued output from artificial intelligence based weather prediction\n(AIWP) and numerical weather prediction (NWP) models, called potential\ncontinuous ranked probability score (PC). In a nutshell, we subject the\ndeterministic backbone of physics-based and data-driven models post hoc to the\nsame statistical postprocessing technique, namely, isotonic distributional\nregression (IDR). Then we find PC as the mean continuous ranked probability\nscore (CRPS) of the postprocessed probabilistic forecasts. The nonnegative PC\nmeasure quantifies potential predictive performance and is invariant under\nstrictly increasing transformations of the model output. PC attains its most\ndesirable value of zero if, and only if, the weather outcome Y is a fixed,\nnon-decreasing function of the model output X. The PC measure is recorded in\nthe unit of the outcome, has an upper bound of one half times the mean absolute\ndifference between outcomes, and serves as a proxy for the mean CRPS of\nreal-time, operational probabilistic products. When applied to WeatherBench 2\ndata, our approach demonstrates that the data-driven GraphCast model\noutperforms the leading, physics-based European Centre for Medium Range Weather\nForecasts (ECMWF) high-resolution (HRES) model. Furthermore, the PC measure for\nthe HRES model aligns exceptionally well with the mean CRPS of the operational\nECMWF ensemble. Across application domains, our approach affords comparisons of\nsingle-valued forecasts in settings where the pre-specification of a loss\nfunction -- which is the usual, and principally superior, procedure in forecast\ncontests, administrative, and benchmarks settings -- places competitors on\nunequal footings.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
    "url": "http://arxiv.org/abs/2506.03210v1",
    "authors": [
      "Qiusheng Huang",
      "Yuan Niu",
      "Xiaohui Zhong",
      "Anboyu Guo",
      "Lei Chen",
      "Dianjun Zhang",
      "Xuefeng Zhang",
      "Hao Li"
    ],
    "published": "2025-06-03",
    "abstract": "Accurate, high-resolution ocean forecasting is crucial for maritime\noperations and environmental monitoring. While traditional numerical models are\ncapable of producing sub-daily, eddy-resolving forecasts, they are\ncomputationally intensive and face challenges in maintaining accuracy at fine\nspatial and temporal scales. In contrast, recent data-driven approaches offer\nimproved computational efficiency and emerging potential, yet typically operate\nat daily resolution and struggle with sub-daily predictions due to error\naccumulation over time. We introduce FuXi-Ocean, the first data-driven global\nocean forecasting model achieving six-hourly predictions at eddy-resolving\n1/12{\\deg} spatial resolution, reaching depths of up to 1500 meters. The model\narchitecture integrates a context-aware feature extraction module with a\npredictive network employing stacked attention blocks. The core innovation is\nthe Mixture-of-Time (MoT) module, which adaptively integrates predictions from\nmultiple temporal contexts by learning variable-specific reliability ,\nmitigating cumulative errors in sequential forecasting. Through comprehensive\nexperimental evaluation, FuXi-Ocean demonstrates superior skill in predicting\nkey variables, including temperature, salinity, and currents, across multiple\ndepths.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Using Diffusion Models to do Data Assimilation",
    "url": "http://arxiv.org/abs/2506.02249v1",
    "authors": [
      "Daniel Hodyss",
      "Matthias Morzfeld"
    ],
    "published": "2025-06-02",
    "abstract": "The recent surge in machine learning (ML) methods for geophysical modeling\nhas raised the question of how these same ML methods might be applied to data\nassimilation (DA). We focus on diffusion modeling (a form of generative\nartificial intelligence) and on systems that can perform the entire DA, rather\nthan on ML-based tools that are used within an otherwise conventional DA\nsystem. We explain that there are (at least) three different types of\ndiffusion-based DA systems and we show in detail that the three systems differ\nin the type of posterior distribution they target for sampling. The different\nposterior distributions correspond to different priors and\\slash or\nlikelihoods, which in turn results in different types of training data sets,\ndifferent computational requirements and different qualities of their state\nestimates. We discuss the implications of these findings for the use of\ndiffusion modeling in DA.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ROC Curves for Spatial Point Patterns and Presence-Absence Data",
    "url": "http://arxiv.org/abs/2506.03414v1",
    "authors": [
      "Adrian Baddeley",
      "Ege Rubak",
      "Suman Rakshit",
      "Gopalan Nair"
    ],
    "published": "2025-06-03",
    "abstract": "Receiver Operating Characteristic (ROC) curves have recently been used to\nevaluate the performance of models for spatial presence-absence or\npresence-only data. Applications include species distribution modelling and\nmineral prospectivity analysis. We clarify the interpretation of the ROC curve\nin this context. Contrary to statements in the literature, ROC does not measure\ngoodness-of-fit of a spatial model, and its interpretation as a measure of\npredictive ability is weak; it is a measure of ranking ability, insensitive to\nthe precise form of the model. To gain insight we draw connections between ROC\nand existing statistical techniques for spatial point pattern data. The area\nunder the ROC curve (AUC) is related to hypothesis tests of the null hypothesis\nthat the explanatory variables have no effect. The shape of the ROC curve has a\ndiagnostic interpretation. This suggests several new techniques, which extend\nthe scope of application of ROC curves for spatial data, to support variable\nselection and model selection, analysis of segregation between different types\nof points, adjustment for a baseline, and analysis of spatial case-control\ndata. The new techniques are illustrated with several real example datasets.\nOpen source R code implementing the techniques is available in the development\nversion of our package spatstat [Baddeley and Turner, 2005, Baddeley et al.,\n2015] and will be included in the next public release.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Modelling benthic animals in space and time using Bayesian Point Process with cross validation: the case of Holoturians",
    "url": "http://arxiv.org/abs/2506.01763v2",
    "authors": [
      "Daniele Poggio",
      "Gian Mario Sangiovanni",
      "Gianluca Mastrantonio",
      "Giovanna Jona Lasinio",
      "Edoardo Casoli",
      "Stefano Moro",
      "Daniele Ventura"
    ],
    "published": "2025-06-02",
    "abstract": "Understanding the spatial distribution of Holothurians is an essential task\nfor ecosystem monitoring and sustainable management, particularly in the\nMediterranean habitats. However, species distribution modeling is often\ncomplicated by the presence-only nature of the data and heterogeneous sampling\ndesigns. This study develops a spatio-temporal framework based on Log-Gaussian\nCox Processes to analyze Holothurians' positions collected across nine survey\ncampaigns conducted from 2022 to 2024 near Giglio Island, Italy. The surveys\ncombined high-resolution photogrammetry with diver-based visual censuses,\nleading to varying detection probabilities across habitats, especially within\nPosidonia oceanica meadows. We adopt a model with a shared spatial Gaussian\nprocess component to accommodate this complexity, accounting for habitat\nstructure, environmental covariates, and temporal variability. Model estimation\nis performed using Integrated Nested Laplace Approximation. We evaluate the\npredictive performances of alternative model specifications through a novel\nk-fold cross-validation strategy for point processes, using the Continuous\nRanked Probability Score. Our approach provides a flexible and computationally\nefficient framework for integrating heterogeneous presence-only data in marine\necology and comparing the predictive ability of alternative models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models",
    "url": "http://arxiv.org/abs/2506.08780v1",
    "authors": [
      "Isaac Corley",
      "Lakshay Sharma",
      "Ruth Crasto"
    ],
    "published": "2025-06-10",
    "abstract": "The Landsat program offers over 50 years of globally consistent Earth\nimagery. However, the lack of benchmarks for this data constrains progress\ntowards Landsat-based Geospatial Foundation Models (GFM). In this paper, we\nintroduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that\nadapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and\nLC100-L. We establish baseline and standardized evaluation methods across both\ncommon architectures and Landsat foundation models pretrained on the SSL4EO-L\ndataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract\nbetter representations for downstream tasks in comparison to ImageNet,\nincluding performance gains of +4% OA and +5.1% mAP on EuroSAT-L and\nBigEarthNet-L.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation",
    "url": "http://arxiv.org/abs/2506.08772v2",
    "authors": [
      "Jiayi Song",
      "Kaiyu Li",
      "Xiangyong Cao",
      "Deyu Meng"
    ],
    "published": "2025-06-10",
    "abstract": "Semantic segmentation in remote sensing images is crucial for various\napplications, yet its performance is heavily reliant on large-scale,\nhigh-quality pixel-wise annotations, which are notoriously expensive and\ntime-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a\npromising alternative to mitigate this data dependency. However, existing SSS\nmethods often struggle with the inherent distribution mismatch between limited\nlabeled data and abundant unlabeled data, leading to suboptimal generalization.\nTo alleviate this issue, we attempt to introduce the Vision Foundation Models\n(VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs\npossess robust generalization capabilities that can effectively bridge this\ndistribution gap and provide strong semantic priors for SSS. Inspired by this,\nwe introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework\nthat leverages the powerful semantic knowledge embedded in VFMs to guide\nsemi-supervised learning in remote sensing. Specifically, RS-MTDF employs\nmultiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing\nfeature-level distillation to align student features with their robust\nrepresentations. To further enhance discriminative power, the distilled\nknowledge is seamlessly fused into the student decoder. Extensive experiments\non three challenging remote sensing datasets demonstrate that RS-MTDF\nconsistently achieves state-of-the-art performance. Notably, our method\noutperforms existing approaches across various label ratios on LoveDA and\nsecures the highest IoU in the majority of semantic categories. These results\nunderscore the efficacy of multi-teacher VFM guidance in significantly\nenhancing both generalization and semantic understanding for remote sensing\nsegmentation. Ablation studies further validate the contribution of each\nproposed module.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial",
    "url": "http://arxiv.org/abs/2506.10386v1",
    "authors": [
      "Jerry Yan",
      "Chinmay Talegaonkar",
      "Nicholas Antipa",
      "Eric Terrill",
      "Sophia Merrifield"
    ],
    "published": "2025-06-12",
    "abstract": "The burial state of anthropogenic objects on the seafloor provides insight\ninto localized sedimentation dynamics and is also critical for assessing\necological risks, potential pollutant transport, and the viability of recovery\nor mitigation strategies for hazardous materials such as munitions. Accurate\nburial depth estimation from remote imagery remains difficult due to partial\nocclusion, poor visibility, and object degradation. This work introduces a\ncomputer vision pipeline, called PoseIDON, which combines deep foundation model\nfeatures with multiview photogrammetry to estimate six degrees of freedom\nobject pose and the orientation of the surrounding seafloor from ROV video.\nBurial depth is inferred by aligning CAD models of the objects with observed\nimagery and fitting a local planar approximation of the seafloor. The method is\nvalidated using footage of 54 objects, including barrels and munitions,\nrecorded at a historic ocean dumpsite in the San Pedro Basin. The model\nachieves a mean burial depth error of approximately 10 centimeters and resolves\nspatial burial patterns that reflect underlying sediment transport processes.\nThis approach enables scalable, non-invasive mapping of seafloor burial and\nsupports environmental assessment at contaminated sites.",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity",
    "url": "http://arxiv.org/abs/2506.08051v1",
    "authors": [
      "Mahmuda Sultana Mimi",
      "Md Monzurul Islam",
      "Anannya Ghosh Tusti",
      "Shriyank Somvanshi",
      "Subasish Das"
    ],
    "published": "2025-06-09",
    "abstract": "Understanding the spatial and temporal dynamics of automated vehicle (AV)\ncrash severity is critical for advancing urban mobility safety and\ninfrastructure planning. In this work, we introduce ST-GraphNet, a\nspatio-temporal graph neural network framework designed to model and predict AV\ncrash severity by using both fine-grained and region-aggregated spatial graphs.\nUsing a balanced dataset of 2,352 real-world AV-related crash reports from\nTexas (2024), including geospatial coordinates, crash timestamps, SAE\nautomation levels, and narrative descriptions, we construct two complementary\ngraph representations: (1) a fine-grained graph with individual crash events as\nnodes, where edges are defined via spatio-temporal proximity; and (2) a\ncoarse-grained graph where crashes are aggregated into Hexagonal Hierarchical\nSpatial Indexing (H3)-based spatial cells, connected through hexagonal\nadjacency. Each node in the graph is enriched with multimodal data, including\nsemantic, spatial, and temporal attributes, including textual embeddings from\ncrash narratives using a pretrained Sentence-BERT model. We evaluate various\ngraph neural network (GNN) architectures, such as Graph Convolutional Networks\n(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN\n(DSTGCN), to classify crash severity and predict high-risk regions. Our\nproposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3\ngraph, achieves a test accuracy of 97.74\\%, substantially outperforming the\nbest fine-grained model (64.7\\% test accuracy). These findings highlight the\neffectiveness of spatial aggregation, dynamic message passing, and multi-modal\nfeature integration in capturing the complex spatio-temporal patterns\nunderlying AV crash severity.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": []
  },
  {
    "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images",
    "url": "http://arxiv.org/abs/2506.09553v1",
    "authors": [
      "Ligao Deng",
      "Yupeng Deng",
      "Yu Meng",
      "Jingbo Chen",
      "Zhihao Xi",
      "Diyou Liu",
      "Qifeng Chu"
    ],
    "published": "2025-06-11",
    "abstract": "Road networks are crucial for mapping, autonomous driving, and disaster\nresponse. While manual annotation is costly, deep learning offers efficient\nextraction. Current methods include postprocessing (prone to errors), global\nparallel (fast but misses nodes), and local iterative (accurate but slow). We\npropose GLD-Road, a two-stage model combining global efficiency and local\nprecision. First, it detects road nodes and connects them via a Connect Module.\nThen, it iteratively refines broken roads using local searches, drastically\nreducing computation. Experiments show GLD-Road outperforms state-of-the-art\nmethods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also\nreduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++\n(local). The experimental results are available at\nhttps://github.com/ucas-dlg/GLD-Road.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2506.06667v1",
    "authors": [
      "Yu-Hsuan Ho",
      "Ali Mostafavi"
    ],
    "published": "2025-06-07",
    "abstract": "Most post-disaster damage classifiers succeed only when destructive forces\nleave clear spectral or structural signatures -- conditions rarely present\nafter inundation. Consequently, existing models perform poorly at identifying\nflood-related building damages. The model presented in this study,\nFlood-DamageSense, addresses this gap as the first deep-learning framework\npurpose-built for building-level flood-damage assessment. The architecture\nfuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical\nbasemaps and an inherent flood-risk layer that encodes long-term exposure\nprobabilities, guiding the network toward plausibly affected structures even\nwhen compositional change is minimal. A multimodal Mamba backbone with a\nsemi-Siamese encoder and task-specific decoders jointly predicts (1) graded\nbuilding-damage states, (2) floodwater extent, and (3) building footprints.\nTraining and evaluation on Hurricane Harvey (2017) imagery from Harris County,\nTexas -- supported by insurance-derived property-damage extents -- show a mean\nF1 improvement of up to 19 percentage points over state-of-the-art baselines,\nwith the largest gains in the frequently misclassified \"minor\" and \"moderate\"\ndamage categories. Ablation studies identify the inherent-risk feature as the\nsingle most significant contributor to this performance boost. An end-to-end\npost-processing pipeline converts pixel-level outputs to actionable,\nbuilding-scale damage maps within minutes of image acquisition. By combining\nrisk-aware modeling with SAR's all-weather capability, Flood-DamageSense\ndelivers faster, finer-grained, and more reliable flood-damage intelligence to\nsupport post-disaster decision-making and resource allocation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Comparative Study of U-Net Architectures for Change Detection in Satellite Images",
    "url": "http://arxiv.org/abs/2506.07925v1",
    "authors": [
      "Yaxita Amin",
      "Naimisha S Trivedi",
      "Rashmi Bhattad"
    ],
    "published": "2025-06-09",
    "abstract": "Remote sensing change detection is essential for monitoring the everchanging\nlandscapes of the Earth. The U-Net architecture has gained popularity for its\ncapability to capture spatial information and perform pixel-wise\nclassification. However, their application in the Remote sensing field remains\nlargely unexplored. Therefore, this paper fill the gap by conducting a\ncomprehensive analysis of 34 papers. This study conducts a comparison and\nanalysis of 18 different U-Net variations, assessing their potential for\ndetecting changes in remote sensing. We evaluate both benefits along with\ndrawbacks of each variation within the framework of this particular\napplication. We emphasize variations that are explicitly built for change\ndetection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.\nThe analysis highlights the significance of aspects such as managing data from\ndifferent time periods and collecting relationships over a long distance to\nenhance the precision of change detection. This study provides valuable\ninsights for researchers and practitioners that choose U-Net versions for\nremote sensing change detection tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "A multi-scale loss formulation for learning a probabilistic model with proper score optimisation",
    "url": "http://arxiv.org/abs/2506.10868v1",
    "authors": [
      "Simon Lang",
      "Martin Leutbecher",
      "Pedro Maciel"
    ],
    "published": "2025-06-12",
    "abstract": "We assess the impact of a multi-scale loss formulation for training\nprobabilistic machine-learned weather forecasting models. The multi-scale loss\nis tested in AIFS-CRPS, a machine-learned weather forecasting model developed\nat the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS-CRPS is\ntrained by directly optimising the almost fair continuous ranked probability\nscore (afCRPS). The multi-scale loss better constrains small scale variability\nwithout negatively impacting forecast skill. This opens up promising directions\nfor future work in scale-aware model training.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Skillful joint probabilistic weather forecasting from marginals",
    "url": "http://arxiv.org/abs/2506.10772v1",
    "authors": [
      "Ferran Alet",
      "Ilan Price",
      "Andrew El-Kadi",
      "Dominic Masters",
      "Stratis Markou",
      "Tom R. Andersson",
      "Jacklynn Stott",
      "Remi Lam",
      "Matthew Willson",
      "Alvaro Sanchez-Gonzalez",
      "Peter Battaglia"
    ],
    "published": "2025-06-12",
    "abstract": "Machine learning (ML)-based weather models have rapidly risen to prominence\ndue to their greater accuracy and speed than traditional forecasts based on\nnumerical weather prediction (NWP), recently outperforming traditional\nensembles in global probabilistic weather forecasting. This paper presents FGN,\na simple, scalable and flexible modeling approach which significantly\noutperforms the current state-of-the-art models. FGN generates ensembles via\nlearned model-perturbations with an ensemble of appropriately constrained\nmodels. It is trained directly to minimize the continuous rank probability\nscore (CRPS) of per-location forecasts. It produces state-of-the-art ensemble\nforecasts as measured by a range of deterministic and probabilistic metrics,\nmakes skillful ensemble tropical cyclone track predictions, and captures joint\nspatial structure despite being trained only on marginals.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Pushing the Limits of Extreme Weather: Constructing Extreme Heatwave Storylines with Differentiable Climate Models",
    "url": "http://arxiv.org/abs/2506.10660v1",
    "authors": [
      "Tim Whittaker",
      "Alejandro Di Luca"
    ],
    "published": "2025-06-12",
    "abstract": "Understanding the plausible upper bounds of extreme weather events is\nessential for risk assessment in a warming climate. Existing methods, based on\nlarge ensembles of physics-based models, are often computationally expensive or\nlack the fidelity needed to simulate rare, high-impact extremes. Here, we\npresent a novel framework that leverages a differentiable hybrid climate model,\nNeuralGCM, to optimize initial conditions and generate physically consistent\nworst-case heatwave trajectories. Applied to the 2021 Pacific Northwest\nheatwave, our method produces temperature anomalies up to 3.7 $^\\circ$C above\nthe most extreme member of a 75-member ensemble. These trajectories feature\nintensified atmospheric blocking and amplified Rossby wave patterns--hallmarks\nof severe heat events. Our results demonstrate that differentiable climate\nmodels can efficiently explore the upper tails of event likelihoods, providing\na powerful new approach for constructing targeted storylines of extreme weather\nunder climate change.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Prediction of steady states in a marine ecosystem model by a machine learning technique",
    "url": "http://arxiv.org/abs/2506.10475v1",
    "authors": [
      "Sarker Miraz Mahfuz",
      "Thomas Slawig"
    ],
    "published": "2025-06-12",
    "abstract": "We used precomputed steady states obtained by a spin-up for a global marine\necosystem model as training data to build a mapping from the small number of\nbiogeochemical model parameters onto the three-dimensional converged steady\nannual cycle. The mapping was performed by a conditional variational\nautoencoder (CVAE) with mass correction. Applied for test data, we show that\nthe prediction obtained by the CVAE already gives a reasonable good\napproximation of the steady states obtained by a regular spin-up. However, the\npredictions do not reach the same level of annual periodicity as those obtained\nin the original spin-up data. Thus, we took the predictions as initial values\nfor a spin-up. We could show that the number of necessary iterations,\ncorresponding to model years, to reach a prescribed stopping criterion in the\nspin-up could be significantly reduced compared to the use of the originally\nuniform, constant initial value. The amount of reduction depends on the applied\nstopping criterion, measuring the periodicity of the solution. The savings in\nneeded iterations and, thus, computing time for the spin-up ranges from 50 to\n95\\%, depending on the stopping criterion for the spin-up. We compared these\nresults with the use of the mean of the training data as an initial value. We\nfound that this also accelerates the spin-up, but only by a much lower factor.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion",
    "url": "http://arxiv.org/abs/2506.10391v1",
    "authors": [
      "Yuanyi Song",
      "Pumeng Lyu",
      "Ben Fei",
      "Fenghua Ling",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "published": "2025-06-12",
    "abstract": "Accurate reconstruction of ocean is essential for reflecting global climate\ndynamics and supporting marine meteorological research. Conventional methods\nface challenges due to sparse data, algorithmic complexity, and high\ncomputational costs, while increasing usage of machine learning (ML) method\nremains limited to reconstruction problems at the sea surface and local\nregions, struggling with issues like cloud occlusion. To address these\nlimitations, this paper proposes ReconMOST, a data-driven guided diffusion\nmodel framework for multi-layer sea temperature reconstruction. Specifically,\nwe first pre-train an unconditional diffusion model using a large collection of\nhistorical numerical simulation data, enabling the model to attain physically\nconsistent distribution patterns of ocean temperature fields. During the\ngeneration phase, sparse yet high-accuracy in-situ observational data are\nutilized as guidance points for the reverse diffusion process, generating\naccurate reconstruction results. Importantly, in regions lacking direct\nobservational data, the physically consistent spatial distribution patterns\nlearned during pre-training enable implicitly guided and physically plausible\nreconstructions. Our method extends ML-based SST reconstruction to a global,\nmulti-layer setting, handling over 92.5% missing data while maintaining\nreconstruction accuracy, spatial resolution, and superior generalization\ncapability. We pre-train our model on CMIP6 numerical simulation data and\nconduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The\nresults of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on\nreconstruction, and 0.633 on total, respectively, demonstrating the\neffectiveness and robustness of the proposed framework. Our source code is\navailable at https://github.com/norsheep/ReconMOST.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Causal Climate Emulation with Bayesian Filtering",
    "url": "http://arxiv.org/abs/2506.09891v1",
    "authors": [
      "Sebastian Hickman",
      "Ilija Trajkovic",
      "Julia Kaltenborn",
      "Francis Pelletier",
      "Alex Archibald",
      "Yaniv Gurwicz",
      "Peer Nowack",
      "David Rolnick",
      "Julien Boussard"
    ],
    "published": "2025-06-11",
    "abstract": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale",
    "url": "http://arxiv.org/abs/2506.09733v1",
    "authors": [
      "Minjong Cheon"
    ],
    "published": "2025-06-11",
    "abstract": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "DEF: Diffusion-augmented Ensemble Forecasting",
    "url": "http://arxiv.org/abs/2506.07324v1",
    "authors": [
      "David Millard",
      "Arielle Carr",
      "St\u00e9phane Gaudreault",
      "Ali Baheri"
    ],
    "published": "2025-06-08",
    "abstract": "We present DEF (\\textbf{\\ul{D}}iffusion-augmented \\textbf{\\ul{E}}nsemble\n\\textbf{\\ul{F}}orecasting), a novel approach for generating initial condition\nperturbations. Modern approaches to initial condition perturbations are\nprimarily designed for numerical weather prediction (NWP) solvers, limiting\ntheir applicability in the rapidly growing field of machine learning for\nweather prediction. Consequently, stochastic models in this domain are often\ndeveloped on a case-by-case basis. We demonstrate that a simple conditional\ndiffusion model can (1) generate meaningful structured perturbations, (2) be\napplied iteratively, and (3) utilize a guidance term to intuitivey control the\nlevel of perturbation. This method enables the transformation of any\ndeterministic neural forecasting system into a stochastic one. With our\nstochastic extended systems, we show that the model accumulates less error over\nlong-term forecasts while producing meaningful forecast distributions. We\nvalidate our approach on the 5.625$^\\circ$ ERA5 reanalysis dataset, which\ncomprises atmospheric and surface variables over a discretized global grid,\nspanning from the 1960s to the present. On this dataset, our method\ndemonstrates improved predictive performance along with reasonable spread\nestimates.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places",
    "url": "http://arxiv.org/abs/2506.14570v1",
    "authors": [
      "Mohammad Hashemi",
      "Andreas Zufle"
    ],
    "published": "2025-06-17",
    "abstract": "Capturing human mobility is essential for modeling how people interact with\nand move through physical spaces, reflecting social behavior, access to\nresources, and dynamic spatial patterns. To support scalable and transferable\nanalysis across diverse geographies and contexts, there is a need for a\ngeneralizable foundation model for spatiotemporal data. While foundation models\nhave transformed language and vision, they remain limited in handling the\nunique challenges posed by the spatial, temporal, and semantic complexity of\nmobility data. This vision paper advocates for a new class of spatial\nfoundation models that integrate geolocation semantics with human mobility\nacross multiple scales. Central to our vision is a shift from modeling discrete\npoints of interest to understanding places: dynamic, context-rich regions\nshaped by human behavior and mobility that may comprise many places of\ninterest. We identify key gaps in adaptability, scalability, and multi-granular\nreasoning, and propose research directions focused on modeling places and\nenabling efficient learning. Our goal is to guide the development of scalable,\ncontext-aware models for next-generation geospatial intelligence. These models\nunlock powerful applications ranging from personalized place discovery and\nlogistics optimization to urban planning, ultimately enabling smarter and more\nresponsive spatial decision-making.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation",
    "url": "http://arxiv.org/abs/2506.13599v1",
    "authors": [
      "Yuwei Du",
      "Jie Feng",
      "Jian Yuan",
      "Yong Li"
    ],
    "published": "2025-06-16",
    "abstract": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset",
    "url": "http://arxiv.org/abs/2506.14765v1",
    "authors": [
      "Nikolaos Dionelis",
      "Jente Bosmans",
      "Riccardo Musto",
      "Giancarlo Paoletti",
      "Simone Sarti",
      "Giacomo Cascarano",
      "Casper Fibaek",
      "Luke Camilleri",
      "Bertrand Le Saux",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-06-17",
    "abstract": "Today, Earth Observation (EO) satellites generate massive volumes of data,\nwith the Copernicus Sentinel-2 constellation alone producing approximately\n1.6TB per day. To fully exploit this information, it is essential to pretrain\nEO Foundation Models (FMs) on large unlabeled datasets, enabling efficient\nfine-tuning for several different downstream tasks with minimal labeled data.\nIn this work, we present the scaling-up of our recently proposed EO Foundation\nModel, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which\ncovers the vast majority of the Earth's surface, as well as on the specialized\nsubset FastTOM 2TB that does not include oceans and ice. We develop and study\nvarious PhilEO model variants with different numbers of parameters and\narchitectures. Finally, we fine-tune the models on the PhilEO Bench for road\ndensity estimation, building density pixel-wise regression, and land cover\nsemantic segmentation, and we evaluate the performance. Our results demonstrate\nthat for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB\nmodel outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots\nfor road density estimation and building density regression, PhilEO 200M\nFastTOM outperforms all the other models. The effectiveness of both dataset and\nmodel scaling is validated using the PhilEO Bench. We also study the impact of\narchitecture scaling, transitioning from U-Net Convolutional Neural Networks\n(CNN) to Vision Transformers (ViT).",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Regression"
    ]
  },
  {
    "title": "Mapping Farmed Landscapes from Remote Sensing",
    "url": "http://arxiv.org/abs/2506.13993v1",
    "authors": [
      "Michelangelo Conserva",
      "Alex Wilson",
      "Charlotte Stanton",
      "Vishal Batchu",
      "Varun Gulshan"
    ],
    "published": "2025-06-16",
    "abstract": "Effective management of agricultural landscapes is critical for meeting\nglobal biodiversity targets, but efforts are hampered by the absence of\ndetailed, large-scale ecological maps. To address this, we introduce\nFarmscapes, the first large-scale (covering most of England), high-resolution\n(25cm) map of rural landscape features, including ecologically vital elements\nlike hedgerows, woodlands, and stone walls. This map was generated using a deep\nlearning segmentation model trained on a novel, dataset of 942 manually\nannotated tiles derived from aerial imagery. Our model accurately identifies\nkey habitats, achieving high f1-scores for woodland (96\\%) and farmed land\n(95\\%), and demonstrates strong capability in segmenting linear features, with\nan F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google\nEarth Engine, we provide a powerful, open-access tool for ecologists and\npolicymakers. This work enables data-driven planning for habitat restoration,\nsupports the monitoring of initiatives like the EU Biodiversity Strategy, and\nlays the foundation for advanced analysis of landscape connectivity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data",
    "url": "http://arxiv.org/abs/2506.13505v1",
    "authors": [
      "Vasiliki Balaska",
      "Ioannis Tsampikos Papapetros",
      "Katerina Maria Oikonomou",
      "Loukas Bampis",
      "Antonios Gasteratos"
    ],
    "published": "2025-06-16",
    "abstract": "The mining sector increasingly adopts digital tools to improve operational\nefficiency, safety, and data-driven decision-making. One of the key challenges\nremains the reliable acquisition of high-resolution, geo-referenced spatial\ninformation to support core activities such as extraction planning and on-site\nmonitoring. This work presents an integrated system architecture that combines\nUAV-based sensing, LiDAR terrain modeling, and deep learning-based object\ndetection to generate spatially accurate information for open-pit mining\nenvironments. The proposed pipeline includes geo-referencing, 3D\nreconstruction, and object localization, enabling structured spatial outputs to\nbe integrated into an industrial digital twin platform. Unlike traditional\nstatic surveying methods, the system offers higher coverage and automation\npotential, with modular components suitable for deployment in real-world\nindustrial contexts. While the current implementation operates in post-flight\nbatch mode, it lays the foundation for real-time extensions. The system\ncontributes to the development of AI-enhanced remote sensing in mining by\ndemonstrating a scalable and field-validated geospatial data workflow that\nsupports situational awareness and infrastructure safety.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder",
    "url": "http://arxiv.org/abs/2506.14906v1",
    "authors": [
      "Robert Czupryniak",
      "Abhishek Chakraborty",
      "Andrew N. Jordan",
      "John C. Howell"
    ],
    "published": "2025-06-17",
    "abstract": "We apply machine learning methods to demonstrate range superresolution in\nremote sensing radar detection. Specifically, we implement a denoising\nautoencoder to estimate the distance between two equal intensity scatterers in\nthe subwavelength regime. The machine learning models are trained on waveforms\nsubject to a bandlimit constraint such that ranges much smaller than the\ninverse bandlimit are optimized in their precision. The autoencoder achieves\neffective dimensionality reduction, with the bottleneck layer exhibiting a\nstrong and consistent correlation with the true scatterer separation. We\nconfirm reproducibility across different training sessions and network\ninitializations by analyzing the scaled encoder outputs and their robustness to\nnoise. We investigate the behavior of the bottleneck layer for the following\ntypes of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse,\nand a theoretically near-optimal pulse created from a spherical Bessel function\nbasis. The Bessel signal performs best, followed by the triangle wave, with the\nsinc signal performing worst, highlighting the crucial role of signal design in\nthe success of machine-learning-based range resolution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Projecting U.S. coastal storm surge risks and impacts with deep learning",
    "url": "http://arxiv.org/abs/2506.13963v1",
    "authors": [
      "Julian R. Rice",
      "Karthik Balaguru",
      "Fadia Ticona Rollano",
      "John Wilson",
      "Brent Daniel",
      "David Judi",
      "Ning Sun",
      "L. Ruby Leung"
    ],
    "published": "2025-06-16",
    "abstract": "Storm surge is one of the deadliest hazards posed by tropical cyclones (TCs),\nyet assessing its current and future risk is difficult due to the phenomenon's\nrarity and physical complexity. Recent advances in artificial intelligence\napplications to natural hazard modeling suggest a new avenue for addressing\nthis problem. We utilize a deep learning storm surge model to efficiently\nestimate coastal surge risk in the United States from 900,000 synthetic TC\nevents, accounting for projected changes in TC behavior and sea levels. The\nderived historical 100-year surge (the event with a 1% yearly exceedance\nprobability) agrees well with historical observations and other modeling\ntechniques. When coupled with an inundation model, we find that heightened TC\nintensities and sea levels by the end of the century result in a 50% increase\nin population at risk. Key findings include markedly heightened risk in\nFlorida, and critical thresholds identified in Georgia and South Carolina.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Towards NoahMP-AI: Enhancing Land Surface Model Prediction with Deep Learning",
    "url": "http://arxiv.org/abs/2506.12919v1",
    "authors": [
      "Mahmoud Mbarak",
      "Manmeet Singh",
      "Naveen Sudharsan",
      "Zong-Liang Yang"
    ],
    "published": "2025-06-15",
    "abstract": "Accurate soil moisture prediction during extreme events remains a critical\nchallenge for Earth system modeling, with profound implications for drought\nmonitoring, flood forecasting, and climate adaptation strategies. While land\nsurface models (LSMs) provide physically-based predictions, they exhibit\nsystematic biases during extreme conditions when their parameterizations\noperate outside calibrated ranges. Here we present NoahMP-AI, a physics-guided\ndeep learning framework that addresses this challenge by leveraging the\ncomplete Noah-MP land surface model as a comprehensive physics-based feature\ngenerator while using machine learning to correct systematic biases against\nsatellite observations. We employ a 3D U-Net architecture that processes\nNoah-MP outputs (soil moisture, latent heat flux, and sensible heat flux) to\npredict SMAP-validated soil moisture across two contrasting extreme events: a\nprolonged drought (March-September 2022) and Hurricane Beryl (July 2024) over\nTexas. Our results demonstrate substantial improvements over standalone Noah-MP\npredictions, with R-squared values improving from -0.57 to 0.46 during drought\nconditions, while maintaining physical consistency and spatial coherence. The\nframework's ability to preserve Noah-MP's physical relationships while learning\nobservation-based corrections represents a significant advance in hybrid Earth\nsystem modeling. This work establishes both a practical tool for operational\nforecasting and a benchmark for investigating the optimal integration of\nphysics-based understanding with data-driven learning in environmental\nprediction systems.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AI-Informed Model Analogs for Subseasonal-to-Seasonal Prediction",
    "url": "http://arxiv.org/abs/2506.14022v1",
    "authors": [
      "Jacob B. Landsberg",
      "Elizabeth A. Barnes",
      "Matthew Newman"
    ],
    "published": "2025-06-16",
    "abstract": "Subseasonal-to-seasonal forecasting is crucial for public health, disaster\npreparedness, and agriculture, and yet it remains a particularly challenging\ntimescale to predict. We explore the use of an interpretable AI-informed model\nanalog forecasting approach, previously employed on longer timescales, to\nimprove S2S predictions. Using an artificial neural network, we learn a mask of\nweights to optimize analog selection and showcase its versatility across three\nvaried prediction tasks: 1) classification of Week 3-4 Southern California\nsummer temperatures; 2) regional regression of Month 1 midwestern U.S. summer\ntemperatures; and 3) classification of Month 1-2 North Atlantic wintertime\nupper atmospheric winds. The AI-informed analogs outperform traditional analog\nforecasting approaches, as well as climatology and persistence baselines, for\ndeterministic and probabilistic skill metrics on both climate model and\nreanalysis data. We find the analog ensembles built using the AI-informed\napproach also produce better predictions of temperature extremes and improve\nrepresentation of forecast uncertainty. Finally, by using an interpretable-AI\nframework, we analyze the learned masks of weights to better understand S2S\nsources of predictability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Digging deeper: deep joint species distribution modeling reveals environmental drivers of Earthworm Communities",
    "url": "http://arxiv.org/abs/2506.13568v1",
    "authors": [
      "Sara Si-moussi",
      "Wilfried Thuiller",
      "Esther Galbrun",
      "Thibaud Deca\u00ebns",
      "Sylvain G\u00e9rard",
      "Daniel F. March\u00e1n",
      "Claire Marsden",
      "Yvan Capowiez",
      "Micka\u00ebl Hedde"
    ],
    "published": "2025-06-16",
    "abstract": "Earthworms are key drivers of soil function, influencing organic matter\nturnover, nutrient cycling, and soil structure. Understanding the environmental\ncontrols on their distribution is essential for predicting the impacts of land\nuse and climate change on soil ecosystems. While local studies have identified\nabiotic drivers of earthworm communities, broad-scale spatial patterns remain\nunderexplored.\n  We developed a multi-species, multi-task deep learning model to jointly\npredict the distribution of 77 earthworm species across metropolitan France,\nusing historical (1960-1970) and contemporary (1990-2020) records. The model\nintegrates climate, soil, and land cover variables to estimate habitat\nsuitability. We applied SHapley Additive exPlanations (SHAP) to identify key\nenvironmental drivers and used species clustering to reveal ecological response\ngroups.\n  The joint model achieved high predictive performance (TSS >= 0.7) and\nimproved predictions for rare species compared to traditional species\ndistribution models. Shared feature extraction across species allowed for more\nrobust identification of common and contrasting environmental responses.\nPrecipitation variability, temperature seasonality, and land cover emerged as\ndominant predictors of earthworm distribution. Species clustering revealed\ndistinct ecological strategies tied to climatic and land use gradients.\n  Our study advances both the methodological and ecological understanding of\nsoil biodiversity. We demonstrate the utility of interpretable deep learning\napproaches for large-scale soil fauna modeling and provide new insights into\nearthworm habitat specialization. These findings support improved soil\nbiodiversity monitoring and conservation planning in the face of global\nenvironmental change.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis",
    "url": "http://arxiv.org/abs/2506.20380v1",
    "authors": [
      "Zhengpeng Feng",
      "Sadiq Jaffer",
      "Jovana Knezevic",
      "Silja Sormunen",
      "Robin Young",
      "Madeline Lisaius",
      "Markus Immitzer",
      "James Ball",
      "Clement Atzberger",
      "David A. Coomes",
      "Anil Madhavapeddy",
      "Andrew Blake",
      "Srinivasan Keshav"
    ],
    "published": "2025-06-25",
    "abstract": "Satellite remote sensing (RS) enables a wide array of downstream Earth\nobservation (EO) applications, including climate modeling, carbon accounting,\nand strategies for conservation and sustainable land use. We present TESSERA, a\nnovel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning\n(SSL) to generate global, robust representations at 10m scale from pixel-level\nsatellite time series data. TESSERA combines information from only optical and\nSAR data streams using two parallel Transformer-based encoders: one dedicated\nto Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected\nspectral bands) to create representations that are then fused using a\nmultilayer perceptron (MLP), resulting in a global representation map covering\nthe years 2017 to 2024. Our precomputed representations set a new\nstate-of-the-art performance benchmark and our open-source approach\ndemocratizes access to high-performance, high-resolution representations. We\nbenchmark the performance of TESSERA in five diverse tasks, comparing our work\nwith state-of-the-art task-specific models and other foundation models. Our\nresults show that TESSERA outperforms both traditional RS baselines and the\nleading geospatial foundation models in these diverse downstream tasks.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition",
    "url": "http://arxiv.org/abs/2506.20174v2",
    "authors": [
      "Man Duc Chuc"
    ],
    "published": "2025-06-25",
    "abstract": "Foundation models are rapidly transforming Earth Observation data mining by\nenabling generalizable and scalable solutions for key tasks such as scene\nclassification and semantic segmentation. While most efforts in the geospatial\ndomain have focused on developing large models trained from scratch using\nmassive Earth Observation datasets, an alternative strategy that remains\nunderexplored is the reuse and combination of existing pretrained models. In\nthis study, we investigate whether foundation models pretrained on remote\nsensing and general vision datasets can be effectively combined to improve\nperformance across a diverse set of key Earth Observation tasks. Using the\nGEO-Bench benchmark, we evaluate several prominent models, including Prithvi,\nHiera, and DOFA, on eleven datasets covering a range of spatial resolutions,\nsensor modalities, and task types. The results show that feature-level\nensembling of smaller pretrained models can match or exceed the performance of\nmuch larger models, while requiring less training time and computational\nresources. Moreover, the study highlights the potential of applying knowledge\ndistillation to transfer the strengths of ensembles into more compact models,\noffering a practical path for deploying foundation models in real-world Earth\nObservation applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2506.19585v1",
    "authors": [
      "Gencer Sumbul",
      "Chang Xu",
      "Emanuele Dalsasso",
      "Devis Tuia"
    ],
    "published": "2025-06-24",
    "abstract": "From optical sensors to microwave radars, leveraging the complementary\nstrengths of remote sensing (RS) sensors is crucial for achieving dense\nspatio-temporal monitoring of our planet. In contrast, recent deep learning\nmodels, whether task-specific or foundational, are often specific to single\nsensors or to fixed combinations: adapting such models to different sensory\ninputs requires both architectural changes and re-training, limiting\nscalability and generalization across multiple RS sensors. On the contrary, a\nsingle model able to modulate its feature representations to accept diverse\nsensors as input would pave the way to agile and flexible multi-sensor RS data\nprocessing. To address this, we introduce SMARTIES, a generic and versatile\nfoundation model lifting sensor-specific/dependent efforts and enabling\nscalability and generalization to diverse RS sensors: SMARTIES projects data\nfrom heterogeneous sensors into a shared spectrum-aware space, enabling the use\nof arbitrary combinations of bands both for training and inference. To obtain\nsensor-agnostic representations, we train a single, unified transformer model\nreconstructing masked multi-sensor data with cross-sensor token mixup. On both\nsingle- and multi-modal tasks across diverse sensors, SMARTIES outperforms\nprevious models that rely on sensor-specific pretraining. Our code and\npretrained models are available at https://gsumbul.github.io/SMARTIES.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2506.21109v1",
    "authors": [
      "Luosheng Xu",
      "Dalin Zhang",
      "Zhaohui Song"
    ],
    "published": "2025-06-26",
    "abstract": "Remote sensing change detection is essential for monitoring urban expansion,\ndisaster assessment, and resource management, offering timely, accurate, and\nlarge-scale insights into dynamic landscape transformations. While deep\nlearning has revolutionized change detection, the increasing complexity and\ncomputational demands of modern models have not necessarily translated into\nsignificant accuracy gains. Instead of following this trend, this study\nexplores a more efficient approach, focusing on lightweight models that\nmaintain high accuracy while minimizing resource consumption, which is an\nessential requirement for on-satellite processing. To this end, we propose\nFlickCD, which means quick flick then get great results, pushing the boundaries\nof the performance-resource trade-off. FlickCD introduces an Enhanced\nDifference Module (EDM) to amplify critical feature differences between\ntemporal phases while suppressing irrelevant variations such as lighting and\nweather changes, thereby reducing computational costs in the subsequent change\ndecoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion\nBlocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global\nSelf-Attention (EGSA) to efficiently capture semantic information at multiple\nscales, preserving both coarse- and fine-grained changes. Extensive experiments\non four benchmark datasets demonstrate that FlickCD reduces computational and\nstorage overheads by more than an order of magnitude while achieving\nstate-of-the-art (SOTA) performance or incurring only a minor (<1\\% F1)\naccuracy trade-off. The implementation code is publicly available at\nhttps://github.com/xulsh8/FlickCD.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Photon Absorption Remote Sensing (PARS): Comprehensive Absorption Imaging Enabling Label-Free Biomolecule Characterization and Mapping",
    "url": "http://arxiv.org/abs/2506.20069v1",
    "authors": [
      "Benjamin R. Ecclestone",
      "James A. Tummon Simmons",
      "James E. D. Tweel",
      "Deepak Dinakaran",
      "Parsin Haji Reza"
    ],
    "published": "2025-06-25",
    "abstract": "Label-free optical absorption microscopy techniques continue to evolve as\npromising tools for label-free histopathological imaging of cells and tissues.\nHowever, critical challenges relating to specificity and contrast, as compared\nto current gold-standard methods continue to hamper adoption. This work\nintroduces Photon Absorption Remote Sensing (PARS), a new absorption microscope\nmodality, which simultaneously captures the dominant de-excitation processes\nfollowing an absorption event. In PARS, radiative (auto-fluorescence) and\nnon-radiative (photothermal and photoacoustic) relaxation processes are\ncollected simultaneously, providing enhanced specificity to a range of\nbiomolecules. As an example, a multiwavelength PARS system featuring UV (266\nnm) and visible (532 nm) excitation is applied to imaging human skin, and\nmurine brain tissue samples. It is shown that PARS can directly characterize,\ndifferentiate, and unmix, clinically relevant biomolecules inside complex\ntissues samples using established statistical processing methods. Gaussian\nmixture models (GMM) are used to characterize clinically relevant biomolecules\n(e.g., white, and gray matter) based on their PARS signals, while non-negative\nleast squares (NNLS) is applied to map the biomolecule abundance in murine\nbrain tissues, without stained ground truth images or deep-learning methods.\nPARS unmixing and abundance estimates are directly validated and compared\nagainst chemically stained ground truth images, and deep learning based-image\ntransforms. Overall, it is found that the PARS unique and rich contrast may\nprovide comprehensive, and otherwise inaccessible, label-free characterization\nof molecular pathology, representing a new source of data to develop AI and\nmachine learning methods for diagnostics and visualization.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Video Compression for Spatiotemporal Earth System Data",
    "url": "http://arxiv.org/abs/2506.19656v1",
    "authors": [
      "Oscar J. Pellicer-Valero",
      "Cesar Aybar",
      "Gustau Camps Valls"
    ],
    "published": "2025-06-24",
    "abstract": "Large-scale Earth system datasets, from high-resolution remote sensing\nimagery to spatiotemporal climate model outputs, exhibit characteristics\nanalogous to those of standard videos. Their inherent spatial, temporal, and\nspectral redundancies can thus be readily exploited by established video\ncompression techniques. Here, we present xarrayvideo, a Python library for\ncompressing multichannel spatiotemporal datasets by encoding them as videos.\nOur approach achieves compression ratios of up to 250x while maintaining high\nfidelity by leveraging standard, well-optimized video codecs through ffmpeg. We\ndemonstrate the library's effectiveness on four real-world multichannel\nspatiotemporal datasets: DynamicEarthNet (very high resolution Planet images),\nDeepExtremeCubes (high resolution Sentinel-2 images), ERA5 (weather reanalysis\ndata), and the SimpleS2 dataset (high resolution multichannel Sentinel-2\nimages), achieving Peak Signal-to-Noise Ratios (PSNRs) of 55.86, 40.60, 46.58,\nand 43.23 dB at 0.1 bits per pixel per band (bpppb) and 65.91, 54.28, 62.90,\nand 55.04 dB at 1 bpppb. We are redistributing two of these datasets,\nDeepExtremeCubes (2.3 Tb) and DynamicEarthNet (525 Gb), in the\nmachine-learning-ready and cloud-ready TACO format through HuggingFace at\nsignificantly reduced sizes (270 Gb and 8.5 Gb, respectively) without\ncompromising quality (PSNR 55.77-56.65 and 60.15). No performance loss is\nobserved when the compressed versions of these datasets are used in their\nrespective deep learning-based downstream tasks (next step reflectance\nprediction and landcover segmentation). In conclusion, xarrayvideo presents an\nefficient solution for handling the rapidly growing size of Earth observation\ndatasets, making advanced compression techniques accessible and practical to\nthe Earth science community. The library is available for use at\nhttps://github.com/IPL-UV/xarrayvideo",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "MambaOutRS: A Hybrid CNN-Fourier Architecture for Remote Sensing Image Classification",
    "url": "http://arxiv.org/abs/2506.19561v1",
    "authors": [
      "Minjong Cheon",
      "Changbae Mun"
    ],
    "published": "2025-06-24",
    "abstract": "Recent advances in deep learning for vision tasks have seen the rise of State\nSpace Models (SSMs) like Mamba, celebrated for their linear scalability.\nHowever, their adaptation to 2D visual data often necessitates complex\nmodifications that may diminish efficiency. In this paper, we introduce\nMambaOutRS, a novel hybrid convolutional architecture for remote sensing image\nclassification that re-evaluates the necessity of recurrent SSMs. MambaOutRS\nbuilds upon stacked Gated CNN blocks for local feature extraction and\nintroduces a novel Fourier Filter Gate (FFG) module that operates in the\nfrequency domain to capture global contextual information efficiently. Our\narchitecture employs a four-stage hierarchical design and was extensively\nevaluated on challenging remote sensing datasets: UC Merced, AID,\nNWPU-RESISC45, and EuroSAT. MambaOutRS consistently achieved state-of-the-art\n(SOTA) performance across these benchmarks. Notably, our MambaOutRS-t variant\n(24.0M parameters) attained the highest F1-scores of 98.41\\% on UC Merced and\n95.99\\% on AID, significantly outperforming existing baselines, including\nlarger transformer models and Mamba-based architectures, despite using\nconsiderably fewer parameters. An ablation study conclusively demonstrates the\ncritical role of the Fourier Filter Gate in enhancing the model's ability to\ncapture global spatial patterns, leading to robust and accurate classification.\nThese results strongly suggest that the complexities of recurrent SSMs can be\neffectively superseded by a judicious combination of gated convolutions for\nspatial mixing and frequency-based gates for spectral global context. Thus,\nMambaOutRS provides a compelling and efficient paradigm for developing\nhigh-performance deep learning models in remote sensing and other vision\ndomains, particularly where computational efficiency is paramount.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Vision Transformer-Based Time-Series Image Reconstruction for Cloud-Filling Applications",
    "url": "http://arxiv.org/abs/2506.19591v1",
    "authors": [
      "Lujun Li",
      "Yiqun Wang",
      "Radu State"
    ],
    "published": "2025-06-24",
    "abstract": "Cloud cover in multispectral imagery (MSI) poses significant challenges for\nearly season crop mapping, as it leads to missing or corrupted spectral\ninformation. Synthetic aperture radar (SAR) data, which is not affected by\ncloud interference, offers a complementary solution, but lack sufficient\nspectral detail for precise crop mapping. To address this, we propose a novel\nframework, Time-series MSI Image Reconstruction using Vision Transformer (ViT),\nto reconstruct MSI data in cloud-covered regions by leveraging the temporal\ncoherence of MSI and the complementary information from SAR from the attention\nmechanism. Comprehensive experiments, using rigorous reconstruction evaluation\nmetrics, demonstrate that Time-series ViT framework significantly outperforms\nbaselines that use non-time-series MSI and SAR or time-series MSI without SAR,\neffectively enhancing MSI image reconstruction in cloud-covered regions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing",
    "url": "http://arxiv.org/abs/2506.18587v1",
    "authors": [
      "Antoine Saget",
      "Baptiste Lafabregue",
      "Antoine Cornu\u00e9jols",
      "Pierre Gan\u00e7arski"
    ],
    "published": "2025-06-23",
    "abstract": "Given the abundance of unlabeled Satellite Image Time Series (SITS) and the\nscarcity of labeled data, contrastive self-supervised pretraining emerges as a\nnatural tool to leverage this vast quantity of unlabeled data. However,\ndesigning effective data augmentations for contrastive learning remains\nchallenging for time series. We introduce a novel resampling-based augmentation\nstrategy that generates positive pairs by upsampling time series and extracting\ndisjoint subsequences while preserving temporal coverage. We validate our\napproach on multiple agricultural classification benchmarks using Sentinel-2\nimagery, showing that it outperforms common alternatives such as jittering,\nresizing, and masking. Further, we achieve state-of-the-art performance on the\nS2-Agri100 dataset without employing spatial information or temporal encodings,\nsurpassing more complex masked-based SSL frameworks. Our method offers a\nsimple, yet effective, contrastive learning augmentation for remote sensing\ntime series.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Extreme Learning Machines for Exoplanet Simulations: A Faster, Lightweight Alternative to Deep Learning",
    "url": "http://arxiv.org/abs/2506.19679v1",
    "authors": [
      "Tara P. A. Tahseen",
      "Lu\u00eds F. Sim\u00f5es",
      "Kai Hou Yip",
      "Nikolaos Nikolaou",
      "Jo\u00e3o M. Mendon\u00e7a",
      "Ingo P. Waldmann"
    ],
    "published": "2025-06-24",
    "abstract": "Increasing resolution and coverage of astrophysical and climate data\nnecessitates increasingly sophisticated models, often pushing the limits of\ncomputational feasibility. While emulation methods can reduce calculation\ncosts, the neural architectures typically used--optimised via gradient\ndescent--are themselves computationally expensive to train, particularly in\nterms of data generation requirements. This paper investigates the utility of\nthe Extreme Learning Machine (ELM) as a lightweight, non-gradient-based machine\nlearning algorithm for accelerating complex physical models.\n  We evaluate ELM surrogate models in two test cases with different data\nstructures: (i) sequentially-structured data, and (ii) image-structured data.\nFor test case (i), where the number of samples $N$ >> the dimensionality of\ninput data $d$, ELMs achieve remarkable efficiency, offering a 100,000$\\times$\nfaster training time and a 40$\\times$ faster prediction speed compared to a\nBi-Directional Recurrent Neural Network (BIRNN), whilst improving upon BIRNN\ntest performance. For test case (ii), characterised by $d >> N$ and image-based\ninputs, a single ELM was insufficient, but an ensemble of 50 individual ELM\npredictors achieves comparable accuracy to a benchmark Convolutional Neural\nNetwork (CNN), with a 16.4$\\times$ reduction in training time, though costing a\n6.9$\\times$ increase in prediction time. We find different sample efficiency\ncharacteristics between the test cases: in test case (i) individual ELMs\ndemonstrate superior sample efficiency, requiring only 0.28% of the training\ndataset compared to the benchmark BIRNN, while in test case (ii) the ensemble\napproach requires 78% of the data used by the CNN to achieve comparable\nresults--representing a trade-off between sample efficiency and model\ncomplexity.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A deep-learning model for predicting daily PM2.5 concentration in response to emission reduction",
    "url": "http://arxiv.org/abs/2506.18018v1",
    "authors": [
      "Shigan Liu",
      "Guannan Geng",
      "Yanfei Xiang",
      "Hejun Hu",
      "Xiaodong Liu",
      "Xiaomeng Huang",
      "Qiang Zhang"
    ],
    "published": "2025-06-22",
    "abstract": "Air pollution remains a leading global health threat, with fine particulate\nmatter (PM2.5) contributing to millions of premature deaths annually. Chemical\ntransport models (CTMs) are essential tools for evaluating how emission\ncontrols improve air quality and save lives, but they are computationally\nintensive. Reduced form models accelerate simulations but sacrifice\nspatial-temporal granularity, accuracy, and flexibility. Here we present\nCleanAir, a deep-learning-based model developed as an efficient alternative to\nCTMs in simulating daily PM2.5 and its chemical compositions in response to\nprecursor emission reductions at 36 km resolution, which could predict PM2.5\nconcentration for a full year within 10 seconds on a single GPU, a speed five\norders of magnitude faster. Built on a Residual Symmetric 3D U-Net architecture\nand trained on more than 2,400 emission reduction scenarios generated by a\nwell-validated Community Multiscale Air Quality (CMAQ) model, CleanAir\ngeneralizes well across unseen meteorological years and emission patterns. It\nproduces results comparable to CMAQ in both absolute concentrations and\nemission-induced changes, enabling efficient, full-coverage simulations across\nshort-term interventions and long-term planning horizons. This advance empowers\nresearchers and policymakers to rapidly evaluate a wide range of air quality\nstrategies and assess the associated health impacts, thereby supporting more\nresponsive and informed environmental decision-making.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks",
    "url": "http://arxiv.org/abs/2506.20181v1",
    "authors": [
      "Ronald Katende"
    ],
    "published": "2025-06-25",
    "abstract": "We develop a principled framework for discovering causal structure in partial\ndifferential equations (PDEs) using physics-informed neural networks and\ncounterfactual perturbations. Unlike classical residual minimization or sparse\nregression methods, our approach quantifies operator-level necessity through\nfunctional interventions on the governing dynamics. We introduce causal\nsensitivity indices and structural deviation metrics to assess the influence of\ncandidate differential operators within neural surrogates. Theoretically, we\nprove exact recovery of the causal operator support under restricted isometry\nor mutual coherence conditions, with residual bounds guaranteeing\nidentifiability. Empirically, we validate the framework on both synthetic and\nreal-world datasets across climate dynamics, tumor diffusion, and ocean flows.\nOur method consistently recovers governing operators even under noise,\nredundancy, and data scarcity, outperforming standard PINNs and DeepONets in\nstructural fidelity. This work positions causal PDE discovery as a tractable\nand interpretable inference task grounded in structural causal models and\nvariational residual analysis.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting",
    "url": "http://arxiv.org/abs/2506.20024v1",
    "authors": [
      "Salva R\u00fchling Cachay",
      "Miika Aittala",
      "Karsten Kreis",
      "Noah Brenowitz",
      "Arash Vahdat",
      "Morteza Mardani",
      "Rose Yu"
    ],
    "published": "2025-06-24",
    "abstract": "Diffusion models are a powerful tool for probabilistic forecasting, yet most\napplications in high-dimensional chaotic systems predict future snapshots\none-by-one. This common approach struggles to model complex temporal\ndependencies and fails to explicitly account for the progressive growth of\nuncertainty inherent to such systems. While rolling diffusion frameworks, which\napply increasing noise to forecasts at longer lead times, have been proposed to\naddress this, their integration with state-of-the-art, high-fidelity diffusion\ntechniques remains a significant challenge. We tackle this problem by\nintroducing Elucidated Rolling Diffusion Models (ERDM), the first framework to\nsuccessfully unify a rolling forecast structure with the principled, performant\ndesign of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM\ncomponents-its noise schedule, network preconditioning, and Heun sampler-to the\nrolling forecast setting. The success of this integration is driven by three\nkey contributions: (i) a novel loss weighting scheme that focuses model\ncapacity on the mid-range forecast horizons where determinism gives way to\nstochasticity; (ii) an efficient initialization strategy using a pre-trained\nEDM for the initial window; and (iii) a bespoke hybrid sequence architecture\nfor robust spatiotemporal feature extraction under progressive denoising. On 2D\nNavier-Stokes simulations and ERA5 global weather forecasting at 1.5^\\circ\nresolution, ERDM consistently outperforms key diffusion-based baselines,\nincluding conditional autoregressive EDM. ERDM offers a flexible and powerful\ngeneral framework for tackling diffusion-based sequence generation problems\nwhere modeling escalating uncertainty is paramount. Code is available at:\nhttps://github.com/salvaRC/erdm",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models",
    "url": "http://arxiv.org/abs/2506.18124v1",
    "authors": [
      "Shaoxiu Wei",
      "Mingchao Liang",
      "Florian Meyer"
    ],
    "published": "2025-06-22",
    "abstract": "Multiobject tracking (MOT) is an important task in applications including\nautonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT\nmethods are model-based and combine sequential Bayesian estimation with data\nassociation and an object birth model. More recent methods are fully\ndata-driven and rely on the training of neural networks. Both approaches offer\ndistinct advantages in specific settings. In particular, model-based methods\nare generally applicable across a wide range of scenarios, whereas data-driven\nMOT achieves superior performance in scenarios where abundant labeled data for\ntraining is available. A natural thought is whether a general framework can\nintegrate the two approaches. This paper introduces a hybrid method that\nutilizes neural networks to enhance specific aspects of the statistical model\nin Bayesian MOT that have been identified as overly simplistic. By doing so,\nthe performance of the prediction and update steps of Bayesian MOT is improved.\nTo ensure tractable computation, our framework uses belief propagation to avoid\nhigh-dimensional operations combined with sequential Monte Carlo methods to\nperform low-dimensional operations efficiently. The resulting method combines\nthe flexibility and robustness of model-based approaches with the capability to\nlearn complex information from data of neural networks. We evaluate the\nperformance of the proposed method based on the nuScenes autonomous driving\ndataset and demonstrate that it has state-of-the-art performance",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation",
    "url": "http://arxiv.org/abs/2507.00356v1",
    "authors": [
      "Zhiwei Yi",
      "Xin Cheng",
      "Jingyu Ma",
      "Ruifei Zhu",
      "Junwei Tian",
      "Yuanxiu Zhou",
      "Xinge Zhao",
      "Hongzhe Li"
    ],
    "published": "2025-07-01",
    "abstract": "Deep learning methods have significantly advanced the development of\nintelligent rinterpretation in remote sensing (RS), with foundational model\nresearch based on large-scale pre-training paradigms rapidly reshaping various\ndomains of Earth Observation (EO). However, compared to the open accessibility\nand high spatiotemporal coverage of medium-resolution data, the limited\nacquisition channels for ultra-high-resolution optical RS imagery have\nconstrained the progress of high-resolution remote sensing vision foundation\nmodels (RSVFM). As the world's largest sub-meter-level commercial RS satellite\nconstellation, the Jilin-1 constellation possesses abundant sub-meter-level\nimage resources. This study proposes CGEarthEye, a RSVFM framework specifically\ndesigned for Jilin-1 satellite characteristics, comprising five backbones with\ndifferent parameter scales with totaling 2.1 billion parameters. To enhance the\nrepresentational capacity of the foundation model, we developed JLSSD, the\nfirst 15-million-scale multi-temporal self-supervised learning (SSL) dataset\nfeaturing global coverage with quarterly temporal sampling within a single\nyear, constructed through multi-level representation clustering and sampling\nstrategies. The framework integrates seasonal contrast, augmentation-based\ncontrast, and masked patch token contrastive strategies for pre-training.\nComprehensive evaluations across 10 benchmark datasets covering four typical RS\ntasks demonstrate that the CGEarthEye consistently achieves state-of-the-art\n(SOTA) performance. Further analysis reveals CGEarthEye's superior\ncharacteristics in feature visualization, model convergence, parameter\nefficiency, and practical mapping applications. This study anticipates that the\nexceptional representation capabilities of CGEarthEye will facilitate broader\nand more efficient applications of Jilin-1 data in traditional EO application.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery",
    "url": "http://arxiv.org/abs/2507.01747v1",
    "authors": [
      "Nora Gourmelon",
      "Marcel Dreier",
      "Martin Mayr",
      "Thorsten Seehaus",
      "Dakota Pyles",
      "Matthias Braun",
      "Andreas Maier",
      "Vincent Christlein"
    ],
    "published": "2025-07-02",
    "abstract": "Glaciers are losing ice mass at unprecedented rates, increasing the need for\naccurate, year-round monitoring to understand frontal ablation, particularly\nthe factors driving the calving process. Deep learning models can extract\ncalving front positions from Synthetic Aperture Radar imagery to track seasonal\nice losses at the calving fronts of marine- and lake-terminating glaciers. The\ncurrent state-of-the-art model relies on ImageNet-pretrained weights. However,\nthey are suboptimal due to the domain shift between the natural images in\nImageNet and the specialized characteristics of remote sensing imagery, in\nparticular for Synthetic Aperture Radar imagery. To address this challenge, we\npropose two novel self-supervised multimodal pretraining techniques that\nleverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14\nSentinel-2 images of Arctic glaciers, with one optical image per glacier in the\ndataset. Additionally, we introduce a novel hybrid model architecture that\ncombines a Swin Transformer encoder with a residual Convolutional Neural\nNetwork (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean\ndistance error of 293 m on the \"CAlving Fronts and where to Find thEm\" (CaFFe)\nbenchmark dataset, outperforming the prior best model by 67 m. Evaluating an\nensemble of the proposed model on a multi-annotator study of the benchmark\ndataset reveals a mean distance error of 75 m, approaching the human\nperformance of 38 m. This advancement enables precise monitoring of seasonal\nchanges in glacier calving fronts.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images",
    "url": "http://arxiv.org/abs/2507.01502v1",
    "authors": [
      "Ozan Durgut",
      "Beril Kallfelz-Sirmacek",
      "Cem Unsalan"
    ],
    "published": "2025-07-02",
    "abstract": "Global warming, loss of biodiversity, and air pollution are among the most\nsignificant problems facing Earth. One of the primary challenges in addressing\nthese issues is the lack of monitoring forests to protect them. To tackle this\nproblem, it is important to leverage remote sensing and computer vision methods\nto automate monitoring applications. Hence, automatic tree crown detection\nalgorithms emerged based on traditional and deep learning methods. In this\nstudy, we first introduce two different tree crown detection methods based on\nthese approaches. Then, we form a novel rule-based approach that integrates\nthese two methods to enhance robustness and accuracy of tree crown detection\nresults. While traditional methods are employed for feature extraction and\nsegmentation of forested areas, deep learning methods are used to detect tree\ncrowns in our method. With the proposed rule-based approach, we post-process\nthese results, aiming to increase the number of detected tree crowns through\nneighboring trees and localized operations. We compare the obtained results\nwith the proposed method in terms of the number of detected tree crowns and\nreport the advantages, disadvantages, and areas for improvement of the obtained\noutcomes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions",
    "url": "http://arxiv.org/abs/2507.01123v1",
    "authors": [
      "Rahul A. Burange",
      "Harsh K. Shinde",
      "Omkar Mutyalwar"
    ],
    "published": "2025-07-01",
    "abstract": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data",
    "url": "http://arxiv.org/abs/2506.22939v1",
    "authors": [
      "Ghufran A. Omran",
      "Wassan Saad Abduljabbar Hayale",
      "Ahmad AbdulQadir AlRababah",
      "Israa Ibraheem Al-Barazanchi",
      "Ravi Sekhar",
      "Pritesh Shah",
      "Sushma Parihar",
      "Harshavardhan Reddy Penubadi"
    ],
    "published": "2025-06-28",
    "abstract": "Scene categorization (SC) in remotely acquired images is an important subject\nwith broad consequences in different fields, including catastrophe control,\necological observation, architecture for cities, and more. Nevertheless, its\nseveral apps, reaching a high degree of accuracy in SC from distant observation\ndata has demonstrated to be difficult. This is because traditional conventional\ndeep learning models require large databases with high variety and high levels\nof noise to capture important visual features. To address these problems, this\ninvestigation file introduces an innovative technique referred to as the\nCuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type\nof scenes in remote sensing data. The investigation compares the execution of\nCO-BRNN with current techniques, including Multilayer Perceptron- Convolutional\nNeural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory\n(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),\nGraph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional\nNeural Networks Data Augmentation (CNN-DA). The results demonstrate that\nCO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,\nMLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance\nof physical confirmation to ensure the efficiency of satellite data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": []
  },
  {
    "title": "AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions",
    "url": "http://arxiv.org/abs/2507.01547v1",
    "authors": [
      "Ubada El Joulani",
      "Tatiana Kalganova",
      "Stergios-Aristoteles Mitoulis",
      "Sotirios Argyroudis"
    ],
    "published": "2025-07-02",
    "abstract": "Critical infrastructure, such as transport networks, underpins economic\ngrowth by enabling mobility and trade. However, ageing assets, climate change\nimpacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging\nfrom natural disasters to cyber attacks and conflicts pose growing risks to\ntheir resilience and functionality. This review paper explores how emerging\ndigital technologies, specifically Artificial Intelligence (AI), can enhance\ndamage assessment and monitoring of transport infrastructure. A systematic\nliterature review examines existing AI models and datasets for assessing damage\nin roads, bridges, and other critical infrastructure impacted by natural\ndisasters. Special focus is given to the unique challenges and opportunities\nassociated with bridge damage detection due to their structural complexity and\ncritical role in connectivity. The integration of SAR (Synthetic Aperture\nRadar) data with AI models is also discussed, with the review revealing a\ncritical research gap: a scarcity of studies applying AI models to SAR data for\ncomprehensive bridge damage assessment. Therefore, this review aims to identify\nthe research gaps and provide foundations for AI-driven solutions for assessing\nand monitoring critical transport infrastructures.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Advancements in Weed Mapping: A Systematic Review",
    "url": "http://arxiv.org/abs/2507.01269v1",
    "authors": [
      "Mohammad Jahanbakht",
      "Alex Olsen",
      "Ross Marchant",
      "Emilie Fillols",
      "Mostafa Rahimi Azghadi"
    ],
    "published": "2025-07-02",
    "abstract": "Weed mapping plays a critical role in precision management by providing\naccurate and timely data on weed distribution, enabling targeted control and\nreduced herbicide use. This minimizes environmental impacts, supports\nsustainable land management, and improves outcomes across agricultural and\nnatural environments. Recent advances in weed mapping leverage ground-vehicle\nRed Green Blue (RGB) cameras, satellite and drone-based remote sensing combined\nwith sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The\nresulting data are processed using advanced techniques including big data\nanalytics and machine learning, significantly improving the spatial and\ntemporal resolution of weed maps and enabling site-specific management\ndecisions. Despite a growing body of research in this domain, there is a lack\nof comprehensive literature reviews specifically focused on weed mapping. In\nparticular, the absence of a structured analysis spanning the entire mapping\npipeline, from data acquisition to processing techniques and mapping tools,\nlimits progress in the field. This review addresses these gaps by\nsystematically examining state-of-the-art methods in data acquisition (sensor\nand platform technologies), data processing (including annotation and\nmodelling), and mapping techniques (such as spatiotemporal analysis and\ndecision support tools). Following PRISMA guidelines, we critically evaluate\nand synthesize key findings from the literature to provide a holistic\nunderstanding of the weed mapping landscape. This review serves as a\nfoundational reference to guide future research and support the development of\nefficient, scalable, and sustainable weed management systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution",
    "url": "http://arxiv.org/abs/2506.23566v1",
    "authors": [
      "Luigi Sigillo",
      "Renato Giamba",
      "Danilo Comminiello"
    ],
    "published": "2025-06-30",
    "abstract": "The acquisition of high-resolution satellite imagery is often constrained by\nthe spatial and temporal limitations of satellite sensors, as well as the high\ncosts associated with frequent observations. These challenges hinder\napplications such as environmental monitoring, disaster response, and\nagricultural management, which require fine-grained and high-resolution data.\nIn this paper, we propose MWT-Diff, an innovative framework for satellite image\nsuper-resolution (SR) that combines latent diffusion models with wavelet\ntransforms to address these challenges. At the core of the framework is a novel\nmetadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates\nembeddings that capture metadata attributes, multi-scale frequency information,\nand temporal relationships. The embedded feature representations steer the\nhierarchical diffusion dynamics, through which the model progressively\nreconstructs high-resolution satellite imagery from low-resolution inputs. This\nprocess preserves critical spatial characteristics including textural patterns,\nboundary discontinuities, and high-frequency spectral components essential for\ndetailed remote sensing analysis. The comparative analysis of MWT-Diff across\nmultiple datasets demonstrated favorable performance compared to recent\napproaches, as measured by standard perceptual quality metrics including FID\nand LPIPS.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification",
    "url": "http://arxiv.org/abs/2506.23462v1",
    "authors": [
      "Manaswi Kulahara",
      "Gautam Siddharth Kashyap",
      "Nipun Joshi",
      "Arpita Soni"
    ],
    "published": "2025-06-30",
    "abstract": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Accurate Mediterranean Sea forecasting via graph-based deep learning",
    "url": "http://arxiv.org/abs/2506.23900v1",
    "authors": [
      "Daniel Holmberg",
      "Emanuela Clementi",
      "Italo Epicoco",
      "Teemu Roos"
    ],
    "published": "2025-06-30",
    "abstract": "Accurate ocean forecasting systems are essential for understanding marine\ndynamics, which play a crucial role in sectors such as shipping, aquaculture,\nenvironmental monitoring, and coastal risk management. Traditional numerical\nsolvers, while effective, are computationally expensive and time-consuming.\nRecent advancements in machine learning have revolutionized weather\nforecasting, offering fast and energy-efficient alternatives. Building on these\nadvancements, we introduce SeaCast, a neural network designed for\nhigh-resolution regional ocean forecasting. SeaCast employs a graph-based\nframework to effectively handle the complex geometry of ocean grids and\nintegrates external forcing data tailored to the regional ocean context. Our\napproach is validated through experiments at a high horizontal resolution using\nthe operational numerical forecasting system of the Mediterranean Sea, along\nwith both numerical and data-driven atmospheric forcings. Results demonstrate\nthat SeaCast consistently outperforms the operational model in forecast skill,\nmarking a significant advancement in regional ocean prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion",
    "url": "http://arxiv.org/abs/2507.01354v1",
    "authors": [
      "Chugang Yi",
      "Minghan Yu",
      "Weikang Qian",
      "Yixin Wen",
      "Haizhao Yang"
    ],
    "published": "2025-07-02",
    "abstract": "Effective hydrological modeling and extreme weather analysis demand\nprecipitation data at a kilometer-scale resolution, which is significantly\nfiner than the 10 km scale offered by standard global products like IMERG. To\naddress this, we propose the Wavelet Diffusion Model (WDM), a generative\nframework that achieves 10x spatial super-resolution (downscaling to 1 km) and\ndelivers a 9x inference speedup over pixel-based diffusion models. WDM is a\nconditional diffusion model that learns the learns the complex structure of\nprecipitation from MRMS radar data directly in the wavelet domain. By focusing\non high-frequency wavelet coefficients, it generates exceptionally realistic\nand detailed 1-km precipitation fields. This wavelet-based approach produces\nvisually superior results with fewer artifacts than pixel-space models, and\ndelivers a significant gains in sampling efficiency. Our results demonstrate\nthat WDM provides a robust solution to the dual challenges of accuracy and\nspeed in geoscience super-resolution, paving the way for more reliable\nhydrological forecasts.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "Guided Unconditional and Conditional Generative Models for Super-Resolution and Inference of Quasi-Geostrophic Turbulence",
    "url": "http://arxiv.org/abs/2507.00719v1",
    "authors": [
      "Anantha Narayanan Suresh Babu",
      "Akhil Sadam",
      "Pierre F. J. Lermusiaux"
    ],
    "published": "2025-07-01",
    "abstract": "Typically, numerical simulations of the ocean, weather, and climate are\ncoarse, and observations are sparse and gappy. In this work, we apply four\ngenerative diffusion modeling approaches to super-resolution and inference of\nforced two-dimensional quasi-geostrophic turbulence on the beta-plane from\ncoarse, sparse, and gappy observations. Two guided approaches minimally adapt a\npre-trained unconditional model: SDEdit modifies the initial condition, and\nDiffusion Posterior Sampling (DPS) modifies the reverse diffusion process\nscore. The other two conditional approaches, a vanilla variant and\nclassifier-free guidance, require training with paired high-resolution and\nobservation data. We consider eight test cases spanning: two regimes, eddy and\nanisotropic-jet turbulence; two Reynolds numbers, 10^3 and 10^4; and two\nobservation types, 4x coarse-resolution fields and coarse, sparse and gappy\nobservations. Our comprehensive skill metrics include norms of the\nreconstructed vorticity fields, turbulence statistical quantities, and\nquantification of the super-resolved probabilistic ensembles and their errors.\nWe also study the sensitivity to tuning parameters such as guidance strength.\nResults show that SDEdit generates unphysical fields, while DPS generates\nreasonable reconstructions at low computational cost but with smoothed\nfine-scale features. Both conditional approaches require re-training, but they\nreconstruct missing fine-scale features, are cycle-consistent with\nobservations, and possess the correct statistics such as energy spectra.\nFurther, their mean model errors are highly correlated with and predictable\nfrom their ensemble standard deviations. Results highlight the trade-offs\nbetween ease of implementation, fidelity (sharpness), and cycle-consistency of\nthe diffusion models, and offer practical guidance for deployment in\ngeophysical inverse problems.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "MAPEX: Modality-Aware Pruning of Experts for Remote Sensing Foundation Models",
    "url": "http://arxiv.org/abs/2507.07527v1",
    "authors": [
      "Joelle Hanna",
      "Linus Scheibenreif",
      "Damian Borth"
    ],
    "published": "2025-07-10",
    "abstract": "Remote sensing data is commonly used for tasks such as flood mapping,\nwildfire detection, or land-use studies. For each task, scientists carefully\nchoose appropriate modalities or leverage data from purpose-built instruments.\nRecent work on remote sensing foundation models pre-trains computer vision\nmodels on large amounts of remote sensing data. These large-scale models tend\nto focus on specific modalities, often optical RGB or multispectral data. For\nmany important applications, this introduces a mismatch between the application\nmodalities and the pre-training data. Moreover, the large size of foundation\nmodels makes them expensive and difficult to fine-tune on typically small\ndatasets for each task. We address this mismatch with MAPEX, a remote sensing\nfoundation model based on mixture-of-modality experts. MAPEX is pre-trained on\nmulti-modal remote sensing data with a novel modality-conditioned token routing\nmechanism that elicits modality-specific experts. To apply the model on a\nspecific task, we propose a modality aware pruning technique, which only\nretains experts specialized for the task modalities. This yields efficient\nmodality-specific models while simplifying fine-tuning and deployment for the\nmodalities of interest. We experimentally validate MAPEX on diverse remote\nsensing datasets and show strong performance compared to fully supervised\ntraining and state-of-the-art remote sensing foundation models. Code is\navailable at https://github.com/HSG-AIML/MAPEX.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models",
    "url": "http://arxiv.org/abs/2507.06231v1",
    "authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Bowen Chen",
      "Jiafan Zhang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-07-08",
    "abstract": "Referring Remote Sensing Image Segmentation provides a flexible and\nfine-grained framework for remote sensing scene analysis via vision-language\ncollaborative interpretation. Current approaches predominantly utilize a\nthree-stage pipeline encompassing dual-modal encoding, cross-modal interaction,\nand pixel decoding. These methods demonstrate significant limitations in\nmanaging complex semantic relationships and achieving precise cross-modal\nalignment, largely due to their coupled processing mechanism that conflates\ntarget localization with boundary delineation. This architectural coupling\namplifies error propagation under semantic ambiguity while restricting model\ngeneralizability and interpretability. To address these issues, we propose\nRSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow\ninto a collaborative dual-stage framework: coarse localization followed by fine\nsegmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with\nSAM's segmentation generalizability through strategic foundation model\ncollaboration. Specifically, CLIP is employed as the dual-modal encoder to\nactivate target features within its pre-aligned semantic space and generate\nlocalization prompts. To mitigate CLIP's misactivation challenges in\nmulti-entity scenarios described by referring texts, a cascaded second-order\nprompter is devised, which enhances precision through implicit reasoning via\ndecomposition of text embeddings into complementary semantic subspaces. These\noptimized semantic prompts subsequently direct the SAM to generate pixel-level\nrefined masks, thereby completing the semantic transmission pipeline. Extensive\nexperiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2\nsurpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex\nsemantic interpretation. Code is available at:\nhttps://github.com/KyanChen/RSRefSeg2.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "From General to Specialized: The Need for Foundational Models in Agriculture",
    "url": "http://arxiv.org/abs/2507.05390v1",
    "authors": [
      "Vishal Nedungadi",
      "Xingguo Xiong",
      "Aike Potze",
      "Ron Van Bree",
      "Tao Lin",
      "Marc Ru\u00dfwurm",
      "Ioannis N. Athanasiadis"
    ],
    "published": "2025-07-07",
    "abstract": "Food security remains a global concern as population grows and climate change\nintensifies, demanding innovative solutions for sustainable agricultural\nproductivity. Recent advances in foundation models have demonstrated remarkable\nperformance in remote sensing and climate sciences, and therefore offer new\nopportunities for agricultural monitoring. However, their application in\nchallenges related to agriculture-such as crop type mapping, crop phenology\nestimation, and crop yield estimation-remains under-explored. In this work, we\nquantitatively evaluate existing foundational models to assess their\neffectivity for a representative set of agricultural tasks. From an\nagricultural domain perspective, we describe a requirements framework for an\nideal agricultural foundation model (CropFM). We then survey and compare\nexisting general-purpose foundational models in this framework and empirically\nevaluate two exemplary of them in three representative agriculture specific\ntasks. Finally, we highlight the need for a dedicated foundational model\ntailored specifically to agriculture.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Time2Agri: Temporal Pretext Tasks for Agricultural Monitoring",
    "url": "http://arxiv.org/abs/2507.04366v1",
    "authors": [
      "Moti Rattan Gupta",
      "Anupam Sobti"
    ],
    "published": "2025-07-06",
    "abstract": "Self Supervised Learning(SSL) has emerged as a prominent paradigm for\nlabel-efficient learning, and has been widely utilized by remote sensing\nfoundation models(RSFMs). Recent RSFMs including SatMAE, DoFA, primarily rely\non masked autoencoding(MAE), contrastive learning or some combination of them.\nHowever, these pretext tasks often overlook the unique temporal characteristics\nof agricultural landscape, namely nature's cycle. Motivated by this gap, we\npropose three novel agriculture-specific pretext tasks, namely Time-Difference\nPrediction(TD), Temporal Frequency Prediction(FP), and Future-Frame\nPrediction(FF). Comprehensive evaluation on SICKLE dataset shows FF achieves\n69.6% IoU on crop mapping and FP reduces yield prediction error to 30.7% MAPE,\noutperforming all baselines, and TD remains competitive on most tasks. Further,\nwe also scale FF to the national scale of India, achieving 54.2% IoU\noutperforming all baselines on field boundary delineation on FTW India dataset.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction",
    "url": "http://arxiv.org/abs/2507.07388v1",
    "authors": [
      "Zesheng Liu",
      "Maryam Rahnemoonfar"
    ],
    "published": "2025-07-10",
    "abstract": "Gaining a deeper understanding of the thickness and variability of internal\nice layers in Radar imagery is essential in monitoring the snow accumulation,\nbetter evaluating ice dynamics processes, and minimizing uncertainties in\nclimate models. Radar sensors, capable of penetrating ice, capture detailed\nradargram images of internal ice layers. In this work, we introduce GRIT, graph\ntransformer for ice layer thickness. GRIT integrates an inductive geometric\ngraph learning framework with an attention mechanism, designed to map the\nrelationships between shallow and deeper ice layers. Compared to baseline graph\nneural networks, GRIT demonstrates consistently lower prediction errors. These\nresults highlight the attention mechanism's effectiveness in capturing temporal\nchanges across ice layers, while the graph transformer combines the strengths\nof transformers for learning long-range dependencies with graph neural networks\nfor capturing spatial patterns, enabling robust modeling of complex\nspatiotemporal dynamics.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction",
    "url": "http://arxiv.org/abs/2507.06806v1",
    "authors": [
      "Eya Cherif",
      "Arthur Ouaknine",
      "Luke A. Brown",
      "Phuong D. Dao",
      "Kyle R. Kovach",
      "Bing Lu",
      "Daniel Mederer",
      "Hannes Feilhauer",
      "Teja Kattenborn",
      "David Rolnick"
    ],
    "published": "2025-07-09",
    "abstract": "Plant traits such as leaf carbon content and leaf mass are essential\nvariables in the study of biodiversity and climate change. However,\nconventional field sampling cannot feasibly cover trait variation at\necologically meaningful spatial scales. Machine learning represents a valuable\nsolution for plant trait prediction across ecosystems, leveraging hyperspectral\ndata from remote sensing. Nevertheless, trait prediction from hyperspectral\ndata is challenged by label scarcity and substantial domain shifts (\\eg across\nsensors, ecological distributions), requiring robust cross-domain methods.\nHere, we present GreenHyperSpectra, a pretraining dataset encompassing\nreal-world cross-sensor and cross-ecosystem samples designed to benchmark trait\nprediction with semi- and self-supervised methods. We adopt an evaluation\nframework encompassing in-distribution and out-of-distribution scenarios. We\nsuccessfully leverage GreenHyperSpectra to pretrain label-efficient\nmulti-output regression models that outperform the state-of-the-art supervised\nbaseline. Our empirical analyses demonstrate substantial improvements in\nlearning spectral representations for trait prediction, establishing a\ncomprehensive methodological framework to catalyze research at the intersection\nof representation learning and plant functional traits assessment. All code and\ndata are available at: https://github.com/echerif18/HyspectraSSL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing",
    "url": "http://arxiv.org/abs/2507.04842v1",
    "authors": [
      "Colin Laganier",
      "Liam Fletcher",
      "Elim Kwan",
      "Richard Walters",
      "Victoria Nockles"
    ],
    "published": "2025-07-07",
    "abstract": "Rapid analysis of satellite data is vital for many remote sensing\napplications, from disaster response to environmental monitoring, but is\nbecoming harder to achieve with the increasing volumes of data generated by\nmodern satellites. On-satellite machine learning (ML) offers a potential\nsolution, by reducing latency associated with transmission of these large data\nvolumes to ground stations, but state-of-the-art models are often too large or\npower-hungry for satellite deployment. Vessel detection using Synthetic\nAperture Radar (SAR) is a critical time-sensitive task for maritime security\nthat exemplifies this challenge. SAR vessel detection has previously been\ndemonstrated only by ML models that either are too large for satellite\ndeployment, have not been developed for sufficiently low-power hardware, or\nhave only been developed and tested on small SAR datasets that do not\nsufficiently represent the real-world task. Here we address this issue by\ndeveloping and deploying a new efficient and highly performant SAR vessel\ndetection model, using a customised YOLOv8 architecture specifically optimized\nfor FPGA-based processing within common satellite power constraints (<10W). We\ntrain and evaluate our model on the largest and most diverse open SAR vessel\ndataset, xView3-SAR, and deploy it on a Kria KV260 MPSoC. We show that our\nFPGA-based model has detection and classification performance only ~2% and 3%\nlower than values from state-of-the-art GPU-based models, despite being two to\nthree orders of magnitude smaller in size. This work demonstrates small yet\nhighly performant ML models for time-critical SAR analysis, paving the way for\nmore autonomous, responsive, and scalable Earth observation systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Generative Lagrangian data assimilation for ocean dynamics under extreme sparsity",
    "url": "http://arxiv.org/abs/2507.06479v1",
    "authors": [
      "Niloofar Asefi",
      "Leonard Lupin-Jimenez",
      "Tianning Wu",
      "Ruoying He",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-07-09",
    "abstract": "Reconstructing ocean dynamics from observational data is fundamentally\nlimited by the sparse, irregular, and Lagrangian nature of spatial sampling,\nparticularly in subsurface and remote regions. This sparsity poses significant\nchallenges for forecasting key phenomena such as eddy shedding and rogue waves.\nTraditional data assimilation methods and deep learning models often struggle\nto recover mesoscale turbulence under such constraints. We leverage a deep\nlearning framework that combines neural operators with denoising diffusion\nprobabilistic models (DDPMs) to reconstruct high-resolution ocean states from\nextremely sparse Lagrangian observations. By conditioning the generative model\non neural operator outputs, the framework accurately captures small-scale,\nhigh-wavenumber dynamics even at $99\\%$ sparsity (for synthetic data) and\n$99.9\\%$ sparsity (for real satellite observations). We validate our method on\nbenchmark systems, synthetic float observations, and real satellite data,\ndemonstrating robust performance under severe spatial sampling limitations as\ncompared to other deep learning baselines.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning in Acoustics: A Review and Open-Source Repository",
    "url": "http://arxiv.org/abs/2507.04419v1",
    "authors": [
      "Ryan A. McCarthy",
      "You Zhang",
      "Samuel A. Verburg",
      "William F. Jenkins",
      "Peter Gerstoft"
    ],
    "published": "2025-07-06",
    "abstract": "Acoustic data provide scientific and engineering insights in fields ranging\nfrom bioacoustics and communications to ocean and earth sciences. In this\nreview, we survey recent advances and the transformative potential of machine\nlearning (ML) in acoustics, including deep learning (DL). Using the Python\nhigh-level programming language, we demonstrate a broad collection of ML\ntechniques to detect and find patterns for classification, regression, and\ngeneration in acoustics data automatically. We have ML examples including\nacoustic data classification, generative modeling for spatial audio, and\nphysics-informed neural networks. This work includes AcousticsML, a set of\npractical Jupyter notebook examples on GitHub demonstrating ML benefits and\nencouraging researchers and practitioners to apply reproducible data-driven\napproaches to acoustic challenges.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "HRRRCast: a data-driven emulator for regional weather forecasting at convection allowing scales",
    "url": "http://arxiv.org/abs/2507.05658v1",
    "authors": [
      "Daniel Abdi",
      "Isidora Jankov",
      "Paul Madden",
      "Vanderlei Vargas",
      "Timothy A. Smith",
      "Sergey Frolov",
      "Montgomery Flora",
      "Corey Potvin"
    ],
    "published": "2025-07-08",
    "abstract": "The High-Resolution Rapid Refresh (HRRR) model is a convection-allowing model\nused in operational weather forecasting across the contiguous United States\n(CONUS). To provide a computationally efficient alternative, we introduce\nHRRRCast, a data-driven emulator built with advanced machine learning\ntechniques. HRRRCast includes two architectures: a ResNet-based model (ResHRRR)\nand a Graph Neural Network-based model (GraphHRRR). ResHRRR uses convolutional\nneural networks enhanced with squeeze-and-excitation blocks and Feature-wise\nLinear Modulation, and supports probabilistic forecasting via the Denoising\nDiffusion Implicit Model (DDIM). To better handle longer lead times, we train a\nsingle model to predict multiple lead times (1h, 3h, and 6h), then use a greedy\nrollout strategy during inference. When evaluated on composite reflectivity\nover the full CONUS domain using ensembles of 3 to 10 members, ResHRRR\noutperforms HRRR forecast at light rainfall threshold (20 dBZ) and achieves\ncompetitive performance at moderate thresholds (30 dBZ). Our work advances the\nStormCast model of Pathak et al. [21] by: a) training on the full CONUS domain,\nb) using multiple lead times to improve long-range skill, c) training on\nanalysis data instead of the +1h post-analysis data inadvertently used in\nStormCast, and d) incorporating future GFS states as inputs, enabling\ndownscaling that improves long-lead accuracy. Grid-, neighborhood-, and\nobject-based metrics confirm better storm placement, lower frequency bias, and\nhigher success ratios than HRRR. HRRRCast ensemble forecasts also maintain\nsharper spatial detail, with power spectra more closely matching HRRR analysis.\nWhile GraphHRRR underperforms in its current form, it lays groundwork for\nfuture graph-based forecasting. HRRRCast represents a step toward efficient,\ndata-driven regional weather prediction with competitive accuracy and ensemble\ncapability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Site-Specific Parameterization of Ocean Spectra for Power Estimates of Wave Energy Converters",
    "url": "http://arxiv.org/abs/2507.05440v1",
    "authors": [
      "Rafael Baez Ramirez",
      "Ethan J. Sloan",
      "Carlos Alejandro Michel\u00e9n Str\u00f6fer"
    ],
    "published": "2025-07-07",
    "abstract": "Estimating the mean annual power of a wave energy converter (WEC) through the\nmethod of bins relies on a parametric representation of all possible sea\nstates. In practice, two-parameter spectra based on significant wave height and\nenergy period are ubiquitous. Two-parameter spectra have been shown\ninsufficient in capturing the range of spectral shapes that can occur in an\nactual ocean environment. Furthermore, through sensitivity analysis, these\ntwo-parameters have been shown to be insufficient for predicting power\nperformance of WECs. Four parameter spectra, which expand the parameter space\nto include two additional shape parameters have been shown sufficient in\ncapturing sea state variance, but their effect on mean power estimates has not\nbeen presented. This work directly looks at the effects of incorporating\n4-parameter spectra into annual power estimates compared to using the\ntraditional 2-parameter spectra. We use two different 4-parameter spectra: one\nfrom the literature and a novel machine learning-based autoencoder, presented\nhere. Both are shown to improve the information retained when parameterizing\nspectra. The site-specific autoencoder performs consistently the best across\ntwo case studies of mean annual power prediction, achieving an error around 1%\nin each instance. The 2-parameter spectra resulted in less consistent\npredictive performance, with errors of -8% and 1% in the two case studies. For\nthe case study where all three models performed well, it is shown that the low\nerror in the 2-parameter model is attributable to a symmetrical distribution of\nlarge errors whereas both 4-parameter spectra result in relatively low errors\nthroughout the parametric space. These results highlight the need for more\nsophisticated resource characterization methods for estimating the power\nperformance of WECs and suggest site-specific machine learning-based spectra\nare an adequate option.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Interpretable Machine Learning for Urban Heat Mitigation: Attribution and Weighting of Multi-Scale Drivers",
    "url": "http://arxiv.org/abs/2507.04802v1",
    "authors": [
      "David Tschan",
      "Zhi Wang",
      "Jan Carmeliet",
      "Yongling Zhao"
    ],
    "published": "2025-07-07",
    "abstract": "Urban heat islands (UHIs) are often accentuated during heat waves (HWs) and\npose a public health risk. Mitigating UHIs requires urban planners to first\nestimate how urban heat is influenced by different land use types (LUTs) and\ndrivers across scales - from synoptic-scale climatic background processes to\nsmall-scale urban- and scale-bridging features. This study proposes to classify\nthese drivers into driving (D), urban (U), and local (L) features,\nrespectively. To increase interpretability and enhance computation efficiency,\na LUT-distinguishing machine learning approach is proposed as a fast emulator\nfor Weather Research and Forecasting model coupled to a Single-Layer Urban\nCanopy Model (WRF-SLUCM) to predict ground- (TSK) and 2-meter air temperature\n(T2). Using random forests (RFs) with extreme gradient boosting (XGB) trained\non WRF-SLUCM output over Zurich, Switzerland, during heatwave (HW) periods in\n2017 and 2019, this study proposes LUT-based (LB) models that categorize\nfeatures by scales and practical controllability, allowing optional categorical\nweighting. This approach enables category-specific feature ranking and\nsensitivity estimation of T2 and TSK to most important small-scale drivers -\nmost notably surface emissivity, albedo, and leaf area index (LAI). Models\nemploying the LB framework are statistically significantly more accurate than\nmodels that do not, with higher performance when more HW data is included in\ntraining. With RF-XGB robustly performing optimal with unit weights, the method\nsubstantially increase interpretability. Despite the needs to reduce\nstatistical uncertainties and testing the method on other cities, the proposed\napproach offers urban planners a direct framework for feasibility-centered UHI\nmitigation assessment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Continental scale habitat modelling with artificial intelligence and multimodal earth observation",
    "url": "http://arxiv.org/abs/2507.09732v1",
    "authors": [
      "Sara Si-Moussi",
      "Stephan Hennekens",
      "Sander Mucher",
      "Stan Los",
      "Wilfried Thuiller"
    ],
    "published": "2025-07-13",
    "abstract": "Habitats integrate the abiotic conditions and biophysical structures that\nsupport biodiversity and sustain nature's contributions to people. As these\necosystems face mounting pressure from human activities, accurate,\nhigh-resolution habitat maps are essential for effective conservation and\nrestoration. Yet current maps often fall short in thematic or spatial\nresolution because they must (1) model several mutually exclusive habitat types\nthat co-occur across landscapes and (2) cope with severe class imbalance that\ncomplicate multi-class training. Here, we evaluated how high-resolution remote\nsensing (RS) data and Artificial Intelligence (AI) tools can improve habitat\nclassification over large geographic extents at fine thematic resolution. Using\nvegetation plots from the European Vegetation Archive, we modelled Level 3\nEUNIS habitats across Europe and assessed multiple modelling strategies against\nindependent validation datasets. Strategies that exploited the hierarchical\nnature of habitat nomenclatures resolved classification ambiguities, especially\nin fragmented landscapes. Integrating multi-spectral (MSI) and synthetic\naperture radar (SAR) imagery, particularly through Earth Observation Foundation\nmodels, enhanced within-formation discrimination and overall performance.\nFinally, ensemble machine learning that corrects class imbalance boosted\naccuracy further. Our methodological framework is transferable beyond Europe\nand adaptable to other classification systems. Future research should advance\ntemporal modelling of dynamic habitats, extend to habitat segmentation and\nquality assessment, and exploit next-generation EO data paired with\nhigher-quality in-situ observations.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges",
    "url": "http://arxiv.org/abs/2507.09562v1",
    "authors": [
      "Yidong Jiang"
    ],
    "published": "2025-07-13",
    "abstract": "The Segment Anything Model (SAM) has revolutionized image segmentation\nthrough its innovative prompt-based approach, yet the critical role of prompt\nengineering in its success remains underexplored. This paper presents the first\ncomprehensive survey focusing specifically on prompt engineering techniques for\nSAM and its variants. We systematically organize and analyze the rapidly\ngrowing body of work in this emerging field, covering fundamental\nmethodologies, practical applications, and key challenges. Our review reveals\nhow prompt engineering has evolved from simple geometric inputs to\nsophisticated multimodal approaches, enabling SAM's adaptation across diverse\ndomains including medical imaging and remote sensing. We identify unique\nchallenges in prompt optimization and discuss promising research directions.\nThis survey fills an important gap in the literature by providing a structured\nframework for understanding and advancing prompt engineering in foundation\nmodels for segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge",
    "url": "http://arxiv.org/abs/2507.09202v1",
    "authors": [
      "Wuxin Wang",
      "Weicheng Ni",
      "Lilan Huang",
      "Tao Hao",
      "Ben Fei",
      "Shuo Ma",
      "Taikang Yuan",
      "Yanlai Zhao",
      "Kefeng Deng",
      "Xiaoyong Li",
      "Boheng Duan",
      "Lei Bai",
      "Kaijun Ren"
    ],
    "published": "2025-07-12",
    "abstract": "Recent advancements in Artificial Intelligence (AI) demonstrate significant\npotential to revolutionize weather forecasting. However, most AI-driven models\nrely on Numerical Weather Prediction (NWP) systems for initial condition\npreparation, which often consumes hours on supercomputers. Here we introduce\nXiChen, the first observation-scalable fully AI-driven global weather\nforecasting system, whose entire pipeline, from Data Assimilation (DA) to\nmedium-range forecasting, can be accomplished within only 17 seconds. XiChen is\nbuilt upon a foundation model that is pre-trained for weather forecasting.\nMeanwhile, this model is subsequently fine-tuned to serve as both observation\noperators and DA models, thereby scalably assimilating conventional and raw\nsatellite observations. Furthermore, the integration of four-dimensional\nvariational knowledge ensures that XiChen's DA and medium-range forecasting\naccuracy rivals that of operational NWP systems, amazingly achieving a skillful\nforecasting lead time exceeding 8.25 days. These findings demonstrate that\nXiChen holds strong potential toward fully AI-driven weather forecasting\nindependent of NWP systems.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation",
    "url": "http://arxiv.org/abs/2507.12857v1",
    "authors": [
      "Shiqi Huang",
      "Shuting He",
      "Huaiyuan Qin",
      "Bihan Wen"
    ],
    "published": "2025-07-17",
    "abstract": "Most existing remote sensing instance segmentation approaches are designed\nfor close-vocabulary prediction, limiting their ability to recognize novel\ncategories or generalize across datasets. This restricts their applicability in\ndiverse Earth observation scenarios. To address this, we introduce\nopen-vocabulary (OV) learning for remote sensing instance segmentation. While\ncurrent OV segmentation models perform well on natural image datasets, their\ndirect application to remote sensing faces challenges such as diverse\nlandscapes, seasonal variations, and the presence of small or ambiguous objects\nin aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$\n($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary\n$\\textbf{RE}$mote sensing instance segmentation), a framework that integrates\nmulti-granularity scene context, i.e., regional context and global context, to\nenhance both visual and textual representations. Specifically, we introduce\nRegion-Aware Integration, which refines class embeddings with regional context\nto improve object distinguishability. Additionally, we propose Global Context\nAdaptation, which enriches naive text embeddings with remote sensing global\ncontext, creating a more adaptable and expressive linguistic latent space for\nthe classifier. We establish new benchmarks for OV remote sensing instance\nsegmentation across diverse datasets. Experimental results demonstrate that,\nour proposed method achieves SOTA performance, which provides a robust solution\nfor large-scale, real-world geospatial analysis. Our code is available at\nhttps://github.com/HuangShiqi128/SCORE.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "A Deep-Learning Framework for Land-Sliding Classification from Remote Sensing Image",
    "url": "http://arxiv.org/abs/2507.12939v1",
    "authors": [
      "Hieu Tang",
      "Truong Vo",
      "Dong Pham",
      "Toan Nguyen",
      "Lam Pham",
      "Truong Nguyen"
    ],
    "published": "2025-07-17",
    "abstract": "The use of satellite imagery combined with deep learning to support automatic\nlandslide detection is becoming increasingly widespread. However, selecting an\nappropriate deep learning architecture to optimize performance while avoiding\noverfitting remains a critical challenge. To address these issues, we propose a\ndeep-learning based framework for landslide detection from remote sensing image\nin this paper. The proposed framework presents an effective combination of the\nonline an offline data augmentation to tackle the imbalanced data, a backbone\nEfficientNet\\_Large deep learning model for extracting robust embedding\nfeatures, and a post-processing SVM classifier to balance and enhance the\nclassification performance. The proposed model achieved an F1-score of 0.8938\non the public test set of the Zindi challenge.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Precision Spatio-Temporal Feature Fusion for Robust Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2507.11523v1",
    "authors": [
      "Buddhi Wijenayake",
      "Athulya Ratnayake",
      "Praveen Sumanasekara",
      "Nichula Wasalathilaka",
      "Mathivathanan Piratheepan",
      "Roshan Godaliyadda",
      "Mervyn Ekanayake",
      "Vijitha Herath"
    ],
    "published": "2025-07-15",
    "abstract": "Remote sensing change detection is vital for monitoring environmental and\nurban transformations but faces challenges like manual feature extraction and\nsensitivity to noise. Traditional methods and early deep learning models, such\nas convolutional neural networks (CNNs), struggle to capture long-range\ndependencies and global context essential for accurate change detection in\ncomplex scenes. While Transformer-based models mitigate these issues, their\ncomputational complexity limits their applicability in high-resolution remote\nsensing. Building upon ChangeMamba architecture, which leverages state space\nmodels for efficient global context modeling, this paper proposes precision\nfusion blocks to capture channel-wise temporal variations and per-pixel\ndifferences for fine-grained change detection. An enhanced decoder pipeline,\nincorporating lightweight channel reduction mechanisms, preserves local details\nwith minimal computational cost. Additionally, an optimized loss function\ncombining Cross Entropy, Dice and Lovasz objectives addresses class imbalance\nand boosts Intersection-over-Union (IoU). Evaluations on SYSU-CD, LEVIR-CD+,\nand WHU-CD datasets demonstrate superior precision, recall, F1 score, IoU, and\noverall accuracy compared to state-of-the-art methods, highlighting the\napproach's robustness for remote sensing change detection. For complete\ntransparency, the codes and pretrained models are accessible at\nhttps://github.com/Buddhi19/MambaCD.git",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images",
    "url": "http://arxiv.org/abs/2507.11143v1",
    "authors": [
      "Lam Pham",
      "Cam Le",
      "Hieu Tang",
      "Khang Truong",
      "Truong Nguyen",
      "Jasmin Lampert",
      "Alexander Schindler",
      "Martin Boyer",
      "Son Phan"
    ],
    "published": "2025-07-15",
    "abstract": "In recent years, landslide disasters have reported frequently due to the\nextreme weather events of droughts, floods , storms, or the consequence of\nhuman activities such as deforestation, excessive exploitation of natural\nresources. However, automatically observing landslide is challenging due to the\nextremely large observing area and the rugged topography such as mountain or\nhighland. This motivates us to propose an end-to-end deep-learning-based model\nwhich explores the remote sensing images for automatically observing landslide\nevents. By considering remote sensing images as the input data, we can obtain\nfree resource, observe large and rough terrains by time. To explore the remote\nsensing images, we proposed a novel neural network architecture which is for\ntwo tasks of landslide detection and landslide segmentation. We evaluated our\nproposed model on three different benchmark datasets of LandSlide4Sense, Bijie,\nand Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,\n93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU\nscores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,\nNepal datasets. These experimental results prove potential to integrate our\nproposed model into real-life landslide observation systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks",
    "url": "http://arxiv.org/abs/2507.10381v1",
    "authors": [
      "Aaryam Sharma"
    ],
    "published": "2025-07-14",
    "abstract": "Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection",
    "url": "http://arxiv.org/abs/2507.09541v1",
    "authors": [
      "Zihao Xiong",
      "Fei Zhou",
      "Fengyi Wu",
      "Shuai Yuan",
      "Maixia Fu",
      "Zhenming Peng",
      "Jian Yang",
      "Yimian Dai"
    ],
    "published": "2025-07-13",
    "abstract": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2507.13120v1",
    "authors": [
      "Xiaozheng Jiang",
      "Wei Zhang",
      "Xuerui Mao"
    ],
    "published": "2025-07-17",
    "abstract": "Detecting tiny objects in remote sensing (RS) imagery has been a\nlong-standing challenge due to their extremely limited spatial information,\nweak feature representations, and dense distributions across complex\nbackgrounds. Despite numerous efforts devoted, mainstream detectors still\nunderperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a\nmulti-stage feature fusion and enhancement model explicitly tailored for RS\ntiny object detection in various RS scenarios. RS-TinyNet comes with two novel\ndesigns: tiny object saliency modeling and feature integrity reconstruction.\nGuided by these principles, we design three step-wise feature enhancement\nmodules. Among them, the multi-dimensional collaborative attention (MDCA)\nmodule employs multi-dimensional attention to enhance the saliency of tiny\nobjects. Additionally, the auxiliary reversible branch (ARB) and a progressive\nfusion detection head (PFDH) module are introduced to preserve information flow\nand fuse multi-level features to bridge semantic gaps and retain structural\ndetail. Comprehensive experiments on public RS dataset AI-TOD show that our\nRS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and\n6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior\ndetection performance in diverse RS scenarios. These results demonstrate that\nthe proposed multi-stage feature fusion strategy offers an effective and\npractical solution for tiny object detection in complex RS environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows",
    "url": "http://arxiv.org/abs/2507.12590v1",
    "authors": [
      "Judy Long",
      "Tao Liu",
      "Sean Alexander Woznicki",
      "Miljana Markovi\u0107",
      "Oskar Marko",
      "Molly Sears"
    ],
    "published": "2025-07-16",
    "abstract": "Crop mapping involves identifying and classifying crop types using spatial\ndata, primarily derived from remote sensing imagery. This study presents the\nfirst comprehensive review of large-scale, pixel-wise crop mapping workflows,\nencompassing both conventional supervised methods and emerging transfer\nlearning approaches. To identify the optimal supervised crop mapping workflows,\nwe conducted systematic experiments, comparing six widely adopted satellite\nimage-based preprocessing methods, alongside eleven supervised pixel-wise\nclassification models. Additionally, we assessed the synergistic impact of\nvaried training sample sizes and variable combinations. Moreover, we identified\noptimal transfer learning techniques for different magnitudes of domain shift.\nThe evaluation of best methods was conducted across five diverse agricultural\nsites. Landsat 8 served as the primary satellite data source. Labels come from\nCDL trusted pixels and field surveys.\n  Our findings reveal three key insights. First, fine-scale interval\npreprocessing paired with Transformer models consistently delivered optimal\nperformance for both supervised and transferable workflows. RF offered rapid\ntraining and competitive performance in conventional supervised learning and\ndirect transfer to similar domains. Second, transfer learning techniques\nenhanced workflow adaptability, with UDA being effective for homogeneous crop\nclasses while fine-tuning remains robust across diverse scenarios. Finally,\nworkflow choice depends heavily on the availability of labeled samples. With a\nsufficient sample size, supervised training typically delivers more accurate\nand generalizable results. Below a certain threshold, transfer learning that\nmatches the level of domain shift is a viable alternative to achieve crop\nmapping. Repository:\nBest-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening",
    "url": "http://arxiv.org/abs/2507.10461v1",
    "authors": [
      "Tao Tang",
      "Chengxu Yang"
    ],
    "published": "2025-07-14",
    "abstract": "Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
=======
    "title": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism",
    "url": "http://arxiv.org/abs/2511.17198v1",
    "authors": [
      "Kaiyu Li",
      "Jiayu Wang",
      "Zhi Wang",
      "Hui Qiao",
      "Weizhan Zhang",
      "Deyu Meng",
      "Xiangyong Cao"
    ],
    "published": "2025-11-21",
    "abstract": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area",
    "url": "http://arxiv.org/abs/2507.10084v1",
    "authors": [
      "Haonan Chen",
      "Xin Tong"
    ],
    "published": "2025-07-14",
    "abstract": "To address the prevalent challenges of domain shift and small sample sizes in\nremote sensing image water body segmentation, this study proposes and validates\na two-stage transfer learning strategy based on the SegFormer model. The\napproach begins by training a foundational segmentation model on a diverse\nsource domain, where it achieves an Intersection over Union (IoU) of 68.80% on\nits validation set, followed by fine-tuning on data from the distinct target\ndomain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by\nhighly complex topography and spectral features -- the experimental results\ndemonstrate that this strategy significantly boosts the IoU for the water body\nsegmentation task from 25.50% (for direct transfer) to 64.84%. This not only\neffectively resolves the model performance degradation caused by domain\ndiscrepancy but also provides an effective technical paradigm for\nhigh-precision thematic information extraction in data-scarce and\nenvironmentally unique remote sensing scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "A Multimodal Data Fusion Generative Adversarial Network for Real Time Underwater Sound Speed Field Construction",
    "url": "http://arxiv.org/abs/2507.11812v1",
    "authors": [
      "Wei Huang",
      "Yuqiang Huang",
      "Yanan Wu",
      "Tianhe Xu",
      "Junting Wang",
      "Hao Zhang"
    ],
    "published": "2025-07-16",
    "abstract": "Sound speed profiles (SSPs) are essential parameters underwater that affects\nthe propagation mode of underwater signals and has a critical impact on the\nenergy efficiency of underwater acoustic communication and accuracy of\nunderwater acoustic positioning. Traditionally, SSPs can be obtained by\nmatching field processing (MFP), compressive sensing (CS), and deep learning\n(DL) methods. However, existing methods mainly rely on on-site underwater sonar\nobservation data, which put forward strict requirements on the deployment of\nsonar observation systems. To achieve high-precision estimation of sound\nvelocity distribution in a given sea area without on-site underwater data\nmeasurement, we propose a multi-modal data-fusion generative adversarial\nnetwork model with residual attention block (MDF-RAGAN) for SSP construction.\nTo improve the model's ability for capturing global spatial feature\ncorrelations, we embedded the attention mechanisms, and use residual modules\nfor deeply capturing small disturbances in the deep ocean sound velocity\ndistribution caused by changes of SST. Experimental results on real open\ndataset show that the proposed model outperforms other state-of-the-art\nmethods, which achieves an accuracy with an error of less than 0.3m/s.\nSpecifically, MDF-RAGAN not only outperforms convolutional neural network (CNN)\nand spatial interpolation (SITP) by nearly a factor of two, but also achieves\nabout 65.8\\% root mean square error (RMSE) reduction compared to mean profile,\nwhich fully reflects the enhancement of overall profile matching by\nmulti-source fusion and cross-modal attention.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
=======
    "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
    "url": "http://arxiv.org/abs/2511.04464v1",
    "authors": [
      "Carnot Braun",
      "Rafael O. Jarczewski",
      "Gabriel U. Talasso",
      "Leandro A. Villas",
      "Allan M. de Souza"
    ],
    "published": "2025-11-06",
    "abstract": "Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale",
    "url": "http://arxiv.org/abs/2507.12144v1",
    "authors": [
      "Boris Bonev",
      "Thorsten Kurth",
      "Ankur Mahesh",
      "Mauro Bisson",
      "Jean Kossaifi",
      "Karthik Kashinath",
      "Anima Anandkumar",
      "William D. Collins",
      "Michael S. Pritchard",
      "Alexander Keller"
    ],
    "published": "2025-07-16",
    "abstract": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency",
    "url": "http://arxiv.org/abs/2507.10893v1",
    "authors": [
      "Minjong Cheon",
      "Eunhan Goo",
      "Su-Hyeon Shin",
      "Muhammad Ahmed",
      "Hyungjun Kim"
    ],
    "published": "2025-07-15",
    "abstract": "Recently, AI-based weather forecast models have achieved impressive advances.\nThese models have reached accuracy levels comparable to traditional NWP\nsystems, marking a significant milestone in data-driven weather prediction.\nHowever, they mostly leverage Transformer-based architectures, which often\nleads to high training complexity and resource demands due to the massive\nparameter sizes. In this study, we introduce a modernized CNN-based model for\nglobal weather forecasting that delivers competitive accuracy while\nsignificantly reducing computational requirements. To present a systematic\nmodernization roadmap, we highlight key architectural enhancements across\nmultiple design scales from an earlier CNN-based approach. KAI-a incorporates a\nscale-invariant architecture and InceptionNeXt-based blocks within a\ngeophysically-aware design, tailored to the structure of Earth system data.\nTrained on the ERA5 daily dataset with 67 atmospheric variables, the model\ncontains about 7 million parameters and completes training in just 12 hours on\na single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the\nperformance of state-of-the-art models in medium-range weather forecasting,\nwhile offering a significantly lightweight design. Furthermore, case studies on\nthe 2018 European heatwave and the East Asian summer monsoon demonstrate\nKAI-a's robust skill in capturing extreme events, reinforcing its practical\nutility.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling",
    "url": "http://arxiv.org/abs/2507.09211v1",
    "authors": [
      "Xinyue Liu",
      "Xiao Peng",
      "Shuyue Yan",
      "Yuntian Chen",
      "Dongxiao Zhang",
      "Zhixiao Niu",
      "Hui-Min Wang",
      "Xiaogang He"
    ],
    "published": "2025-07-12",
    "abstract": "Observed records of climate extremes provide an incomplete picture of risk,\nmissing \"unseen\" extremes that exceed historical bounds. In parallel,\nneglecting spatial dependence undervalues the risk of synchronized hazards that\namplify impacts. To address these challenges, we develop DeepX-GAN\n(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial\nNetwork), a knowledge-informed deep generative model designed to better capture\nthe spatial structure of rare extremes. The zero-shot generalizability of\nDeepX-GAN enables simulation of unseen extremes that fall outside historical\nexperience yet remain statistically plausible. We define two types of unseen\nextremes: \"checkmate\" extremes that directly hit targets, and \"stalemate\"\nextremes that narrowly miss. These unrealized scenarios expose latent risks in\nfragile systems and may reinforce a false sense of resilience if overlooked.\nNear misses, in particular, can prompt either proactive adaptation or dangerous\ncomplacency, depending on how they are interpreted. Applying DeepX-GAN to the\nMiddle East and North Africa (MENA), we find that these unseen extremes\ndisproportionately affect regions with high vulnerability and low socioeconomic\nreadiness, but differ in urgency and interpretation. Future warming could\nexpand and redistribute these unseen extremes, with emerging exposure hotspots\nin Indo-Pakistan and Central Africa. This distributional shift highlights\ncritical blind spots in conventional hazard planning and underscores the need\nto develop spatially adaptive policies that anticipate emergent risk hotspots\nrather than simply extrapolating from historical patterns.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": []
  },
  {
    "title": "Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates",
    "url": "http://arxiv.org/abs/2507.09166v1",
    "authors": [
      "Louise Largeau",
      "Erwan Koch",
      "David Leutwyler",
      "Gregoire Mariethoz",
      "Valerie Chavez-Demoulin",
      "Tom Beucler"
    ],
    "published": "2025-07-12",
    "abstract": "The coarse spatial resolution of gridded climate models, such as general\ncirculation models, limits their direct use in projecting socially relevant\nvariables like extreme precipitation. Most downscaling methods estimate the\nconditional distributions of extremes by generating large ensembles,\ncomplicating the assessment of robustness under distributional shifts, such as\nthose induced by climate change. To better understand and potentially improve\nrobustness, we propose super-resolving the parameters of the target variable's\nprobability distribution directly using analytically tractable mappings. Within\na perfect-model framework over Switzerland, we demonstrate that vector\ngeneralized linear and additive models can super-resolve the generalized\nextreme value distribution of summer hourly precipitation extremes from coarse\nprecipitation fields and topography. We introduce the notion of a \"robustness\ngap\", defined as the difference in predictive error between present-trained and\nfuture-trained models, and use it to diagnose how model structure affects the\ngeneralization of each quantile to a pseudo-global warming scenario. By\nevaluating multiple model configurations, we also identify an upper limit on\nthe super-resolution factor based on the spatial auto- and cross-correlation of\nprecipitation and elevation, beyond which coarse precipitation loses predictive\nvalue. Our framework is broadly applicable to variables governed by parametric\ndistributions and offers a model-agnostic diagnostic for understanding when and\nwhy empirical downscaling generalizes to climate change and extremes.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Uncovering symmetric and asymmetric species associations from community and environmental data",
    "url": "http://arxiv.org/abs/2507.09317v1",
    "authors": [
      "Sara Si-Moussi",
      "Esther Galbrun",
      "Mickael Hedde",
      "Giovanni Poggiato",
      "Matthias Rohr",
      "Wilfried Thuiller"
    ],
    "published": "2025-07-12",
    "abstract": "There is no much doubt that biotic interactions shape community assembly and\nultimately the spatial co-variations between species. There is a hope that the\nsignal of these biotic interactions can be observed and retrieved by\ninvestigating the spatial associations between species while accounting for the\ndirect effects of the environment. By definition, biotic interactions can be\nboth symmetric and asymmetric. Yet, most models that attempt to retrieve\nspecies associations from co-occurrence or co-abundance data internally assume\nsymmetric relationships between species. Here, we propose and validate a\nmachine-learning framework able to retrieve bidirectional associations by\nanalyzing species community and environmental data.\n  Our framework (1) models pairwise species associations as directed influences\nfrom a source to a target species, parameterized with two species-specific\nlatent embeddings: the effect of the source species on the community, and the\nresponse of the target species to the community; and (2) jointly fits these\nassociations within a multi-species conditional generative model with different\nmodes of interactions between environmental drivers and biotic associations.\nUsing both simulated and empirical data, we demonstrate the ability of our\nframework to recover known asymmetric and symmetric associations and highlight\nthe properties of the learned association networks. By comparing our approach\nto other existing models such as joint species distribution models and\nprobabilistic graphical models, we show its superior capacity at retrieving\nsymmetric and asymmetric interactions. The framework is intuitive, modular and\nbroadly applicable across various taxonomic groups.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation",
    "url": "http://arxiv.org/abs/2507.16716v1",
    "authors": [
      "Yiguo He",
      "Junjie Zhu",
      "Yiying Li",
      "Xiaoyu Zhang",
      "Chunping Qiu",
      "Jun Wang",
      "Qiangjuan Huang",
      "Ke Yang"
    ],
    "published": "2025-07-22",
    "abstract": "The application of Vision-language foundation models (VLFMs) to remote\nsensing (RS) imagery has garnered significant attention due to their superior\ncapability in various downstream tasks. A key challenge lies in the scarcity of\nhigh-quality, large-scale, image-text paired training data. Recently, several\nworks introduced extensive image-text datasets for RS and trained their VLFMs.\nHowever, due to the rudimentary methods used for generating captions, the\nquality of datasets is suboptimal, requiring larger volumes of training data,\nwhile only yielding modest performance improvements. In this paper, we propose\na two-stage method named MpGI(Multi-Perspective Generation and Integration) for\ngenerating high-quality text captions for RS images. Firstly, we generate\ndistinct and detailed descriptions from different perspectives using\nRule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs\ngeneration methods. Next, we utilize Large Language Models (LLMs) to integrate\nthese diverse descriptions into comprehensive captions, capturing details from\nmultiple perspectives. Finally, we have created the HQRS-IT-210K dataset,\nincluding about 210,000 RS images and 1.3 million captions. We fine-tuned two\nVLFMs using our dataset: CLIP, a discriminative model, and CoCa, an\nimage-to-text generative model. This process resulted in our proposed HQRS-CLIP\nand RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed\nthe previous SOTA RS CLIP model in various downstream tasks while using only\n4.2\\% of the training data. RS-CoCa outperforms other advanced approaches\nacross benchmark datasets and can generate captions for RS images that rival or\neven exceed manual annotations. Dataset, pre-trained models, and codes will be\nreleased at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "CLIP"
    ],
    "applications": []
  },
  {
    "title": "A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges",
    "url": "http://arxiv.org/abs/2507.18376v1",
    "authors": [
      "Xing Hua",
      "Haodong Chen",
      "Qianqian Duan",
      "Danfeng Hong",
      "Ruijiao Li",
      "Huiliang Shang",
      "Linghua Jiang",
      "Haima Yang",
      "Dawei Zhang"
    ],
    "published": "2025-07-24",
    "abstract": "With the global population growing and arable land resources becoming\nincreasingly scarce,smart agriculture and precision agriculture have emerged as\nkey directions for the future ofagricultural development.Artificial\nintelligence (AI) technologies, particularly deep learning models, have found\nwidespread applications in areas such as crop monitoring and pest detection. As\nan emerging generative model, diffusion models have shown significant promise\nin tasks like agricultural image processing, data augmentation, and remote\nsensing. Compared to traditional generative adversarial networks (GANs),\ndiffusion models offer superior training stability and generation quality,\neffectively addressing challenges such as limited agricultural data and\nimbalanced image samples. This paper reviews the latest advancements in the\napplication of diffusion models in agriculture, focusing on their potential in\ncrop pest and disease detection, remote sensing image enhancement, crop growth\nprediction, and agricultural resource management. Experimental results\ndemonstrate that diffusion models significantly improve model accuracy and\nrobustness in data augmentation, image generation, and denoising, especially in\ncomplex environments. Despite challenges related to computational efficiency\nand generalization capabilities, diffusion models are expected to play an\nincreasingly important role in smart and precision agriculture as technology\nadvances, providing substantial support for the sustainable development of\nglobal agriculture.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN",
      "Diffusion Models"
    ],
    "applications": [
      "Detection",
      "Image Generation",
      "Forecast"
    ]
  },
  {
    "title": "Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels for Building Detection",
    "url": "http://arxiv.org/abs/2507.16657v1",
    "authors": [
      "Shuang Song",
      "Yang Tang",
      "Rongjun Qin"
    ],
    "published": "2025-07-22",
    "abstract": "Deep learning has significantly advanced building segmentation in remote\nsensing, yet models struggle to generalize on data of diverse geographic\nregions due to variations in city layouts and the distribution of building\ntypes, sizes and locations. However, the amount of time-consuming annotated\ndata for capturing worldwide diversity may never catch up with the demands of\nincreasingly data-hungry models. Thus, we propose a novel approach: re-training\nmodels at test time using synthetic data tailored to the target region's city\nlayout. This method generates geo-typical synthetic data that closely\nreplicates the urban structure of a target area by leveraging geospatial data\nsuch as street network from OpenStreetMap. Using procedural modeling and\nphysics-based rendering, very high-resolution synthetic images are created,\nincorporating domain randomization in building shapes, materials, and\nenvironmental illumination. This enables the generation of virtually unlimited\ntraining samples that maintain the essential characteristics of the target\nenvironment. To overcome synthetic-to-real domain gaps, our approach integrates\ngeo-typical data into an adversarial domain adaptation framework for building\nsegmentation. Experiments demonstrate significant performance enhancements,\nwith median improvements of up to 12%, depending on the domain gap. This\nscalable and cost-effective method blends partial geographic knowledge with\nsynthetic imagery, providing a promising solution to the \"model collapse\" issue\nin purely synthetic datasets. It offers a practical pathway to improving\ngeneralization in remote sensing building segmentation without extensive\nreal-world annotations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing",
    "url": "http://arxiv.org/abs/2507.16228v1",
    "authors": [
      "Shreelekha Revankar",
      "Utkarsh Mall",
      "Cheng Perng Phoo",
      "Kavita Bala",
      "Bharath Hariharan"
    ],
    "published": "2025-07-22",
    "abstract": "Natural disasters cause devastating damage to communities and infrastructure\nevery year. Effective disaster response is hampered by the difficulty of\naccessing affected areas during and after events. Remote sensing has allowed us\nto monitor natural disasters in a remote way. More recently there have been\nadvances in computer vision and deep learning that help automate satellite\nimagery analysis, However, they remain limited by their narrow focus on\nspecific disaster types, reliance on manual expert interpretation, and lack of\ndatasets with sufficient temporal granularity or natural language annotations\nfor tracking disaster progression. We present MONITRS, a novel multimodal\ndataset of more than 10,000 FEMA disaster events with temporal satellite\nimagery and natural language annotations from news articles, accompanied by\ngeotagged locations, and question-answer pairs. We demonstrate that fine-tuning\nexisting MLLMs on our dataset yields significant performance improvements for\ndisaster monitoring tasks, establishing a new benchmark for machine\nlearning-assisted disaster response systems. Code can be found at:\nhttps://github.com/ShreelekhaR/MONITRS",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery",
    "url": "http://arxiv.org/abs/2507.16849v1",
    "authors": [
      "Yi-Shan Chu",
      "Hsuan-Cheng Wei"
    ],
    "published": "2025-07-21",
    "abstract": "We propose a vision transformer (ViT)-based deep learning framework to refine\ndisaster-affected area segmentation from remote sensing imagery, aiming to\nsupport and enhance the Emergent Value Added Product (EVAP) developed by the\nTaiwan Space Agency (TASA). The process starts with a small set of manually\nannotated regions. We then apply principal component analysis (PCA)-based\nfeature space analysis and construct a confidence index (CI) to expand these\nlabels, producing a weakly supervised training set. These expanded labels are\nthen used to train ViT-based encoder-decoder models with multi-band inputs from\nSentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder\nvariants and multi-stage loss strategies to improve performance under limited\nsupervision. During the evaluation, model predictions are compared with\nhigher-resolution EVAP output to assess spatial coherence and segmentation\nconsistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes\nwildfire demonstrate that our framework improves the smoothness and reliability\nof segmentation results, offering a scalable approach for disaster mapping when\naccurate ground truth is unavailable.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover",
    "url": "http://arxiv.org/abs/2507.18099v1",
    "authors": [
      "Naman Srivastava",
      "Joel D Joy",
      "Yash Dixit",
      "Swarup E",
      "Rakshit Ramesh"
    ],
    "published": "2025-07-24",
    "abstract": "Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning, and is one of the key elements in developing smart and sustainable\ncities.This study evaluates advanced LULC mapping techniques, focusing on\nLook-Up Table (LUT)-based Atmospheric Correction applied to Cartosat\nMultispectral (MX) sensor images, followed by supervised and semi-supervised\nlearning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo\nSupervision (CPS). The CPS model is further refined with dynamic weighting,\nenhancing pseudo-label reliability during training. This comprehensive approach\nanalyses the accuracy and utility of LULC mapping techniques for various urban\nplanning applications. A case study of Hyderabad, India, illustrates\nsignificant land use changes due to rapid urbanization. By analyzing Cartosat\nMX images over time, we highlight shifts such as urban sprawl, shrinking green\nspaces, and expanding industrial areas. This demonstrates the practical utility\nof these techniques for urban planners and policymakers.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents",
    "url": "http://arxiv.org/abs/2507.18067v1",
    "authors": [
      "Abdessamad El-Kabid",
      "Loubna Benabbou",
      "Redouane Lguensat",
      "Alex Hern\u00e1ndez-Garc\u00eda"
    ],
    "published": "2025-07-24",
    "abstract": "Accurate modeling of physical systems governed by partial differential\nequations is a central challenge in scientific computing. In oceanography,\nhigh-resolution current data are critical for coastal management, environmental\nmonitoring, and maritime safety. However, available satellite products, such as\nCopernicus data for sea water velocity at ~0.08 degrees spatial resolution and\nglobal ocean models, often lack the spatial granularity required for detailed\nlocal analyses. In this work, we (a) introduce a supervised deep learning\nframework based on neural operators for solving PDEs and providing arbitrary\nresolution solutions, and (b) propose downscaling models with an application to\nCopernicus ocean current data. Additionally, our method can model surrogate\nPDEs and predict solutions at arbitrary resolution, regardless of the input\nresolution. We evaluated our model on real-world Copernicus ocean current data\nand synthetic Navier-Stokes simulation datasets.",
    "categories": [
      "ocean"
=======
    "title": "From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL",
    "url": "http://arxiv.org/abs/2510.25997v1",
    "authors": [
      "Manu Redd",
      "Tao Zhe",
      "Dongjie Wang"
    ],
    "published": "2025-10-29",
    "abstract": "Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing access to structured data, allowing users to query databases without learning SQL. Yet existing systems struggle with realistic spatio-temporal queries, where success requires aligning vague user phrasing with schema-specific categories, handling temporal reasoning, and choosing appropriate outputs. We present an agentic pipeline that extends a naive text-to-SQL baseline (llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The agent can plan, decompose, and adapt queries through schema inspection, SQL generation, execution, and visualization tools. We evaluate on 35 natural-language queries over the NYC and Tokyo check-in dataset, covering spatial, temporal, and multi-dataset reasoning. The agent achieves substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and enhances usability through maps, plots, and structured natural-language summaries. Crucially, our design enables more natural human-database interaction, supporting users who lack SQL expertise, detailed schema knowledge, or prompting skill. We conclude that agentic orchestration, rather than stronger SQL generators alone, is a promising foundation for interactive geospatial assistants.",
    "categories": [
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "Bayesian Deep Learning for Convective Initiation Nowcasting Uncertainty Estimation",
    "url": "http://arxiv.org/abs/2507.16219v1",
    "authors": [
      "Da Fan",
      "David John Gagne II",
      "Steven J. Greybush",
      "Eugene E. Clothiaux",
      "John S. Schreck",
      "Chaopeng Shen"
    ],
    "published": "2025-07-22",
    "abstract": "This study evaluated the probability and uncertainty forecasts of five\nrecently proposed Bayesian deep learning methods relative to a deterministic\nresidual neural network (ResNet) baseline for 0-1 h convective initiation (CI)\nnowcasting using GOES-16 satellite infrared observations. Uncertainty was\nassessed by how well probabilistic forecasts were calibrated and how well\nuncertainty separated forecasts with large and small errors. Most of the\nBayesian deep learning methods produced probabilistic forecasts that\noutperformed the deterministic ResNet, with one, the initial-weights ensemble +\nMonte Carlo (MC) dropout, an ensemble of deterministic ResNets with different\ninitial weights to start training and dropout activated during inference,\nproducing the most skillful and well-calibrated forecasts. The initial-weights\nensemble + MC dropout benefited from generating multiple solutions that more\nthoroughly sampled the hypothesis space. The Bayesian ResNet ensemble was the\nonly one that performed worse than the deterministic ResNet at longer lead\ntimes, likely due to the challenge of optimizing a larger number of parameters.\nTo address this issue, the Bayesian-MOPED (MOdel Priors with Empirical Bayes\nusing Deep neural network) ResNet ensemble was adopted, and it enhanced\nforecast skill by constraining the hypothesis search near the deterministic\nResNet hypothesis. All Bayesian methods demonstrated well-calibrated\nuncertainty and effectively separated cases with large and small errors. In\ncase studies, the initial-weights ensemble + MC dropout demonstrated better\nforecast skill than the Bayesian-MOPED ensemble and the deterministic ResNet on\nselected CI events in clear-sky regions. However, the initial-weights ensemble\n+ MC dropout exhibited poorer generalization in clear-sky and anvil cloud\nregions without CI occurrence compared to the deterministic ResNet and\nBayesian-MOPED ensemble.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet",
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A comparison of stretched-grid and limited-area modelling for data-driven regional weather forecasting",
    "url": "http://arxiv.org/abs/2507.18378v1",
    "authors": [
      "Jasper S. Wijnands",
      "Michiel Van Ginderachter",
      "Bastien Fran\u00e7ois",
      "Sophie Buurman",
      "Piet Termonia",
      "Dieter Van den Bleeken"
    ],
    "published": "2025-07-24",
    "abstract": "Regional machine learning weather prediction (MLWP) models based on graph\nneural networks have recently demonstrated remarkable predictive accuracy,\noutperforming numerical weather prediction models at lower computational costs.\nIn particular, limited-area model (LAM) and stretched-grid model (SGM)\napproaches have emerged for generating high-resolution regional forecasts,\nbased on initial conditions from a regional (re)analysis. While LAM uses\nlateral boundaries from an external global model, SGM incorporates a global\ndomain at lower resolution. This study aims to understand how the differences\nin model design impact relative performance and potential applications.\nSpecifically, the strengths and weaknesses of these two approaches are\nidentified for generating deterministic regional forecasts over Europe. Using\nthe Anemoi framework, models of both types are built by minimally adapting a\nshared architecture and trained using global and regional reanalyses in a\nnear-identical setup. Several inference experiments have been conducted to\nexplore their relative performance and highlight key differences. Results show\nthat both LAM and SGM are competitive deterministic MLWP models with generally\naccurate and comparable forecasting performance over the regional domain.\nVarious differences were identified in the performance of the models across\napplications. LAM is able to successfully exploit high-quality boundary\nforcings to make predictions within the regional domain and is suitable in\ncontexts where global data is difficult to acquire. SGM is fully self-contained\nfor easier operationalisation, can take advantage of more training data and\nsignificantly surpasses LAM in terms of (temporal) generalisability. Our paper\ncan serve as a starting point for meteorological institutes to guide their\nchoice between LAM and SGM in developing an operational data-driven forecasting\nsystem.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "EarthLink: A Self-Evolving AI Agent for Climate Science",
    "url": "http://arxiv.org/abs/2507.17311v2",
    "authors": [
      "Zijie Guo",
      "Jiong Wang",
      "Xiaoyu Yue",
      "Wangxu Wei",
      "Zhe Jiang",
      "Wanghan Xu",
      "Ben Fei",
      "Wenlong Zhang",
      "Xinyu Gu",
      "Lijing Cheng",
      "Jing-Jia Luo",
      "Chao Li",
      "Yaqiang Wang",
      "Tao Chen",
      "Wanli Ouyang",
      "Fenghua Ling",
      "Lei Bai"
    ],
    "published": "2025-07-23",
    "abstract": "Modern Earth science is at an inflection point. The vast, fragmented, and\ncomplex nature of Earth system data, coupled with increasingly sophisticated\nanalytical demands, creates a significant bottleneck for rapid scientific\ndiscovery. Here we introduce EarthLink, the first AI agent designed as an\ninteractive copilot for Earth scientists. It automates the end-to-end research\nworkflow, from planning and code generation to multi-scenario analysis. Unlike\nstatic diagnostic tools, EarthLink can learn from user interaction,\ncontinuously refining its capabilities through a dynamic feedback loop. We\nvalidated its performance on a number of core scientific tasks of climate\nchange, ranging from model-observation comparisons to the diagnosis of complex\nphenomena. In a multi-expert evaluation, EarthLink produced scientifically\nsound analyses and demonstrated an analytical competency that was rated as\ncomparable to specific aspects of a human junior researcher's workflow.\nAdditionally, its transparent, auditable workflows and natural language\ninterface empower scientists to shift from laborious manual execution to\nstrategic oversight and hypothesis generation. EarthLink marks a pivotal step\ntowards an efficient, trustworthy, and collaborative paradigm for Earth system\nresearch in an era of accelerating global change. The system is accessible at\nour website https://earthlink.intern-ai.org.cn.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model",
    "url": "http://arxiv.org/abs/2507.22675v1",
    "authors": [
      "Meiqi Hu",
      "Lingzhi Lu",
      "Chengxi Han",
      "Xiaoping Liu"
    ],
    "published": "2025-07-30",
    "abstract": "Recently, large foundation models trained on vast datasets have demonstrated\nexceptional capabilities in feature extraction and general feature\nrepresentation. The ongoing advancements in deep learning-driven large models\nhave shown great promise in accelerating unsupervised change detection methods,\nthereby enhancing the practical applicability of change detection technologies.\nBuilding on this progress, this paper introduces MergeSAM, an innovative\nunsupervised change detection method for high-resolution remote sensing\nimagery, based on the Segment Anything Model (SAM). Two novel strategies,\nMaskMatching and MaskSplitting, are designed to address real-world complexities\nsuch as object splitting, merging, and other intricate changes. The proposed\nmethod fully leverages SAM's object segmentation capabilities to construct\nmultitemporal masks that capture complex changes, embedding the spatial\nstructure of land cover into the change detection process.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning",
    "url": "http://arxiv.org/abs/2507.20776v1",
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Yingchao Feng",
      "Kaiwen Wei",
      "Wenxin Yin",
      "Wenhui Diao",
      "Mengyu Wang",
      "Hanbo Bi",
      "Kaiyue Kang",
      "Tong Ling",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2025-07-28",
    "abstract": "Remote sensing (RS) images from multiple modalities and platforms exhibit\ndiverse details due to differences in sensor characteristics and imaging\nperspectives. Existing vision-language research in RS largely relies on\nrelatively homogeneous data sources. Moreover, they still remain limited to\nconventional visual perception tasks such as classification or captioning. As a\nresult, these methods fail to serve as a unified and standalone framework\ncapable of effectively handling RS imagery from diverse sources in real-world\napplications. To address these issues, we propose RingMo-Agent, a model\ndesigned to handle multi-modal and multi-platform data that performs perception\nand reasoning tasks based on user textual instructions. Compared with existing\nmodels, RingMo-Agent 1) is supported by a large-scale vision-language dataset\nnamed RS-VL3M, comprising over 3 million image-text pairs, spanning optical,\nSAR, and infrared (IR) modalities collected from both satellite and UAV\nplatforms, covering perception and challenging reasoning tasks; 2) learns\nmodality adaptive representations by incorporating separated embedding layers\nto construct isolated features for heterogeneous modalities and reduce\ncross-modal interference; 3) unifies task modeling by introducing task-specific\ntokens and employing a token-based high-dimensional hidden state decoding\nmechanism designed for long-horizon spatial tasks. Extensive experiments on\nvarious RS vision-language tasks demonstrate that RingMo-Agent not only proves\neffective in both visual understanding and sophisticated analytical tasks, but\nalso exhibits strong generalizability across different platforms and sensing\nmodalities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data",
    "url": "http://arxiv.org/abs/2507.22291v1",
    "authors": [
      "Christopher F. Brown",
      "Michal R. Kazmierski",
      "Valerie J. Pasquarella",
      "William J. Rucklidge",
      "Masha Samsikova",
      "Chenhui Zhang",
      "Evan Shelhamer",
      "Estefania Lahera",
      "Olivia Wiles",
      "Simon Ilyushchenko",
      "Noel Gorelick",
      "Lihui Lydia Zhang",
      "Sophia Alj",
      "Emily Schechter",
      "Sean Askay",
      "Oliver Guinan",
      "Rebecca Moore",
      "Alexis Boukouvalas",
      "Pushmeet Kohli"
    ],
    "published": "2025-07-29",
    "abstract": "Unprecedented volumes of Earth observation data are continually collected\naround the world, but high-quality labels remain scarce given the effort\nrequired to make physical measurements and observations. This has led to\nconsiderable investment in bespoke modeling efforts translating sparse labels\ninto maps. Here we introduce AlphaEarth Foundations, an embedding field model\nyielding a highly general, geospatial representation that assimilates spatial,\ntemporal, and measurement contexts across multiple sources, enabling accurate\nand efficient production of maps and monitoring systems from local to global\nscales. The embeddings generated by AlphaEarth Foundations are the only to\nconsistently outperform all previous featurization approaches tested on a\ndiverse set of mapping evaluations without re-training. We will release a\ndataset of global, annual, analysis-ready embedding field layers from 2017\nthrough 2024.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SCANet: Split Coordinate Attention Network for Building Footprint Extraction",
    "url": "http://arxiv.org/abs/2507.20809v1",
    "authors": [
      "Chunshi Wang",
      "Bin Zhao",
      "Shuxue Ding"
    ],
    "published": "2025-07-28",
    "abstract": "Building footprint extraction holds immense significance in remote sensing\nimage analysis and has great value in urban planning, land use, environmental\nprotection and disaster assessment. Despite the progress made by conventional\nand deep learning approaches in this field, they continue to encounter\nsignificant challenges. This paper introduces a novel plug-and-play attention\nmodule, Split Coordinate Attention (SCA), which ingeniously captures spatially\nremote interactions by employing two spatial range of pooling kernels,\nstrategically encoding each channel along x and y planes, and separately\nperforms a series of split operations for each feature group, thus enabling\nmore efficient semantic feature extraction. By inserting into a 2D CNN to form\nan effective SCANet, our SCANet outperforms recent SOTA methods on the public\nWuhan University (WHU) Building Dataset and Massachusetts Building Dataset in\nterms of various metrics. Particularly SCANet achieves the best IoU, 91.61% and\n75.49% for the two datasets. Our code is available at\nhttps://github.com/AiEson/SCANet",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
=======
    "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL",
    "url": "http://arxiv.org/abs/2510.21045v2",
    "authors": [
      "Ali Khosravi Kazazi",
      "Zhenlong Li",
      "M. Naser Lessani",
      "Guido Cervone"
    ],
    "published": "2025-10-23",
    "abstract": "The complexity of Structured Query Language (SQL) and the specialized nature of geospatial functions in tools like PostGIS present significant barriers to non-experts seeking to analyze spatial data. While Large Language Models (LLMs) offer promise for translating natural language into SQL (Text-to-SQL), single-agent approaches often struggle with the semantic and syntactic complexities of spatial queries. To address this, we propose a multi-agent framework designed to accurately translate natural language questions into spatial SQL queries. The framework integrates several innovative components, including a knowledge base with programmatic schema profiling and semantic enrichment, embeddings for context retrieval, and a collaborative multi-agent pipeline as its core. This pipeline comprises specialized agents for entity extraction, metadata retrieval, query logic formulation, SQL generation, and a review agent that performs programmatic and semantic validation of the generated SQL to ensure correctness (self-verification). We evaluate our system using both the non-spatial KaggleDBQA benchmark and a new, comprehensive SpatialQueryQA benchmark that includes diverse geometry types, predicates, and three levels of query complexity. On KaggleDBQA, the system achieved an overall accuracy of 81.2% (221 out of 272 questions) after the review agent's review and corrections. For spatial queries, the system achieved an overall accuracy of 87.7% (79 out of 90 questions), compared with 76.7% without the review agent. Beyond accuracy, results also show that in some instances the system generates queries that are more semantically aligned with user intent than those in the benchmarks. This work makes spatial analysis more accessible, and provides a robust, generalizable foundation for spatial Text-to-SQL systems, advancing the development of autonomous GIS.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit",
    "url": "http://arxiv.org/abs/2507.20623v1",
    "authors": [
      "Yang Zhao",
      "Shusheng Li",
      "Xueshang Feng"
    ],
    "published": "2025-07-28",
    "abstract": "As the development of lightweight deep learning algorithms, various deep\nneural network (DNN) models have been proposed for the remote sensing scene\nclassification (RSSC) application. However, it is still challenging for these\nRSSC models to achieve optimal performance among model accuracy, inference\nlatency, and energy consumption on resource-constrained edge devices. In this\npaper, we propose a lightweight RSSC framework, which includes a distilled\nglobal filter network (GFNet) model and an early-exit mechanism designed for\nedge devices to achieve state-of-the-art performance. Specifically, we first\napply frequency domain distillation on the GFNet model to reduce model size.\nThen we design a dynamic early-exit model tailored for DNN models on edge\ndevices to further improve model inference efficiency. We evaluate our E3C\nmodel on three edge devices across four datasets. Extensive experimental\nresults show that it achieves an average of 1.3x speedup on model inference and\nover 40% improvement on energy efficiency, while maintaining high\nclassification accuracy.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations",
    "url": "http://arxiv.org/abs/2507.23154v1",
    "authors": [
      "Sofiane Bouaziz",
      "Adel Hafiane",
      "Raphael Canals",
      "Rachid Nedjai"
    ],
    "published": "2025-07-30",
    "abstract": "Urban heatwaves, droughts, and land degradation are pressing and growing\nchallenges in the context of climate change. A valuable approach to studying\nthem requires accurate spatio-temporal information on land surface conditions.\nOne of the most important variables for assessing and understanding these\nphenomena is Land Surface Temperature (LST), which is derived from satellites\nand provides essential information about the thermal state of the Earth's\nsurface. However, satellite platforms inherently face a trade-off between\nspatial and temporal resolutions. To bridge this gap, we propose FuseTen, a\nnovel generative framework that produces daily LST observations at a fine 10 m\nspatial resolution by fusing spatio-temporal observations derived from\nSentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative\narchitecture trained using an averaging-based supervision strategy grounded in\nphysical principles. It incorporates attention and normalization modules within\nthe fusion process and uses a PatchGAN discriminator to enforce realism.\nExperiments across multiple dates show that FuseTen outperforms linear\nbaselines, with an average 32.06% improvement in quantitative metrics and\n31.42% in visual fidelity. To the best of our knowledge, this is the first\nnon-linear method to generate daily LST estimates at such fine spatial\nresolution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SpecBPP: A Self-Supervised Learning Approach for Hyperspectral Representation and Soil Organic Carbon Estimation",
    "url": "http://arxiv.org/abs/2507.19781v1",
    "authors": [
      "Daniel La'ah Ayuba",
      "Jean-Yves Guillemaut",
      "Belen Marti-Cardona",
      "Oscar Mendez Maldonado"
    ],
    "published": "2025-07-26",
    "abstract": "Self-supervised learning has revolutionized representation learning in vision\nand language, but remains underexplored for hyperspectral imagery (HSI), where\nthe sequential structure of spectral bands offers unique opportunities. In this\nwork, we propose Spectral Band Permutation Prediction (SpecBPP), a novel\nself-supervised learning framework that leverages the inherent spectral\ncontinuity in HSI. Instead of reconstructing masked bands, SpecBPP challenges a\nmodel to recover the correct order of shuffled spectral segments, encouraging\nglobal spectral understanding. We implement a curriculum-based training\nstrategy that progressively increases permutation difficulty to manage the\nfactorial complexity of the permutation space. Applied to Soil Organic Carbon\n(SOC) estimation using EnMAP satellite data, our method achieves\nstate-of-the-art results, outperforming both masked autoencoder (MAE) and\njoint-embedding predictive (JEPA) baselines. Fine-tuned on limited labeled\nsamples, our model yields an $R^2$ of 0.9456, RMSE of 1.1053%, and RPD of 4.19,\nsignificantly surpassing traditional and self-supervised benchmarks. Our\nresults demonstrate that spectral order prediction is a powerful pretext task\nfor hyperspectral understanding, opening new avenues for scientific\nrepresentation learning in remote sensing and beyond.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "JEPA",
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Discrete Gaussian Vector Fields On Meshes",
    "url": "http://arxiv.org/abs/2507.20024v1",
    "authors": [
      "Michael Gillan",
      "Stefan Siegert",
      "Ben Youngman"
    ],
    "published": "2025-07-26",
    "abstract": "Though the underlying fields associated with vector-valued environmental data\nare continuous, observations themselves are discrete. For example, climate\nmodels typically output grid-based representations of wind fields or ocean\ncurrents, and these are often downscaled to a discrete set of points. By\ntreating the area of interest as a two-dimensional manifold that can be\nrepresented as a triangular mesh and embedded in Euclidean space, this work\nshows that discrete intrinsic Gaussian processes for vector-valued data can be\ndeveloped from discrete differential operators defined with respect to a mesh.\nThese Gaussian processes account for the geometry and curvature of the manifold\nwhilst also providing a flexible and practical formulation that can be readily\napplied to any two-dimensional mesh. We show that these models can capture\nharmonic flows, incorporate boundary conditions, and model non-stationary data.\nFinally, we apply these models to downscaling stationary and non-stationary\ngridded wind data on the globe, and to inference of ocean currents from sparse\nobservations in bounded domains.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Machine Learning Framework for Predicting Microphysical Properties of Ice Crystals from Cloud Particle Imagery",
    "url": "http://arxiv.org/abs/2507.19759v1",
    "authors": [
      "Joseph Ko",
      "Jerry Harrington",
      "Kara Sulia",
      "Vanessa Przybylo",
      "Marcus van Lier-Walqui",
      "Kara Lamb"
    ],
    "published": "2025-07-26",
    "abstract": "The microphysical properties of ice crystals are important because they\nsignificantly alter the radiative properties and spatiotemporal distributions\nof clouds, which in turn strongly affect Earth's climate. However, it is\nchallenging to measure key properties of ice crystals, such as mass or\nmorphological features. Here, we present a framework for predicting\nthree-dimensional (3D) microphysical properties of ice crystals from in situ\ntwo-dimensional (2D) imagery. First, we computationally generate synthetic ice\ncrystals using 3D modeling software along with geometric parameters estimated\nfrom the 2021 Ice Cryo-Encapsulation Balloon (ICEBall) field campaign. Then, we\nuse synthetic crystals to train machine learning (ML) models to predict\neffective density ($\\rho_{e}$), effective surface area ($A_e$), and number of\nbullets ($N_b$) from synthetic rosette imagery. When tested on unseen synthetic\nimages, we find that our ML models can predict microphysical properties with\nhigh accuracy. For $\\rho_{e}$ and $A_e$, respectively, our best-performing\nsingle view models achieved $R^2$ values of 0.99 and 0.98. For $N_b$, our best\nsingle view model achieved a balanced accuracy and F1 score of 0.91. We also\nquantify the marginal prediction improvements from incorporating a second view.\nA stereo view ResNet-18 model reduced RMSE by 40% for both $\\rho_e$ and $A_e$,\nrelative to a single view ResNet-18 model. For $N_b$, we find that a stereo\nview ResNet-18 model improved the F1 score by 8%. This work provides a novel\nML-driven framework for estimating ice microphysical properties from in situ\nimagery, which will allow for downstream constraints on microphysical\nparameterizations, such as the mass-size relationship.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Integrating Activity Predictions in Knowledge Graphs",
    "url": "http://arxiv.org/abs/2507.19733v1",
    "authors": [
      "Alec Sculley",
      "Cameron Stockton",
      "Forrest Hare"
    ],
    "published": "2025-07-26",
    "abstract": "We argue that ontology-structured knowledge graphs can play a crucial role in\ngenerating predictions about future events. By leveraging the semantic\nframework provided by Basic Formal Ontology (BFO) and Common Core Ontologies\n(CCO), we demonstrate how data such as the movements of a fishing vessel can be\norganized in and retrieved from a knowledge graph. These query results are then\nused to create Markov chain models, allowing us to predict future states based\non the vessel's history. To fully support this process, we introduce the term\n`spatiotemporal instant' to complete the necessary structural semantics.\nAdditionally, we critique the prevailing ontological model of probability,\nwhich conflates probability with likelihood and relies on the problematic\nconcept of modal measurements: measurements of future entities. We propose an\nalternative view, where probabilities are treated as being about process\nprofiles, which better captures the dynamics of real world phenomena. Finally,\nwe demonstrate how our Markov chain based probability calculations can be\nseamlessly integrated back into the knowledge graph, enabling further analysis\nand decision-making. Keywords: predictive analytics, ontology, Markov chains,\nprobability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks",
    "url": "http://arxiv.org/abs/2508.03566v1",
    "authors": [
      "Xinyu Xiong",
      "Zihuang Wu",
      "Lei Zhang",
      "Lei Lu",
      "Ming Li",
      "Guanbin Li"
    ],
    "published": "2025-08-05",
    "abstract": "Recent studies have highlighted the potential of adapting the Segment\nAnything Model (SAM) for various downstream tasks. However, constructing a more\npowerful and generalizable encoder to further enhance performance remains an\nopen challenge. In this work, we propose SAM2-UNeXT, an advanced framework that\nbuilds upon the core principles of SAM2-UNet while extending the\nrepresentational capacity of SAM2 through the integration of an auxiliary\nDINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue\nlayer, our approach enables more accurate segmentation with a simple\narchitecture, relaxing the need for complex decoder designs. Extensive\nexperiments conducted on four benchmarks, including dichotomous image\nsegmentation, camouflaged object detection, marine animal segmentation, and\nremote sensing saliency detection, demonstrate the superior performance of our\nproposed method. The code is available at\nhttps://github.com/WZH0120/SAM2-UNeXT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models",
    "url": "http://arxiv.org/abs/2508.01731v1",
    "authors": [
      "Yuxiang Zhang",
      "Wei Li",
      "Mengmeng Zhang",
      "Jiawei Han",
      "Ran Tao",
      "Shunlin Liang"
    ],
    "published": "2025-08-03",
    "abstract": "Recent advances in Remote Sensing Foundation Models (RSFMs) have led to\nsignificant breakthroughs in the field. While many RSFMs have been pretrained\nwith massive optical imagery, more multispectral/hyperspectral data remain lack\nof the corresponding foundation models. To leverage the advantages of spectral\nimagery in earth observation, we explore whether existing RSFMs can be\neffectively adapted to process diverse spectral modalities without requiring\nextensive spectral pretraining. In response to this challenge, we proposed\nSpectralX, an innovative parameter-efficient fine-tuning framework that adapt\nexisting RSFMs as backbone while introducing a two-stage training approach to\nhandle various spectral inputs, thereby significantly improving domain\ngeneralization performance. In the first stage, we employ a\nmasked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) to\nextract attribute tokens from both spatial and spectral dimensions.\nSimultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA)\nthat dynamically aggregates multi-attribute expert knowledge while performing\nlayer-wise fine-tuning. With semantic segmentation as downstream task in the\nsecond stage, we insert an Attribute-refined Adapter (Are-adapter) into the\nfirst stage framework. By iteratively querying low-level semantic features with\nhigh-level representations, the model learns to focus on task-beneficial\nattributes, enabling customized adjustment of RSFMs. Following this two-phase\nadaptation process, SpectralX is capable of interpreting spectral imagery from\nnew regions or seasons. The codes will be available from the website:\nhttps://github.com/YuxiangZhang-BIT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.05271v1",
    "authors": [
      "Xiaoyang Zhang",
      "Guodong Fan",
      "Guang-Yong Chen",
      "Zhen Hua",
      "Jinjiang Li",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "published": "2025-08-07",
    "abstract": "Change detection in remote sensing imagery plays a vital role in various\nengineering applications, such as natural disaster monitoring, urban expansion\ntracking, and infrastructure management. Despite the remarkable progress of\ndeep learning in recent years, most existing methods still rely on\nspatial-domain modeling, where the limited diversity of feature representations\nhinders the detection of subtle change regions. We observe that\nfrequency-domain feature modeling particularly in the wavelet domain an amplify\nfine-grained differences in frequency components, enhancing the perception of\nedge changes that are challenging to capture in the spatial domain. Thus, we\npropose a method called Wavelet-Guided Dual-Frequency Encoding (WGDF).\nSpecifically, we first apply Discrete Wavelet Transform (DWT) to decompose the\ninput images into high-frequency and low-frequency components, which are used\nto model local details and global structures, respectively. In the\nhigh-frequency branch, we design a Dual-Frequency Feature Enhancement (DFFE)\nmodule to strengthen edge detail representation and introduce a\nFrequency-Domain Interactive Difference (FDID) module to enhance the modeling\nof fine-grained changes. In the low-frequency branch, we exploit Transformers\nto capture global semantic relationships and employ a Progressive Contextual\nDifference Module (PCDM) to progressively refine change regions, enabling\nprecise structural semantic characterization. Finally, the high- and\nlow-frequency features are synergistically fused to unify local sensitivity\nwith global discriminability. Extensive experiments on multiple remote sensing\ndatasets demonstrate that WGDF significantly alleviates edge ambiguity and\nachieves superior detection accuracy and robustness compared to\nstate-of-the-art methods. The code will be available at\nhttps://github.com/boshizhang123/WGDF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Deep learning framework for crater detection and identification on the Moon and Mars",
    "url": "http://arxiv.org/abs/2508.03920v1",
    "authors": [
      "Yihan Ma",
      "Zeyang Yu",
      "Rohitash Chandra"
    ],
    "published": "2025-08-05",
    "abstract": "Impact craters are among the most prominent geomorphological features on\nplanetary surfaces and are of substantial significance in planetary science\nresearch. Their spatial distribution and morphological characteristics provide\ncritical information on planetary surface composition, geological history, and\nimpact processes. In recent years, the rapid advancement of deep learning\nmodels has fostered significant interest in automated crater detection. In this\npaper, we apply advancements in deep learning models for impact crater\ndetection and identification. We use novel models, including Convolutional\nNeural Networks (CNNs) and variants such as YOLO and ResNet. We present a\nframework that features a two-stage approach where the first stage features\ncrater identification using simple classic CNN, ResNet-50 and YOLO. In the\nsecond stage, our framework employs YOLO-based detection for crater\nlocalisation. Therefore, we detect and identify different types of craters and\npresent a summary report with remote sensing data for a selected region. We\nconsider selected regions for craters and identification from Mars and the Moon\nbased on remote sensing data. Our results indicate that YOLO demonstrates the\nmost balanced crater detection performance, while ResNet-50 excels in\nidentifying large craters with high precision.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "ResNet"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling",
    "url": "http://arxiv.org/abs/2508.03774v1",
    "authors": [
      "Rui Zhu",
      "Yuexing Peng",
      "Peng Wang",
      "George C. Alexandropoulos",
      "Wenbo Wang",
      "Wei Xiang"
    ],
    "published": "2025-08-05",
    "abstract": "Electromagnetic (EM) scattering modeling is critical for radar remote\nsensing, however, its inherent complexity introduces significant computational\nchallenges. Traditional numerical solvers offer high accuracy, but suffer from\nscalability issues and substantial computational costs. Pure data-driven deep\nlearning approaches, while efficient, lack physical constraints embedding\nduring training and require extensive labeled data, limiting their\napplicability and generalization. To overcome these limitations, we propose a\nU-shaped Physics-Informed Network (U-PINet), the first fully\ndeep-learning-based, physics-informed hierarchical framework for computational\nEM designed to ensure physical consistency while maximizing computational\nefficiency. Motivated by the hierarchical decomposition strategy in EM solvers\nand the inherent sparsity of local EM coupling, the U-PINet models the\ndecomposition and coupling of near- and far-field interactions through a\nmultiscale processing neural network architecture, while employing a\nphysics-inspired sparse graph representation to efficiently model both self-\nand mutual- coupling among mesh elements of complex $3$-Dimensional (3D)\nobjects. This principled approach enables end-to-end multiscale EM scattering\nmodeling with improved efficiency, generalization, and physical consistency.\nExperimental results showcase that the U-PINet accurately predicts surface\ncurrent distributions, achieving close agreement with traditional solver, while\nsignificantly reducing computational time and outperforming conventional deep\nlearning baselines in both accuracy and robustness. Furthermore, our\nevaluations on radar cross section prediction tasks confirm the feasibility of\nthe U-PINet for downstream EM scattering applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MGCR-Net:Multimodal Graph-Conditioned Vision-Language Reconstruction Network for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.01555v1",
    "authors": [
      "Chengming Wang",
      "Guodong Fan",
      "Jinjiang Li",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "published": "2025-08-03",
    "abstract": "With the advancement of remote sensing satellite technology and the rapid\nprogress of deep learning, remote sensing change detection (RSCD) has become a\nkey technique for regional monitoring. Traditional change detection (CD)\nmethods and deep learning-based approaches have made significant contributions\nto change analysis and detection, however, many outstanding methods still face\nlimitations in the exploration and application of multimodal data. To address\nthis, we propose the multimodal graph-conditioned vision-language\nreconstruction network (MGCR-Net) to further explore the semantic interaction\ncapabilities of multimodal data. Multimodal large language models (MLLM) have\nattracted widespread attention for their outstanding performance in computer\nvision, particularly due to their powerful visual-language understanding and\ndialogic interaction capabilities. Specifically, we design a MLLM-based\noptimization strategy to generate multimodal textual data from the original CD\nimages, which serve as textual input to MGCR. Visual and textual features are\nextracted through a dual encoder framework. For the first time in the RSCD\ntask, we introduce a multimodal graph-conditioned vision-language\nreconstruction mechanism, which is integrated with graph attention to construct\na semantic graph-conditioned reconstruction module (SGCM), this module\ngenerates vision-language (VL) tokens through graph-based conditions and\nenables cross-dimensional interaction between visual and textual features via\nmultihead attention. The reconstructed VL features are then deeply fused using\nthe language vision transformer (LViT), achieving fine-grained feature\nalignment and high-level semantic interaction. Experimental results on four\npublic datasets demonstrate that MGCR achieves superior performance compared to\nmainstream CD methods. Our code is available on\nhttps://github.com/cn-xvkong/MGCR",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
=======
    "title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response",
    "url": "http://arxiv.org/abs/2510.12061v1",
    "authors": [
      "Yiheng Chen",
      "Lingyao Li",
      "Zihui Ma",
      "Qikai Hu",
      "Yilun Zhu",
      "Min Deng",
      "Runlong Yu"
    ],
    "published": "2025-10-14",
    "abstract": "Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "LLM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
<<<<<<< HEAD
    "title": "CGCCE-Net:Change-Guided Cross Correlation Enhancement Network for Remote Sensing Building Change Detection",
    "url": "http://arxiv.org/abs/2508.01549v1",
    "authors": [
      "ChengMing Wang"
    ],
    "published": "2025-08-03",
    "abstract": "Change detection encompasses a variety of task types, and the goal of\nbuilding change detection (BCD) tasks is to accurately locate buildings and\ndistinguish changed building areas. In recent years, various deep\nlearning-based BCD methods have achieved significant success in detecting\ndifference regions by using different change information enhancement\ntechniques, effectively improving the precision of BCD tasks. To address the\nissue of BCD with special colors, we propose the change-guided cross\ncorrelation enhancement network (CGCCE-Net). We design the change-guided\nresidual refinement (CGRR) Branch, which focuses on extending shallow texture\nfeatures to multiple scale features obtained from PVT, enabling early attention\nand acquisition of special colors. Then, channel spatial attention is used in\nthe deep features to achieve independent information enhancement. Additionally,\nwe construct the global cross correlation module (GCCM) to facilitate semantic\ninformation interaction between bi-temporal images, establishing building and\ntarget recognition relationships between different images. Further semantic\nfeature enhancement is achieved through the semantic cognitive enhancement\nmodule (SCEM), and finally, the cross fusion decoder (CFD) is used for change\ninformation fusion and image reconstruction. Extensive experiments on three\npublic datasets demonstrate that our CGCCE-Net outperforms mainstream BCD\nmethods with outstanding performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Recognition"
    ]
  },
  {
    "title": "Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets",
    "url": "http://arxiv.org/abs/2508.02871v1",
    "authors": [
      "J. Alex Hurt",
      "Trevor M. Bajkowski",
      "Grant J. Scott",
      "Curt H. Davis"
    ],
    "published": "2025-08-04",
    "abstract": "In 2012, AlexNet established deep convolutional neural networks (DCNNs) as\nthe state-of-the-art in CV, as these networks soon led in visual tasks for many\ndomains, including remote sensing. With the publication of Visual Transformers,\nwe are witnessing the second modern leap in computational vision, and as such,\nit is imperative to understand how various transformer-based neural networks\nperform on satellite imagery. While transformers have shown high levels of\nperformance in natural language processing and CV applications, they have yet\nto be compared on a large scale to modern remote sensing data. In this paper,\nwe explore the use of transformer-based neural networks for object detection in\nhigh-resolution electro-optical satellite imagery, demonstrating\nstate-of-the-art performance on a variety of publicly available benchmark data\nsets. We compare eleven distinct bounding-box detection and localization\nalgorithms in this study, of which seven were published since 2020, and all\neleven since 2015. The performance of five transformer-based architectures is\ncompared with six convolutional networks on three state-of-the-art opensource\nhigh-resolution remote sensing imagery datasets ranging in size and complexity.\nFollowing the training and evaluation of thirty-three deep neural models, we\nthen discuss and analyze model performance across various feature extraction\nmethodologies and detection algorithms.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2508.01941v1",
    "authors": [
      "Andrea Dosi",
      "Semanto Mondal",
      "Rajib Chandra Ghosh",
      "Massimo Brescia",
      "Giuseppe Longo"
    ],
    "published": "2025-08-03",
    "abstract": "This work presents the results of a methodological transfer from remote\nsensing to healthcare, adapting AMBER -- a transformer-based model originally\ndesigned for multiband images, such as hyperspectral data -- to the task of 3D\nmedical datacube segmentation. In this study, we use the AMBER architecture\nwith Adaptive Fourier Neural Operators (AFNO) in place of the multi-head\nself-attention mechanism. While existing models rely on various forms of\nattention to capture global context, AMBER-AFNO achieves this through\nfrequency-domain mixing, enabling a drastic reduction in model complexity. This\ndesign reduces the number of trainable parameters by over 80% compared to\nUNETR++, while maintaining a FLOPs count comparable to other state-of-the-art\narchitectures. Model performance is evaluated on two benchmark 3D medical\ndatasets -- ACDC and Synapse -- using standard metrics such as Dice Similarity\nCoefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO\nachieves competitive or superior accuracy with significant gains in training\nefficiency, inference speed, and memory usage.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Bridging ocean wave physics and deep learning: Physics-informed neural operators for nonlinear wavefield reconstruction in real-time",
    "url": "http://arxiv.org/abs/2508.03315v1",
    "authors": [
      "Svenja Ehlers",
      "Merten Stender",
      "Norbert Hoffmann"
    ],
    "published": "2025-08-05",
    "abstract": "Accurate real-time prediction of phase-resolved ocean wave fields remains a\ncritical yet largely unsolved problem, primarily due to the absence of\npractical data assimilation methods for reconstructing initial conditions from\nsparse or indirect wave measurements. While recent advances in supervised deep\nlearning have shown potential for this purpose, they require large labelled\ndatasets of ground truth wave data, which are infeasible to obtain in\nreal-world scenarios. To overcome this limitation, we propose a\nPhysics-Informed Neural Operator (PINO) framework for reconstructing spatially\nand temporally phase-resolved, nonlinear ocean wave fields from sparse\nmeasurements, without the need for ground truth data during training. This is\nachieved by embedding residuals of the free surface boundary conditions of\nocean gravity waves into the loss function of the PINO, constraining the\nsolution space in a soft manner. After training, we validate our approach using\nhighly realistic synthetic wave data and demonstrate the accurate\nreconstruction of nonlinear wave fields from both buoy time series and radar\nsnapshots. Our results indicate that PINOs enable accurate, real-time\nreconstruction and generalize robustly across a wide range of wave conditions,\nthereby paving the way for operational, data-driven wave reconstruction and\nprediction in realistic marine environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models",
    "url": "http://arxiv.org/abs/2508.05260v1",
    "authors": [
      "Zhouyao Qian",
      "Yang Chen",
      "Baodian Li",
      "Shuyi Zhang",
      "Zhen Tian",
      "Gongsen Wang",
      "Tianyue Gu",
      "Xinyu Zhou",
      "Huilin Chen",
      "Xinyi Li",
      "Hao Zhu",
      "Shuyao Zhang",
      "Zongheng Li",
      "Siyuan Wang"
    ],
    "published": "2025-08-07",
    "abstract": "Marine chlorophyll concentration is an important indicator of ecosystem\nhealth and carbon cycle strength, and its accurate prediction is crucial for\nred tide warning and ecological response. In this paper, we propose a LSTM-RF\nhybrid model that combines the advantages of LSTM and RF, which solves the\ndeficiencies of a single model in time-series modelling and nonlinear feature\nportrayal. Trained with multi-source ocean data(temperature, salinity,\ndissolved oxygen, etc.), the experimental results show that the LSTM-RF model\nhas an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test\nset, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2\n=0.4934) alone , respectively. The standardised treatment and sliding window\napproach improved the prediction accuracy of the model and provided an\ninnovative solution for high-frequency prediction of marine ecological\nvariables.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Operational convection-permitting COSMO/ICON ensemble predictions at observation sites (CIENS)",
    "url": "http://arxiv.org/abs/2508.03845v1",
    "authors": [
      "Sebastian Lerch",
      "Benedikt Schulz",
      "Reinhold Hess",
      "Annette M\u00f6ller",
      "Cristina Primo",
      "Sebastian Trepte",
      "Susanne Theis"
    ],
    "published": "2025-08-05",
    "abstract": "We present the CIENS dataset, which contains ensemble weather forecasts from\nthe operational convection-permitting numerical weather prediction model of the\nGerman Weather Service. It comprises forecasts for 55 meteorological variables\nmapped to the locations of synoptic stations, as well as additional spatially\naggregated forecasts from surrounding grid points, available for a subset of\nthese variables. Forecasts are available at hourly lead times from 0 to 21\nhours for two daily model runs initialized at 00 and 12 UTC, covering the\nperiod from December 2010 to June 2023. Additionally, the dataset provides\nstation observations for six key variables at 170 locations across Germany:\npressure, temperature, hourly precipitation accumulation, wind speed, wind\ndirection, and wind gusts. Since the forecast are mapped to the observed\nlocations, the data is delivered in a convenient format for analysis. The CIENS\ndataset complements the growing collection of benchmark datasets for weather\nand climate modeling. A key distinguishing feature is its long temporal extent,\nwhich encompasses multiple updates to the underlying numerical weather\nprediction model and thus supports investigations into how forecasting methods\ncan account for such changes. In addition to detailing the design and contents\nof the CIENS dataset, we outline potential applications in ensemble\npost-processing, forecast verification, and related research areas. A use case\nfocused on ensemble post-processing illustrates the benefits of incorporating\nthe rich set of available model predictors into machine learning-based\nforecasting models.",
    "categories": [
      "ocean"
=======
    "title": "Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany",
    "url": "http://arxiv.org/abs/2510.10640v1",
    "authors": [
      "Piyush Pant",
      "Marcellius William Suntoro",
      "Ayesha Siddiqua",
      "Muhammad Shehryaar Sharif",
      "Daniyal Ahmed"
    ],
    "published": "2025-10-12",
    "abstract": "This paper presents EA-GeoAI, an integrated framework for demand forecasting and equitable hospital planning in Germany through 2030. We combine district-level demographic shifts, aging population density, and infrastructure balances into a unified Equity Index. An interpretable Agentic AI optimizer then allocates beds and identifies new facility sites to minimize unmet need under budget and travel-time constraints. This approach bridges GeoAI, long-term forecasting, and equity measurement to deliver actionable recommendations for policymakers.",
    "categories": [
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
<<<<<<< HEAD
    "title": "Synthetic medical data generation: state of the art and application to trauma mechanism classification",
    "url": "http://arxiv.org/abs/2508.02771v1",
    "authors": [
      "Oc\u00e9ane Doremus",
      "Ariel Guerra-Adames",
      "Marta Avalos-Fernandez",
      "Vianney Jouhet",
      "C\u00e9dric Gil-Jardin\u00e9",
      "Emmanuel Lagarde"
    ],
    "published": "2025-08-04",
    "abstract": "Faced with the challenges of patient confidentiality and scientific\nreproducibility, research on machine learning for health is turning towards the\nconception of synthetic medical databases. This article presents a brief\noverview of state-of-the-art machine learning methods for generating synthetic\ntabular and textual data, focusing their application to the automatic\nclassification of trauma mechanisms, followed by our proposed methodology for\ngenerating high-quality, synthetic medical records combining tabular and\nunstructured text data.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "NICE^k Metrics: Unified and Multidimensional Framework for Evaluating Deterministic Solar Forecasting Accuracy",
    "url": "http://arxiv.org/abs/2508.01457v1",
    "authors": [
      "Cyril Voyant",
      "Milan Despotovic",
      "Luis Garcia-Gutierrez",
      "Rodrigo Amaro e Silva",
      "Philippe Lauret",
      "Ted Soubdhan",
      "Nadjem Bailek"
    ],
    "published": "2025-08-02",
    "abstract": "Accurate solar energy output prediction is key for integrating renewables\ninto grids, maintaining stability, and improving energy management. However,\nstandard error metrics such as Root Mean Squared Error (RMSE), Mean Absolute\nError (MAE), and Skill Scores (SS) fail to capture the multidimensional nature\nof solar irradiance forecasting. These metrics lack sensitivity to\nforecastability, rely on arbitrary baselines (e.g., clear-sky models), and are\npoorly suited for operational use.\n  To address this, we introduce the NICEk framework (Normalized Informed\nComparison of Errors, with k = 1, 2, 3, Sigma), offering a robust and\ninterpretable evaluation of forecasting models. Each NICEk score corresponds to\nan Lk norm: NICE1 targets average errors, NICE2 emphasizes large deviations,\nNICE3 highlights outliers, and NICESigma combines all.\n  Using Monte Carlo simulations and data from 68 stations in the Spanish SIAR\nnetwork, we evaluated methods including autoregressive models, extreme\nlearning, and smart persistence. Theoretical and empirical results align when\nassumptions hold (e.g., R^2 ~ 1.0 for NICE2). Most importantly, NICESigma\nconsistently shows higher discriminative power (p < 0.05), outperforming\ntraditional metrics (p > 0.05).\n  The NICEk metrics exhibit stronger statistical significance (e.g., p-values\nfrom 10^-6 to 0.004 across horizons) and greater generalizability. They offer a\nunified and operational alternative to standard error metrics in deterministic\nsolar forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.10568v1",
    "authors": [
      "Humza Naveed",
      "Xina Zeng",
      "Mitch Bryson",
      "Nagita Mehrseresht"
    ],
    "published": "2025-08-14",
    "abstract": "Foundational models have achieved significant success in diverse domains of\ncomputer vision. They learn general representations that are easily\ntransferable to tasks not seen during training. One such foundational model is\nSegment anything model (SAM), which can accurately segment objects in images.\nWe propose adapting the SAM encoder via fine-tuning for remote sensing change\ndetection (RSCD) along with spatial-temporal feature enhancement (STFE) and\nmulti-scale decoder fusion (MSDF) to detect changes robustly at multiple\nscales. Additionally, we propose a novel cross-entropy masking (CEM) loss to\nhandle high class imbalance in change detection datasets. Our method\noutperforms state-of-the-art (SOTA) methods on four change detection datasets,\nLevir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on\na large complex S2Looking dataset. The code is available at:\nhttps://github.com/humza909/SAM-CEM-CD",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss",
    "url": "http://arxiv.org/abs/2508.09453v1",
    "authors": [
      "Abdul Matin",
      "Tanjim Bin Faruk",
      "Shrideep Pallickara",
      "Sangmi Lee Pallickara"
    ],
    "published": "2025-08-13",
    "abstract": "The proliferation of foundation models, pretrained on large-scale unlabeled\ndatasets, has emerged as an effective approach in creating adaptable and\nreusable architectures that can be leveraged for various downstream tasks using\nsatellite observations. However, their direct application to hyperspectral\nremote sensing remains challenging due to inherent spectral disparities and the\nscarcity of available observations. In this work, we present HyperKD, a novel\nknowledge distillation framework that enables transferring learned\nrepresentations from a teacher model into a student model for effective\ndevelopment of a foundation model on hyperspectral images. Unlike typical\nknowledge distillation frameworks, which use a complex teacher to guide a\nsimpler student, HyperKD enables an inverse form of knowledge transfer across\ndifferent types of spectral data, guided by a simpler teacher model. Building\nupon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi\nfoundational model into a student tailored for EnMAP hyperspectral imagery.\nHyperKD addresses the inverse domain adaptation problem with spectral gaps by\nintroducing a feature-based strategy that includes spectral range-based channel\nalignment, spatial feature-guided masking, and an enhanced loss function\ntailored for hyperspectral images. HyperKD bridges the substantial spectral\ndomain gap, enabling the effective use of pretrained foundation models for\ngeospatial applications. Extensive experiments show that HyperKD significantly\nimproves representation learning in MAEs, leading to enhanced reconstruction\nfidelity and more robust performance on downstream tasks such as land cover\nclassification, crop type identification, and soil organic carbon prediction,\nunderpinning the potential of knowledge distillation frameworks in remote\nsensing analytics with hyperspectral imagery.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders",
    "url": "http://arxiv.org/abs/2508.07020v1",
    "authors": [
      "Tanjim Bin Faruk",
      "Abdul Matin",
      "Shrideep Pallickara",
      "Sangmi Lee Pallickara"
    ],
    "published": "2025-08-09",
    "abstract": "Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of\ncontiguous spectral bands, enabling fine-grained mapping of soils, crops, and\nland cover. While self-supervised Masked Autoencoders excel on RGB and low-band\nmultispectral data, they struggle to exploit the intricate spatial-spectral\ncorrelations in 200+ band hyperspectral images. We introduce TerraMAE, a novel\nHSI encoding framework specifically designed to learn highly representative\nspatial-spectral embeddings for diverse geospatial analyses. TerraMAE features\nan adaptive channel grouping strategy, based on statistical reflectance\nproperties to capture spectral similarities, and an enhanced reconstruction\nloss function that incorporates spatial and spectral quality metrics. We\ndemonstrate TerraMAE's effectiveness through superior spatial-spectral\ninformation preservation in high-fidelity image reconstruction. Furthermore, we\nvalidate its practical utility and the quality of its learned representations\nthrough strong performance on three key downstream geospatial tasks: crop\nidentification, land cover classification, and soil texture prediction.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning",
    "url": "http://arxiv.org/abs/2508.09555v1",
    "authors": [
      "Ahmet \u00d6ztel",
      "\u0130smet Karaca"
    ],
    "published": "2025-08-13",
    "abstract": "Objective - This study presents a biometric identification method based on\ntopological invariants from 2D iris images, representing iris texture via\nformally defined digital homology and evaluating classification performance.\n  Methods - Each normalized iris image (48x482 pixels) is divided into grids\n(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their\nratio using a recent algorithm for homology groups in 2D digital images. The\nresulting invariants form a feature matrix used with logistic regression, KNN,\nand SVM (with PCA and 100 randomized repetitions). A convolutional neural\nnetwork (CNN) is trained on raw images for comparison.\n  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,\noutperforming CNN (96.44 +/- 1.32%) and other feature-based models. The\ntopological features showed high accuracy with low variance.\n  Conclusion - This is the first use of topological invariants from formal\ndigital homology for iris recognition. The method offers a compact,\ninterpretable, and accurate alternative to deep learning, useful when\nexplainability or limited data is important. Beyond iris recognition, it can\napply to other biometrics, medical imaging, materials science, remote sensing,\nand interpretable AI. It runs efficiently on CPU-only systems and produces\nrobust, explainable features valuable for security-critical domains.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification",
      "Recognition",
      "Regression"
    ]
  },
  {
    "title": "Can Multitask Learning Enhance Model Explainability?",
    "url": "http://arxiv.org/abs/2508.06966v1",
    "authors": [
      "Hiba Najjar",
      "Bushra Alshbib",
      "Andreas Dengel"
    ],
    "published": "2025-08-09",
    "abstract": "Remote sensing provides satellite data in diverse types and formats. The\nusage of multimodal learning networks exploits this diversity to improve model\nperformance, except that the complexity of such networks comes at the expense\nof their interpretability. In this study, we explore how modalities can be\nleveraged through multitask learning to intrinsically explain model behavior.\nIn particular, instead of additional inputs, we use certain modalities as\nadditional targets to be predicted along with the main task. The success of\nthis approach relies on the rich information content of satellite data, which\nremains as input modalities. We show how this modeling context provides\nnumerous benefits: (1) in case of data scarcity, the additional modalities do\nnot need to be collected for model inference at deployment, (2) the model\nperformance remains comparable to the multimodal baseline performance, and in\nsome cases achieves better scores, (3) prediction errors in the main task can\nbe explained via the model behavior in the auxiliary task(s). We demonstrate\nthe efficiency of our approach on three datasets, including segmentation,\nclassification, and regression tasks. Code available at\ngit.opendfki.de/hiba.najjar/mtl_explainability/.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Leveraging GNN to Enhance MEF Method in Predicting ENSO",
    "url": "http://arxiv.org/abs/2508.07410v1",
    "authors": [
      "Saghar Ganji",
      "Mohammad Naisipour"
    ],
    "published": "2025-08-10",
    "abstract": "Reliable long-lead forecasting of the El Nino Southern Oscillation (ENSO)\nremains a long-standing challenge in climate science. The previously developed\nMultimodal ENSO Forecast (MEF) model uses 80 ensemble predictions by two\nindependent deep learning modules: a 3D Convolutional Neural Network (3D-CNN)\nand a time-series module. In their approach, outputs of the two modules are\ncombined using a weighting strategy wherein one is prioritized over the other\nas a function of global performance. Separate weighting or testing of\nindividual ensemble members did not occur, however, which may have limited the\nmodel to optimize the use of high-performing but spread-out forecasts. In this\nstudy, we propose a better framework that employs graph-based analysis to\ndirectly model similarity between all 80 members of the ensemble. By\nconstructing an undirected graph whose vertices are ensemble outputs and whose\nweights on edges measure similarity (via RMSE and correlation), we identify and\ncluster structurally similar and accurate predictions. From which we obtain an\noptimized subset of 20 members using community detection methods. The final\nprediction is then obtained by averaging this optimized subset. This method\nimproves the forecast skill through noise removal and emphasis on ensemble\ncoherence. Interestingly, our graph-based selection shows robust statistical\ncharacteristics among top performers, offering new ensemble behavior insights.\nIn addition, we observe that while the GNN-based approach does not always\noutperform the baseline MEF under every scenario, it produces more stable and\nconsistent outputs, particularly in compound long-lead situations. The approach\nis model-agnostic too, suggesting that it can be applied directly to other\nforecasting models with gargantuan ensemble outputs, such as statistical,\nphysical, or hybrid models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "GNN"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Machine Learning for Cloud Detection in IASI Measurements: A Data-Driven SVM Approach with Physical Constraints",
    "url": "http://arxiv.org/abs/2508.10120v1",
    "authors": [
      "Chiara Zugarini",
      "Cristina Sgattoni",
      "Luca Sgheri"
    ],
    "published": "2025-08-13",
    "abstract": "Cloud detection is essential for atmospheric retrievals, climate studies, and\nweather forecasting. We analyze infrared radiances from the Infrared\nAtmospheric Sounding Interferometer (IASI) onboard Meteorological Operational\n(MetOp) satellites to classify scenes as clear or cloudy.\n  We apply the Support Vector Machine (SVM) approach, based on kernel methods\nfor non-separable data. In this study, the method is implemented for Cloud\nIdentification (CISVM) to classify the test set using radiances or brightness\ntemperatures, with dimensionality reduction through Principal Component\nAnalysis (PCA) and cloud-sensitive channel selection to focus on the most\ninformative features. Our best configuration achieves 88.30 percent agreement\nwith reference labels and shows strong consistency with cloud masks from the\nModerate Resolution Imaging Spectroradiometer (MODIS), with the largest\ndiscrepancies in polar regions due to sensor differences.\n  These results demonstrate that CISVM is a robust, flexible, and efficient\nmethod for automated cloud classification from infrared radiances, suitable for\noperational retrievals and future missions such as Far infrared Outgoing\nRadiation Understanding and Monitoring (FORUM), the ninth European Space Agency\nEarth Explorer Mission.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Knowledge-guided machine learning for disentangling Pacific sea surface temperature variability across timescales",
    "url": "http://arxiv.org/abs/2508.08490v1",
    "authors": [
      "Kyle J. C. Hall",
      "Maria J. Molina",
      "Emily F. Wisinski",
      "Gerald A. Meehl",
      "Antonietta Capotondi"
    ],
    "published": "2025-08-11",
    "abstract": "Global weather patterns and regimes are heavily influenced by the dominant\nmodes of Pacific sea surface temperature (SST) variability, including the El\nNi\\~no-Southern Oscillation (ENSO), Tropical Pacific Decadal Variability\n(TPDV), North Pacific Meridional Mode (NPMM), and the Pacific Decadal\nOscillation (PDO). However, separating these modes of variability remains\nchallenging due to their spatial overlap and possible nonlinear coupling, which\nviolates the assumptions of traditional linear methods. We develop a\nKnowledge-Guided AutoEncoder (KGAE) that uses spectral constraints to identify\nphysically interpretable modes, without the need for predefined temporal\nfilters or thresholds. The KGAE separates ENSO-like modes on 2- and 3-7-year\ntimescales and a decadal mode with characteristics reminiscent of the PDO and\nthe NPMM, each with distinct spatial patterns. We demonstrate that the decadal\nmode modulates ENSO diversity (central Pacific versus eastern Pacific), and\nthat a quasibiennial mode leads and follows the interannual mode, suggesting a\nrole in ENSO onset and decay. When applied to climate model output, KGAEs\nreveal model-specific biases in ENSO diversity and seasonal timing. Finally,\nresidual training isolates a primarily equatorial decadal mode, which may be a\ncomponent of TPDV-related decadal variability, likely originating from\nadvection linked to upwelling near the Gal\\'apagos Islands and the South\nEquatorial Current. Our results highlight how machine learning can uncover\nphysically meaningful modes of Earth system variability and improve the\nrepresentation and evaluation of variability across models and timescales.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
=======
    "title": "Context-Aware Visual Prompting: Automating Geospatial Web Dashboards with Large Language Models and Agent Self-Validation for Decision Support",
    "url": "http://arxiv.org/abs/2511.20656v1",
    "authors": [
      "Haowen Xu",
      "Jose Tupayachi",
      "Xiao-Ying Yu"
    ],
    "published": "2025-10-10",
    "abstract": "The development of web-based geospatial dashboards for risk analysis and decision support is often challenged by the difficulty in visualization of big, multi-dimensional environmental data, implementation complexity, and limited automation. We introduce a generative AI framework that harnesses Large Language Models (LLMs) to automate the creation of interactive geospatial dashboards from user-defined inputs including UI wireframes, requirements, and data sources. By incorporating a structured knowledge graph, the workflow embeds domain knowledge into the generation process and enable accurate and context-aware code completions. A key component of our approach is the Context-Aware Visual Prompting (CAVP) mechanism, which extracts encodes and interface semantics from visual layouts to guide LLM driven generation of codes. The new framework also integrates a self-validation mechanism that uses an agent-based LLM and Pass@k evaluation alongside semantic metrics to assure output reliability. Dashboard snippets are paired with data visualization codebases and ontological representations, enabling a pipeline that produces scalable React-based completions using the MVVM architectural pattern. Our results demonstrate improved performance over baseline approaches and expanded functionality over third party platforms, while incorporating multi-page, fully functional interfaces. We successfully developed a framework to implement LLMs, demonstrated the pipeline for automated code generation, deployment, and performed chain-of-thought AI agents in self-validation. This integrative approach is guided by structured knowledge and visual prompts, providing an innovative geospatial solution in enhancing risk analysis and decision making.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "Nonparametric Reaction Coordinate Optimization with Histories: A Framework for Rare Event Dynamics",
    "url": "http://arxiv.org/abs/2508.07326v1",
    "authors": [
      "Polina V. Banushkina",
      "Sergei V. Krivov"
    ],
    "published": "2025-08-10",
    "abstract": "Rare but critical events in complex systems, such as protein folding,\nchemical reactions, disease progression, and extreme weather or climate\nphenomena, are governed by complex, high-dimensional, stochastic dynamics.\nIdentifying an optimal reaction coordinate (RC) that accurately captures the\nprogress of these dynamics is crucial for understanding and simulating such\nprocesses. This work introduces a nonparametric RC optimization framework that\nincorporates trajectory histories, enabling robust analysis even for irregular\nor incomplete data. The power of the method is demonstrated through\nincreasingly challenging analyses of protein folding dynamics, where it\nprovides accurate committor estimates that pass a stringent validation test and\nyield high-resolution free energy profiles. Its generality is further\nillustrated through applications to dynamics in phase space, a conceptual ocean\ncirculation model, and a longitudinal clinical dataset. These results\ndemonstrate that rare event dynamics can be accurately characterized without\nexhaustive sampling of the configuration space, establishing a general,\nflexible, and robust framework for analyzing complex dynamical systems and\nlongitudinal datasets.",
    "categories": [
      "ocean"
=======
    "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models",
    "url": "http://arxiv.org/abs/2509.21593v1",
    "authors": [
      "Peng Luo",
      "Xiayin Lou",
      "Yu Zheng",
      "Zhuo Zheng",
      "Stefano Ermon"
    ],
    "published": "2025-09-25",
    "abstract": "Geospatial modeling provides critical solutions for pressing global challenges such as sustainability and climate change. Existing large language model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at evolving generic code but lack the domain knowledge and multi-step reasoning required for complex geospatial problems. We introduce GeoEvolve, a multi-agent LLM framework that couples evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms. GeoEvolve operates in two nested loops: an inner loop leverages a code evolver to generate and mutate candidate solutions, while an outer agentic controller evaluates global elites and queries a GeoKnowRAG module -- a structured geospatial knowledge base that injects theoretical priors from geography. This knowledge-guided evolution steers the search toward theoretically meaningful and computationally efficient algorithms. We evaluate GeoEvolve on two fundamental and classical tasks: spatial interpolation (kriging) and spatial uncertainty quantification (geospatial conformal prediction). Across these benchmarks, GeoEvolve automatically improves and discovers new algorithms, incorporating geospatial theory on top of classical models. It reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution. These results demonstrate that GeoEvolve provides a scalable path toward automated, knowledge-driven geospatial modeling, opening new opportunities for trustworthy and efficient AI-for-Science discovery.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
    "url": "http://arxiv.org/abs/2509.19002v2",
    "authors": [
      "Hao Wang",
      "Eiki Murata",
      "Lingfang Zhang",
      "Ayako Sato",
      "So Fukuda",
      "Ziqi Yin",
      "Wentao Hu",
      "Keisuke Nakao",
      "Yusuke Nakamura",
      "Sebastian Zwirner",
      "Yi-Chia Chen",
      "Hiroyuki Otomo",
      "Hiroki Ouchi",
      "Daisuke Kawahara"
    ],
    "published": "2025-09-23",
    "abstract": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents",
    "url": "http://arxiv.org/abs/2509.18633v2",
    "authors": [
      "Yara Mohajerani"
    ],
    "published": "2025-09-23",
    "abstract": "Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems. We present a novel geospatial agent-based model that integrates climate hazard data with evolutionary learning for economic agents. Our framework combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviours that allow firms to evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation. We demonstrate the framework using riverine flood projections under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to converge with baseline (no hazard) production levels after decades of disruption due to climate stress. Our results reveal systemic risks where even agents that are not directly exposed to floods face impacts through supply chain disruptions, with the end-of-century average price of goods 5.6% higher under RCP8.5 compared to the baseline in our illustrative economic network. This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.",
    "categories": [
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "Taking the Garbage Out of Data-Driven Prediction Across Climate Timescales",
    "url": "http://arxiv.org/abs/2508.07062v1",
    "authors": [
      "Jason C. Furtado",
      "Maria J. Molina",
      "Marybeth C. Arcodia",
      "Weston Anderson",
      "Tom Beucler",
      "John A. Callahan",
      "Laura M. Ciasto",
      "Vittorio A. Gensini",
      "Michelle L'Heureux",
      "Kathleen Pegion",
      "Jhayron S. P\u00e9rez-Carrasquilla",
      "Maike Sonnewald",
      "Ken Takahashi",
      "Baoqiang Xiang",
      "Brian G. Zimmerman"
    ],
    "published": "2025-08-09",
    "abstract": "Artificial intelligence (AI) -- and specifically machine learning (ML) --\napplications for climate prediction across timescales are proliferating\nquickly. The emergence of these methods prompts a revisit to the impact of data\npreprocessing, a topic familiar to the climate community, as more traditional\nstatistical models work with relatively small sample sizes. Indeed, the skill\nand confidence in the forecasts produced by data-driven models are directly\ninfluenced by the quality of the datasets and how they are treated during model\ndevelopment, thus yielding the colloquialism \"garbage in, garbage out.\" As\nsuch, this article establishes protocols for the proper preprocessing of input\ndata for AI/ML models designed for climate prediction (i.e., subseasonal to\ndecadal and longer). The three aims are to: (1) educate researchers,\ndevelopers, and end users on the effects that preprocessing has on climate\npredictions; (2) provide recommended practices for data preprocessing for such\napplications; and (3) empower end users to decipher whether the models they are\nusing are properly designed for their objectives. Specific topics covered in\nthis article include the creation of (standardized) anomalies, dealing with\nnon-stationarity and the spatiotemporally correlated nature of climate data,\nand handling of extreme values and variables with potentially complex\ndistributions. Case studies will illustrate how using different preprocessing\ntechniques can produce different predictions from the same model, which can\ncreate confusion and decrease confidence in the overall process. Ultimately,\nimplementing the recommended practices set forth in this article will enhance\nthe robustness and transparency of AI/ML in climate prediction studies.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing",
    "url": "http://arxiv.org/abs/2508.12409v1",
    "authors": [
      "Liang Lv",
      "Di Wang",
      "Jing Zhang",
      "Lefei Zhang"
    ],
    "published": "2025-08-17",
    "abstract": "Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)\nanalysis by leveraging unlabeled data through pseudo-labeling and consistency\nlearning. However, existing S4 studies often rely on small-scale datasets and\nmodels, limiting their practical applicability. To address this, we propose S5,\nthe first scalable framework for semi-supervised semantic segmentation in RS,\nwhich unlocks the potential of vast unlabeled Earth observation data typically\nunderutilized due to costly pixel-level annotations. Built upon existing\nlarge-scale RS datasets, S5 introduces a data selection strategy that\nintegrates entropy-based filtering and diversity expansion, resulting in the\nRS4P-1M dataset. Using this dataset, we systematically scales S4 methods by\npre-training RS foundation models (RSFMs) of varying sizes on this extensive\ncorpus, significantly boosting their performance on land cover segmentation and\nobject detection tasks. Furthermore, during fine-tuning, we incorporate a\nMixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which\nenables efficient adaptation to multiple RS benchmarks with fewer parameters.\nThis approach improves the generalization and versatility of RSFMs across\ndiverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance\nacross all benchmarks, underscoring the viability of scaling semi-supervised\nlearning for RS applications. All datasets, code, and models will be released\nat https://github.com/MiliLab/S5",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
=======
    "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2509.13164v2",
    "authors": [
      "Jiawei Wang",
      "Haowei Sun",
      "Xintao Yan",
      "Shuo Feng",
      "Jun Gao",
      "Henry X. Liu"
    ],
    "published": "2025-09-16",
    "abstract": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires extensive and diverse data, particularly safety-critical events. Existing data are mostly generated from simulators with a significant sim-to-real gap or collected from on-road testing that is costly and unsafe. This paper presents TeraSim-World, an automated pipeline that synthesizes realistic and geographically diverse safety-critical data for E2E autonomous driving at anywhere in the world. Starting from an arbitrary location, TeraSim-World retrieves real-world maps and traffic demand from geospatial data sources. Then, it simulates agent behaviors from naturalistic driving datasets, and orchestrates diverse adversities to create corner cases. Informed by street views of the same location, it achieves photorealistic, geographically grounded sensor rendering via the frontier video generation model Cosmos-Drive. By bridging agent and sensor simulations, TeraSim-World provides a scalable and critical data synthesis framework for training and evaluation of E2E autonomous driving systems. Codes and videos are available at https://wjiawei.com/terasim-world-web/ .",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation",
    "url": "http://arxiv.org/abs/2509.08863v3",
    "authors": [
      "Qianqian Luo",
      "Qingming Lin",
      "Liuchang Xu",
      "Sensen Wu",
      "Ruichen Mao",
      "Chao Wang",
      "Hailin Feng",
      "Bo Huang",
      "Zhenhong Du"
    ],
    "published": "2025-09-10",
    "abstract": "Large Language Models (LLMs) have demonstrated substantial progress in task automation and natural language understanding. However, without domain expertise in geographic information science (GIS), they continue to encounter limitations including reduced accuracy and unstable performance when processing complex tasks. To address these challenges, we propose GeoJSON Agents-a novel multi-agent LLM architecture specifically designed for geospatial analysis. This framework transforms natural language instructions into structured GeoJSON operations through two LLM enhancement techniques: Function Calling and Code Generation. The architecture integrates three core components: task parsing, agent collaboration, and result integration. The Planner agent systematically decomposes user-defined tasks into executable subtasks, while Worker agents perform spatial data processing and analysis either by invoking predefined function APIs or by generating and executing Python-based analytical code. The system produces reusable, standards-compliant GeoJSON outputs through iterative refinement. To evaluate both approaches, we constructed a benchmark comprising 70 tasks spanning basic, intermediate, and advanced complexity levels, conducting experiments with OpenAI's GPT-4o as the core model. Results indicate that the Code Generation-based agent achieved 97.14% accuracy, while the Function Calling-based agent attained 85.71%-both significantly outperforming the best-performing general-purpose model (48.57%). Comparative analysis reveals Code Generation offers superior flexibility for complex, open-ended tasks, whereas Function Calling provides enhanced execution stability for structured operations. This study represents the first systematic integration of GeoJSON data with a multi-agent LLM framework and provides empirical evidence comparing two mainstream enhancement methodologies in geospatial context.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration",
    "url": "http://arxiv.org/abs/2509.05933v2",
    "authors": [
      "Md Hasebul Hasan",
      "Mahir Labib Dihan",
      "Tanzima Hashem",
      "Mohammed Eunus Ali",
      "Md Rizwan Parvez"
    ],
    "published": "2025-09-07",
    "abstract": "Agentic AI has significantly extended the capabilities of large language models (LLMs) by enabling complex reasoning and tool use. However, most existing frameworks are tailored to domains such as mathematics, coding, or web automation, and fall short on geospatial tasks that require spatial reasoning, multi-hop planning, and real-time map interaction. To address these challenges, we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with customized toolsets and agentic scaffolds for map-integrated geospatial reasoning. Unlike existing flat agent-based approaches that treat tools uniformly-often overwhelming the LLM when handling similar but subtly different geospatial APIs-MapAgent decouples planning from execution. A high-level planner decomposes complex queries into subgoals, which are routed to specialized modules. For tool-heavy modules-such as map-based services-we then design a dedicated map-tool agent that efficiently orchestrates related APIs adaptively in parallel to effectively fetch geospatial data relevant for the query, while simpler modules (e.g., solution generation or answer extraction) operate without additional agent overhead. This hierarchical design reduces cognitive load, improves tool selection accuracy, and enables precise coordination across similar APIs. We evaluate MapAgent on four diverse geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented and agentic baselines. We open-source our framwork at https://github.com/Hasebul/MapAgent.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
  },
  {
    "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries",
    "url": "http://arxiv.org/abs/2508.15752v1",
    "authors": [
      "Jon E. Froehlich",
      "Jared Hwang",
      "Zeyu Wang",
      "John S. O'Meara",
      "Xia Su",
      "William Huang",
      "Yang Zhang",
      "Alex Fiannaca",
      "Philip Nelson",
      "Shaun Kane"
    ],
    "published": "2025-08-21",
<<<<<<< HEAD
    "abstract": "Interactive digital maps have revolutionized how people travel and learn\nabout the world; however, they rely on pre-existing structured data in GIS\ndatabases (e.g., road networks, POI indices), limiting their ability to address\ngeo-visual questions related to what the world looks like. We introduce our\nvision for Geo-Visual Agents--multimodal AI agents capable of understanding and\nresponding to nuanced visual-spatial inquiries about the world by analyzing\nlarge-scale repositories of geospatial images, including streetscapes (e.g.,\nGoogle Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial\nimagery (e.g., satellite photos) combined with traditional GIS data sources. We\ndefine our vision, describe sensing and interaction approaches, provide three\nexemplars, and enumerate key challenges and opportunities for future work.",
    "categories": [
      "foundation_model"
=======
    "abstract": "Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.",
    "categories": [
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "CuMoLoS-MAE: A Masked Autoencoder for Remote Sensing Data Reconstruction",
    "url": "http://arxiv.org/abs/2508.14957v1",
    "authors": [
      "Anurup Naskar",
      "Nathanael Zhixin Wong",
      "Sara Shamekh"
    ],
    "published": "2025-08-20",
    "abstract": "Accurate atmospheric profiles from remote sensing instruments such as Doppler\nLidar, Radar, and radiometers are frequently corrupted by low-SNR (Signal to\nNoise Ratio) gates, range folding, and spurious discontinuities. Traditional\ngap filling blurs fine-scale structures, whereas deep models lack confidence\nestimates. We present CuMoLoS-MAE, a Curriculum-Guided Monte Carlo Stochastic\nEnsemble Masked Autoencoder designed to (i) restore fine-scale features such as\nupdraft and downdraft cores, shear lines, and small vortices, (ii) learn a\ndata-driven prior over atmospheric fields, and (iii) quantify pixel-wise\nuncertainty. During training, CuMoLoS-MAE employs a mask-ratio curriculum that\nforces a ViT decoder to reconstruct from progressively sparser context. At\ninference, we approximate the posterior predictive by Monte Carlo over random\nmask realisations, evaluating the MAE multiple times and aggregating the\noutputs to obtain the posterior predictive mean reconstruction together with a\nfinely resolved per-pixel uncertainty map. Together with high-fidelity\nreconstruction, this novel deep learning-based workflow enables enhanced\nconvection diagnostics, supports real-time data assimilation, and improves\nlong-term climate reanalysis.",
    "categories": [
      "ocean",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
=======
    "title": "GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement",
    "url": "http://arxiv.org/abs/2508.04080v1",
    "authors": [
      "Jinfan Tang",
      "Kunming Wu",
      "Ruifeng Gongxie",
      "Yuya He",
      "Yuankai Wu"
    ],
    "published": "2025-08-06",
    "abstract": "Recent studies have extended the application of large language models (LLMs) to geographic problems, revealing surprising geospatial competence even without explicit spatial supervision. However, LLMs still face challenges in spatial consistency, multi-hop reasoning, and geographic bias. To address these issues, we propose GeoSR, a self-refining agentic reasoning framework that embeds core geographic principles -- most notably Tobler's First Law of Geography -- into an iterative prediction loop. In GeoSR, the reasoning process is decomposed into three collaborating agents: (1) a variable-selection agent that selects relevant covariates from the same location; (2) a point-selection agent that chooses reference predictions at nearby locations generated by the LLM in previous rounds; and (3) a refine agent that coordinates the iterative refinement process by evaluating prediction quality and triggering further rounds when necessary. This agentic loop progressively improves prediction quality by leveraging both spatial dependencies and inter-variable relationships. We validate GeoSR on tasks ranging from physical-world property estimation to socioeconomic prediction. Experimental results show consistent improvements over standard prompting strategies, demonstrating that incorporating geostatistical priors and spatially structured reasoning into LLMs leads to more accurate and equitable geospatial predictions. The code of GeoSR is available at https://github.com/JinfanTang/GeoSR.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GeoFlow: Agentic Workflow Automation for Geospatial Tasks",
    "url": "http://arxiv.org/abs/2508.04719v1",
    "authors": [
      "Amulya Bhattaram",
      "Justin Chung",
      "Stanley Chung",
      "Ranit Gupta",
      "Janani Ramamoorthy",
      "Kartikeya Gullapalli",
      "Diana Marculescu",
      "Dimitrios Stamoulis"
    ],
    "published": "2025-08-05",
    "abstract": "We present GeoFlow, a method that automatically generates agentic workflows for geospatial tasks. Unlike prior work that focuses on reasoning decomposition and leaves API selection implicit, our method provides each agent with detailed tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow increases agentic success by 6.8% and reduces token usage by up to fourfold across major LLM families compared to state-of-the-art approaches.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives",
    "url": "http://arxiv.org/abs/2508.14558v1",
    "authors": [
      "Juepeng Zheng",
      "Zi Ye",
      "Yibin Wen",
      "Jianxi Huang",
      "Zhiwei Zhang",
      "Qingmei Li",
      "Qiong Hu",
      "Baodong Xu",
      "Lingyuan Zhao",
      "Haohuan Fu"
    ],
    "published": "2025-08-20",
    "abstract": "Powered by advances in multiple remote sensing sensors, the production of\nhigh spatial resolution images provides great potential to achieve\ncost-efficient and high-accuracy agricultural inventory and analysis in an\nautomated way. Lots of studies that aim at providing an inventory of the level\nof each agricultural parcel have generated many methods for Agricultural Parcel\nand Boundary Delineation (APBD). This review covers APBD methods for detecting\nand delineating agricultural parcels and systematically reviews the past and\npresent of APBD-related research applied to remote sensing images. With the\ngoal to provide a clear knowledge map of existing APBD efforts, we conduct a\ncomprehensive review of recent APBD papers to build a meta-data analysis,\nincluding the algorithm, the study site, the crop type, the sensor type, the\nevaluation method, etc. We categorize the methods into three classes: (1)\ntraditional image processing methods (including pixel-based, edge-based and\nregion-based); (2) traditional machine learning methods (such as random forest,\ndecision tree); and (3) deep learning-based methods. With deep\nlearning-oriented approaches contributing to a majority, we further discuss\ndeep learning-based methods like semantic segmentation-based, object\ndetection-based and Transformer-based methods. In addition, we discuss five\nAPBD-related issues to further comprehend the APBD domain using remote sensing\ndata, such as multi-sensor data in APBD task, comparisons between single-task\nlearning and multi-task learning in the APBD domain, comparisons among\ndifferent algorithms and different APBD tasks, etc. Finally, this review\nproposes some APBD-related applications and a few exciting prospects and\npotential hot topics in future APBD research. We hope this review help\nresearchers who involved in APBD domain to keep track of its development and\ntendency.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
=======
    "title": "HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis",
    "url": "http://arxiv.org/abs/2508.05666v1",
    "authors": [
      "Alejandro Godinez"
    ],
    "published": "2025-08-01",
    "abstract": "We present HySemRAG, a framework that combines Extract, Transform, Load (ETL) pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale literature synthesis and identify methodological research gaps. The system addresses limitations in existing RAG architectures through a multi-layered approach: hybrid retrieval combining semantic search, keyword filtering, and knowledge graph traversal; an agentic self-correction framework with iterative quality assurance; and post-hoc citation verification ensuring complete traceability. Our implementation processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis using modified Docling architecture, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. The system creates dual data products - a Neo4j knowledge graph enabling complex relationship queries and Qdrant vector collections supporting semantic search - serving as foundational infrastructure for verifiable information synthesis. Evaluation across 643 observations from 60 testing sessions demonstrates structured field extraction achieving 35.1% higher semantic similarity scores (0.655 $\\pm$ 0.178) compared to PDF chunking approaches (0.485 $\\pm$ 0.204, p < 0.000001). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy in validated responses. Applied to geospatial epidemiology literature on ozone exposure and cardiovascular disease, the system identifies methodological trends and research gaps, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Towards Urban Planing AI Agent in the Age of Agentic AI",
    "url": "http://arxiv.org/abs/2507.14730v4",
    "authors": [
      "Rui Liu",
      "Tao Zhe",
      "Zhong-Ren Peng",
      "Necati Catbas",
      "Xinyue Ye",
      "Dongjie Wang",
      "Yanjie Fu"
    ],
    "published": "2025-07-19",
    "abstract": "Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. Existing studies conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints and reshape automated urban design. We further identify critical gaps of existing generative urban planning studies: 1) the generative structure has to be predefined with strong assumption: all of adversarial generator-discriminator, forward and inverse diffusion structures, hierarchical zone-POI generative structure are predefined by humans; 2) ignore the power of domain expert developed tools: domain urban planners have developed various tools in the urban planning process guided by urban theory, while existing pure neural networks based generation ignore the power of the tools developed by urban planner practitioners. To address these limitations, we outline a future research direction agentic urban AI planner, calling for a new synthesis of agentic AI and participatory urbanism.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Start from the End: A Framework for Computational Policy Exploration to Inform Effective and Geospatially Consistent Interventions applied to COVID-19 in St. Louis",
    "url": "http://arxiv.org/abs/2507.10870v2",
    "authors": [
      "David O'Gara",
      "Matt Kasman",
      "Matthew D. Haslam",
      "Ross A. Hammond"
    ],
    "published": "2025-07-15",
    "abstract": "Mathematical models are a powerful tool to study infectious disease dynamics and intervention strategies against them in social systems. However, due to their detailed implementation and steep computational requirements, practitioners and stakeholders are typically only able to explore a small subset of all possible intervention scenarios, a severe limitation when preparing for disease outbreaks. In this work, we propose a parameter exploration framework utilizing emulator models to make uncertainty-aware predictions of high-dimensional parameter spaces and identify large numbers of feasible response strategies. We apply our framework to a case study of a large-scale agent-based disease model of the COVID-19 ``Omicron wave'' in St. Louis, Missouri that took place from December 2021 to February 2022. We identify large numbers of response strategies that would have been estimated to have reduced disease spread by a substantial amount. We also identify policy interventions that would have been able to reduce the geospatial variation in disease spread, which has additional implications for designing thoughtful response strategies.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "IMAIA: Interactive Maps AI Assistant for Travel Planning and Geo-Spatial Intelligence",
    "url": "http://arxiv.org/abs/2507.06993v3",
    "authors": [
      "Jieren Deng",
      "Zhizhang Hu",
      "Ziyan He",
      "Aleksandar Cvetkovic",
      "Pak Kiu Chung",
      "Dragomir Yankov",
      "Chiqun Zhang"
    ],
    "published": "2025-07-09",
    "abstract": "Map applications are still largely point-and-click, making it difficult to ask map-centric questions or connect what a camera sees to the surrounding geospatial context with view-conditioned inputs. We introduce IMAIA, an interactive Maps AI Assistant that enables natural-language interaction with both vector (street) maps and satellite imagery, and augments camera inputs with geospatial intelligence to help users understand the world. IMAIA comprises two complementary components. Maps Plus treats the map as first-class context by parsing tiled vector/satellite views into a grid-aligned representation that a language model can query to resolve deictic references (e.g., ``the flower-shaped building next to the park in the top-right''). Places AI Smart Assistant (PAISA) performs camera-aware place understanding by fusing image--place embeddings with geospatial signals (location, heading, proximity) to ground a scene, surface salient attributes, and generate concise explanations. A lightweight multi-agent design keeps latency low and exposes interpretable intermediate decisions. Across map-centric QA and camera-to-place grounding tasks, IMAIA improves accuracy and responsiveness over strong baselines while remaining practical for user-facing deployments. By unifying language, maps, and geospatial cues, IMAIA moves beyond scripted tools toward conversational mapping that is both spatially grounded and broadly usable.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "\"Hi AirStar, Guide Me to the Badminton Court.\"",
    "url": "http://arxiv.org/abs/2507.04430v1",
    "authors": [
      "Ziqin Wang",
      "Jinyu Chen",
      "Xiangyi Zheng",
      "Qinan Liao",
      "Linjiang Huang",
      "Si Liu"
    ],
    "published": "2025-07-06",
    "abstract": "Unmanned Aerial Vehicles, operating in environments with relatively few obstacles, offer high maneuverability and full three-dimensional mobility. This allows them to rapidly approach objects and perform a wide range of tasks often challenging for ground robots, making them ideal for exploration, inspection, aerial imaging, and everyday assistance. In this paper, we introduce AirStar, a UAV-centric embodied platform that turns a UAV into an intelligent aerial assistant: a large language model acts as the cognitive core for environmental understanding, contextual reasoning, and task planning. AirStar accepts natural interaction through voice commands and gestures, removing the need for a remote controller and significantly broadening its user base. It combines geospatial knowledge-driven long-distance navigation with contextual reasoning for fine-grained short-range control, resulting in an efficient and accurate vision-and-language navigation (VLN) capability.Furthermore, the system also offers built-in capabilities such as cross-modal question answering, intelligent filming, and target tracking. With a highly extensible framework, it supports seamless integration of new functionalities, paving the way toward a general-purpose, instruction-driven intelligent UAV agent. The supplementary PPT is available at \\href{https://buaa-colalab.github.io/airstar.github.io}{https://buaa-colalab.github.io/airstar.github.io}.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Generative AI as a Pillar for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning",
    "url": "http://arxiv.org/abs/2506.02485v2",
    "authors": [
      "Haowen Xu",
      "Sisi Zlatanova",
      "Ruiyu Liang",
      "Ismet Canbulat"
    ],
    "published": "2025-06-03",
    "abstract": "Wildfires increasingly threaten human life, ecosystems, and infrastructure, with events like the 2025 Palisades and Eaton fires in Los Angeles County underscoring the urgent need for more advanced prediction frameworks. Existing physics-based and deep learning models struggle to capture dynamic wildfire spread across both 2D and 3D domains, especially when incorporating real-time, multimodal geospatial data. This paper explores how generative Artificial Intelligence (AI) models-such as GANs, VAEs, and Transformers-can serve as transformative tools for wildfire prediction and simulation. These models offer superior capabilities in managing uncertainty, integrating multimodal inputs, and generating realistic, scalable wildfire scenarios. We introduce a new paradigm that leverages large language models (LLMs) for literature synthesis, classification, and knowledge extraction, conducting a systematic review of recent studies applying generative AI to fire prediction and monitoring. We highlight how generative approaches uniquely address challenges faced by traditional simulation and deep learning methods. Finally, we outline five key future directions for generative AI in wildfire management, including unified multimodal modeling of 2D and 3D dynamics, agentic AI systems and chatbots for decision intelligence, and real-time scenario generation on mobile devices, along with a discussion of critical challenges. Our findings advocate for a paradigm shift toward multimodal generative frameworks to support proactive, data-informed wildfire response.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer",
      "GAN",
      "Autoencoder",
      "LLM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings",
    "url": "http://arxiv.org/abs/2506.00634v1",
    "authors": [
      "Adam Visokay",
      "Ruth Bagley",
      "Ian Kennedy",
      "Chris Hess",
      "Kyle Crowder",
      "Rob Voigt",
      "Denis Peskoff"
    ],
    "published": "2025-05-31",
    "abstract": "Rental listings offer a unique window into how urban space is socially constructed through language. We analyze Chicago Craigslist rental advertisements from 2018 to 2024 to examine how listing agents characterize neighborhoods, identifying mismatches between institutional boundaries and neighborhood claims. Through manual and large language model annotation, we classify unstructured listings from Craigslist according to their neighborhood. Geospatial analysis reveals three distinct patterns: properties with conflicting neighborhood designations due to competing spatial definitions, border properties with valid claims to adjacent neighborhoods, and ``reputation laundering\" where listings claim association with distant, desirable neighborhoods. Through topic modeling, we identify patterns that correlate with spatial positioning: listings further from neighborhood centers emphasize different amenities than centrally-located units. Our findings demonstrate that natural language processing techniques can reveal how definitions of urban spaces are contested in ways that traditional methods overlook.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "MapStory: Prototyping Editable Map Animations with LLM Agents",
    "url": "http://arxiv.org/abs/2505.21966v2",
    "authors": [
      "Aditya Gunturu",
      "Ben Pearman",
      "Keiichi Ihara",
      "Morteza Faraji",
      "Bryan Wang",
      "Rubaiat Habib Kazi",
      "Ryo Suzuki"
    ],
    "published": "2025-05-28",
    "abstract": "We introduce MapStory, an LLM-powered animation prototyping tool that generates editable map animation sequences directly from natural language text by leveraging a dual-agent LLM architecture. Given a user written script, MapStory automatically produces a scene breakdown, which decomposes the text into key map animation primitives such as camera movements, visual highlights, and animated elements. Our system includes a researcher agent that accurately queries geospatial information by leveraging an LLM with web search, enabling automatic extraction of relevant regions, paths, and coordinates while allowing users to edit and query for changes or additional information to refine the results. Additionally, users can fine-tune parameters of these primitive blocks through an interactive timeline editor. We detail the system's design and architecture, informed by formative interviews with professional animators and by an analysis of 200 existing map animation videos. Our evaluation, which includes expert interviews (N=5) and a usability study (N=12), demonstrates that MapStory enables users to create map animations with ease, facilitates faster iteration, encourages creative exploration, and lowers barriers to creating map-centric stories.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning",
    "url": "http://arxiv.org/abs/2505.21863v3",
    "authors": [
      "Shikhhar Siingh",
      "Abhinav Rawat",
      "Chitta Baral",
      "Vivek Gupta"
    ],
    "published": "2025-05-28",
    "abstract": "Publicly significant images from events hold valuable contextual information, crucial for journalism and education. However, existing methods often struggle to extract this relevance accurately. To address this, we introduce GETReason (Geospatial Event Temporal Reasoning), a framework that moves beyond surface-level image descriptions to infer deeper contextual meaning. We propose that extracting global event, temporal, and geospatial information enhances understanding of an image's significance. Additionally, we introduce GREAT (Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric for evaluating reasoning-based image understanding. Our layered multi-agent approach, assessed using a reasoning-weighted metric, demonstrates that meaningful insights can be inferred, effectively linking images to their broader event context.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "AI-in-the-Loop Planning for Transportation Electrification: Case Studies from Austin, Texas",
    "url": "http://arxiv.org/abs/2504.21185v2",
    "authors": [
      "Seung Jun Choi"
    ],
    "published": "2025-04-29",
    "abstract": "This study explores the integration of AI in transportation electrification planning in Austin, TX, focusing on the use of Geospatial AI (GeoAI), Generative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site selection, localized GenAI models support meta-level estimations, and LLMs enable scenario simulations. These AI applications require human oversight. GeoAI outputs must be evaluated with land use data, GenAI models are not always accurate, and LLMs are prone to hallucinations. To ensure accountable planning, human planners must work alongside AI agents. Establishing a community feedback loop is essential to audit automated decisions. Planners should place Community Experience (CX) at the center of Urban Planning AI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation",
    "url": "http://arxiv.org/abs/2504.09587v3",
    "authors": [
      "Haotian Xu",
      "Yue Hu",
      "Chen Gao",
      "Zhengqiu Zhu",
      "Yong Zhao",
      "Yong Li",
      "Quanjun Yin"
    ],
    "published": "2025-04-13",
    "abstract": "Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Geo-OLM: Enabling Sustainable Earth Observation Studies with Cost-Efficient Open Language Models & State-Driven Workflows",
    "url": "http://arxiv.org/abs/2504.04319v1",
    "authors": [
      "Dimitrios Stamoulis",
      "Diana Marculescu"
    ],
    "published": "2025-04-06",
    "abstract": "Geospatial Copilots hold immense potential for automating Earth observation (EO) and climate monitoring workflows, yet their reliance on large-scale models such as GPT-4o introduces a paradox: tools intended for sustainability studies often incur unsustainable costs. Using agentic AI frameworks in geospatial applications can amass thousands of dollars in API charges or requires expensive, power-intensive GPUs for deployment, creating barriers for researchers, policymakers, and NGOs. Unfortunately, when geospatial Copilots are deployed with open language models (OLMs), performance often degrades due to their dependence on GPT-optimized logic. In this paper, we present Geo-OLM, a tool-augmented geospatial agent that leverages the novel paradigm of state-driven LLM reasoning to decouple task progression from tool calling. By alleviating the workflow reasoning burden, our approach enables low-resource OLMs to complete geospatial tasks more effectively. When downsizing to small models below 7B parameters, Geo-OLM outperforms the strongest prior geospatial baselines by 32.8% in successful query completion rates. Our method performs comparably to proprietary models achieving results within 10% of GPT-4o, while reducing inference costs by two orders of magnitude from \\$500-\\$1000 to under \\$10. We present an in-depth analysis with geospatial downstream benchmarks, providing key insights to help practitioners effectively deploy OLMs for EO applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GIScience in the Era of Artificial Intelligence: A Research Agenda Towards Autonomous GIS",
    "url": "http://arxiv.org/abs/2503.23633v5",
    "authors": [
      "Zhenlong Li",
      "Huan Ning",
      "Song Gao",
      "Krzysztof Janowicz",
      "Wenwen Li",
      "Samantha T. Arundel",
      "Chaowei Yang",
      "Budhendra Bhaduri",
      "Shaowen Wang",
      "A-Xing Zhu",
      "Mark Gahegan",
      "Shashi Shekhar",
      "Xinyue Ye",
      "Grant McKenzie",
      "Guido Cervone",
      "Michael E. Hodgson"
    ],
    "published": "2025-03-31",
    "abstract": "The advent of generative AI exemplified by large language models (LLMs) opens new ways to represent and compute geographic information and transcends the process of geographic knowledge production, driving geographic information systems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core, autonomous GIS can independently generate and execute geoprocessing workflows to perform spatial analysis. In this vision paper, we further elaborate on the concept of autonomous GIS and present a conceptual framework that defines its five autonomous goals, five autonomous levels, five core functions, and three operational scales. We demonstrate how autonomous GIS could perform geospatial data retrieval, spatial analysis, and map making with four proof-of-concept GIS agents. We conclude by identifying critical challenges and future research directions, including fine-tuning and self-growing decision-cores, autonomous modeling, and examining the societal and practical implications of autonomous GIS. By establishing the groundwork for a paradigm shift in GIScience, this paper envisions a future where GIS moves beyond traditional workflows to autonomously reason, derive, innovate, and advance geospatial solutions to pressing global challenges. Meanwhile, as we design and deploy increasingly intelligent geospatial systems, we carry a responsibility to ensure they are developed in socially responsible ways, serve the public good, and support the continued value of human geographic insight in an AI-augmented future.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks",
    "url": "http://arxiv.org/abs/2503.18129v2",
    "authors": [
      "Varvara Krechetova",
      "Denis Kochedykov"
    ],
    "published": "2025-03-23",
    "abstract": "This paper establishes a benchmark for evaluating tool-calling capabilities of large language models (LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet 3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o, GPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23 geospatial functions. Our benchmark comprises tasks in four categories of increasing complexity, with both solvable and intentionally unsolvable tasks to test rejection accuracy. We develop a LLM-as-Judge evaluation framework to compare agent solutions against reference solutions. Results show o4-mini and Claude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1, GPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the last two are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due its preference to provide any solution rather than reject a task, proved to be less accurate. We observe significant differences in token usage, with Anthropic models consuming more tokens than competitors. Common errors include misunderstanding geometrical relationships, relying on outdated knowledge, and inefficient data manipulation. The resulting benchmark set, evaluation framework, and data generation pipeline are released as open-source resources (available at https://github.com/Solirinai/GeoBenchX), providing one more standardized method for the ongoing evaluation of LLMs for GeoAI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Towards a Barrier-free GeoQA Portal: Natural Language Interaction with Geospatial Data Using Multi-Agent LLMs and Semantic Search",
    "url": "http://arxiv.org/abs/2503.14251v1",
    "authors": [
      "Yu Feng",
      "Puzhen Zhang",
      "Guohui Xiao",
      "Linfang Ding",
      "Liqiu Meng"
    ],
    "published": "2025-03-18",
    "abstract": "A Barrier-Free GeoQA Portal: Enhancing Geospatial Data Accessibility with a Multi-Agent LLM Framework\n  Geoportals are vital for accessing and analyzing geospatial data, promoting open spatial data sharing and online geo-information management. Designed with GIS-like interaction and layered visualization, they often challenge non-expert users with complex functionalities and overlapping layers that obscure spatial relationships. We propose a GeoQA Portal using a multi-agent Large Language Model framework for seamless natural language interaction with geospatial data. Complex queries are broken into subtasks handled by specialized agents, retrieving relevant geographic data efficiently. Task plans are shown to users, boosting transparency. The portal supports default and custom data inputs for flexibility. Semantic search via word vector similarity aids data retrieval despite imperfect terms. Case studies, evaluations, and user tests confirm its effectiveness for non-experts, bridging GIS complexity and public access, and offering an intuitive solution for future geoportals.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Social Cyber Geographical Worldwide Inventory of Bots",
    "url": "http://arxiv.org/abs/2501.18839v1",
    "authors": [
      "Lynnette Hui Xian Ng",
      "Kathleen M. Carley"
    ],
    "published": "2025-01-31",
    "abstract": "Social Cyber Geography is the space in the digital cyber realm that is produced through social relations. Communication in the social media ecosystem happens not only because of human interactions, but is also fueled by algorithmically controlled bot agents. Most studies have not looked at the social cyber geography of bots because they focus on bot activity within a single country. Since creating a bot uses universal programming technology, bots, how prevalent are these bots throughout the world? To quantify bot activity worldwide, we perform a multilingual and geospatial analysis on a large dataset of social data collected from X during the Coronavirus pandemic in 2021. This pandemic affected most of the world, and thus is a common topic of discussion. Our dataset consists of ~100 mil posts generated by ~31mil users. Most bot studies focus only on English-speaking countries, because most bot detection algorithms are built for the English language. However, only 47\\% of the bots write in the English language. To accommodate multiple languages in our bot detection algorithm, we built Multilingual BotBuster, a multi-language bot detection algorithm to identify the bots in this diverse dataset. We also create a Geographical Location Identifier to swiftly identify the countries a user affiliates with in his description. Our results show that bots can appear to move from one country to another, but the language they write in remains relatively constant. Bots distribute narratives on distinct topics related to their self-declared country affiliation. Finally, despite the diverse distribution of bot locations around the world, the proportion of bots per country is about 20%. Our work stresses the importance of a united analysis of the cyber and physical realms, where we combine both spheres to inventorize the language and location of social media bots and understand communication strategies.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "Detection"
    ]
  },
  {
<<<<<<< HEAD
    "title": "Generative AI models enable efficient and physically consistent sea-ice simulations",
    "url": "http://arxiv.org/abs/2508.14984v1",
    "authors": [
      "Tobias Sebastian Finn",
      "Marc Bocquet",
      "Pierre Rampal",
      "Charlotte Durand",
      "Flavia Porro",
      "Alban Farchi",
      "Alberto Carrassi"
    ],
    "published": "2025-08-20",
    "abstract": "Sea ice is governed by highly complex, scale-invariant, and anisotropic\nprocesses that are challenging to represent in Earth system models. While\nadvanced numerical models have improved our understanding of the sea-ice\ndynamics, their computational costs often limit their application in ensemble\nforecasting and climate simulations. Here, we introduce GenSIM, the first\ngenerative AI-based pan-Arctic model that predicts the evolution of all\nrelevant key properties, including concentration, thickness, and drift, in a\n12-hour window with improved accuracy over deterministic predictions and high\ncomputational efficiency, while remaining physically consistent. Trained on a\nlong simulation from a state-of-the-art sea-ice--ocean system, GenSIM robustly\nreproduces statistics as observed in numerical models and observations,\nexhibiting brittle-like short-term dynamics while also depicting the long-term\nsea-ice decline. Driven solely by atmospheric forcings, we attribute GenSIM's\nemergent extrapolation capabilities to patterns that reflect the long-term\nimpact of the ocean: it seemingly has learned an internal ocean emulator. This\nability to infer slowly evolving climate-relevant dynamics from short-term\npredictions underlines the large potential of generative models to generalise\nfor unseen climates and to encode hidden physics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models",
    "url": "http://arxiv.org/abs/2508.14315v1",
    "authors": [
      "Pritthijit Nath",
      "Sebastian Schemm",
      "Henry Moss",
      "Peter Haynes",
      "Emily Shuckburgh",
      "Mark Webb"
    ],
    "published": "2025-08-19",
    "abstract": "Sub-grid parameterisations in climate models are traditionally static and\ntuned offline, limiting adaptability to evolving states. This work introduces\nFedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors\nthe spatial decomposition used in general circulation models (GCMs) by\nassigning agents to latitude bands, enabling local parameter learning with\nperiodic global aggregation. Using a hierarchy of simplified energy-balance\nclimate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble\n(ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under\ndifferent FedRL configurations. Results show that Deep Deterministic Policy\nGradient (DDPG) consistently outperforms both static and single-agent\nbaselines, with faster convergence and lower area-weighted RMSE in tropical and\nmid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to\ntransfer across hyperparameters and low computational cost make it well-suited\nfor geographically adaptive parameter learning. This capability offers a\nscalable pathway towards high-complexity GCMs and provides a prototype for\nphysically aligned, online-learning climate models that can evolve with a\nchanging climate. Code accessible at\nhttps://github.com/p3jitnath/climate-rl-fedrl.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network",
    "url": "http://arxiv.org/abs/2508.13099v1",
    "authors": [
      "Mingyu Kim",
      "Daniel Stilwell",
      "Jorge Jimenez"
    ],
    "published": "2025-08-18",
    "abstract": "This paper presents a framework for classifying and detecting spatial\ncommission outliers in maritime environments using seabed acoustic sensor\nnetworks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as\na mixture of normal and outlier processes, we estimate the probability that a\nnewly observed event is an outlier. We propose a second-order approximation of\nthis probability that incorporates both the mean and variance of the normal\nintensity function, providing improved classification accuracy compared to\nmean-only approaches. We analytically show that our method yields a tighter\nbound to the true probability using Jensen's inequality. To enhance detection,\nwe integrate a real-time, near-optimal sensor placement strategy that\ndynamically adjusts sensor locations based on the evolving outlier intensity.\nThe proposed framework is validated using real ship traffic data near Norfolk,\nVirginia, where numerical results demonstrate the effectiveness of our approach\nin improving both classification performance and outlier detection through\nsensor deployment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams",
    "url": "http://arxiv.org/abs/2508.12198v1",
    "authors": [
      "ChangJae Lee",
      "Heecheol Yang",
      "Jonghak Choi"
    ],
    "published": "2025-08-17",
    "abstract": "Forecasting from atmospheric soundings is a fundamental task in operational\nmeteorology, often requiring structured visual reasoning over Skew-T log-P\ndiagrams by human forecasters. While recent advances in Vision-Language Models\n(VLMs) have shown promise in other scientific domains, their application to\nmeteorological diagram interpretation remains largely unexplored. In this\nstudy, we present a lightweight AI assistant that interprets Skew-T diagrams\nusing a small language model (LM) and a small VLM fine-tuned to emulate human\nforecasters. Using a curriculum learning framework, we first train the models\nto identify key atmospheric features from diagrams through visual question\nanswering, followed by chain-of-thought reasoning tasks that estimate\nprecipitation probability based on the derived visual groundings. Model inputs\ninclude either textual summaries or generated Skew-T diagrams derived from\noperational Numerical Weather Prediction (NWP) forecasts, paired with\nthree-hour precipitation observations from South Korea's Auto Weather Stations\nnetwork. Evaluation results demonstrate that the fine-tuned VLM achieves skill\ncomparable to an operational NWP model, despite relying solely on static\natmospheric profiles. Ablation studies reveal that visual grounding and\nreasoning supervision are critical for performance, while attention map\nanalysis confirms that the model learns to focus on relevant meteorological\nfeatures. These findings highlight the potential of compact, interpretable\nmultimodal models to support weather forecasting tasks. The approach offers a\ncomputationally efficient alternative to large-scale systems, and future work\ncould extend it to more complex applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement",
    "url": "http://arxiv.org/abs/2508.20954v1",
    "authors": [
      "Amir Jmal",
      "Chaima Chtourou",
      "Mahdi Louati",
      "Abdelaziz Kallel",
      "Houda Khmila"
    ],
    "published": "2025-08-28",
    "abstract": "In the context of proven climate change, maintaining olive biodiversity\nthrough early anomaly detection and treatment using remote sensing technology\nis crucial, offering effective management solutions. This paper presents an\ninnovative approach to olive tree segmentation from satellite images. By\nleveraging foundational models and advanced segmentation techniques, the study\nintegrates the Segment Anything Model (SAM) to accurately identify and segment\nolive trees in agricultural plots. The methodology includes SAM segmentation\nand corrections based on trees alignement in the field and a learanble\nconstraint about the shape and the size. Our approach achieved a 98\\% accuracy\nrate, significantly surpassing the initial SAM performance of 82\\%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
=======
    "title": "Multi-Agent Geospatial Copilots for Remote Sensing Workflows",
    "url": "http://arxiv.org/abs/2501.16254v1",
    "authors": [
      "Chaehong Lee",
      "Varatheepan Paramanayakam",
      "Andreas Karatzas",
      "Yanan Jian",
      "Michael Fore",
      "Heming Liao",
      "Fuxun Yu",
      "Ruopu Li",
      "Iraklis Anagnostopoulos",
      "Dimitrios Stamoulis"
    ],
    "published": "2025-01-27",
    "abstract": "We present GeoLLM-Squad, a geospatial Copilot that introduces the novel multi-agent paradigm to remote sensing (RS) workflows. Unlike existing single-agent approaches that rely on monolithic large language models (LLM), GeoLLM-Squad separates agentic orchestration from geospatial task-solving, by delegating RS tasks to specialized sub-agents. Built on the open-source AutoGen and GeoLLM-Engine frameworks, our work enables the modular integration of diverse applications, spanning urban monitoring, forestry protection, climate analysis, and agriculture studies. Our results demonstrate that while single-agent systems struggle to scale with increasing RS task complexity, GeoLLM-Squad maintains robust performance, achieving a 17% improvement in agentic correctness over state-of-the-art baselines. Our findings highlight the potential of multi-agent AI in advancing RS workflows.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Probabilistic Mission Design in Neuro-Symbolic Systems",
    "url": "http://arxiv.org/abs/2501.01439v1",
    "authors": [
      "Simon Kohaut",
      "Benedict Flade",
      "Daniel Ochs",
      "Devendra Singh Dhami",
      "Julian Eggert",
      "Kristian Kersting"
    ],
    "published": "2024-12-25",
    "abstract": "Advanced Air Mobility (AAM) is a growing field that demands accurate modeling of legal concepts and restrictions in navigating intelligent vehicles. In addition, any implementation of AAM needs to face the challenges posed by inherently dynamic and uncertain human-inhabited spaces robustly. Nevertheless, the employment of Unmanned Aircraft Systems (UAS) beyond visual line of sight (BVLOS) is an endearing task that promises to enhance significantly today's logistics and emergency response capabilities. To tackle these challenges, we present a probabilistic and neuro-symbolic architecture to encode legal frameworks and expert knowledge over uncertain spatial relations and noisy perception in an interpretable and adaptable fashion. More specifically, we demonstrate Probabilistic Mission Design (ProMis), a system architecture that links geospatial and sensory data with declarative, Hybrid Probabilistic Logic Programs (HPLP) to reason over the agent's state space and its legality. As a result, ProMis generates Probabilistic Mission Landscapes (PML), which quantify the agent's belief that a set of mission conditions is satisfied across its navigation space. Extending prior work on ProMis' reasoning capabilities and computational characteristics, we show its integration with potent machine learning models such as Large Language Models (LLM) and Transformer-based vision models. Hence, our experiments underpin the application of ProMis with multi-modal input data and how our method applies to many important AAM scenarios.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis",
    "url": "http://arxiv.org/abs/2411.03205v4",
    "authors": [
      "Temitope Akinboyewa",
      "Zhenlong Li",
      "Huan Ning",
      "M. Naser Lessani"
    ],
    "published": "2024-11-05",
    "abstract": "Recent advancements in Generative AI offer promising capabilities for spatial analysis. Despite their potential, the integration of generative AI with established GIS platforms remains underexplored. In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example. Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters. The implementation of this framework resulted in the development of a \"GIS Copilot\" that allows GIS users to interact with QGIS using natural language commands for spatial analysis. The GIS Copilot was evaluated with over 100 spatial analysis tasks with three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps. The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks. This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise. While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "An LLM Agent for Automatic Geospatial Data Analysis",
    "url": "http://arxiv.org/abs/2410.18792v2",
    "authors": [
      "Yuxing Chen",
      "Weijie Wang",
      "Sylvain Lobry",
      "Camille Kurtz"
    ],
    "published": "2024-10-24",
    "abstract": "Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors. Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries. To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively. GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing. In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks. This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization. By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks. Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls. Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion. In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated Shapefile Processing",
    "url": "http://arxiv.org/abs/2410.12376v2",
    "authors": [
      "Qingming Lin",
      "Rui Hu",
      "Huaxia Li",
      "Sensen Wu",
      "Yadong Li",
      "Kai Fang",
      "Hailin Feng",
      "Zhenhong Du",
      "Liuchang Xu"
    ],
    "published": "2024-10-16",
    "abstract": "Vector data is one of the two core data structures in geographic information science (GIS), essential for accurately storing and representing geospatial information. Shapefile, the most widely used vector data format, has become the industry standard supported by all major geographic information systems. However, processing this data typically requires specialized GIS knowledge and skills, creating a barrier for researchers from other fields and impeding interdisciplinary research in spatial data analysis. Moreover, while large language models (LLMs) have made significant advancements in natural language processing and task automation, they still face challenges in handling the complex spatial and topological relationships inherent in GIS vector data. To address these challenges, we propose ShapefileGPT, an innovative framework powered by LLMs, specifically designed to automate Shapefile tasks. ShapefileGPT utilizes a multi-agent architecture, in which the planner agent is responsible for task decomposition and supervision, while the worker agent executes the tasks. We developed a specialized function library for handling Shapefiles and provided comprehensive API documentation, enabling the worker agent to operate Shapefiles efficiently through function calling. For evaluation, we developed a benchmark dataset based on authoritative textbooks, encompassing tasks in categories such as geometric operations and spatial queries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the GPT series models. In comparison to traditional LLMs, ShapefileGPT effectively handles complex vector data analysis tasks, overcoming the limitations of traditional LLMs in spatial analysis. This breakthrough opens new pathways for advancing automation and intelligence in the GIS field, with significant potential in interdisciplinary data analysis and application contexts.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Spatiotemporal Modeling and Forecasting at Scale with Dynamic Generalized Linear Models",
    "url": "http://arxiv.org/abs/2410.07161v1",
    "authors": [
      "Pranay Pherwani",
      "Nicholas Hass",
      "Anna K. Yanchenko"
    ],
    "published": "2024-10-09",
    "abstract": "Spatiotemporal data consisting of timestamps, GPS coordinates, and IDs occurs in many settings. Modeling approaches for this type of data must address challenges in terms of sensor noise, uneven sampling rates, and non-persistent IDs. In this work, we characterize and forecast human mobility at scale with dynamic generalized linear models (DGLMs). We represent mobility data as occupancy counts of spatial cells over time and use DGLMs to model the occupancy counts for each spatial cell in an area of interest. DGLMs are flexible to varying numbers of occupancy counts across spatial cells, are dynamic, and easily incorporate daily and weekly seasonality in the aggregate-level behavior. Our overall approach is robust to various types of noise and scales linearly in the number of spatial cells, time bins, and agents. Our results show that DGLMs provide accurate occupancy count forecasts over a variety of spatial resolutions and forecast horizons. We also present scaling results for spatiotemporal data consisting of hundreds of millions of observations. Our approach is flexible to support several downstream applications, including characterizing human mobility, forecasting occupancy counts, and anomaly detection for aggregate-level behaviors.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast",
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "Anomaly Detection"
    ]
  },
  {
<<<<<<< HEAD
    "title": "Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory",
    "url": "http://arxiv.org/abs/2508.18829v1",
    "authors": [
      "Takayuki Ishikawa",
      "Carmelo Bonannella",
      "Bas J. W. Lerink",
      "Marc Ru\u00dfwurm"
    ],
    "published": "2025-08-26",
    "abstract": "National Forest Inventory (NFI)s serve as the primary source of forest\ninformation, providing crucial tree species distribution data. However,\nmaintaining these inventories requires labor-intensive on-site campaigns.\nRemote sensing approaches, particularly when combined with machine learning,\noffer opportunities to update NFIs more frequently and at larger scales. While\nthe use of Satellite Image Time Series has proven effective for distinguishing\ntree species through seasonal canopy reflectance patterns, current approaches\nrely primarily on Random Forest classifiers with hand-designed features and\nphenology-based metrics. Using deep features from an available pre-trained\nremote sensing foundation models offers a complementary strategy. These\npre-trained models leverage unannotated global data and are meant to used for\ngeneral-purpose applications and can then be efficiently fine-tuned with\nsmaller labeled datasets for specific classification tasks. This work\nsystematically investigates how deep features improve tree species\nclassification accuracy in the Netherlands with few annotated data. Data-wise,\nwe extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites\ndata and SRTM data using Google Earth Engine. Our results demonstrate that\nfine-tuning a publicly available remote sensing time series foundation model\noutperforms the current state-of-the-art in NFI classification in the\nNetherlands by a large margin of up to 10% across all datasets. This\ndemonstrates that classic hand-defined harmonic features are too simple for\nthis task and highlights the potential of using deep AI features for\ndata-limited application like NFI classification. By leveraging openly\navailable satellite data and pre-trained models, this approach significantly\nimproves classification accuracy compared to traditional methods and can\neffectively complement existing forest inventory processes.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images",
    "url": "http://arxiv.org/abs/2508.18067v1",
    "authors": [
      "Kaiyu Li",
      "Xiangyong Cao",
      "Ruixun Liu",
      "Shihong Wang",
      "Zixuan Jiang",
      "Zhi Wang",
      "Deyu Meng"
    ],
    "published": "2025-08-25",
    "abstract": "Semantic segmentation of remote sensing (RS) images is pivotal for\ncomprehensive Earth observation, but the demand for interpreting new object\ncategories, coupled with the high expense of manual annotation, poses\nsignificant challenges. Although open-vocabulary semantic segmentation (OVSS)\noffers a promising solution, existing frameworks designed for natural images\nare insufficient for the unique complexities of RS data. They struggle with\nvast scale variations and fine-grained details, and their adaptation often\nrelies on extensive, costly annotations. To address this critical gap, this\npaper introduces SegEarth-OV, the first framework for annotation-free\nopen-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,\na universal upsampler that robustly restores high-resolution spatial details\nfrom coarse features, correcting distorted target shapes without any\ntask-specific post-training. We also present a simple yet effective Global Bias\nAlleviation operation to subtract the inherent global context from patch\nfeatures, significantly enhancing local semantic fidelity. These components\nempower SegEarth-OV to effectively harness the rich semantics of pre-trained\nVLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the\nframework's universality to other challenging RS modalities like SAR images,\nwhere large-scale VLMs are unavailable and expensive to create, we introduce\nAlignEarth, which is a distillation-based strategy and can efficiently transfer\nsemantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the\nneed to build SAR foundation models from scratch and enabling universal OVSS\nacross diverse sensor types. Extensive experiments on both optical and SAR\ndatasets validate that SegEarth-OV can achieve dramatic improvements over the\nSOTA methods, establishing a robust foundation for annotation-free and\nopen-world Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "GRASP: Geospatial pixel Reasoning viA Structured Policy learning",
    "url": "http://arxiv.org/abs/2508.17102v1",
    "authors": [
      "Chengjie Jiang",
      "Yunqi Zhou",
      "Jiafeng Yan",
      "Jing Li"
    ],
    "published": "2025-08-23",
    "abstract": "Geospatial pixel reasoning is a nascent remote-sensing task that aims to\ngenerate segmentation masks directly from natural-language instructions.\nPrevailing MLLM-based systems co-train a language model and a mask decoder with\ndense pixel supervision, which is expensive and often weak on out-of-domain\n(OOD) data. We introduce GRASP, a structured policy-learning framework. In our\ndesign, a multimodal large language model first emits task-relevant bounding\nboxes and positive points from a vision-language instruction. These outputs are\nthen passed to a pre-trained segmentation model, which consumes them as prompts\nto generate the final mask. Instead of supervised fine-tuning, we optimize the\nsystem purely with reinforcement learning: the model is trained solely with\nGRPO, guided by format rewards and accuracy rewards computed on boxes and\npoints (no mask supervision). This leverages strong priors in foundation\nmodels, minimizes trainable parameters, and enables learning from inexpensive\nannotations. We additionally curate GRASP-1k, which contains\nreasoning-intensive queries, detailed reasoning traces, and fine-grained\nsegmentation annotations. Evaluations on both in-domain and out-of-domain test\nsets show state-of-the-art results: about 4% improvement in-domain and up to\n54% on OOD benchmarks. The experiment results evidence our model's robust\ngeneralization and demonstrate that complex geospatial segmentation behaviors\ncan be learned via RL from weak spatial cues. Code and the dataset will be\nreleased open-source.",
    "categories": [
      "foundation_model"
=======
    "title": "AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction",
    "url": "http://arxiv.org/abs/2408.13986v2",
    "authors": [
      "Jie Feng",
      "Yuwei Du",
      "Jie Zhao",
      "Yong Li"
    ],
    "published": "2024-08-26",
    "abstract": "Next location prediction plays a crucial role in various real-world applications. Recently, due to the limitation of existing deep learning methods, attempts have been made to apply large language models (LLMs) to zero-shot next location prediction task. However, they directly generate the final output using LLMs without systematic design, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this paper, we introduce AgentMove, a systematic agentic prediction framework to achieve generalized next location prediction. In AgentMove, we first decompose the mobility prediction task and design specific modules to complete them, including spatial-temporal memory for individual mobility pattern mining, world knowledge generator for modeling the effects of urban structure and collective knowledge extractor for capturing the shared patterns among population. Finally, we combine the results of three modules and conduct a reasoning step to generate the final predictions. Extensive experiments utilizing mobility data from two distinct sources reveal that AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of 12 metrics and it shows robust predictions with various LLMs as base and also less geographical bias across cities. Our codes are available via https://github.com/tsinghua-fib-lab/AgentMove.",
    "categories": [
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
<<<<<<< HEAD
      "Segmentation",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities",
    "url": "http://arxiv.org/abs/2508.19305v1",
    "authors": [
      "Chen Chu",
      "Cyrus Shahabi"
    ],
    "published": "2025-08-26",
    "abstract": "Spatial representation learning is essential for GeoAI applications such as\nurban analytics, enabling the encoding of shapes, locations, and spatial\nrelationships (topological and distance-based) of geo-entities like points,\npolylines, and polygons. Existing methods either target a single geo-entity\ntype or, like Poly2Vec, decompose entities into simpler components to enable\nFourier transformation, introducing high computational cost. Moreover, since\nthe transformed space lacks geometric alignment, these methods rely on uniform,\nnon-adaptive sampling, which blurs fine-grained features like edges and\nboundaries. To address these limitations, we introduce Geo2Vec, a novel method\ninspired by signed distance fields (SDF) that operates directly in the original\nspace. Geo2Vec adaptively samples points and encodes their signed distances\n(positive outside, negative inside), capturing geometry without decomposition.\nA neural network trained to approximate the SDF produces compact,\ngeometry-aware, and unified representations for all geo-entity types.\nAdditionally, we propose a rotation-invariant positional encoding to model\nhigh-frequency spatial variations and construct a structured and robust\nembedding space for downstream GeoAI models. Empirical results show that\nGeo2Vec consistently outperforms existing methods in representing shape and\nlocation, capturing topological and distance relationships, and achieving\ngreater efficiency in real-world GeoAI applications. Code and Data can be found\nat: https://github.com/chuchen2017/GeoNeuralRepresentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "The point is the mask: scaling coral reef segmentation with weak supervision",
    "url": "http://arxiv.org/abs/2508.18958v1",
    "authors": [
      "Matteo Contini",
      "Victor Illien",
      "Sylvain Poulain",
      "Serge Bernard",
      "Julien Barde",
      "Sylvain Bonhommeau",
      "Alexis Joly"
    ],
    "published": "2025-08-26",
    "abstract": "Monitoring coral reefs at large spatial scales remains an open challenge,\nessential for assessing ecosystem health and informing conservation efforts.\nWhile drone-based aerial imagery offers broad spatial coverage, its limited\nresolution makes it difficult to reliably distinguish fine-scale classes, such\nas coral morphotypes. At the same time, obtaining pixel-level annotations over\nlarge spatial extents is costly and labor-intensive, limiting the scalability\nof deep learning-based segmentation methods for aerial imagery. We present a\nmulti-scale weakly supervised semantic segmentation framework that addresses\nthis challenge by transferring fine-scale ecological information from\nunderwater imagery to aerial data. Our method enables large-scale coral reef\nmapping from drone imagery with minimal manual annotation, combining\nclassification-based supervision, spatial interpolation and self-distillation\ntechniques. We demonstrate the efficacy of the approach, enabling large-area\nsegmentation of coral morphotypes and demonstrating flexibility for integrating\nnew classes. This study presents a scalable, cost-effective methodology for\nhigh-resolution reef monitoring, combining low-cost data collection, weakly\nsupervised deep learning and multi-scale remote sensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency",
    "url": "http://arxiv.org/abs/2508.18693v1",
    "authors": [
      "Zhitong Cheng",
      "Yiran Jiang",
      "Yulong Ge",
      "Yufeng Li",
      "Zhongheng Qin",
      "Rongzhi Lin",
      "Jianwei Ma"
    ],
    "published": "2025-08-26",
    "abstract": "Domain shift, characterized by degraded model performance during transition\nfrom labeled source domains to unlabeled target domains, poses a persistent\nchallenge for deploying deep learning systems. Current unsupervised domain\nadaptation (UDA) methods predominantly rely on fine-tuning feature extractors -\nan approach limited by inefficiency, reduced interpretability, and poor\nscalability to modern architectures.\n  Our analysis reveals that models pretrained on large-scale data exhibit\ndomain-invariant geometric patterns in their feature space, characterized by\nintra-class clustering and inter-class separation, thereby preserving\ntransferable discriminative structures. These findings indicate that domain\nshifts primarily manifest as boundary misalignment rather than feature\ndegradation.\n  Unlike fine-tuning entire pre-trained models - which risks introducing\nunpredictable feature distortions - we propose the Feature-space Planes\nSearcher (FPS): a novel domain adaptation framework that optimizes decision\nboundaries by leveraging these geometric patterns while keeping the feature\nencoder frozen. This streamlined approach enables interpretative analysis of\nadaptation while substantially reducing memory and computational costs through\noffline feature extraction, permitting full-dataset optimization in a single\ncomputation cycle.\n  Evaluations on public benchmarks demonstrate that FPS achieves competitive or\nsuperior performance to state-of-the-art methods. FPS scales efficiently with\nmultimodal large models and shows versatility across diverse domains including\nprotein structure prediction, remote sensing classification, and earthquake\ndetection. We anticipate FPS will provide a simple, effective, and\ngeneralizable paradigm for transfer learning, particularly in domain adaptation\ntasks. .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
=======
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "Forecast"
    ]
  },
  {
<<<<<<< HEAD
    "title": "Inferring geometry and material properties from Mueller matrices with machine learning",
    "url": "http://arxiv.org/abs/2508.19713v1",
    "authors": [
      "Lars Doorenbos",
      "C. H. Lucas Patty",
      "Raphael Sznitman",
      "Pablo M\u00e1rquez-Neila"
    ],
    "published": "2025-08-27",
    "abstract": "Mueller matrices (MMs) encode information on geometry and material\nproperties, but recovering both simultaneously is an ill-posed problem. We\nexplore whether MMs contain sufficient information to infer surface geometry\nand material properties with machine learning. We use a dataset of spheres of\nvarious isotropic materials, with MMs captured over the full angular domain at\nfive visible wavelengths (450-650 nm). We train machine learning models to\npredict material properties and surface normals using only these MMs as input.\nWe demonstrate that, even when the material type is unknown, surface normals\ncan be predicted and object geometry reconstructed. Moreover, MMs allow models\nto identify material types correctly. Further analyses show that diagonal\nelements are key for material characterization, and off-diagonal elements are\ndecisive for normal estimation.",
    "categories": [
      "remote_sensing"
=======
    "title": "An Autonomous GIS Agent Framework for Geospatial Data Retrieval",
    "url": "http://arxiv.org/abs/2407.21024v2",
    "authors": [
      "Huan Ning",
      "Zhenlong Li",
      "Temitope Akinboyewa",
      "M. Naser Lessani"
    ],
    "published": "2024-07-13",
    "abstract": "Powered by the emerging large language models (LLMs), autonomous geographic information systems (GIS) agents have the potential to accomplish spatial analyses and cartographic tasks. However, a research gap exists to support fully autonomous GIS agents: how to enable agents to discover and download the necessary data for geospatial analyses. This study proposes an autonomous GIS agent framework capable of retrieving required geospatial data by generating, executing, and debugging programs. The framework utilizes the LLM as the decision-maker, selects the appropriate data source (s) from a pre-defined source list, and fetches the data from the chosen source. Each data source has a handbook that records the metadata and technical details for data retrieval. The proposed framework is designed in a plug-and-play style to ensure flexibility and extensibility. Human users or autonomous data scrawlers can add new data sources by adding new handbooks. We developed a prototype agent based on the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a Python program. Experiment results demonstrate its capability of retrieving data from various sources including OpenStreetMap, administrative boundaries and demographic data from the US Census Bureau, satellite basemaps from ESRI World Imagery, global digital elevation model (DEM) from OpenTopography.org, weather data from a commercial provider, the COVID-19 cases from the NYTimes GitHub. Our study is among the first attempts to develop an autonomous geospatial data retrieval agent.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Geospatial Trajectory Generation via Efficient Abduction: Deployment for Independent Testing",
    "url": "http://arxiv.org/abs/2407.06447v2",
    "authors": [
      "Divyagna Bavikadi",
      "Dyuman Aditya",
      "Devendra Parkar",
      "Paulo Shakarian",
      "Graham Mueller",
      "Chad Parvis",
      "Gerardo I. Simari"
    ],
    "published": "2024-07-08",
    "abstract": "The ability to generate artificial human movement patterns while meeting location and time constraints is an important problem in the security community, particularly as it enables the study of the analog problem of detecting such patterns while maintaining privacy.  We frame this problem as an instance of abduction guided by a novel parsimony function represented as an aggregate truth value over an annotated logic program.  This approach has the added benefit of affording explainability to an analyst user.  By showing that any subset of such a program can provide a lower bound on this parsimony requirement, we are able to abduce movement trajectories efficiently through an informed (i.e., A*) search.  We describe how our implementation was enhanced with the application of multiple techniques in order to be scaled and integrated with a cloud-based software stack that included bottom-up rule learning, geolocated knowledge graph retrieval/management, and interfaces with government systems for independently conducted government-run tests for which we provide results.  We also report on our own experiments showing that we not only provide exact results but also scale to very large scenarios and provide realistic agent trajectories that can go undetected by machine learning anomaly detectors.",
    "categories": [
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "Global Forecasting of Tropical Cyclone Intensity Using Neural Weather Models",
    "url": "http://arxiv.org/abs/2508.17903v2",
    "authors": [
      "Milton Gomez",
      "Louis Poulain--Auzeau",
      "Alexis Berne",
      "Tom Beucler"
    ],
    "published": "2025-08-25",
    "abstract": "Numerical Weather Prediction (NWP) models that integrate coupled physical\nequations forward in time are the traditional tools for simulating atmospheric\nprocesses and forecasting weather. With recent advancements in deep learning,\nNeural Weather Models (NeWMs) have emerged as competent medium-range NWP\nemulators, with performances that compare favorably to state-of-the-art NWP\nmodels. However, they are commonly trained on reanalyses with limited spatial\nresolution (e.g., 0.25{\\deg} horizontal grid spacing), which smooths out key\nfeatures of weather systems. For example, tropical cyclones (TCs)-among the\nmost impactful weather events due to their devastating effects on human\nactivities-are challenging to forecast, as extrema like wind gusts, used as\nproxies for TC intensity, are smoothed in deterministic forecasts at 0.25{\\deg}\nresolution. To address this, we use our best observational estimates of wind\ngusts and minimum sea level pressure to train a hierarchy of post-processing\nmodels on NeWM outputs. Applied to Pangu-Weather and FourCastNet v2, the\npost-processing models produce accurate and reliable forecasts of TC intensity\nup to five days ahead. Our post-processing algorithm is tracking-independent,\npreventing full misses, and we demonstrate that even linear models extract\npredictive information from NeWM outputs beyond what is encoded in their\ninitial conditions. While spatial masking improves probabilistic forecast\nconsistency, we do not find clear advantages of convolutional architectures\nover simple multilayer perceptrons for our NeWM post-processing purposes.\nOverall, by combining the efficiency of NeWMs with a lightweight,\ntracking-independent post-processing framework, our approach improves the\naccessibility of global TC intensity forecasts, marking a step toward their\ndemocratization.",
    "categories": [
      "ocean"
=======
    "title": "IQLS: Framework for leveraging Metadata to enable Large Language Model based queries to complex, versatile Data",
    "url": "http://arxiv.org/abs/2405.15792v1",
    "authors": [
      "Sami Azirar",
      "Hossam A. Gabbar",
      "Chaouki Regoui"
    ],
    "published": "2024-05-04",
    "abstract": "As the amount and complexity of data grows, retrieving it has become a more difficult task that requires greater knowledge and resources. This is especially true for the logistics industry, where new technologies for data collection provide tremendous amounts of interconnected real-time data. The Intelligent Query and Learning System (IQLS) simplifies the process by allowing natural language use to simplify data retrieval . It maps structured data into a framework based on the available metadata and available data models. This framework creates an environment for an agent powered by a Large Language Model. The agent utilizes the hierarchical nature of the data to filter iteratively by making multiple small context-aware decisions instead of one-shot data retrieval. After the Data filtering, the IQLS enables the agent to fulfill tasks given by the user query through interfaces. These interfaces range from multimodal transportation information retrieval to route planning under multiple constraints. The latter lets the agent define a dynamic object, which is determined based on the query parameters. This object represents a driver capable of navigating a road network. The road network is depicted as a graph with attributes based on the data. Using a modified version of the Dijkstra algorithm, the optimal route under the given constraints can be determined. Throughout the entire process, the user maintains the ability to interact and guide the system. The IQLS is showcased in a case study on the Canadian logistics sector, allowing geospatial, visual, tabular and text data to be easily queried semantically in natural language.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Evaluating Tool-Augmented Agents in Remote Sensing Platforms",
    "url": "http://arxiv.org/abs/2405.00709v1",
    "authors": [
      "Simranjit Singh",
      "Michael Fore",
      "Dimitrios Stamoulis"
    ],
    "published": "2024-04-23",
    "abstract": "Tool-augmented Large Language Models (LLMs) have shown impressive capabilities in remote sensing (RS) applications. However, existing benchmarks assume question-answering input templates over predefined image-text data pairs. These standalone instructions neglect the intricacies of realistic user-grounded tasks. Consider a geospatial analyst: they zoom in a map area, they draw a region over which to collect satellite imagery, and they succinctly ask \"Detect all objects here\". Where is `here`, if it is not explicitly hardcoded in the image-text template, but instead is implied by the system state, e.g., the live map positioning? To bridge this gap, we present GeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual, and click-based actions on a real UI platform. Through in-depth evaluation of state-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights towards stronger agents for RS applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots",
    "url": "http://arxiv.org/abs/2404.15500v1",
    "authors": [
      "Simranjit Singh",
      "Michael Fore",
      "Dimitrios Stamoulis"
    ],
    "published": "2024-04-23",
    "abstract": "Geospatial Copilots unlock unprecedented potential for performing Earth Observation (EO) applications through natural language instructions. However, existing agents rely on overly simplified single tasks and template-based prompts, creating a disconnect with real-world scenarios. In this work, we present GeoLLM-Engine, an environment for tool-augmented agents with intricate tasks routinely executed by analysts on remote sensing platforms. We enrich our environment with geospatial API tools, dynamic maps/UIs, and external multimodal knowledge bases to properly gauge an agent's proficiency in interpreting realistic high-level natural language commands and its functional correctness in task completions. By alleviating overheads typically associated with human-in-the-loop benchmark curation, we harness our massively parallel engine across 100 GPT-4-Turbo nodes, scaling to over half a million diverse multi-tool tasks and across 1.1 million satellite images. By moving beyond traditional single-task image-caption paradigms, we investigate state-of-the-art agents and prompting techniques against long-horizon prompts.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions",
    "url": "http://arxiv.org/abs/2402.16364v2",
    "authors": [
      "Tzuf Paz-Argaman",
      "Sayali Kulkarni",
      "John Palowitch",
      "Jason Baldridge",
      "Reut Tsarfaty"
    ],
    "published": "2024-02-26",
    "abstract": "When communicating routes in natural language, the concept of acquired spatial knowledge is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are typically non-sequential, contain allocentric relations, with multiple spatial relations and implicit actions, without any explicit verification. This paper introduces the Rendezvous (RVS) task and dataset, which includes 10,404 examples of English geospatial instructions for reaching a target location using map-knowledge. Our analysis reveals that RVS exhibits a richer use of spatial allocentric relations, and requires resolving more spatial relations simultaneously compared to previous text-based navigation benchmarks.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction",
    "url": "http://arxiv.org/abs/2402.06861v2",
    "authors": [
      "Yansong Ning",
      "Hao Liu"
    ],
    "published": "2024-02-10",
    "abstract": "Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments",
    "url": "http://arxiv.org/abs/2401.04290v1",
    "authors": [
      "Sean Kulinski",
      "Nicholas R. Waytowich",
      "James Z. Hare",
      "David I. Inouye"
    ],
    "published": "2024-01-09",
    "abstract": "Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com",
    "categories": [
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [],
    "applications": [
      "Forecast",
<<<<<<< HEAD
      "Tracking"
    ]
  },
  {
    "title": "Huracan: A skillful end-to-end data-driven system for ensemble data assimilation and weather prediction",
    "url": "http://arxiv.org/abs/2508.18486v1",
    "authors": [
      "Zekun Ni",
      "Jonathan Weyn",
      "Hang Zhang",
      "Yanfei Xiang",
      "Jiang Bian",
      "Weixin Jin",
      "Kit Thambiratnam",
      "Qi Zhang",
      "Haiyu Dong",
      "Hongyu Sun"
    ],
    "published": "2025-08-25",
    "abstract": "Over the past few years, machine learning-based data-driven weather\nprediction has been transforming operational weather forecasting by providing\nmore accurate forecasts while using a mere fraction of computing power compared\nto traditional numerical weather prediction (NWP). However, those models still\nrely on initial conditions from NWP, putting an upper limit on their forecast\nabilities. A few end-to-end systems have since been proposed, but they have yet\nto match the forecast skill of state-of-the-art NWP competitors. In this work,\nwe propose Huracan, an observation-driven weather forecasting system which\ncombines an ensemble data assimilation model with a forecast model to produce\nhighly accurate forecasts relying only on observations as inputs. Huracan is\nnot only the first to provide ensemble initial conditions and end-to-end\nensemble weather forecasts, but also the first end-to-end system to achieve an\naccuracy comparable with that of ECMWF ENS, the state-of-the-art NWP\ncompetitor, despite using a smaller amount of available observation data.\nNotably, Huracan matches or exceeds the continuous ranked probability score of\nECMWF ENS on 75.4% of the variable and lead time combinations. Our work is a\nmajor step forward in end-to-end data-driven weather prediction and opens up\nopportunities for further improving and revolutionizing operational weather\nforecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Finetuning AI Foundation Models to Develop Subgrid-Scale Parameterizations: A Case Study on Atmospheric Gravity Waves",
    "url": "http://arxiv.org/abs/2509.03816v1",
    "authors": [
      "Aman Gupta",
      "Aditi Sheshadri",
      "Sujit Roy",
      "Johannes Schmude",
      "Vishal Gaur",
      "Wei Ji Leong",
      "Manil Maskey",
      "Rahul Ramachandran"
    ],
    "published": "2025-09-04",
    "abstract": "Global climate models parameterize a range of atmospheric-oceanic processes\nlike gravity waves, clouds, moist convection, and turbulence that cannot be\nsufficiently resolved. These subgrid-scale closures for unresolved processes\nare a leading source of model uncertainty. Here, we present a new approach to\ndeveloping machine learning parameterizations of small-scale climate processes\nby fine-tuning a pre-trained AI foundation model (FM). FMs are largely\nunexplored in climate research. A pre-trained encoder-decoder from a 2.3\nbillion parameter FM (NASA and IBM Research's Prithvi WxC) -- which contains a\nlatent probabilistic representation of atmospheric evolution -- is fine-tuned\n(or reused) to create a deep learning parameterization for atmospheric gravity\nwaves (GWs). The parameterization captures GW effects for a coarse-resolution\nclimate model by learning the fluxes from an atmospheric reanalysis with 10\ntimes finer resolution. A comparison of monthly averages and instantaneous\nevolution with a machine learning model baseline (an Attention U-Net) reveals\nsuperior predictive performance of the FM parameterization throughout the\natmosphere, even in regions excluded from pre-training. This performance boost\nis quantified using the Hellinger distance, which is 0.11 for the baseline and\n0.06 for the fine-tuned model. Our findings emphasize the versatility and\nreusability of FMs, which could be used to accomplish a range of atmosphere-\nand climate-related applications, leading the way for the creation of\nobservations-driven and physically accurate parameterizations for more\nearth-system processes.",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework",
    "url": "http://arxiv.org/abs/2509.01910v1",
    "authors": [
      "Furong Jia",
      "Lanxin Liu",
      "Ce Hou",
      "Fan Zhang",
      "Xinyan Liu",
      "Yu Liu"
    ],
    "published": "2025-09-02",
    "abstract": "Worldwide geo-localization involves determining the exact geographic location\nof images captured globally, typically guided by geographic cues such as\nclimate, landmarks, and architectural styles. Despite advancements in\ngeo-localization models like GeoCLIP, which leverages images and location\nalignment via contrastive learning for accurate predictions, the\ninterpretability of these models remains insufficiently explored. Current\nconcept-based interpretability methods fail to align effectively with\nGeo-alignment image-location embedding objectives, resulting in suboptimal\ninterpretability and performance. To address this gap, we propose a novel\nframework integrating global geo-localization with concept bottlenecks. Our\nmethod inserts a Concept-Aware Alignment Module that jointly projects image and\nlocation embeddings onto a shared bank of geographic concepts (e.g., tropical\nclimate, mountain, cathedral) and minimizes a concept-level loss, enhancing\nalignment in a concept-specific subspace and enabling robust interpretability.\nTo our knowledge, this is the first work to introduce interpretability into\ngeo-localization. Extensive experiments demonstrate that our approach surpasses\nGeoCLIP in geo-localization accuracy and boosts performance across diverse\ngeospatial prediction tasks, revealing richer semantic insights into geographic\ndecision-making processes.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2509.03961v1",
    "authors": [
      "Yijun Zhou",
      "Yikui Zhai",
      "Zilu Ying",
      "Tingfeng Xian",
      "Wenlve Zhou",
      "Zhiheng Zhou",
      "Xiaolin Tian",
      "Xudong Jia",
      "Hongsheng Zhang",
      "C. L. Philip Chen"
    ],
    "published": "2025-09-04",
    "abstract": "Although deep learning has advanced remote sensing change detection (RSCD),\nmost methods rely solely on image modality, limiting feature representation,\nchange pattern modeling, and generalization especially under illumination and\nnoise disturbances. To address this, we propose MMChange, a multimodal RSCD\nmethod that combines image and text modalities to enhance accuracy and\nrobustness. An Image Feature Refinement (IFR) module is introduced to highlight\nkey regions and suppress environmental noise. To overcome the semantic\nlimitations of image features, we employ a vision language model (VLM) to\ngenerate semantic descriptions of bitemporal images. A Textual Difference\nEnhancement (TDE) module then captures fine grained semantic shifts, guiding\nthe model toward meaningful changes. To bridge the heterogeneity between\nmodalities, we design an Image Text Feature Fusion (ITFF) module that enables\ndeep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and\nSYSUCD demonstrate that MMChange consistently surpasses state of the art\nmethods across multiple metrics, validating its effectiveness for multimodal\nRSCD. Code is available at: https://github.com/yikuizhai/MMChange.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing",
    "url": "http://arxiv.org/abs/2509.03376v1",
    "authors": [
      "Hui Chen",
      "Liangyu Liu",
      "Xianchao Xiu",
      "Wanquan Liu"
    ],
    "published": "2025-09-03",
    "abstract": "Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote\nsensing images into a set of endmembers and their corresponding abundances.\nDespite significant progress in this field using deep learning, most methods\nfail to simultaneously characterize global dependencies and local consistency,\nmaking it difficult to preserve both long-range interactions and boundary\ndetails. This letter proposes a novel transformer-guided content-adaptive graph\nunmixing framework (T-CAGU), which overcomes these challenges by employing a\ntransformer to capture global dependencies and introducing a content-adaptive\ngraph neural network to enhance local relationships. Unlike previous work,\nT-CAGU integrates multiple propagation orders to dynamically learn the graph\nstructure, ensuring robustness against noise. Furthermore, T-CAGU leverages a\ngraph residual mechanism to preserve global information and stabilize training.\nExperimental results demonstrate its superiority over the state-of-the-art\nmethods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Information transmission: Inferring change area from change moment in time series remote sensing images",
    "url": "http://arxiv.org/abs/2509.03112v1",
    "authors": [
      "Jialu Li",
      "Chen Wu",
      "Meiqi Hu"
    ],
    "published": "2025-09-03",
    "abstract": "Time series change detection is a critical task for exploring ecosystem\ndynamics using time series remote sensing images, because it can simultaneously\nindicate where and when change occur. While deep learning has shown excellent\nperformance in this domain, it continues to approach change area detection and\nchange moment identification as distinct tasks. Given that change area can be\ninferred from change moment, we propose a time series change detection network,\nnamed CAIM-Net (Change Area Inference from Moment Network), to ensure\nconsistency between change area and change moment results. CAIM-Net infers\nchange area from change moment based on the intrinsic relationship between time\nseries analysis and spatial change detection. The CAIM-Net comprises three key\nsteps: Difference Extraction and Enhancement, Coarse Change Moment Extraction,\nand Fine Change Moment Extraction and Change Area Inference. In the Difference\nExtraction and Enhancement, a lightweight encoder with batch dimension stacking\nis designed to rapidly extract difference features. Subsequently, boundary\nenhancement convolution is applied to amplify these difference features. In the\nCoarse Change Moment Extraction, the enhanced difference features from the\nfirst step are used to spatiotemporal correlation analysis, and then two\ndistinct methods are employed to determine coarse change moments. In the Fine\nChange Moment Extraction and Change Area Inference, a multiscale temporal Class\nActivation Mapping (CAM) module first increases the weight of the\nchange-occurring moment from coarse change moments. Then the weighted change\nmoment is used to infer change area based on the fact that pixels with the\nchange moment must have undergone a change.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision",
    "url": "http://arxiv.org/abs/2509.01882v2",
    "authors": [
      "Shubham Laxmikant Deshmukh",
      "Matthew Wilchek",
      "Feras A. Batarseh"
    ],
    "published": "2025-09-02",
    "abstract": "Ongoing advancements in computer vision, particularly in pattern recognition\nand scene classification, have enabled new applications in environmental\nmonitoring. Deep learning now offers non-contact methods for assessing water\nquality and detecting contamination, both critical for disaster response and\npublic health protection. This work introduces HydroVision, a deep\nlearning-based scene classification framework that estimates optically active\nwater quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored\nDissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and\nTurbidity from standard Red-Green-Blue (RGB) images of surface water.\nHydroVision supports early detection of contamination trends and strengthens\nmonitoring by regulatory agencies during external environmental stressors,\nindustrial activities, and force majeure events. The model is trained on more\nthan 500,000 seasonally varied images collected from the United States\nGeological Survey Hydrologic Imagery Visualization and Information System\nbetween 2022 and 2024. This approach leverages widely available RGB imagery as\na scalable, cost-effective alternative to traditional multispectral and\nhyperspectral remote sensing. Four state-of-the-art convolutional neural\nnetworks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer\nare evaluated through transfer learning to identify the best-performing\narchitecture. DenseNet121 achieves the highest validation performance, with an\nR2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for\nreal-world water quality monitoring across diverse conditions. While the\ncurrent model is optimized for well-lit imagery, future work will focus on\nimproving robustness under low-light and obstructed scenarios to expand its\noperational utility.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "VGG",
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Classification",
      "Recognition"
    ]
  },
  {
    "title": "SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment",
    "url": "http://arxiv.org/abs/2509.01183v1",
    "authors": [
      "Bingnan Yang",
      "Mi Zhang",
      "Zhili Zhang",
      "Zhan Zhang",
      "Yuanxin Zhao",
      "Xiangyun Hu",
      "Jianya Gong"
    ],
    "published": "2025-09-01",
    "abstract": "High-quality image segmentation is fundamental to pixel-level geospatial\nanalysis in remote sensing, necessitating robust segmentation quality\nassessment (SQA), particularly in unsupervised settings lacking ground truth.\nAlthough recent deep learning (DL) based unsupervised SQA methods show\npotential, they often suffer from coarse evaluation granularity, incomplete\nassessments, and poor transferability. To overcome these limitations, this\npaper introduces Panoramic Quality Mapping (PQM) as a new paradigm for\ncomprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning\nframework realizing this approach. SegAssess distinctively formulates SQA as a\nfine-grained, four-class panoramic segmentation task, classifying pixels within\na segmentation mask under evaluation into true positive (TP), false positive\n(FP), true negative (TN), and false negative (FN) categories, thereby\ngenerating a complete quality map. Leveraging an enhanced Segment Anything\nModel (SAM) architecture, SegAssess uniquely employs the input mask as a prompt\nfor effective feature integration via cross-attention. Key innovations include\nan Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF)\nmodule to refine predictions near challenging object edges, and an Augmented\nMixup Sampling (AMS) training strategy integrating multi-source masks to\nsignificantly boost cross-domain robustness and zero-shot transferability.\nComprehensive experiments across 32 datasets derived from 6 sources demonstrate\nthat SegAssess achieves state-of-the-art (SOTA) performance and exhibits\nremarkable zero-shot transferability to unseen masks, establishing PQM via\nSegAssess as a robust and transferable solution for unsupervised SQA. The code\nis available at https://github.com/Yangbn97/SegAssess.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification",
    "url": "http://arxiv.org/abs/2509.00677v1",
    "authors": [
      "Qingyu Wang",
      "Xue Jiang",
      "Guozheng Xu"
    ],
    "published": "2025-08-31",
    "abstract": "Multimodal fusion has made great progress in the field of remote sensing\nimage classification due to its ability to exploit the complementary\nspatial-spectral information. Deep learning methods such as CNN and Transformer\nhave been widely used in these domains. State Space Models recently highlighted\nthat prior methods suffer from quadratic computational complexity. As a result,\nmodeling longer-range dependencies of spatial-spectral features imposes an\noverwhelming burden on the network. Mamba solves this problem by incorporating\ntime-varying parameters into ordinary SSM and performing hardware optimization,\nbut it cannot perform feature fusion directly. In order to make full use of\nMamba's low computational burden and explore the potential of internal\nstructure in multimodal feature fusion, we propose Cross State Fusion Mamba\n(CSFMamba) Network. Specifically, we first design the preprocessing module of\nremote sensing image information for the needs of Mamba structure, and combine\nit with CNN to extract multi-layer features. Secondly, a cross-state module\nbased on Mamba operator is creatively designed to fully fuse the feature of the\ntwo modalities. The advantages of Mamba and CNN are combined by designing a\nmore powerful backbone. We capture the fusion relationship between HSI and\nLiDAR modalities with stronger full-image understanding. The experimental\nresults on two datasets of MUUFL and Houston2018 show that the proposed method\noutperforms the experimental results of Transformer under the premise of\nreducing the network training burden.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation",
    "url": "http://arxiv.org/abs/2509.00598v1",
    "authors": [
      "Boyi Li",
      "Ce Zhang",
      "Richard M. Timmerman",
      "Wenxuan Bao"
    ],
    "published": "2025-08-30",
    "abstract": "The emergence of vision language models (VLMs) has bridged vision and\nlanguage, enabling joint multimodal understanding beyond traditional\nvisual-only deep learning models. However, transferring VLMs from the natural\nimage domain to remote sensing (RS) segmentation remains challenging due to the\nlimited category diversity in RS datasets and the domain gap between natural\nand RS imagery. Here, we propose a training-free framework, DGL-RSIS, that\ndecouples visual and textual inputs, performing visual-language alignment at\nboth the local semantic and global contextual levels through tailored\nstrategies. Specifically, we first introduce a global-local decoupling (GLD)\nmodule, where text inputs are divided into local class nouns and global\nmodifiers using natural language processing (NLP) techniques; image inputs are\npartitioned into a set of class-agnostic mask proposals via unsupervised mask\nproposal networks. Second, visual and textual features are aligned at local\nscale, through a novel context-aware cropping strategy for extracting image\npatches with proper boundaries and introducing RS-specific knowledge to enrich\nthe text inputs. By matching the enhanced text features with mask-guided visual\nfeatures, we enable the mask classification, supporting open-vocabulary\nsemantic segmentation (OVSS). Third, at the global scale, we propose a\nCross-Scale Grad-CAM module to refine Grad-CAM maps using contextual\ninformation from global modifiers. A subsequent mask selection module\nintegrates pixel-level Grad-CAM activations into the mask-level segmentation\noutput, such that accurate and interpretable alignment can be realized across\nglobal and local dimensions for referring expression segmentation (RES).",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "DAOVI: Distortion-Aware Omnidirectional Video Inpainting",
    "url": "http://arxiv.org/abs/2509.00396v1",
    "authors": [
      "Ryosuke Seshimo",
      "Mariko Isogawa"
    ],
    "published": "2025-08-30",
    "abstract": "Omnidirectional videos that capture the entire surroundings are employed in a\nvariety of fields such as VR applications and remote sensing. However, their\nwide field of view often causes unwanted objects to appear in the videos. This\nproblem can be addressed by video inpainting, which enables the natural removal\nof such objects while preserving both spatial and temporal consistency.\nNevertheless, most existing methods assume processing ordinary videos with a\nnarrow field of view and do not tackle the distortion in equirectangular\nprojection of omnidirectional videos. To address this issue, this paper\nproposes a novel deep learning model for omnidirectional video inpainting,\ncalled Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI\nintroduces a module that evaluates temporal motion information in the image\nspace considering geodesic distance, as well as a depth-aware feature\npropagation module in the feature space that is designed to address the\ngeometric distortion inherent to omnidirectional videos. The experimental\nresults demonstrate that our proposed method outperforms existing methods both\nquantitatively and qualitatively.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena",
    "url": "http://arxiv.org/abs/2509.03500v1",
    "authors": [
      "Itai Zilberstein",
      "Alberto Candela",
      "Steve Chien"
    ],
    "published": "2025-09-03",
    "abstract": "Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Invariant Features for Global Crop Type Classification",
    "url": "http://arxiv.org/abs/2509.03497v2",
    "authors": [
      "Xin-Yi Tong",
      "Sherrie Wang"
    ],
    "published": "2025-09-03",
    "abstract": "Accurately obtaining crop type and its spatial distribution at a global scale\nis critical for food security, agricultural policy-making, and sustainable\ndevelopment. Remote sensing offers an efficient solution for large-scale crop\nclassification, but the limited availability of reliable ground samples in many\nregions constrains applicability across geographic areas. To address\nperformance declines under geospatial shifts, this study identifies remote\nsensing features that are invariant to geographic variation and proposes\nstrategies to enhance cross-regional generalization. We construct CropGlobe, a\nglobal crop type dataset with 300,000 pixel-level samples from eight countries\nacross five continents, covering six major food and industrial crops (corn,\nsoybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage,\nCropGlobe enables a systematic evaluation under cross-country, cross-continent,\nand cross-hemisphere transfer. We compare the transferability of temporal\nmulti-spectral features (Sentinel-2-based 1D/2D median features and harmonic\ncoefficients) and hyperspectral features (from EMIT). To improve generalization\nunder spectral and phenological shifts, we design CropNet, a lightweight and\nrobust CNN tailored for pixel-level crop classification, coupled with temporal\ndata augmentation (time shift, time scale, and magnitude warping) that\nsimulates realistic cross-regional phenology. Experiments show that 2D median\ntemporal features from Sentinel-2 consistently exhibit the strongest invariance\nacross all transfer scenarios, and augmentation further improves robustness,\nparticularly when training data diversity is limited. Overall, the work\nidentifies more invariant feature representations that enhance geographic\ntransferability and suggests a promising path toward scalable, low-cost crop\ntype applications across globally diverse regions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "From Image Denoisers to Regularizing Imaging Inverse Problems: An Overview",
    "url": "http://arxiv.org/abs/2509.03475v1",
    "authors": [
      "Hong Ye Tan",
      "Subhadip Mukherjee",
      "Junqi Tang"
    ],
    "published": "2025-09-03",
    "abstract": "Inverse problems lie at the heart of modern imaging science, with broad\napplications in areas such as medical imaging, remote sensing, and microscopy.\nRecent years have witnessed a paradigm shift in solving imaging inverse\nproblems, where data-driven regularizers are used increasingly, leading to\nremarkably high-fidelity reconstruction. A particularly notable approach for\ndata-driven regularization is to use learned image denoisers as implicit priors\nin iterative image reconstruction algorithms. This survey presents a\ncomprehensive overview of this powerful and emerging class of algorithms,\ncommonly referred to as plug-and-play (PnP) methods. We begin by providing a\nbrief background on image denoising and inverse problems, followed by a short\nreview of traditional regularization strategies. We then explore how proximal\nsplitting algorithms, such as the alternating direction method of multipliers\n(ADMM) and proximal gradient descent (PGD), can naturally accommodate learned\ndenoisers in place of proximal operators, and under what conditions such\nreplacements preserve convergence. The role of Tweedie's formula in connecting\noptimal Gaussian denoisers and score estimation is discussed, which lays the\nfoundation for regularization-by-denoising (RED) and more recent\ndiffusion-based posterior sampling methods. We discuss theoretical advances\nregarding the convergence of PnP algorithms, both within the RED and proximal\nsettings, emphasizing the structural assumptions that the denoiser must satisfy\nfor convergence, such as non-expansiveness, Lipschitz continuity, and local\nhomogeneity. We also address practical considerations in algorithm design,\nincluding choices of denoiser architecture and acceleration strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "LUCIE-3D: A three-dimensional climate emulator for forced responses",
    "url": "http://arxiv.org/abs/2509.02061v1",
    "authors": [
      "Haiwen Guan",
      "Troy Arcomano",
      "Ashesh Chattopadhyay",
      "Romit Maulik"
    ],
    "published": "2025-09-02",
    "abstract": "We introduce LUCIE-3D, a lightweight three-dimensional climate emulator\ndesigned to capture the vertical structure of the atmosphere, respond to\nclimate change forcings, and maintain computational efficiency with long-term\nstability. Building on the original LUCIE-2D framework, LUCIE-3D employs a\nSpherical Fourier Neural Operator (SFNO) backbone and is trained on 30 years of\nERA5 reanalysis data spanning eight vertical {\\sigma}-levels. The model\nincorporates atmospheric CO2 as a forcing variable and optionally integrates\nprescribed sea surface temperature (SST) to simulate coupled ocean--atmosphere\ndynamics. Results demonstrate that LUCIE-3D successfully reproduces\nclimatological means, variability, and long-term climate change signals,\nincluding surface warming and stratospheric cooling under increasing CO2\nconcentrations. The model further captures key dynamical processes such as\nequatorial Kelvin waves, the Madden--Julian Oscillation, and annular modes,\nwhile showing credible behavior in the statistics of extreme events. Despite\nrequiring longer training than its 2D predecessor, LUCIE-3D remains efficient,\ntraining in under five hours on four GPUs. Its combination of stability,\nphysical consistency, and accessibility makes it a valuable tool for rapid\nexperimentation, ablation studies, and the exploration of coupled climate\ndynamics, with potential applications extending to paleoclimate research and\nfuture Earth system emulation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "An Observations-focused Assessment of Global AI Weather Prediction Models During the South Asian Monsoon",
    "url": "http://arxiv.org/abs/2509.01879v1",
    "authors": [
      "Aman Gupta",
      "Aditi Sheshadri",
      "Dhruv Suri"
    ],
    "published": "2025-09-02",
    "abstract": "Seven state-of-the-art AI weather models (FourCastNet, FourCastNet-SFNO,\nPangu-Weather, GraphCast, Aurora, AIFS, and GenCast) are evaluated against\nobservational data during the South Asian Monsoon. The models are tested on\ntemperature, winds, global kinetic energy spectrum, regional precipitation,\ncloud cover, cyclone trajectory prediction, and hyperlocal predictions around\nextreme weather events. The models forecast large-scale dynamics with\nreasonable accuracy, but fall short on key metrics critical to Monsoon-time\nweather prediction. The models exhibit substantially higher errors when\ncompared against ground-based weather station data than against reanalysis or\nconventional forecasts. The AI weather prediction models show key differences\nin mesoscale kinetic energy and extreme precipitation during the Monsoon, and\npredict markedly different Monsoon-time cyclone trajectories over the Indian\nsubcontinent, raising questions about their readiness for operational\napplications. Our analysis finds that ECMWF's deterministic AIFS model offers\nthe most reliable performance and usability, with GraphCast and GenCast being\nclose seconds.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "AT Loss: Advanced Torrential Loss Function for Precipitation Forecasting",
    "url": "http://arxiv.org/abs/2509.01348v1",
    "authors": [
      "Jaeho Choi",
      "Hyeri Kim",
      "Kwang-Ho Kim",
      "Jaesung Lee"
    ],
    "published": "2025-09-01",
    "abstract": "Accurate precipitation forecasting is becoming increasingly important in the\ncontext of climate change. In response, machine learning-based approaches have\nrecently gained attention as an emerging alternative to traditional methods\nsuch as numerical weather prediction and climate models. Nonetheless, many\nrecent approaches still rely on off-the-shelf loss functions, and even the more\nadvanced ones merely involve optimization processes based on the critical\nsuccess index (CSI). The problem, however, is that CSI may become ineffective\nduring extended dry periods when precipitation remains below the threshold,\nrendering it less than ideal as a criterion for optimization. To address this\nlimitation, we introduce a simple penalty expression and reinterpret it as a\nquadratic unconstrained binary optimization (QUBO) formulation. Ultimately, the\nresulting QUBO formulation is relaxed into a differentiable advanced torrential\n(AT) loss function through an approximation process. The proposed AT loss\ndemonstrates its superiority through the Lipschitz constant, forecast\nperformance evaluations, consistency experiments, and ablation studies with the\noperational model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Imputing Missing Long-Term Spatiotemporal Multivariate Atmospheric Data with CNN-Transformer Machine Learning",
    "url": "http://arxiv.org/abs/2509.01141v1",
    "authors": [
      "Jiahui Hu",
      "Wenjun Dong",
      "Alan Z. Liu"
    ],
    "published": "2025-09-01",
    "abstract": "Continuous physical domains are important for scientific investigations of\ndynamical processes in the atmosphere. However, missing data arising from\noperational constraints and adverse environmental conditions pose significant\nchallenges to accurate analysis and modeling. To address this limitation, we\npropose a novel hybrid Convolutional Neural Network (CNN) Transformer machine\nlearning model for multivariable atmospheric data imputation, termed CT-MVP.\nThis framework integrates CNNs for local feature extraction with transformers\nfor capturing long-range dependencies across time and altitude. The model is\ntrained and evaluated on a testbed using the Specified Dynamics Whole\nAtmosphere Community Climate Model with thermosphere and ionosphere extension\n(SD-WACCM-X) dataset spanning 13 years, which provides continuous global\ncoverage of atmospheric variables, including temperature and zonal and\nmeridional winds. This setup ensures that the ML approach can be rigorously\nassessed under diverse data-gap conditions. The hybrid framework enables\neffective reconstruction of missing values in high-dimensional atmospheric\ndatasets, with comparative evaluations against traditional methods and a simple\ntransformer. The results demonstrate that CT-MVP achieves superior performance\ncompared with traditional approaches, particularly in cases involving extended\nperiods of missing data, and slightly outperforms a simple transformer with the\nsame hyper-parameters.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef",
    "url": "http://arxiv.org/abs/2509.01019v1",
    "authors": [
      "Scarlett Raine",
      "Benjamin Moshirian",
      "Tobias Fischer"
    ],
    "published": "2025-08-31",
    "abstract": "Coral reefs are on the brink of collapse, with climate change, ocean\nacidification, and pollution leading to a projected 70-90% loss of coral\nspecies within the next decade. Restoration efforts are crucial, but their\nsuccess hinges on introducing automation to upscale efforts. We present\nautomated deployment of coral re-seeding devices powered by artificial\nintelligence, computer vision, and robotics. Specifically, we perform automated\nsubstrate classification, enabling detection of areas of the seafloor suitable\nfor coral growth, thus significantly reducing reliance on human experts and\nincreasing the range and efficiency of restoration. Real-world testing of the\nalgorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,\nsub-image patch classification of 89.1%, and real-time model inference at 5.5\nframes per second. Further, we present and publicly contribute a large\ncollection of annotated substrate image data to foster future research in this\narea.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India",
    "url": "http://arxiv.org/abs/2509.00653v1",
    "authors": [
      "Tung Nguyen",
      "Harkanwar Singh",
      "Nilay Naharas",
      "Lucas Bandarkar",
      "Aditya Grover"
    ],
    "published": "2025-08-31",
    "abstract": "Regional weather forecasting is a critical problem for localized climate\nadaptation, disaster mitigation, and sustainable development. While machine\nlearning has shown impressive progress in global weather forecasting, regional\nforecasting remains comparatively underexplored. Existing efforts often use\ndifferent datasets and experimental setups, limiting fair comparison and\nreproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for\ndata-driven regional weather forecasting focused on the Indian subcontinent.\nIndiaWeatherBench provides a curated dataset built from high-resolution\nregional reanalysis products, along with a suite of deterministic and\nprobabilistic metrics to facilitate consistent training and evaluation. To\nestablish strong baselines, we implement and evaluate a range of models across\ndiverse architectures, including UNets, Transformers, and Graph-based networks,\nas well as different boundary conditioning strategies and training objectives.\nWhile focused on India, IndiaWeatherBench is easily extensible to other\ngeographic regions. We open-source all raw and preprocessed datasets, model\nimplementations, and evaluation pipelines to promote accessibility and future\ndevelopment. We hope IndiaWeatherBench will serve as a foundation for advancing\nregional weather forecasting research. Code is available at\nhttps://github.com/tung-nd/IndiaWeatherBench.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers",
    "url": "http://arxiv.org/abs/2509.00631v1",
    "authors": [
      "Giacomo Acciarini",
      "Simone Mestici",
      "Halil Kelebek",
      "Linnea Wolniewicz",
      "Michael Vergalla",
      "Madhulika Guhathakurta",
      "Umaa Rebbapragada",
      "Bala Poduval",
      "At\u0131l\u0131m G\u00fcne\u015f Baydin",
      "Frank Soboczenski"
    ],
    "published": "2025-08-30",
    "abstract": "The ionosphere critically influences Global Navigation Satellite Systems\n(GNSS), satellite communications, and Low Earth Orbit (LEO) operations, yet\naccurate prediction of its variability remains challenging due to nonlinear\ncouplings between solar, geomagnetic, and thermospheric drivers. Total Electron\nContent (TEC), a key ionospheric parameter, is derived from GNSS observations,\nbut its reliable forecasting is limited by the sparse nature of global\nmeasurements and the limited accuracy of empirical models, especially during\nstrong space weather conditions. In this work, we present a machine learning\nframework for ionospheric TEC forecasting that leverages Temporal Fusion\nTransformers (TFT) to predict sparse ionosphere data. Our approach accommodates\nheterogeneous input sources, including solar irradiance, geomagnetic indices,\nand GNSS-derived vertical TEC, and applies preprocessing and temporal alignment\nstrategies. Experiments spanning 2010-2025 demonstrate that the model achieves\nrobust predictions up to 24 hours ahead, with root mean square errors as low as\n3.33 TECU. Results highlight that solar EUV irradiance provides the strongest\npredictive signals. Beyond forecasting accuracy, the framework offers\ninterpretability through attention-based analysis, supporting both operational\napplications and scientific discovery. To encourage reproducibility and\ncommunity-driven development, we release the full implementation as the\nopen-source toolkit \\texttt{ionopy}.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2509.09572v1",
    "authors": [
      "Sijun Dong",
      "Yuxuan Hu",
      "LiBo Wang",
      "Geng Chen",
      "Xiaoliang Meng"
    ],
    "published": "2025-09-11",
    "abstract": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples,\nand the difficulty of cross-domain generalization in multi-temporal and\nmulti-source remote sensing imagery, we propose PeftCD, a change detection\nframework built upon Vision Foundation Models (VFMs) with Parameter-Efficient\nFine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese\nencoder derived from a VFM, into which LoRA and Adapter modules are seamlessly\nintegrated. This design enables highly efficient task adaptation by training\nonly a minimal set of additional parameters. To fully unlock the potential of\nVFMs, we investigate two leading backbones: the Segment Anything Model v2\n(SAM2), renowned for its strong segmentation priors, and DINOv3, a\nstate-of-the-art self-supervised representation learner. The framework is\ncomplemented by a deliberately lightweight decoder, ensuring the focus remains\non the powerful feature representations from the backbones. Extensive\nexperiments demonstrate that PeftCD achieves state-of-the-art performance\nacross multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD\n(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and\nLEVIR-CD (85.62%), with notably precise boundary delineation and strong\nsuppression of pseudo-changes. In summary, PeftCD presents an optimal balance\nof accuracy, efficiency, and generalization. It offers a powerful and scalable\nparadigm for adapting large-scale VFMs to real-world remote sensing change\ndetection applications. The code and pretrained models will be released at\nhttps://github.com/dyzy41/PeftCD.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia",
    "url": "http://arxiv.org/abs/2509.08303v1",
    "authors": [
      "M. Warizmi Wafiq",
      "Peter Cutter",
      "Ate Poortinga",
      "Daniel Marc G. dela Torre",
      "Karis Tenneson",
      "Vanna Teck",
      "Enikoe Bihari",
      "Chanarun Saisaward",
      "Weraphong Suaruang",
      "Andrea McMahon",
      "Andi Vika Faradiba Muin",
      "Karno B. Batiran",
      "Chairil A",
      "Nurul Qomar",
      "Arya Arismaya Metananda",
      "David Ganz",
      "David Saah"
    ],
    "published": "2025-09-10",
    "abstract": "Oil palm cultivation remains one of the leading causes of deforestation in\nIndonesia. To better track and address this challenge, detailed and reliable\nmapping is needed to support sustainability efforts and emerging regulatory\nframeworks. We present an open-access geospatial dataset of oil palm\nplantations and related land cover types in Indonesia, produced through expert\nlabeling of high-resolution satellite imagery from 2020 to 2024. The dataset\nprovides polygon-based, wall-to-wall annotations across a range of\nagro-ecological zones and includes a hierarchical typology that distinguishes\noil palm planting stages as well as similar perennial crops. Quality was\nensured through multi-interpreter consensus and field validation. The dataset\nwas created using wall-to-wall digitization over large grids, making it\nsuitable for training and benchmarking both conventional convolutional neural\nnetworks and newer geospatial foundation models. Released under a CC-BY\nlicense, it fills a key gap in training data for remote sensing and aims to\nimprove the accuracy of land cover types mapping. By supporting transparent\nmonitoring of oil palm expansion, the resource contributes to global\ndeforestation reduction goals and follows FAIR data principles.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery",
    "url": "http://arxiv.org/abs/2509.08949v1",
    "authors": [
      "Yibin Wang",
      "Wondimagegn Beshah",
      "Padmanava Dash",
      "Haifeng Wang"
    ],
    "published": "2025-09-10",
    "abstract": "The use of unmanned aerial systems (UASs) has increased tremendously in the\ncurrent decade. They have significantly advanced remote sensing with the\ncapability to deploy and image the terrain as per required spatial, spectral,\ntemporal, and radiometric resolutions for various remote sensing applications.\nOne of the major advantages of UAS imagery is that images can be acquired in\ncloudy conditions by flying the UAS under the clouds. The limitation to the\ntechnology is that the imagery is often sullied by cloud shadows. Images taken\nover water are additionally affected by sun glint. These are two pose serious\nissues for estimating water quality parameters from the UAS images. This study\nproposes a novel machine learning approach first to identify and extract\nregions with cloud shadows and sun glint and separate such regions from\nnon-obstructed clear sky regions and sun-glint unaffected regions. The data was\nextracted from the images at pixel level to train an U-Net based deep learning\nmodel and best settings for model training was identified based on the various\nevaluation metrics from test cases. Using this evaluation, a high-quality image\ncorrection model was determined, which was used to recover the cloud shadow and\nsun glint areas in the images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping",
    "url": "http://arxiv.org/abs/2509.09408v1",
    "authors": [
      "Jonas Schmidinger",
      "Viacheslav Barkov",
      "Sebastian Vogel",
      "Martin Atzmueller",
      "Gerard B M Heuvelink"
    ],
    "published": "2025-09-11",
    "abstract": "Machine learning and geostatistics are two fundamentally different frameworks\nfor predicting and spatially mapping soil properties. Geostatistics leverages\nthe spatial structure of soil properties, while machine learning captures the\nrelationship between available environmental features and soil properties. We\npropose a hybrid framework that enriches ML with spatial context through\nengineering of 'spatial lag' features from ordinary kriging. We call this\napproach 'kriging prior regression' (KpR), as it follows the inverse logic of\nregression kriging. To evaluate this approach, we assessed both the point and\nprobabilistic prediction performance of KpR, using the TabPFN model across six\nfieldscale datasets from LimeSoDa. These datasets included soil organic carbon,\nclay content, and pH, along with features derived from remote sensing and\nin-situ proximal soil sensing. KpR with TabPFN demonstrated reliable\nuncertainty estimates and more accurate predictions in comparison to several\nother spatial techniques (e.g., regression/residual kriging with TabPFN), as\nwell as to established non-spatial machine learning algorithms (e.g., random\nforest). Most notably, it significantly improved the average R2 by around 30%\ncompared to machine learning algorithms without spatial context. This\nimprovement was due to the strong prediction performance of the TabPFN\nalgorithm itself and the complementary spatial information provided by KpR\nfeatures. TabPFN is particularly effective for prediction tasks with small\nsample sizes, common in precision agriculture, whereas KpR can compensate for\nweak relationships between sensing features and soil properties when proximal\nsoil sensing data are limited. Hence, we conclude that KpR with TabPFN is a\nvery robust and versatile modelling framework for digital soil mapping in\nprecision agriculture.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "A Benchmark Dataset for Satellite-Based Estimation and Detection of Rain",
    "url": "http://arxiv.org/abs/2509.08816v2",
    "authors": [
      "Simon Pfreundschuh",
      "Malarvizhi Arulraj",
      "Ali Behrangi",
      "Linda Bogerd",
      "Alan James Peixoto Calheiros",
      "Daniele Casella",
      "Neda Dolatabadi",
      "Clement Guilloteau",
      "Jie Gong",
      "Christian D. Kummerow",
      "Pierre Kirstetter",
      "Gyuwon Lee",
      "Maximilian Maahn",
      "Lisa Milani",
      "Giulia Panegrossi",
      "Rayana Palharini",
      "Veljko Petkovi\u0107",
      "Soorok Ryu",
      "Paolo San\u00f2",
      "Jackson Tan"
    ],
    "published": "2025-09-10",
    "abstract": "Accurately tracking the global distribution and evolution of precipitation is\nessential for both research and operational meteorology. Satellite observations\nremain the only means of achieving consistent, global-scale precipitation\nmonitoring. While machine learning has long been applied to satellite-based\nprecipitation retrieval, the absence of a standardized benchmark dataset has\nhindered fair comparisons between methods and limited progress in algorithm\ndevelopment.\n  To address this gap, the International Precipitation Working Group has\ndeveloped SatRain, the first AI-ready benchmark dataset for satellite-based\ndetection and estimation of rain, snow, graupel, and hail. SatRain includes\nmulti-sensor satellite observations representative of the major platforms\ncurrently used in precipitation remote sensing, paired with high-quality\nreference estimates from ground-based radars corrected using rain gauge\nmeasurements. It offers a standardized evaluation protocol to enable robust and\nreproducible comparisons across machine learning approaches.\n  In addition to supporting algorithm evaluation, the diversity of sensors and\ninclusion of time-resolved geostationary observations make SatRain a valuable\nfoundation for developing next-generation AI models to deliver more accurate,\ndetailed, and globally consistent precipitation estimates.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Self-supervised Learning for Hyperspectral Images of Trees",
    "url": "http://arxiv.org/abs/2509.05630v1",
    "authors": [
      "Moqsadur Rahman",
      "Saurav Kumar",
      "Santosh S. Palmate",
      "M. Shahriar Hossain"
    ],
    "published": "2025-09-06",
    "abstract": "Aerial remote sensing using multispectral and RGB imagers has provided a\ncritical impetus to precision agriculture. Analysis of the hyperspectral images\nwith limited or no labels is challenging. This paper focuses on self-supervised\nlearning to create neural network embeddings reflecting vegetation properties\nof trees from aerial hyperspectral images of crop fields. Experimental results\ndemonstrate that a constructed tree representation, using a vegetation\nproperty-related embedding space, performs better in downstream machine\nlearning tasks compared to the direct use of hyperspectral vegetation\nproperties as tree representations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics",
    "url": "http://arxiv.org/abs/2509.09599v1",
    "authors": [
      "Ira J. S. Shokar",
      "Rich R. Kerswell",
      "Peter H. Haynes"
    ],
    "published": "2025-09-11",
    "abstract": "We present a deep learning emulator for stochastic and chaotic\nspatio-temporal systems, explicitly conditioned on the parameter values of the\nunderlying partial differential equations (PDEs). Our approach involves\npre-training the model on a single parameter domain, followed by fine-tuning on\na smaller, yet diverse dataset, enabling generalisation across a broad range of\nparameter values. By incorporating local attention mechanisms, the network is\ncapable of handling varying domain sizes and resolutions. This enables\ncomputationally efficient pre-training on smaller domains while requiring only\na small additional dataset to learn how to generalise to larger domain sizes.\nWe demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky\nequation and stochastically-forced beta-plane turbulence, showcasing its\nability to capture phenomena at interpolated parameter values. The emulator\nprovides significant computational speed-ups over conventional numerical\nintegration, facilitating efficient exploration of parameter space, while a\nprobabilistic variant of the emulator provides uncertainty quantification,\nallowing for the statistical study of rare events.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction",
    "url": "http://arxiv.org/abs/2509.09128v1",
    "authors": [
      "Emam Hossain",
      "Md Osman Gani"
    ],
    "published": "2025-09-11",
    "abstract": "Conventional machine learning and deep learning models typically rely on\ncorrelation-based learning, which often fails to distinguish genuine causal\nrelationships from spurious associations, limiting their robustness,\ninterpretability, and ability to generalize. To overcome these limitations, we\nintroduce a causality-aware deep learning framework that integrates\nMultivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection\nwithin a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic\nSea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily\nand monthly resolutions, the proposed method identifies causally influential\npredictors, prioritizes direct causes of SIE dynamics, reduces unnecessary\nfeatures, and enhances computational efficiency. Experimental results show that\nincorporating causal inputs leads to improved prediction accuracy and\ninterpretability across varying lead times. While demonstrated on Arctic SIE\nforecasting, the framework is broadly applicable to other dynamic,\nhigh-dimensional domains, offering a scalable approach that advances both the\ntheoretical foundations and practical performance of causality-informed\npredictive modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon",
    "url": "http://arxiv.org/abs/2509.06227v1",
    "authors": [
      "Saghar Ganji",
      "Mohammad Naisipour",
      "Alireza Hassani",
      "Arash Adib"
    ],
    "published": "2025-09-07",
    "abstract": "The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO)\nis still one of the biggest challenges in climate science. While it is true\nthat short-to medium-range performance has been improved significantly using\nthe advances in deep learning, statistical dynamical hybrids, most operational\nsystems still use the simple mean of all ensemble members, implicitly assuming\nequal skill across members. In this study, we demonstrate, through a strictly\na-posteriori evaluation , for any large enough ensemble of ENSO forecasts,\nthere is a subset of members whose skill is substantially higher than that of\nthe ensemble mean. Using a state-of-the-art ENSO forecast system\ncross-validated against the 1986-2017 observed Nino3.4 index, we identify two\nTop-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on\nhighest Pearson correlation. Generally across all leads, these outstanding\nmembers show higher correlation and lower RMSE, with the advantage rising\nenormously with lead time. Whereas at short leads (1 month) raises the mean\ncorrelation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\\deg}C\nor by 23.3% compared to the All-40 mean, at extreme leads (23 months) the\ncorrelation is raised by +0.43 (+172%) and RMSE by 0.18 {\\deg}C or by 22.5%\ndecrease. The enhancements are largest during crucial ENSO transition periods\nsuch as SON and DJF, when accurate amplitude and phase forecasting is of\ngreatest socio-economic benefit, and furthermore season-dependent e.g.,\nmid-year months such as JJA and MJJ have incredibly large RMSE reductions. This\nstudy provides a solid foundation for further investigations to identify\nreliable clues for detecting high-quality ensemble members, thereby enhancing\nforecasting skill.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MoWE : A Mixture of Weather Experts",
    "url": "http://arxiv.org/abs/2509.09052v1",
    "authors": [
      "Dibyajyoti Chakraborty",
      "Romit Maulik",
      "Peter Harrington",
      "Dallas Foster",
      "Mohammad Amin Nabian",
      "Sanjay Choudhry"
    ],
    "published": "2025-09-10",
    "abstract": "Data-driven weather models have recently achieved state-of-the-art\nperformance, yet progress has plateaued in recent years. This paper introduces\na Mixture of Experts (MoWE) approach as a novel paradigm to overcome these\nlimitations, not by creating a new forecaster, but by optimally combining the\noutputs of existing models. The MoWE model is trained with significantly lower\ncomputational resources than the individual experts. Our model employs a Vision\nTransformer-based gating network that dynamically learns to weight the\ncontributions of multiple \"expert\" models at each grid point, conditioned on\nforecast lead time. This approach creates a synthesized deterministic forecast\nthat is more accurate than any individual component in terms of Root Mean\nSquared Error (RMSE). Our results demonstrate the effectiveness of this method,\nachieving up to a 10% lower RMSE than the best-performing AI weather model on a\n2-day forecast horizon, significantly outperforming individual experts as well\nas a simple average across experts. This work presents a computationally\nefficient and scalable strategy to push the state of the art in data-driven\nweather prediction by making the most out of leading high-quality forecast\nmodels.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Using machine learning to downscale coarse-resolution environmental variables for understanding the spatial frequency of convective storms",
    "url": "http://arxiv.org/abs/2509.08802v1",
    "authors": [
      "Hungjui Yu",
      "Lander Ver Hoef",
      "Kristen L. Rasmussen",
      "Imme Ebert-Uphoff"
    ],
    "published": "2025-09-10",
    "abstract": "Global climate models (GCMs), typically run at ~100-km resolution, capture\nlarge-scale environmental conditions but cannot resolve convection and cloud\nprocesses at kilometer scales. Convection-permitting models offer\nhigher-resolution simulations that explicitly simulate convection but are\ncomputationally expensive and impractical for large ensemble runs. This study\nexplores machine learning (ML) as a bridge between these approaches. We train\nsimple, pixel-based neural networks to predict convective storm frequency from\nenvironmental variables produced by a regional convection-permitting model. The\nML models achieve promising results, with structural similarity index measure\n(SSIM) values exceeding 0.8, capturing the diurnal cycle and orographic\nconvection without explicit temporal or spatial coordinates as input. Model\nperformance declines when fewer input features are used or specific regions are\nexcluded, underscoring the role of diverse physical mechanisms in convective\nactivity. These findings highlight ML potential as a computationally efficient\ntool for representing convection and as a means of scientific discovery,\noffering insights into convective processes. Unlike convolutional neural\nnetworks, which depend on spatial structure and grid size, the pixel-based\nmodel treats each grid point independently, enabling value-to-value prediction\nwithout spatial context. This design enhances adaptability to resolution\nchanges and supports generalization to unseen environmental regimes, making it\nparticularly suited for linking environmental conditions to convective features\nand for application across diverse model grids or climate scenarios.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Seasonal forecasting using the GenCast probabilistic machine learning model",
    "url": "http://arxiv.org/abs/2509.06457v1",
    "authors": [
      "Bobby Antonio",
      "Kristian Strommen",
      "Hannah M. Christensen"
    ],
    "published": "2025-09-08",
    "abstract": "Machine-learnt weather prediction (MLWP) models are now well established as\nbeing competitive with conventional numerical weather prediction (NWP) models\nin the medium range. However, there is still much uncertainty as to how this\nperformance extends to longer timescales, where interactions with slower\ncomponents of the earth system become important. We take GenCast, a\nstate-of-the-art probabilistic MLWP model, and apply it to the task of seasonal\nforecasting with prescribed sea surface temperature (SST), by providing\nanomalies persisted over climatology (GenCast-Persisted) or forcing with\nobservations (GenCast-Forced). The forecasts are compared to the European\nCentre for Medium-Range Weather Forecasts seasonal forecasting system, SEAS5.\nOur results indicate that, despite being trained at short timescales,\nGenCast-Persisted produces much of the correct precipitation patterns in\nresponse to El Ni\\~{n}o and La Ni\\~{n}a events, with several erroneous patterns\nin GenCast-Persisted corrected with GenCast-Forced. The uncertainty in\nprecipitation response, as represented by the ensemble, compares favourably to\nSEAS5. Whilst SEAS5 achieves superior skill in the tropics for 2-metre\ntemperature and mean sea level pressure (MSLP), GenCast-Persisted achieves\nsignificantly higher skill in some areas in higher latitudes, including\nmountainous areas, with notable improvements for MSLP in particular; this is\nreflected in a higher correlation with the observed NAO index. Reliability\ndiagrams indicate that GenCast-Persisted is overconfident compared to SEAS5,\nwhilst GenCast-Forced produces well-calibrated seasonal 2-metre temperature\npredictions. These results provide an indication of the potential of MLWP\nmodels similar to GenCast for the `full' seasonal forecasting problem, where\nthe atmospheric model is coupled to ocean, land and cryosphere models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent and Independent Data under Preferential Sampling",
    "url": "http://arxiv.org/abs/2509.09336v1",
    "authors": [
      "Daniela Silva",
      "Raquel Menezes",
      "Gon\u00e7alo Ara\u00fajo",
      "Ana Machado",
      "Renato Rosa",
      "Ana Moreno",
      "Alexandra Silva",
      "Susana Garrido"
    ],
    "published": "2025-09-11",
    "abstract": "Sustainable management of marine ecosystems is vital for maintaining healthy\nfishery resources, and benefits from advanced scientific tools to accurately\nassess species distribution patterns. In fisheries science, two primary data\nsources are used: fishery-independent data (FID), collected through systematic\nsurveys, and fishery-dependent data (FDD), obtained from commercial fishing\nactivities. While these sources provide complementary information, their\ndistinct sampling schemes - systematic for FID and preferential for FDD - pose\nsignificant integration challenges. This study introduces a novel\nspatio-temporal model that integrates FID and FDD, addressing challenges\nassociated with zero-inflation and preferential sampling (PS) common in\necological data. The model employs a six-layer structure to differentiate\nbetween presence-absence and biomass observations, offering a robust framework\nfor ecological studies affected by PS biases. Simulation results demonstrate\nthe model's accuracy in parameter estimation across diverse PS scenarios and\nits ability to detect preferential signals. Application to the study of the\ndistribution patterns of the European sardine populations along the southern\nPortuguese continental shelf illustrates the model's effectiveness in\nintegrating diverse data sources and incorporating environmental and\nvessel-specific covariates. The model reveals spatio-temporal variability in\nsardine presence and biomass, providing actionable insights for fisheries\nmanagement. Beyond ecology, this framework offers broad applicability to data\nintegration challenges in other disciplines.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts",
    "url": "http://arxiv.org/abs/2509.14104v1",
    "authors": [
      "Leonard Hackel",
      "Tom Burgert",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-09-17",
    "abstract": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation",
    "url": "http://arxiv.org/abs/2509.13229v1",
    "authors": [
      "Hugo Carlesso",
      "Josiane Mothe",
      "Radu Tudor Ionescu"
    ],
    "published": "2025-09-16",
    "abstract": "Hyperspectral imaging (HSI) captures detailed spectral signatures across\nhundreds of contiguous bands per pixel, being indispensable for remote sensing\napplications such as land-cover classification, change detection, and\nenvironmental monitoring. Due to the high dimensionality of HSI data and the\nslow rate of data transfer in satellite-based systems, compact and efficient\nmodels are required to support onboard processing and minimize the transmission\nof redundant or low-value data, e.g. cloud-covered areas. To this end, we\nintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)\nframework designed for lightweight architectures for HSI analysis. CMTSSL\nintegrates masked image modeling with decoupled spatial and spectral jigsaw\npuzzle solving, guided by a curriculum learning strategy that progressively\nincreases data complexity during self-supervision. This enables the encoder to\njointly capture fine-grained spectral continuity, spatial structure, and global\nsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously\naddresses spatial and spectral reasoning within a unified and computationally\nefficient design, being particularly suitable for training lightweight models\nfor onboard satellite deployment. We validate our approach on four public\nbenchmark datasets, demonstrating consistent gains in downstream segmentation\ntasks, using architectures that are over 16,000x lighter than some\nstate-of-the-art models. These results highlight the potential of CMTSSL in\ngeneralizable representation learning with lightweight architectures for\nreal-world HSI applications. Our code is publicly available at\nhttps://github.com/hugocarlesso/CMTSSL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji",
    "url": "http://arxiv.org/abs/2509.13388v1",
    "authors": [
      "Yadvendra Gurjar",
      "Ruoni Wan",
      "Ehsan Farahbakhsh",
      "Rohitash Chandra"
    ],
    "published": "2025-09-16",
    "abstract": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Vessel Detection and Localization Using Distributed Acoustic Sensing in Submarine Optical Fiber Cables",
    "url": "http://arxiv.org/abs/2509.11614v2",
    "authors": [
      "Erick Eduardo Ramirez-Torres",
      "Javier Macias-Guarasa",
      "Daniel Pizarro-Perez",
      "Javier Tejedor",
      "Sira Elena Palazuelos-Cagigas",
      "Pedro J. Vidal-Moreno",
      "Sonia Martin-Lopez",
      "Miguel Gonzalez-Herraez",
      "Roel Vanthillo"
    ],
    "published": "2025-09-15",
    "abstract": "Submarine cables play a critical role in global internet connectivity, energy\ntransmission, and communication but remain vulnerable to accidental damage and\nsabotage. Recent incidents in the Baltic Sea highlighted the need for enhanced\nmonitoring to protect this vital infrastructure. Traditional vessel detection\nmethods, such as synthetic aperture radar, video surveillance, and\nmultispectral satellite imagery, face limitations in real-time processing,\nadverse weather conditions, and coverage range. This paper explores Distributed\nAcoustic Sensing (DAS) as an alternative by repurposing submarine\ntelecommunication cables as large-scale acoustic sensor arrays. DAS offers\ncontinuous real-time monitoring, operates independently of cooperative systems\nlike the \"Automatic Identification System\" (AIS), being largely unaffected by\nlighting or weather conditions. However, existing research on DAS for vessel\ntracking is limited in scale and lacks validation under real-world conditions.\nTo address these gaps, a general and systematic methodology is presented for\nvessel detection and distance estimation using DAS. Advanced machine learning\nmodels are applied to improve detection and localization accuracy in dynamic\nmaritime environments. The approach is evaluated over a continuous ten-day\nperiod, covering diverse ship and operational conditions, representing one of\nthe largest-scale DAS-based vessel monitoring studies to date, and for which we\nrelease the full evaluation dataset. Results demonstrate DAS as a practical\ntool for maritime surveillance, with an overall F1-score of over 90% in vessel\ndetection, and a mean average error of 141 m for vessel distance estimation,\nbridging the gap between experimental research and real-world deployment.",
    "categories": [
      "remote_sensing",
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ]
  },
  {
    "title": "Artificial neural networks ensemble methodology to predict significant wave height",
    "url": "http://arxiv.org/abs/2509.14020v1",
    "authors": [
      "Felipe Crivellaro Minuzzi",
      "Leandro Farina"
    ],
    "published": "2025-09-17",
    "abstract": "The forecast of wave variables are important for several applications that\ndepend on a better description of the ocean state. Due to the chaotic behaviour\nof the differential equations which model this problem, a well know strategy to\novercome the difficulties is basically to run several simulations, by for\ninstance, varying the initial condition, and averaging the result of each of\nthese, creating an ensemble. Moreover, in the last few years, considering the\namount of available data and the computational power increase, machine learning\nalgorithms have been applied as surrogate to traditional numerical models,\nyielding comparative or better results. In this work, we present a methodology\nto create an ensemble of different artificial neural networks architectures,\nnamely, MLP, RNN, LSTM, CNN and a hybrid CNN-LSTM, which aims to predict\nsignificant wave height on six different locations in the Brazilian coast. The\nnetworks are trained using NOAA's numerical reforecast data and target the\nresidual between observational data and the numerical model output. A new\nstrategy to create the training and target datasets is demonstrated. Results\nshow that our framework is capable of producing high efficient forecast, with\nan average accuracy of $80\\%$, that can achieve up to $88\\%$ in the best case\nscenario, which means $5\\%$ reduction in error metrics if compared to NOAA's\nnumerical model, and a increasingly reduction of computational cost.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM",
      "RNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SamudrACE: Fast and Accurate Coupled Climate Modeling with 3D Ocean and Atmosphere Emulators",
    "url": "http://arxiv.org/abs/2509.12490v1",
    "authors": [
      "James P. C. Duncan",
      "Elynn Wu",
      "Surya Dheeshjith",
      "Adam Subel",
      "Troy Arcomano",
      "Spencer K. Clark",
      "Brian Henn",
      "Anna Kwa",
      "Jeremy McGibbon",
      "W. Andre Perkins",
      "William Gregory",
      "Carlos Fernandez-Granda",
      "Julius Busecke",
      "Oliver Watt-Meyer",
      "William J. Hurlin",
      "Alistair Adcroft",
      "Laure Zanna",
      "Christopher Bretherton"
    ],
    "published": "2025-09-15",
    "abstract": "Traditional numerical global climate models simulate the full Earth system by\nexchanging boundary conditions between separate simulators of the atmosphere,\nocean, sea ice, land surface, and other geophysical processes. This paradigm\nallows for distributed development of individual components within a common\nframework, unified by a coupler that handles translation between realms via\nspatial or temporal alignment and flux exchange. Following a similar approach\nadapted for machine learning-based emulators, we present SamudrACE: a coupled\nglobal climate model emulator which produces centuries-long simulations at\n1-degree horizontal, 6-hourly atmospheric, and 5-daily oceanic resolution, with\n145 2D fields spanning 8 atmospheric and 19 oceanic vertical levels, plus sea\nice, surface, and top-of-atmosphere variables. SamudrACE is highly stable and\nhas low climate biases comparable to those of its components with prescribed\nboundary forcing, with realistic variability in coupled climate phenomena such\nas ENSO that is not possible to simulate in uncoupled mode.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models",
    "url": "http://arxiv.org/abs/2509.13047v1",
    "authors": [
      "Nolan Platt",
      "Pragyansmita Nayak"
    ],
    "published": "2025-09-16",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "Branched Broomrape Detection in Tomato Farms Using Satellite Imagery and Time-Series Analysis",
    "url": "http://arxiv.org/abs/2509.10804v1",
    "authors": [
      "Mohammadreza Narimani",
      "Alireza Pourreza",
      "Ali Moghimi",
      "Parastoo Farajpoor",
      "Hamid Jafarbiglu",
      "Mohsen Mesgaran"
    ],
    "published": "2025-09-13",
    "abstract": "Branched broomrape (Phelipanche ramosa (L.) Pomel) is a chlorophyll-deficient\nparasitic plant that threatens tomato production by extracting nutrients from\nthe host, with reported yield losses up to 80 percent. Its mostly subterranean\nlife cycle and prolific seed production (more than 200,000 seeds per plant,\nviable for up to 20 years) make early detection essential. We present an\nend-to-end pipeline that uses Sentinel-2 imagery and time-series analysis to\nidentify broomrape-infested tomato fields in California. Regions of interest\nwere defined from farmer-reported infestations, and images with less than 10\npercent cloud cover were retained. We processed 12 spectral bands and\nsun-sensor geometry, computed 20 vegetation indices (e.g., NDVI, NDMI), and\nderived five plant traits (Leaf Area Index, Leaf Chlorophyll Content, Canopy\nChlorophyll Content, Fraction of Absorbed Photosynthetically Active Radiation,\nand Fractional Vegetation Cover) using a neural network calibrated with\nground-truth and synthetic data. Trends in Canopy Chlorophyll Content\ndelineated transplanting-to-harvest periods, and phenology was aligned using\ngrowing degree days. Vegetation pixels were segmented and used to train a Long\nShort-Term Memory (LSTM) network on 18,874 pixels across 48 growing-degree-day\ntime points. The model achieved 88 percent training accuracy and 87 percent\ntest accuracy, with precision 0.86, recall 0.92, and F1 0.89. Permutation\nfeature importance ranked NDMI, Canopy Chlorophyll Content, FAPAR, and a\nchlorophyll red-edge index as most informative, consistent with the\nphysiological effects of infestation. Results show the promise of\nsatellite-driven time-series modeling for scalable detection of parasitic\nstress in tomato farms.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Modelling species distributions using remote sensing predictors: Comparing Dynamic Habitat Index and LULC",
    "url": "http://arxiv.org/abs/2509.14862v1",
    "authors": [
      "Ma\u00efri Souza Oliveira",
      "Cl\u00e9mentine Pr\u00e9au",
      "Samuel Alleaume",
      "Maxime Lenormand",
      "Sandra Luque"
    ],
    "published": "2025-09-18",
    "abstract": "This study compares the predictive capacity of the Dynamic Habitat Index\n(DHI) - a remote sensing (RS)-based measure of habitat productivity and\nvariability - against traditional land-use/land-cover (LULC) metrics in species\ndistribution modelling (SDM) applications. RS and LULC-based SDMs were built\nusing distribution data for eleven bird, amphibian, and mammal species in\n\\^Ile-de-France. Predictor variables were derived from Sentinel-2 RS data and\nLULC classifications, with the latter incorporating Euclidean distance to\nhabitat types. Ensemble SDMs were built using nine algorithms and evaluated\nwith the Continuous Boyce Index (CBI) and a calibrated AUC. Habitat suitability\nscores and their binary transformations were assessed using niche overlap\nindices (Schoener, Warren, and Spearman rank correlation coefficient). Both RS\nand LULC approaches exhibited similar predictive accuracy overall. After\nbinarisation however, the resulting niche maps diverged significantly. While\nLULC-based models exhibited spatial constraints (habitat suitability decreased\nas distance from recorded occurrences increased), RS-based models, which used\ncontinuous data, were not affected by geographic bias or distance effects.\nThese results underscore the need to account for spatial biases in LULC-based\nSDMs. The DHI may offer a more spatially neutral alternative, making it a\npromising predictor for modelling species niches at regional scales.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Sentinel-3 foundation model for ocean colour",
    "url": "http://arxiv.org/abs/2509.21273v1",
    "authors": [
      "Geoffrey Dawson",
      "Remy Vandaele",
      "Andrew Taylor",
      "David Moffat",
      "Helen Tamura-Wicks",
      "Sarah Jackson",
      "Rosie Lickorish",
      "Paolo Fraccaro",
      "Hywel Williams",
      "Chunbo Luo",
      "Anne Jones"
    ],
    "published": "2025-09-25",
    "abstract": "Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive\nunlabelled datasets, have the potential to drastically change AI applications\nin ocean science, where labelled data are often sparse and expensive to\ncollect. In this work, we describe a new foundation model using the Prithvi-EO\nVision Transformer architecture which has been pre-trained to reconstruct data\nfrom the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the\nmodel by fine-tuning on two downstream marine earth observation tasks. We first\nassess model performance compared to current baseline models used to quantify\nchlorophyll concentration. We then evaluate the FMs ability to refine remote\nsensing-based estimates of ocean primary production. Our results demonstrate\nthe utility of self-trained FMs for marine monitoring, in particular for making\nuse of small amounts of high quality labelled data and in capturing detailed\nspatial patterns of ocean colour whilst matching point observations. We\nconclude that this new generation of geospatial AI models has the potential to\nprovide more robust, data-driven insights into ocean ecosystems and their role\nin global climate processes.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2509.18711v1",
    "authors": [
      "Ke Li",
      "Di Wang",
      "Ting Wang",
      "Fuyu Dong",
      "Yiming Zhang",
      "Luyao Zhang",
      "Xiangyu Wang",
      "Shaofeng Li",
      "Quan Wang"
    ],
    "published": "2025-09-23",
    "abstract": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Visual Instruction Pretraining for Domain-Specific Foundation Models",
    "url": "http://arxiv.org/abs/2509.17562v2",
    "authors": [
      "Yuxuan Li",
      "Yicheng Zhang",
      "Wenhao Tang",
      "Yimian Dai",
      "Ming-Ming Cheng",
      "Xiang Li",
      "Jian Yang"
    ],
    "published": "2025-09-22",
    "abstract": "Modern computer vision is converging on a closed loop in which perception,\nreasoning and generation mutually reinforce each other. However, this loop\nremains incomplete: the top-down influence of high-level reasoning on the\nfoundational learning of low-level perceptual features is not yet\nunderexplored. This paper addresses this gap by proposing a new paradigm for\npretraining foundation models in downstream domains. We introduce Visual\ninsTruction Pretraining (ViTP), a novel approach that directly leverages\nreasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)\nbackbone within a Vision-Language Model and pretrains it end-to-end using a\nrich corpus of visual instruction data curated from target downstream domains.\nViTP is powered by our proposed Visual Robustness Learning (VRL), which compels\nthe ViT to learn robust and domain-relevant features from a sparse set of\nvisual tokens. Extensive experiments on 16 challenging remote sensing and\nmedical imaging benchmarks demonstrate that ViTP establishes new\nstate-of-the-art performance across a diverse range of downstream tasks. The\ncode is available at https://github.com/zcablii/ViTP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model",
    "url": "http://arxiv.org/abs/2509.16617v1",
    "authors": [
      "David Kreismann"
    ],
    "published": "2025-09-20",
    "abstract": "As urbanization and climate change progress, urban heat island effects are\nbecoming more frequent and severe. To formulate effective mitigation plans,\ncities require detailed air temperature data. However, predictive analytics\nmethods based on conventional machine learning models and limited data\ninfrastructure often provide inaccurate predictions, especially in underserved\nareas. In this context, geospatial foundation models trained on unstructured\nglobal data demonstrate strong generalization and require minimal fine-tuning,\noffering an alternative for predictions where traditional approaches are\nlimited. This study fine-tunes a geospatial foundation model to predict urban\nland surface temperatures under future climate scenarios and explores its\nresponse to land cover changes using simulated vegetation strategies. The\nfine-tuned model achieved pixel-wise downscaling errors below 1.74 {\\deg}C and\naligned with ground truth patterns, demonstrating an extrapolation capacity up\nto 3.62 {\\deg}C.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images",
    "url": "http://arxiv.org/abs/2509.20918v1",
    "authors": [
      "Qinfeng Zhu",
      "Han Li",
      "Liang He",
      "Lei Fan"
    ],
    "published": "2025-09-25",
    "abstract": "Semantic segmentation of remote sensing imagery is a fundamental task in\ncomputer vision, supporting a wide range of applications such as land use\nclassification, urban planning, and environmental monitoring. However, this\ntask is often challenged by the high spatial resolution, complex scene\nstructures, and diverse object scales present in remote sensing data. To\naddress these challenges, various deep learning architectures have been\nproposed, including convolutional neural networks, Vision Transformers, and the\nrecently introduced Vision Mamba. Vision Mamba features a global receptive\nfield and low computational complexity, demonstrating both efficiency and\neffectiveness in image segmentation. However, its reliance on global scanning\ntends to overlook critical local features, such as textures and edges, which\nare essential for achieving accurate segmentation in remote sensing contexts.\nTo tackle this limitation, we propose SwinMamba, a novel framework inspired by\nthe Swin Transformer. SwinMamba integrates localized Mamba-style scanning\nwithin shifted windows with a global receptive field, to enhance the model's\nperception of both local and global features. Specifically, the first two\nstages of SwinMamba perform local scanning to capture fine-grained details,\nwhile its subsequent two stages leverage global scanning to fuse broader\ncontextual information. In our model, the use of overlapping shifted windows\nenhances inter-region information exchange, facilitating more robust feature\nintegration across the entire image. Extensive experiments on the LoveDA and\nISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art\nmethods, underscoring its effectiveness and potential as a superior solution\nfor semantic segmentation of remotely sensed imagery.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
    "url": "http://arxiv.org/abs/2509.20234v1",
    "authors": [
      "Tom Burgert",
      "Oliver Stoll",
      "Paolo Rota",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-09-24",
    "abstract": "The hypothesis that Convolutional Neural Networks (CNNs) are inherently\ntexture-biased has shaped much of the discourse on feature use in deep\nlearning. We revisit this hypothesis by examining limitations in the\ncue-conflict experiment by Geirhos et al. To address these limitations, we\npropose a domain-agnostic framework that quantifies feature reliance through\nsystematic suppression of shape, texture, and color cues, avoiding the\nconfounds of forced-choice conflicts. By evaluating humans and neural networks\nunder controlled suppression conditions, we find that CNNs are not inherently\ntexture-biased but predominantly rely on local shape features. Nonetheless,\nthis reliance can be substantially mitigated through modern training strategies\nor architectures (ConvNeXt, ViTs). We further extend the analysis across\ncomputer vision, medical imaging, and remote sensing, revealing that reliance\npatterns differ systematically: computer vision models prioritize shape,\nmedical imaging models emphasize color, and remote sensing models exhibit a\nstronger reliance towards texture. Code is available at\nhttps://github.com/tomburgert/feature-reliance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Optimal Transport Based Hyperspectral Unmixing for Highly Mixed Observations",
    "url": "http://arxiv.org/abs/2509.20417v1",
    "authors": [
      "D. Doutsas",
      "B. Figliuzzi"
    ],
    "published": "2025-09-24",
    "abstract": "We propose a novel approach based on optimal transport (OT) for tackling the\nproblem of highly mixed data in blind hyperspectral unmixing. Our method\nconstrains the distribution of the estimated abundance matrix to resemble a\ntargeted Dirichlet distribution more closely. The novelty lies in using OT to\nmeasure the discrepancy between the targeted and true abundance distributions,\nwhich we incorporate as a regularization term in our optimization problem. We\ndemonstrate the efficiency of our method through a case study involving an\nunsupervised deep learning approach. Our experiments show that the proposed\napproach allows for a better estimation of the endmembers in the presence of\nhighly mixed data, while displaying robustness to the choice of target\nabundance distribution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy",
    "url": "http://arxiv.org/abs/2509.19665v1",
    "authors": [
      "Manuel Perez-Carrasco",
      "Maya Nasr",
      "Sebastien Roche",
      "Chris Chan Miller",
      "Zhan Zhang",
      "Core Francisco Park",
      "Eleanor Walker",
      "Cecilia Garraffo",
      "Douglas Finkbeiner",
      "Ritesh Gautam",
      "Steven Wofsy"
    ],
    "published": "2025-09-24",
    "abstract": "Effective cloud and cloud shadow detection is a critical prerequisite for\naccurate retrieval of concentrations of atmospheric methane or other trace\ngases in hyperspectral remote sensing. This challenge is especially pertinent\nfor MethaneSAT and for its airborne companion mission, MethaneAIR. In this\nstudy, we use machine learning methods to address the cloud and cloud shadow\ndetection problem for sensors with these high spatial resolutions instruments.\nCloud and cloud shadows in remote sensing data need to be effectively screened\nout as they bias methane retrievals in remote sensing imagery and impact the\nquantification of emissions. We deploy and evaluate conventional techniques\nincluding Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),\nwith advanced deep learning architectures, namely UNet and a Spectral Channel\nAttention Network (SCAN) method. Our results show that conventional methods\nstruggle with spatial coherence and boundary definition, affecting the\ndetection of clouds and cloud shadows. Deep learning models substantially\nimprove detection quality: UNet performs best in preserving spatial structure,\nwhile SCAN excels at capturing fine boundary details. Notably, SCAN surpasses\nUNet on MethaneSAT data, underscoring the benefits of incorporating spectral\nattention for satellite specific features. This in depth assessment of various\ndisparate machine learning techniques demonstrates the strengths and\neffectiveness of advanced deep learning architectures in providing robust,\nscalable solutions for clouds and cloud shadow screening towards enhancing\nmethane emission quantification capacity of existing and next generation\nhyperspectral missions. Our data and code is publicly available at\nhttps://doi.org/10.7910/DVN/IKLZOJ",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection",
      "Regression"
    ]
  },
  {
    "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2509.19087v1",
    "authors": [
      "Ganesh Mallya",
      "Yotam Gigi",
      "Dahun Kim",
      "Maxim Neumann",
      "Genady Beryozkin",
      "Tomer Shekel",
      "Anelia Angelova"
    ],
    "published": "2025-09-23",
    "abstract": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Can multimodal representation learning by alignment preserve modality-specific information?",
    "url": "http://arxiv.org/abs/2509.17943v1",
    "authors": [
      "Romain Thoreau",
      "Jessie Levillain",
      "Dawa Derksen"
    ],
    "published": "2025-09-22",
    "abstract": "Combining multimodal data is a key issue in a wide range of machine learning\ntasks, including many remote sensing problems. In Earth observation, early\nmultimodal data fusion methods were based on specific neural network\narchitectures and supervised learning. Ever since, the scarcity of labeled data\nhas motivated self-supervised learning techniques. State-of-the-art multimodal\nrepresentation learning techniques leverage the spatial alignment between\nsatellite data from different modalities acquired over the same geographic area\nin order to foster a semantic alignment in the latent space. In this paper, we\ninvestigate how this methods can preserve task-relevant information that is not\nshared across modalities. First, we show, under simplifying assumptions, when\nalignment strategies fundamentally lead to an information loss. Then, we\nsupport our theoretical insight through numerical experiments in more realistic\nsettings. With those theoretical and empirical evidences, we hope to support\nnew developments in contrastive learning for the combination of multimodal\nsatellite data. Our code and data is publicly available at\nhttps://github.com/Romain3Ch216/alg_maclean_25.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Remote Sensing-Oriented World Model",
    "url": "http://arxiv.org/abs/2509.17808v1",
    "authors": [
      "Yuxi Lu",
      "Biao Wu",
      "Zhidong Li",
      "Kunqi Li",
      "Chenya Huang",
      "Huacan Wang",
      "Qizhen Lan",
      "Ronghao Chen",
      "Ling Chen",
      "Bin Liang"
    ],
    "published": "2025-09-22",
    "abstract": "World models have shown potential in artificial intelligence by predicting\nand reasoning about world states beyond direct observations. However, existing\napproaches are predominantly evaluated in synthetic environments or constrained\nscene settings, limiting their validation in real-world contexts with broad\nspatial coverage and complex semantics. Meanwhile, remote sensing applications\nurgently require spatial reasoning capabilities for disaster response and urban\nplanning. This paper bridges these gaps by introducing the first framework for\nworld modeling in remote sensing. We formulate remote sensing world modeling as\ndirection-conditioned spatial extrapolation, where models generate semantically\nconsistent adjacent image tiles given a central observation and directional\ninstruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing\nWorld-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks\nacross four scenarios: general, flood, urban, and rural. RSWISE combines visual\nfidelity assessment with instruction compliance evaluation using GPT-4o as a\nsemantic judge, ensuring models genuinely perform spatial reasoning rather than\nsimple replication. Afterwards, we present RemoteBAGEL, a unified multimodal\nmodel fine-tuned on remote sensing data for spatial extrapolation tasks.\nExtensive experiments demonstrate that RemoteBAGEL consistently outperforms\nstate-of-the-art baselines on RSWISE.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation",
    "url": "http://arxiv.org/abs/2509.17206v1",
    "authors": [
      "Gunner Stone",
      "Sushmita Sarker",
      "Alireza Tavakkoli"
    ],
    "published": "2025-09-21",
    "abstract": "Generating realistic 3D point clouds is a fundamental problem in computer\nvision with applications in remote sensing, robotics, and digital object\nmodeling. Existing generative approaches primarily capture geometry, and when\nsemantics are considered, they are typically imposed post hoc through external\nsegmentation or clustering rather than integrated into the generative process\nitself. We propose a diffusion-based framework that embeds per-point semantic\nconditioning directly within generation. Each point is associated with a\nconditional variable corresponding to its semantic label, which guides the\ndiffusion dynamics and enables the joint synthesis of geometry and semantics.\nThis design produces point clouds that are both structurally coherent and\nsegmentation-aware, with object parts explicitly represented during synthesis.\nThrough a comparative analysis of guided and unguided diffusion processes, we\ndemonstrate the significant impact of conditional variables on diffusion\ndynamics and generation quality. Extensive experiments validate the efficacy of\nour approach, producing detailed and accurate 3D point clouds tailored to\nspecific parts and features.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures",
    "url": "http://arxiv.org/abs/2509.17734v1",
    "authors": [
      "Pablo Rodr\u00edguez-Bocca",
      "Guillermo Pereira",
      "Diego Kiedanski",
      "Soledad Collazo",
      "Sebasti\u00e1n Basterrech",
      "Gerardo Rubino"
    ],
    "published": "2025-09-22",
    "abstract": "In recent years, great progress has been made in the field of forecasting\nmeteorological variables. Recently, deep learning architectures have made a\nmajor breakthrough in forecasting the daily average temperature over a ten-day\nhorizon. However, advances in forecasting events related to the maximum\ntemperature over short horizons remain a challenge for the community. A problem\nthat is even more complex consists in making predictions of the maximum daily\ntemperatures in the short, medium, and long term. In this work, we focus on\nforecasting events related to the maximum daily temperature over medium-term\nperiods (90 days). Therefore, instead of addressing the problem from a\nmeteorological point of view, this article tackles it from a climatological\npoint of view. Due to the complexity of this problem, a common approach is to\nframe the study as a temporal classification problem with the classes: maximum\ntemperature \"above normal\", \"normal\" or \"below normal\". From a practical point\nof view, we created a large historical dataset (from 1981 to 2018) collecting\ninformation from weather stations located in South America. In addition, we\nalso integrated exogenous information from the Pacific, Atlantic, and Indian\nOcean basins. We applied the AutoGluonTS platform to solve the above-mentioned\nproblem. This AutoML tool shows competitive forecasting performance with\nrespect to large operational platforms dedicated to tackling this\nclimatological problem; but with a \"relatively\" low computational cost in terms\nof time and resources.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations",
    "url": "http://arxiv.org/abs/2509.19384v1",
    "authors": [
      "Hongyuan Shi",
      "Yilin Zhai",
      "Ping Dong",
      "Zaijin You",
      "Chao Zhan",
      "Qing Wang"
    ],
    "published": "2025-09-21",
    "abstract": "Reconstructing high-resolution regional significant wave height fields from\nsparse and uneven buoy observations remains a core challenge for ocean\nmonitoring and risk-aware operations. We introduce AUWave, a hybrid deep\nlearning framework that fuses a station-wise sequence encoder (MLP) with a\nmulti-scale U-Net enhanced by a bottleneck self-attention layer to recover\n32$\\times$32 regional SWH fields. A systematic Bayesian hyperparameter search\nwith Optuna identifies the learning rate as the dominant driver of\ngeneralization, followed by the scheduler decay and the latent dimension. Using\nNDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave\nattains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE\ndistribution. Spatial errors are lowest near observation sites and increase\nwith distance, reflecting identifiability limits under sparse sampling.\nSensitivity experiments show that AUWave consistently outperforms a\nrepresentative baseline in data-richer configurations, while the baseline is\nonly marginally competitive in the most underdetermined single-buoy cases. The\narchitecture's multi-scale and attention components translate into accuracy\ngains when minimal but non-trivial spatial anchoring is available. Error maps\nand buoy ablations reveal key anchor stations whose removal disproportionately\ndegrades performance, offering actionable guidance for network design. AUWave\nprovides a scalable pathway for gap filling, high-resolution priors for data\nassimilation, and contingency reconstruction.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations",
    "url": "http://arxiv.org/abs/2509.20422v1",
    "authors": [
      "Yiling Ma",
      "Nathan Luke Abraham",
      "Stefan Versick",
      "Roland Ruhnke",
      "Andrea Schneidereit",
      "Ulrike Niemeier",
      "Felix Back",
      "Peter Braesicke",
      "Peer Nowack"
    ],
    "published": "2025-09-24",
    "abstract": "Atmospheric ozone is a crucial absorber of solar radiation and an important\ngreenhouse gas. However, most climate models participating in the Coupled Model\nIntercomparison Project (CMIP) still lack an interactive representation of\nozone due to the high computational costs of atmospheric chemistry schemes.\nHere, we introduce a machine learning parameterization (mloz) to interactively\nmodel daily ozone variability and trends across the troposphere and\nstratosphere in standard climate sensitivity simulations, including two-way\ninteractions of ozone with the Quasi-Biennial Oscillation. We demonstrate its\nhigh fidelity on decadal timescales and its flexible use online across two\ndifferent climate models -- the UK Earth System Model (UKESM) and the German\nICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile\ninformation as the only input, mloz produces stable ozone predictions around 31\ntimes faster than the chemistry scheme in UKESM, contributing less than 4\npercent of the respective total climate model runtimes. In particular, we also\ndemonstrate its transferability to different climate models without chemistry\nschemes by transferring the parameterization from UKESM to ICON. This\nhighlights the potential for widespread adoption in CMIP-level climate models\nthat lack interactive chemistry for future climate change assessments,\nparticularly when focusing on climate sensitivity simulations, where ozone\ntrends and variability are known to significantly modulate atmospheric feedback\nprocesses.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Discovering strategies for coastal resilience with AI-based prediction and optimization",
    "url": "http://arxiv.org/abs/2509.19263v1",
    "authors": [
      "Jared Markowitz",
      "Alexander New",
      "Jennifer Sleeman",
      "Chace Ashcraft",
      "Jay Brett",
      "Gary Collins",
      "Stella In",
      "Nathaniel Winstead"
    ],
    "published": "2025-09-23",
    "abstract": "Tropical storms cause extensive property damage and loss of life, making them\none of the most destructive types of natural hazards. The development of\npredictive models that identify interventions effective at mitigating storm\nimpacts has considerable potential to reduce these adverse outcomes. In this\nstudy, we use an artificial intelligence (AI)-driven approach for optimizing\nintervention schemes that improve resilience to coastal flooding. We combine\nthree different AI models to optimize the selection of intervention types,\nsites, and scales in order to minimize the expected cost of flooding damage in\na given region, including the cost of installing and maintaining interventions.\nOur approach combines data-driven generation of storm surge fields, surrogate\nmodeling of intervention impacts, and the solving of a continuous-armed bandit\nproblem. We applied this methodology to optimize the selection of sea wall and\noyster reef interventions near Tyndall Air Force Base (AFB) in Florida, an area\nthat was catastrophically impacted by Hurricane Michael. Our analysis predicts\nthat intervention optimization could be used to potentially save billions of\ndollars in storm damage, far outpacing greedy or non-optimal solutions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling",
    "url": "http://arxiv.org/abs/2509.19032v2",
    "authors": [
      "Kashaf Ul Emaan"
    ],
    "published": "2025-09-23",
    "abstract": "Detection of credit card fraud is an acute issue of financial security\nbecause transaction datasets are highly lopsided, with fraud cases being only a\ndrop in the ocean. Balancing datasets using the most popular methods of\ntraditional oversampling such as the Synthetic Minority Oversampling Technique\n(SMOTE) generally create simplistic synthetic samples that are not readily\napplicable to complex fraud patterns. Recent industry advances that include\nConditional Tabular Generative Adversarial Networks (CTGAN) and Tabular\nVariational Autoencoders (TVAE) have demonstrated increased efficiency in\ntabular synthesis, yet all these models still exhibit issues with\nhigh-dimensional dependence modelling. Now we will present our hybrid approach\nwhere we use a Generative Adversarial Network (GAN) with a Transformer encoder\nblock to produce realistic fraudulent transactions samples. The GAN\narchitecture allows training realistic generators adversarial, and the\nTransformer allows the model to learn rich feature interactions by\nself-attention. Such a hybrid strategy overcomes the limitations of SMOTE,\nCTGAN, and TVAE by producing a variety of high-quality synthetic minority\nclasses samples. We test our algorithm on the publicly-available Credit Card\nFraud Detection dataset and compare it to conventional and generative\nresampling strategies with a variety of classifiers, such as Logistic\nRegression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and\nSupport Vector Machine (SVM). Findings indicate that our Transformer-based GAN\nshows substantial gains in Recall, F1-score and Area Under the Receiver\nOperating Characteristic Curve (AUC), which indicates that it is effective in\novercoming the severe class imbalance inherent in the task of fraud detection.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "GAN",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Regression"
    ]
  },
  {
    "title": "An update to ECMWF's machine-learned weather forecast model AIFS",
    "url": "http://arxiv.org/abs/2509.18994v1",
    "authors": [
      "Gabriel Moldovan",
      "Ewan Pinnington",
      "Ana Prieto Nemesio",
      "Simon Lang",
      "Zied Ben Bouall\u00e8gue",
      "Jesper Dramsch",
      "Mihai Alexe",
      "Mario Santa Cruz",
      "Sara Hahner",
      "Harrison Cook",
      "Helen Theissen",
      "Mariana Clare",
      "Cathal O'Brien",
      "Jan Polster",
      "Linus Magnusson",
      "Gert Mertes",
      "Florian Pinault",
      "Baudouin Raoult",
      "Patricia de Rosnay",
      "Richard Forbes",
      "Matthew Chantry"
    ],
    "published": "2025-09-23",
    "abstract": "We present an update to ECMWF's machine-learned weather forecasting model\nAIFS Single with several key improvements. The model now incorporates physical\nconsistency constraints through bounding layers, an updated training schedule,\nand an expanded set of variables. The physical constraints substantially\nimprove precipitation forecasts and the new variables show a high level of\nskill. Upper-air headline scores also show improvement over the previous AIFS\nversion. The AIFS has been fully operational at ECMWF since the 25th of\nFebruary 2025.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Training-Free Data Assimilation with GenCast",
    "url": "http://arxiv.org/abs/2509.18811v1",
    "authors": [
      "Thomas Savary",
      "Fran\u00e7ois Rozet",
      "Gilles Louppe"
    ],
    "published": "2025-09-23",
    "abstract": "Data assimilation is widely used in many disciplines such as meteorology,\noceanography, and robotics to estimate the state of a dynamical system from\nnoisy observations. In this work, we propose a lightweight and general method\nto perform data assimilation using diffusion models pre-trained for emulating\ndynamical systems. Our method builds on particle filters, a class of data\nassimilation algorithms, and does not require any further training. As a\nguiding example throughout this work, we illustrate our methodology on GenCast,\na diffusion-based model that generates global ensemble weather forecasts.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Climate-Adaptive and Cascade-Constrained Machine Learning Prediction for Sea Surface Height under Greenhouse Warming",
    "url": "http://arxiv.org/abs/2509.18741v1",
    "authors": [
      "Tianmu Zheng",
      "Ru Chen",
      "Xin Su",
      "Gang Huang",
      "Bingzheng Yan"
    ],
    "published": "2025-09-23",
    "abstract": "Machine learning (ML) has achieved remarkable success in climate and marine\nscience. Given that greenhouse warming fundamentally reshapes ocean conditions\nsuch as stratification, circulation patterns and eddy activity, evaluating the\nclimate adaptability of the ML model is crucial. While physical constraints\nhave been shown to enhance the performance of ML models, kinetic energy (KE)\ncascade has not been used as a constraint despite its importance in regulating\nmulti-scale ocean motions. Here we develop two sea surface height (SSH)\nprediction models (with and without KE cascade constraint) and quantify their\nclimate adaptability at the Kuroshio Extension. Our results demonstrate that\nboth models exhibit only slight performance degradation under greenhouse\nwarming conditions. Incorporating the KE cascade as a physical constraint\nsignificantly improve the model performance, reducing eddy kinetic energy\nerrors by 14.7% in the present climate and 15.9% under greenhouse warming. This\nwork presents the first application of the kinetic energy (KE) cascade as a\nphysical constraint for ML based ocean state prediction and demonstrates its\nrobust adaptability across climates, offering guidance for the further\ndevelopment of global ML models for both present and future conditions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning",
    "url": "http://arxiv.org/abs/2509.18553v1",
    "authors": [
      "Richa Rawat",
      "Faisal Ahmed"
    ],
    "published": "2025-09-23",
    "abstract": "Cancer is one of the leading health challenges for women, specifically breast\nand ovarian cancer. Early detection can help improve the survival rate through\ntimely intervention and treatment. Traditional methods of detecting cancer\ninvolve manually examining mammograms, CT scans, ultrasounds, and other imaging\ntypes. However, this makes the process labor-intensive and requires the\nexpertise of trained pathologists. Hence, making it both time-consuming and\nresource-intensive. In this paper, we introduce a novel vision transformer\n(ViT)-based method for detecting and classifying breast and ovarian cancer. We\nuse a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both\nbinary and multi-class classification tasks using publicly available\nhistopathological image datasets. Further, we use a preprocessing pipeline that\nconverts raw histophological images into standardized PyTorch tensors, which\nare compatible with the ViT architecture and also help improve the model\nperformance. We evaluated the performance of our model on two benchmark\ndatasets: the BreakHis dataset for binary classification and the UBC-OCEAN\ndataset for five-class classification without any data augmentation. Our model\nsurpasses existing CNN, ViT, and topological data analysis-based approaches in\nbinary classification. For multi-class classification, it is evaluated against\nrecent topological methods and demonstrates superior performance. Our study\nhighlights the effectiveness of Vision Transformer-based transfer learning\ncombined with efficient preprocessing in oncological diagnostics.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Technical overview and architecture of the FastNet Machine Learning weather prediction model, version 1.0",
    "url": "http://arxiv.org/abs/2509.17658v1",
    "authors": [
      "Eric G. Daub",
      "Tom Dunstan",
      "Thusal Bennett",
      "Matthew Burnand",
      "James Chappell",
      "Alejandro Coca-Castro",
      "Noushin Eftekhari",
      "J. Scott Hosking",
      "Manvendra Janmaijaya",
      "Jon Lillis",
      "David Salvador-Jasin",
      "Nathan Simpson",
      "Oliver T Strickson",
      "Ryan Sze-Yin Chan",
      "Mohamad Elmasri",
      "Lydia Allegranza France",
      "Sam Madge",
      "James Robinson",
      "Adam A. Scaife",
      "David Walters",
      "Peter Yatsyshin",
      "Theo McCaie",
      "Levan Bokeria",
      "Hannah Brown",
      "Tom Dodds",
      "David Llewellyn-Jones",
      "Sophia Moreton",
      "Tom Potter",
      "Iain Stenson",
      "Louisa van Zeeland",
      "Karina Bett-Williams",
      "Kirstine Ida Dale"
    ],
    "published": "2025-09-22",
    "abstract": "We present FastNet version 1.0, a data-driven medium range numerical weather\nprediction (NWP) model based on a Graph Neural Network architecture, developed\njointly between the Alan Turing Institute and the Met Office. FastNet uses an\nencode-process-decode structure to produce deterministic global weather\npredictions out to 10 days. The architecture is independent of spatial\nresolution and we have trained models at 1$^{\\circ}$ and 0.25$^{\\circ}$\nresolution, with a six hour time step. FastNet uses a multi-level mesh in the\nprocessor, which is able to capture both short-range and long-range patterns in\nthe spatial structure of the atmosphere. The model is pre-trained on ECMWF's\nERA5 reanalysis data and then fine-tuned on additional autoregressive rollout\nsteps, which improves accuracy over longer time horizons. We evaluate the model\nperformance at 1.5$^{\\circ}$ resolution using 2022 as a hold-out year and\ncompare with the Met Office Global Model, finding that FastNet surpasses the\nskill of the current Met Office Global Model NWP system using a variety of\nevaluation metrics on a number of atmospheric variables. Our results show that\nboth our 1$^{\\circ}$ and 0.25$^{\\circ}$ FastNet models outperform the current\nGlobal Model and produce results with predictive skill approaching those of\nother data-driven models trained on 0.25$^{\\circ}$ ERA5.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FastNet: Improving the physical consistency of machine-learning weather prediction models through loss function design",
    "url": "http://arxiv.org/abs/2509.17601v1",
    "authors": [
      "Tom Dunstan",
      "Oliver Strickson",
      "Thusal Bennett",
      "Jack Bowyer",
      "Matthew Burnand",
      "James Chappell",
      "Alejandro Coca-Castro",
      "Kirstine Ida Dale",
      "Eric G. Daub",
      "Noushin Eftekhari",
      "Manvendra Janmaijaya",
      "Jon Lillis",
      "David Salvador-Jasin",
      "Nathan Simpson",
      "Ryan Sze-Yin Chan",
      "Mohamad Elmasri",
      "Lydia Allegranza France",
      "Sam Madge",
      "Levan Bokeria",
      "Hannah Brown",
      "Tom Dodds",
      "Anna-Louise Ellis",
      "David Llewellyn-Jones",
      "Theo McCaie",
      "Sophia Moreton",
      "Tom Potter",
      "James Robinson",
      "Adam A. Scaife",
      "Iain Stenson",
      "David Walters",
      "Karina Bett-Williams",
      "Louisa van Zeeland",
      "Peter Yatsyshin",
      "J. Scott Hosking"
    ],
    "published": "2025-09-22",
    "abstract": "Machine learning weather prediction (MLWP) models have demonstrated\nremarkable potential in delivering accurate forecasts at significantly reduced\ncomputational cost compared to traditional numerical weather prediction (NWP)\nsystems. However, challenges remain in ensuring the physical consistency of\nMLWP outputs, particularly in deterministic settings. This study presents\nFastNet, a graph neural network (GNN)-based global prediction model, and\ninvestigates the impact of alternative loss function designs on improving the\nphysical realism of its forecasts. We explore three key modifications to the\nstandard mean squared error (MSE) loss: (1) a modified spherical harmonic (MSH)\nloss that penalises spectral amplitude errors to reduce blurring and enhance\nsmall-scale structure retention; (2) inclusion of horizontal gradient terms in\nthe loss to suppress non-physical artefacts; and (3) an alternative wind\nrepresentation that decouples speed and direction to better capture extreme\nwind events. Results show that while the MSH and gradient-based losses\n\\textit{alone} may slightly degrade RMSE scores, when trained in combination\nthe model exhibits very similar MSE performance to an MSE-trained model while\nat the same time significantly improving spectral fidelity and physical\nconsistency. The alternative wind representation further improves wind speed\naccuracy and reduces directional bias. Collectively, these findings highlight\nthe importance of loss function design as a mechanism for embedding domain\nknowledge into MLWP models and advancing their operational readiness.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GeoLink: Empowering Remote Sensing Foundation Model with OpenStreetMap Data",
    "url": "http://arxiv.org/abs/2509.26016v1",
    "authors": [
      "Lubian Bai",
      "Xiuyuan Zhang",
      "Siqi Zhang",
      "Zepeng Zhang",
      "Haoyu Wang",
      "Wei Qin",
      "Shihong Du"
    ],
    "published": "2025-09-30",
    "abstract": "Integrating ground-level geospatial data with rich geographic context, like\nOpenStreetMap (OSM), into remote sensing (RS) foundation models (FMs) is\nessential for advancing geospatial intelligence and supporting a broad spectrum\nof tasks. However, modality gap between RS and OSM data, including differences\nin data structure, content, and spatial granularity, makes effective synergy\nhighly challenging, and most existing RS FMs focus on imagery alone. To this\nend, this study presents GeoLink, a multimodal framework that leverages OSM\ndata to enhance RS FM during both the pretraining and downstream task stages.\nSpecifically, GeoLink enhances RS self-supervised pretraining using\nmulti-granularity learning signals derived from OSM data, guided by cross-modal\nspatial correlations for information interaction and collaboration. It also\nintroduces image mask-reconstruction to enable sparse input for efficient\npretraining. For downstream tasks, GeoLink generates both unimodal and\nmultimodal fine-grained encodings to support a wide range of applications, from\ncommon RS interpretation tasks like land cover classification to more\ncomprehensive geographic tasks like urban function zone mapping. Extensive\nexperiments show that incorporating OSM data during pretraining enhances the\nperformance of the RS image encoder, while fusing RS and OSM data in downstream\ntasks improves the FM's adaptability to complex geographic scenarios. These\nresults underscore the potential of multimodal synergy in advancing high-level\ngeospatial artificial intelligence. Moreover, we find that spatial correlation\nplays a crucial role in enabling effective multimodal geospatial data\nintegration. Code, checkpoints, and using examples are released at\nhttps://github.com/bailubin/GeoLink_NeurIPS2025",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Environment-Aware Satellite Image Generation with Diffusion Models",
    "url": "http://arxiv.org/abs/2509.24875v1",
    "authors": [
      "Nikos Kostagiolas",
      "Pantelis Georgiades",
      "Yannis Panagakis",
      "Mihalis A. Nicolaou"
    ],
    "published": "2025-09-29",
    "abstract": "Diffusion-based foundation models have recently garnered much attention in\nthe field of generative modeling due to their ability to generate images of\nhigh quality and fidelity. Although not straightforward, their recent\napplication to the field of remote sensing signaled the first successful trials\ntowards harnessing the large volume of publicly available datasets containing\nmultimodal information. Despite their success, existing methods face\nconsiderable limitations: they rely on limited environmental context, struggle\nwith missing or corrupted data, and often fail to reliably reflect user\nintentions in generated outputs. In this work, we propose a novel diffusion\nmodel conditioned on environmental context, that is able to generate satellite\nimages by conditioning from any combination of three different control signals:\na) text, b) metadata, and c) visual data. In contrast to previous works, the\nproposed method is i) to our knowledge, the first of its kind to condition\nsatellite image generation on dynamic environmental conditions as part of its\ncontrol signals, and ii) incorporating a metadata fusion strategy that models\nattribute embedding interactions to account for partially corrupt and/or\nmissing observations. Our method outperforms previous methods both\nqualitatively (robustness to missing metadata, higher responsiveness to control\ninputs) and quantitatively (higher fidelity, accuracy, and quality of\ngenerations measured using 6 different metrics) in the trials of single-image\nand temporal generation. The reported results support our hypothesis that\nconditioning on environmental context can improve the performance of foundation\nmodels for satellite imagery, and render our model a promising candidate for\nusage in downstream tasks. The collected 3-modal dataset is to our knowledge,\nthe first publicly-available dataset to combine data from these three different\nmediums.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ]
  },
  {
    "title": "SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing",
    "url": "http://arxiv.org/abs/2509.23927v1",
    "authors": [
      "Yi Yang",
      "Xiaokun Zhang",
      "Qingchen Fang",
      "Ziqi Ye",
      "Rui Li",
      "Li Liu",
      "Haipeng Wang"
    ],
    "published": "2025-09-28",
    "abstract": "Cross-modal artificial intelligence has garnered widespread attention in\nrecent years, achieving significant progress in the study of natural images.\nHowever, existing methods are mostly designed for RGB imagery, leaving a\nsignificant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with\nits all-day, all-weather imaging capabilities, plays an irreplaceable role in\nremote sensing scene understanding. To address this gap, this paper proposes\nSAR-KnowLIP, the first universal SAR multimodal foundational model, along with\nreusable data and evaluation baselines. Specifically: (1) This work introduces\nthe critical yet long-overlooked attribute of geographic information into\nremote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR\ndataset with complete geographic projection properties), covering multiple\nsatellite platforms, 120,000 images, and 135 cities. (2) Aligned structured\ntext is generated through a hierarchical cognitive chain-of-thought (HCoT),\nproviding more than one million multi-dimensional semantic annotations of\nlandforms, regional functions, target attributes, and spatial relationships.\n(3) We design a Self-Consistent Iterative Optimization mechanism that\ncontinuously enhances cross-modal alignment through a self-supervised closed\nloop of contrastive, matching, and reconstruction learning on a transferable\nmultimodal encoder. (4) A unified evaluation benchmark is established across 11\nrepresentative downstream vision and vision-language tasks, with comparisons\nagainst 14 leading foundation models, where SAR-KnowLIP demonstrates leading\nperformance, particularly in object counting and land-cover classification. We\nexpect that SAR-KnowLIP's large-scale multimodal data, transferable model\narchitecture, and comprehensive experimental benchmark will significantly\nadvance the development of SAR multimodal baseline models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Learning Regional Monsoon Patterns with a Multimodal Attention U-Net",
    "url": "http://arxiv.org/abs/2509.23267v1",
    "authors": [
      "Swaib Ilias Mazumder",
      "Manish Kumar",
      "Aparajita Khan"
    ],
    "published": "2025-09-27",
    "abstract": "Accurate monsoon rainfall prediction is vital for India's agriculture, water\nmanagement, and climate risk planning, yet remains challenging due to sparse\nground observations and complex regional variability. We present a multimodal\ndeep learning framework for high-resolution precipitation classification that\nleverages satellite and Earth observation data. Unlike previous rainfall\nprediction models based on coarse 5-50 km grids, we curate a new 1 km\nresolution dataset for five Indian states, integrating seven key geospatial\nmodalities: land surface temperature, vegetation (NDVI), soil moisture,\nrelative humidity, wind speed, elevation, and land use, covering the\nJune-September 2024 monsoon season. Our approach uses an attention-guided U-Net\narchitecture to capture spatial patterns and temporal dependencies across\nmodalities, combined with focal and dice loss functions to handle rainfall\nclass imbalance defined by the India Meteorological Department (IMD).\nExperiments demonstrate that our multimodal framework consistently outperforms\nunimodal baselines and existing deep learning methods, especially in extreme\nrainfall categories. This work contributes a scalable framework, benchmark\ndataset, and state-of-the-art results for regional monsoon forecasting, climate\nresilience, and geospatial AI applications in India.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "A Weather Foundation Model for the Power Grid",
    "url": "http://arxiv.org/abs/2509.25268v1",
    "authors": [
      "Cristian Bodnar",
      "Rapha\u00ebl Rousseau-Rizzi",
      "Nikhil Shankar",
      "James Merleau",
      "Stylianos Flampouris",
      "Guillem Candille",
      "Slavica Antic",
      "Fran\u00e7ois Miralles",
      "Jayesh K. Gupta"
    ],
    "published": "2025-09-28",
    "abstract": "Weather foundation models (WFMs) have recently set new benchmarks in global\nforecast skill, yet their concrete value for the weather-sensitive\ninfrastructure that powers modern society remains largely unexplored. In this\nstudy, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting\nTransformer (GFT), on a rich archive of Hydro-Qu\\'ebec asset\nobservations--including transmission-line weather stations, wind-farm met-mast\nstreams, and icing sensors--to deliver hyper-local, asset-level forecasts for\nfive grid-critical variables: surface temperature, precipitation, hub-height\nwind speed, wind-turbine icing risk, and rime-ice accretion on overhead\nconductors. Across 6-72 h lead times, the tailored model surpasses\nstate-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE)\nby 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%.\nMost importantly, it attains an average precision score of 0.72 for day-ahead\nrime-ice detection, a capability absent from existing operational systems,\nwhich affords several hours of actionable warning for potentially catastrophic\noutage events. These results show that WFMs, when post-trained with small\namounts of high-fidelity, can serve as a practical foundation for\nnext-generation grid-resilience intelligence.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Neighbor-aware informal settlement mapping with graph convolutional networks",
    "url": "http://arxiv.org/abs/2509.26171v1",
    "authors": [
      "Thomas Hallopeau",
      "Joris Gu\u00e9rin",
      "Laurent Demagistri",
      "Christovam Barcellos",
      "Nadine Dessay"
    ],
    "published": "2025-09-30",
    "abstract": "Mapping informal settlements is crucial for addressing challenges related to\nurban planning, public health, and infrastructure in rapidly growing cities.\nGeospatial machine learning has emerged as a key tool for detecting and mapping\nthese areas from remote sensing data. However, existing approaches often treat\nspatial units independently, neglecting the relational structure of the urban\nfabric. We propose a graph-based framework that explicitly incorporates local\ngeographical context into the classification process. Each spatial unit (cell)\nis embedded in a graph structure along with its adjacent neighbors, and a\nlightweight Graph Convolutional Network (GCN) is trained to classify whether\nthe central cell belongs to an informal settlement. Experiments are conducted\non a case study in Rio de Janeiro using spatial cross-validation across five\ndistinct zones, ensuring robustness and generalizability across heterogeneous\nurban landscapes. Our method outperforms standard baselines, improving Kappa\ncoefficient by 17 points over individual cell classification. We also show that\ngraph-based modeling surpasses simple feature concatenation of neighboring\ncells, demonstrating the benefit of encoding spatial structure for urban scene\nunderstanding.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Overview of GeoLifeCLEF 2023: Species Composition Prediction with High Spatial Resolution at Continental Scale Using Remote Sensing",
    "url": "http://arxiv.org/abs/2509.25816v1",
    "authors": [
      "Christophe Botella",
      "Benjamin Deneu",
      "Diego Marcos",
      "Maximilien Servajean",
      "Theo Larcher",
      "Cesar Leblanc",
      "Joaquim Estopinan",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2025-09-30",
    "abstract": "Understanding the spatio-temporal distribution of species is a cornerstone of\necology and conservation. By pairing species observations with geographic and\nenvironmental predictors, researchers can model the relationship between an\nenvironment and the species which may be found there. To advance the\nstate-of-the-art in this area with deep learning models and remote sensing\ndata, we organized an open machine learning challenge called GeoLifeCLEF 2023.\nThe training dataset comprised 5 million plant species observations (single\npositive label per sample) distributed across Europe and covering most of its\nflora, high-resolution rasters: remote sensing imagery, land cover, elevation,\nin addition to coarse-resolution data: climate, soil and human footprint\nvariables. In this multi-label classification task, we evaluated models ability\nto predict the species composition in 22 thousand small plots based on\nstandardized surveys. This paper presents an overview of the competition,\nsynthesizes the approaches used by the participating teams, and analyzes the\nmain results. In particular, we highlight the biases faced by the methods\nfitted to single positive labels when it comes to the multi-label evaluation,\nand the new and effective learning strategy combining single and multi-label\ndata in training.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Spatial-Spectral Binarized Neural Network for Panchromatic and Multi-spectral Images Fusion",
    "url": "http://arxiv.org/abs/2509.23321v2",
    "authors": [
      "Yizhen Jiang",
      "Mengting Ma",
      "Anqi Zhu",
      "Xiaowen Ma",
      "Jiaxin Li",
      "Wei Zhang"
    ],
    "published": "2025-09-27",
    "abstract": "Remote sensing pansharpening aims to reconstruct spatial-spectral properties\nduring the fusion of panchromatic (PAN) images and low-resolution\nmulti-spectral (LR-MS) images, finally generating the high-resolution\nmulti-spectral (HR-MS) images. Although deep learning-based models have\nachieved excellent performance, they often come with high computational\ncomplexity, which hinder their applications on resource-limited devices. In\nthis paper, we explore the feasibility of applying the binary neural network\n(BNN) to pan-sharpening. Nevertheless, there are two main issues with\nbinarizing pan-sharpening models: (i) the binarization will cause serious\nspectral distortion due to the inconsistent spectral distribution of the\nPAN/LR-MS images; (ii) the common binary convolution kernel is difficult to\nadapt to the multi-scale and anisotropic spatial features of remote sensing\nobjects, resulting in serious degradation of contours. To address the above\nissues, we design the customized spatial-spectral binarized convolution\n(S2B-Conv), which is composed of the Spectral-Redistribution Mechanism (SRM)\nand Gabor Spatial Feature Amplifier (GSFA). Specifically, SRM employs an affine\ntransformation, generating its scaling and bias parameters through a dynamic\nlearning process. GSFA, which randomly selects different frequencies and angles\nwithin a preset range, enables to better handle multi-scale and-directional\nspatial features. A series of S2B-Conv form a brand-new binary network for\npan-sharpening, dubbed as S2BNet. Extensive quantitative and qualitative\nexperiments have shown our high-efficiency binarized pan-sharpening method can\nattain a promising performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification",
    "url": "http://arxiv.org/abs/2509.23310v1",
    "authors": [
      "Hao Liu",
      "Yongjie Zheng",
      "Yuhan Kang",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-09-27",
    "abstract": "Deep learning-based techniques for the analysis of multimodal remote sensing\ndata have become popular due to their ability to effectively integrate\ncomplementary spatial, spectral, and structural information from different\nsensors. Recently, denoising diffusion probabilistic models (DDPMs) have\nattracted attention in the remote sensing community due to their powerful\nability to capture robust and complex spatial-spectral distributions. However,\npre-training multimodal DDPMs may result in modality imbalance, and effectively\nleveraging diffusion features to guide complementary diversity feature\nextraction remains an open question. To address these issues, this paper\nproposes a balanced diffusion-guided fusion (BDGF) framework that leverages\nmultimodal diffusion features to guide a multi-branch network for land-cover\nclassification. Specifically, we propose an adaptive modality masking strategy\nto encourage the DDPMs to obtain a modality-balanced rather than spectral\nimage-dominated data distribution. Subsequently, these diffusion features\nhierarchically guide feature extraction among CNN, Mamba, and transformer\nnetworks by integrating feature fusion, group channel attention, and\ncross-attention mechanisms. Finally, a mutual learning strategy is developed to\nenhance inter-branch collaboration by aligning the probability entropy and\nfeature similarity of individual subnetworks. Extensive experiments on four\nmultimodal remote sensing datasets demonstrate that the proposed method\nachieves superior classification performance. The code is available at\nhttps://github.com/HaoLiu-XDU/BDGF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
    "url": "http://arxiv.org/abs/2509.26536v1",
    "authors": [
      "Yida Xue",
      "Mingjun Mao",
      "Xiangyuan Ru",
      "Yuqi Zhu",
      "Baochang Ren",
      "Shuofei Qiao",
      "Mengru Wang",
      "Shumin Deng",
      "Xinyu An",
      "Ningyu Zhang",
      "Ying Chen",
      "Huajun Chen"
    ],
    "published": "2025-09-30",
    "abstract": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "EnScale: Temporally-consistent multivariate generative downscaling via proper scoring rules",
    "url": "http://arxiv.org/abs/2509.26258v1",
    "authors": [
      "Maybritt Schillinger",
      "Maxim Samarin",
      "Xinwei Shen",
      "Reto Knutti",
      "Nicolai Meinshausen"
    ],
    "published": "2025-09-30",
    "abstract": "The practical use of future climate projections from global circulation\nmodels (GCMs) is often limited by their coarse spatial resolution, requiring\ndownscaling to generate high-resolution data. Regional climate models (RCMs)\nprovide this refinement, but are computationally expensive. To address this\nissue, machine learning models can learn the downscaling function, mapping\ncoarse GCM outputs to high-resolution fields. Among these, generative\napproaches aim to capture the full conditional distribution of RCM data given\ncoarse-scale GCM data, which is characterized by large variability and thus\nchallenging to model accurately. We introduce EnScale, a generative machine\nlearning framework that emulates the full GCM-to-RCM map by training on\nmultiple pairs of GCM and corresponding RCM data. It first adjusts large-scale\nmismatches between GCM and coarsened RCM data, followed by a super-resolution\nstep to generate high-resolution fields. Both steps employ generative models\noptimized with the energy score, a proper scoring rule. Compared to\nstate-of-the-art ML downscaling approaches, our setup reduces computational\ncost by about one order of magnitude. EnScale jointly emulates multiple\nvariables -- temperature, precipitation, solar radiation, and wind -- spatially\nconsistent over an area in Central Europe. In addition, we propose a variant\nEnScale-t that enables temporally consistent downscaling. We establish a\ncomprehensive evaluation framework across various categories including\ncalibration, spatial structure, extremes, and multivariate dependencies.\nComparison with diverse benchmarks demonstrates EnScale's strong performance\nand computational efficiency. EnScale offers a promising approach for accurate\nand temporally consistent RCM emulation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields",
    "url": "http://arxiv.org/abs/2509.26005v1",
    "authors": [
      "Rui-Yang Zhang",
      "Henry B. Moss",
      "Lachlan Astfalck",
      "Edward Cripps",
      "David S. Leslie"
    ],
    "published": "2025-09-30",
    "abstract": "We introduce a formal active learning methodology for guiding the placement\nof Lagrangian observers to infer time-dependent vector fields -- a key task in\noceanography, marine science, and ocean engineering -- using a physics-informed\nspatio-temporal Gaussian process surrogate model. The majority of existing\nplacement campaigns either follow standard `space-filling' designs or\nrelatively ad-hoc expert opinions. A key challenge to applying principled\nactive learning in this setting is that Lagrangian observers are continuously\nadvected through the vector field, so they make measurements at different\nlocations and times. It is, therefore, important to consider the likely future\ntrajectories of placed observers to account for the utility of candidate\nplacement locations. To this end, we present BALLAST: Bayesian Active Learning\nwith Look-ahead Amendment for Sea-drifter Trajectories. We observe noticeable\nbenefits of BALLAST-aided sequential observer placement strategies on both\nsynthetic and high-fidelity ocean current models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise",
    "url": "http://arxiv.org/abs/2509.25730v1",
    "authors": [
      "Indu Kant Deo",
      "Akash Venkateshwaran",
      "Rajeev K. Jaiman"
    ],
    "published": "2025-09-30",
    "abstract": "Ship traffic is an increasing source of underwater radiated noise in coastal\nwaters, motivating real-time digital twins of ocean acoustics for operational\nnoise mitigation. We present a physics-guided probabilistic framework to\npredict three-dimensional transmission loss in realistic ocean environments. As\na case study, we consider the Salish Sea along shipping routes from the Pacific\nOcean to the Port of Vancouver. A dataset of over 30 million source-receiver\npairs was generated with a Gaussian beam solver across seasonal sound speed\nprofiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We\nfirst assess sparse variational Gaussian processes (SVGP) and then incorporate\nphysics-based mean functions combining spherical spreading with\nfrequency-dependent absorption. To capture nonlinear effects, we examine deep\nsigma-point processes and stochastic variational deep kernel learning. The\nfinal framework integrates four components: (i) a learnable physics-informed\nmean that represents dominant propagation trends, (ii) a convolutional encoder\nfor bathymetry along the source-receiver track, (iii) a neural encoder for\nsource, receiver, and frequency coordinates, and (iv) a residual SVGP layer\nthat provides calibrated predictive uncertainty. This probabilistic digital\ntwin facilitates the construction of sound-exposure bounds and worst-case\nscenarios for received levels. We further demonstrate the application of the\nframework to ship speed optimization, where predicted transmission loss\ncombined with near-field source models provides sound exposure level estimates\nfor minimizing acoustic impacts on marine mammals. The proposed framework\nadvances uncertainty-aware digital twins for ocean acoustics and illustrates\nhow physics-guided machine learning can support sustainable maritime\noperations.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data",
    "url": "http://arxiv.org/abs/2509.25263v1",
    "authors": [
      "Yifang Zhang",
      "Pengfei Duan",
      "Henan Wang",
      "Shengwu Xiong"
    ],
    "published": "2025-09-28",
    "abstract": "Rainfall nowcasting, which aims to predict precipitation within the next 0 to\n3 hours, is critical for disaster mitigation and real-time response planning.\nHowever, most time series forecasting benchmarks in meteorology are evaluated\non variables with strong periodicity, such as temperature and humidity, which\nfail to reflect model capabilities in more complex and practically meteorology\nscenarios like rainfall nowcasting. To address this gap, we propose\nRainfallBench, a benchmark designed for rainfall nowcasting, a highly\nchallenging and practically relevant task characterized by zero inflation,\ntemporal decay, and non-stationarity, focused on predicting precipitation\nwithin the next 0 to 3 hours. The dataset is derived from five years of\nmeteorological observations, recorded at 15-minute intervals across six\nessential variables, and collected from more than 12,000 GNSS stations\nglobally. In particular, it incorporates precipitable water vapor (PWV), a\ncrucial indicator of rainfall that is absent in other datasets. We further\ndesign specialized evaluation strategies to assess model performance on key\nmeteorological challenges, such as multi-scale prediction and extreme rainfall\nevents, and evaluate over 20 state-of-the-art models across six major\narchitectures on RainfallBench. Additionally, to address the zero-inflation and\ntemporal decay issues overlooked by existing models, we introduce Bi-Focus\nPrecipitation Forecaster (BFPF), a plug-and-play module that incorporates\ndomain-specific priors to enhance rainfall time series forecasting. Statistical\nanalysis and ablation studies validate the comprehensiveness of our dataset as\nwell as the superiority of our methodology. Code and datasets are available at\nhttps://anonymous.4open.science/r/RainfallBench-A710.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment",
    "url": "http://arxiv.org/abs/2510.05617v1",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Iffanice Houndayi",
      "Rym Oualha",
      "Mohamed Aziz Cherif",
      "Kobby Panford-Quainoo",
      "Arnu Pretorius"
    ],
    "published": "2025-10-07",
    "abstract": "Open-access multispectral imagery from missions like Landsat 8-9 and\nSentinel-2 has fueled the development of geospatial foundation models (GFMs)\nfor humanitarian and environmental applications. Yet, their deployment remains\nlimited by (i) the absence of automated geospatial data pipelines and (ii) the\nlarge size of fine-tuned models. Existing GFMs lack workflows for processing\nraw satellite imagery, and downstream adaptations often retain the full\ncomplexity of the original encoder.\n  We present InstaGeo, an open-source, end-to-end framework that addresses\nthese challenges by integrating: (1) automated data curation to transform raw\nimagery into model-ready datasets; (2) task-specific model distillation to\nderive compact, compute-efficient models; and (3) seamless deployment as\ninteractive web-map applications. Using InstaGeo, we reproduced datasets from\nthree published studies and trained models with marginal mIoU differences of\n-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for\ndesert locust prediction. The distilled models are up to 8x smaller than\nstandard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal\naccuracy loss.\n  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger\ncrop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp\nimprovement over prior baselines. Moreover, InstaGeo enables users to progress\nfrom raw data to model deployment within a single working day.\n  By unifying data preparation, model compression, and deployment, InstaGeo\ntransforms research-grade GFMs into practical, low-carbon tools for real-time,\nlarge-scale Earth observation. This approach shifts geospatial AI toward data\nquality and application-driven innovation. Source code, datasets, and model\ncheckpoints are available at:\nhttps://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ]
  },
  {
    "title": "Zephyrus: An Agentic Framework for Weather Science",
    "url": "http://arxiv.org/abs/2510.04017v1",
    "authors": [
      "Sumanth Varambally",
      "Marshall Fisher",
      "Jas Thakker",
      "Yiwei Chen",
      "Zhirui Xia",
      "Yasaman Jafari",
      "Ruijia Niu",
      "Manas Jain",
      "Veeramakali Vignesh Manivannan",
      "Zachary Novack",
      "Luyu Han",
      "Srikar Eranky",
      "Salva R\u00fchling Cachay",
      "Taylor Berg-Kirkpatrick",
      "Duncan Watson-Parris",
      "Yi-An Ma",
      "Rose Yu"
    ],
    "published": "2025-10-05",
    "abstract": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Hyperspectral data augmentation with transformer-based diffusion models",
    "url": "http://arxiv.org/abs/2510.08363v1",
    "authors": [
      "Mattia Ferrari",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-09",
    "abstract": "The introduction of new generation hyperspectral satellite sensors, combined\nwith advancements in deep learning methodologies, has significantly enhanced\nthe ability to discriminate detailed land-cover classes at medium-large scales.\nHowever, a significant challenge in deep learning methods is the risk of\noverfitting when training networks with small labeled datasets. In this work,\nwe propose a data augmentation technique that leverages a guided diffusion\nmodel. To effectively train the model with a limited number of labeled samples\nand to capture complex patterns in the data, we implement a lightweight\ntransformer network. Additionally, we introduce a modified weighted loss\nfunction and an optimized cosine variance scheduler, which facilitate fast and\neffective training on small datasets. We evaluate the effectiveness of the\nproposed method on a forest classification task with 10 different forest types\nusing hyperspectral images acquired by the PRISMA satellite. The results\ndemonstrate that the proposed method outperforms other data augmentation\ntechniques in both average and weighted average accuracy. The effectiveness of\nthe method is further highlighted by the stable training behavior of the model,\nwhich addresses a common limitation in the practical application of deep\ngenerative models for data augmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models",
    "url": "http://arxiv.org/abs/2510.07008v1",
    "authors": [
      "Gianmarco Perantoni",
      "Giulio Weikmann",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-08",
    "abstract": "The temporal consistency of yearly land-cover maps is of great importance to\nmodel the evolution and change of the land cover over the years. In this paper,\nwe focus the attention on a novel approach to classification of yearly\nsatellite image time series (SITS) that combines deep learning with Bayesian\nmodelling, using Hidden Markov Models (HMMs) integrated with Transformer\nEncoder (TE) based DNNs. The proposed approach aims to capture both i)\nintricate temporal correlations in yearly SITS and ii) specific patterns in\nmultiyear crop type sequences. It leverages the cascade classification of an\nHMM layer built on top of the TE, discerning consistent yearly crop-type\nsequences. Validation on a multiyear crop type classification dataset spanning\n47 crop types and six years of Sentinel-2 acquisitions demonstrates the\nimportance of modelling temporal consistency in the predicted labels. HMMs\nenhance the overall performance and F1 scores, emphasising the effectiveness of\nthe proposed approach.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Label-frugal satellite image change detection with generative virtual exemplar learning",
    "url": "http://arxiv.org/abs/2510.06926v1",
    "authors": [
      "Hichem Sahbi"
    ],
    "published": "2025-10-08",
    "abstract": "Change detection is a major task in remote sensing which consists in finding\nall the occurrences of changes in multi-temporal satellite or aerial images.\nThe success of existing methods, and particularly deep learning ones, is\ntributary to the availability of hand-labeled training data that capture the\nacquisition conditions and the subjectivity of the user (oracle). In this\npaper, we devise a novel change detection algorithm, based on active learning.\nThe main contribution of our work resides in a new model that measures how\nimportant is each unlabeled sample, and provides an oracle with only the most\ncritical samples (also referred to as virtual exemplars) for further labeling.\nThese exemplars are generated, using an invertible graph convnet, as the\noptimum of an adversarial loss that (i) measures representativity, diversity\nand ambiguity of the data, and thereby (ii) challenges (the most) the current\nchange detection criteria, leading to a better re-estimate of these criteria in\nthe subsequent iterations of active learning. Extensive experiments show the\npositive impact of our label-efficient learning model against comparative\nmethods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Explaining raw data complexity to improve satellite onboard processing",
    "url": "http://arxiv.org/abs/2510.06858v2",
    "authors": [
      "Adrien Dorise",
      "Marjorie Bellizzi",
      "Adrien Girard",
      "Benjamin Francesconi",
      "St\u00e9phane May"
    ],
    "published": "2025-10-08",
    "abstract": "With increasing processing power, deploying AI models for remote sensing\ndirectly onboard satellites is becoming feasible. However, new constraints\narise, mainly when using raw, unprocessed sensor data instead of preprocessed\nground-based products. While current solutions primarily rely on preprocessed\nsensor images, few approaches directly leverage raw data. This study\ninvestigates the effects of utilising raw data on deep learning models for\nobject detection and classification tasks. We introduce a simulation workflow\nto generate raw-like products from high-resolution L1 imagery, enabling\nsystemic evaluation. Two object detection models (YOLOv11n and YOLOX-S) are\ntrained on both raw and L1 datasets, and their performance is compared using\nstandard detection metrics and explainability tools. Results indicate that\nwhile both models perform similarly at low to medium confidence thresholds, the\nmodel trained on raw data struggles with object boundary identification at high\nconfidence levels. It suggests that adapting AI architectures with improved\ncontouring methods can enhance object detection on raw images, improving\nonboard AI for remote sensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data",
    "url": "http://arxiv.org/abs/2510.05760v1",
    "authors": [
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-07",
    "abstract": "Deep learning has gained broad interest in remote sensing image scene\nclassification thanks to the effectiveness of deep neural networks in\nextracting the semantics from complex data. However, deep networks require\nlarge amounts of training samples to obtain good generalization capabilities\nand are sensitive to errors in the training labels. This is a problem in remote\nsensing since highly reliable labels can be obtained at high costs and in\nlimited amount. However, many sources of less reliable labeled data are\navailable, e.g., obsolete digital maps. In order to train deep networks with\nlarger datasets, we propose both the combination of single or multiple weak\nsources of labeled data with a small but reliable dataset to generate\nmultisource labeled datasets and a novel training strategy where the\nreliability of each source is taken in consideration. This is done by\nexploiting the transition matrices describing the statistics of the errors of\neach source. The transition matrices are embedded into the labels and used\nduring the training process to weigh each label according to the related\nsource. The proposed method acts as a weighting scheme at gradient level, where\neach instance contributes with different weights to the optimization of\ndifferent classes. The effectiveness of the proposed method is validated by\nexperiments on different datasets. The results proved the robustness and\ncapability of leveraging on unreliable source of labels of the proposed method.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images",
    "url": "http://arxiv.org/abs/2510.04916v1",
    "authors": [
      "Giulio Weikmann",
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-06",
    "abstract": "Deep learning has become increasingly important in remote sensing image\nclassification due to its ability to extract semantic information from complex\ndata. Classification tasks often include predefined label hierarchies that\nrepresent the semantic relationships among classes. However, these hierarchies\nare frequently overlooked, and most approaches focus only on fine-grained\nclassification schemes. In this paper, we present a novel Semantics-Aware\nHierarchical Consensus (SAHC) method for learning hierarchical features and\nrelationships by integrating hierarchy-specific classification heads within a\ndeep network architecture, each specialized in different degrees of class\ngranularity. The proposed approach employs trainable hierarchy matrices, which\nguide the network through the learning of the hierarchical structure in a\nself-supervised manner. Furthermore, we introduce a hierarchical consensus\nmechanism to ensure consistent probability distributions across different\nhierarchical levels. This mechanism acts as a weighted ensemble being able to\neffectively leverage the inherent structure of the hierarchical classification\ntask. The proposed SAHC method is evaluated on three benchmark datasets with\ndifferent degrees of hierarchical complexity on different tasks, using distinct\nbackbone architectures to effectively emphasize its adaptability. Experimental\nresults show both the effectiveness of the proposed approach in guiding network\nlearning and the robustness of the hierarchical consensus for remote sensing\nimage classification tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification",
    "url": "http://arxiv.org/abs/2510.04628v1",
    "authors": [
      "Hao Liu",
      "Yunhao Gao",
      "Wei Li",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-06",
    "abstract": "Deep learning-based methods have achieved significant success in remote\nsensing Earth observation data analysis. Numerous feature fusion techniques\naddress multimodal remote sensing image classification by integrating global\nand local features. However, these techniques often struggle to extract\nstructural and detail features from heterogeneous and redundant multimodal\nimages. With the goal of introducing frequency domain learning to model key and\nsparse detail features, this paper introduces the spatial-spectral-frequency\ninteraction network (S$^2$Fin), which integrates pairwise fusion modules across\nthe spatial, spectral, and frequency domains. Specifically, we propose a\nhigh-frequency sparse enhancement transformer that employs sparse\nspatial-spectral attention to optimize the parameters of the high-frequency\nfilter. Subsequently, a two-level spatial-frequency fusion strategy is\nintroduced, comprising an adaptive frequency channel module that fuses\nlow-frequency structures with enhanced high-frequency details, and a\nhigh-frequency resonance mask that emphasizes sharp edges via phase similarity.\nIn addition, a spatial-spectral attention fusion module further enhances\nfeature extraction at intermediate layers of the network. Experiments on four\nbenchmark multimodal datasets with limited labeled data demonstrate that\nS$^2$Fin performs superior classification, outperforming state-of-the-art\nmethods. The code is available at https://github.com/HaoLiu-XDU/SSFin.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks",
    "url": "http://arxiv.org/abs/2510.03725v1",
    "authors": [
      "Thomas Hallopeau",
      "Joris Gu\u00e9rin",
      "Laurent Demagistri",
      "Youssef Fouzai",
      "Renata Gracie",
      "Vanderlei Pascoal De Matos",
      "Helen Gurgel",
      "Nadine Dessay"
    ],
    "published": "2025-10-04",
    "abstract": "While deep learning methods for detecting informal settlements have already\nbeen developed, they have not yet fully utilized the potential offered by\nrecent pretrained neural networks. We compare two types of pretrained neural\nnetworks for detecting the favelas of Rio de Janeiro: 1. Generic networks\npretrained on large diverse datasets of unspecific images, 2. A specialized\nnetwork pretrained on satellite imagery. While the latter is more specific to\nthe target task, the former has been pretrained on significantly more images.\nHence, this research investigates whether task specificity or data volume\nyields superior performance in urban informal settlement detection.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping",
    "url": "http://arxiv.org/abs/2510.06769v1",
    "authors": [
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-08",
    "abstract": "The quantity and the quality of the training labels are central problems in\nhigh-resolution land-cover mapping with machine-learning-based solutions. In\nthis context, weak labels can be gathered in large quantities by leveraging on\nexisting low-resolution or obsolete products. In this paper, we address the\nproblem of training land-cover classifiers using high-resolution imagery (e.g.,\nSentinel-2) and weak low-resolution reference data (e.g., MODIS -derived\nland-cover maps). Inspired by recent works in Deep Multiple Instance Learning\n(DMIL), we propose a method that trains pixel-level multi-class classifiers and\npredicts low-resolution labels (i.e., patch-level classification), where the\nactual high-resolution labels are learned implicitly without direct\nsupervision. This is achieved with flexible pooling layers that are able to\nlink the semantics of the pixels in the high-resolution imagery to the\nlow-resolution reference labels. Then, the Multiple Instance Learning (MIL)\nproblem is re-framed in a multi-class and in a multi-label setting. In the\nformer, the low-resolution annotation represents the majority of the pixels in\nthe patch. In the latter, the annotation only provides us information on the\npresence of one of the land-cover classes in the patch and thus multiple labels\ncan be considered valid for a patch at a time, whereas the low-resolution\nlabels provide us only one label. Therefore, the classifier is trained with a\nPositive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020\nIEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed\nframework compared to standard training strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Do Superpixel Segmentation Methods Influence Deforestation Image Classification?",
    "url": "http://arxiv.org/abs/2510.04645v1",
    "authors": [
      "Hugo Resende",
      "Fabio A. Faria",
      "Eduardo B. Neto",
      "Isabela Borlido",
      "Victor Sundermann",
      "Silvio Jamil F. Guimar\u00e3es",
      "\u00c1lvaro L. Fazenda"
    ],
    "published": "2025-10-06",
    "abstract": "Image segmentation is a crucial step in various visual applications,\nincluding environmental monitoring through remote sensing. In the context of\nthe ForestEyes project, which combines citizen science and machine learning to\ndetect deforestation in tropical forests, image segments are used for labeling\nby volunteers and subsequent model training. Traditionally, the Simple Linear\nIterative Clustering (SLIC) algorithm is adopted as the segmentation method.\nHowever, recent studies have indicated that other superpixel-based methods\noutperform SLIC in remote sensing image segmentation, and might suggest that\nthey are more suitable for the task of detecting deforested areas. In this\nsense, this study investigated the impact of the four best segmentation\nmethods, together with SLIC, on the training of classifiers for the target\napplication. Initially, the results showed little variation in performance\namong segmentation methods, even when selecting the top five classifiers using\nthe PyCaret AutoML library. However, by applying a classifier fusion approach\n(ensemble of classifiers), noticeable improvements in balanced accuracy were\nobserved, highlighting the importance of both the choice of segmentation method\nand the combination of machine learning-based models for deforestation\ndetection tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Zeeman: A Deep Learning Regional Atmospheric Chemistry Transport Model",
    "url": "http://arxiv.org/abs/2510.06140v1",
    "authors": [
      "Mijie Pang",
      "Jianbing Jin",
      "Arjo Segers",
      "Hai Xiang Lin",
      "Guoqiang Wang",
      "Hong Liao",
      "Wei Han"
    ],
    "published": "2025-10-07",
    "abstract": "Atmospheric chemistry encapsulates the emission of various pollutants, the\ncomplex chemistry reactions, and the meteorology dominant transport, which form\na dynamic system that governs air quality. While deep learning (DL) models have\nshown promise in capturing intricate patterns for forecasting individual\natmospheric component - such as PM2.5 and ozone - the critical interactions\namong multiple pollutants and the combined influence of emissions and\nmeteorology are often overlook. This study introduces an advanced DL-based\natmospheric chemistry transport model Zeeman for multi-component atmospheric\nchemistry simulation. Leveraging an attention mechanism, our model effectively\ncaptures the nuanced relationships among these constituents. Performance\nmetrics demonstrate that our approach rivals numerical models, offering an\nefficient solution for atmospheric chemistry. In the future, this model could\nbe further integrated with data assimilation techniques to facilitate efficient\nand accurate atmospheric emission estimation and concentration forecast.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Deep Learning Reconstruction of Tropical Cyclogenesis in the Western North Pacific from Climate Reanalysis Dataset",
    "url": "http://arxiv.org/abs/2510.06118v1",
    "authors": [
      "Duc-Trong Le",
      "Tran-Binh Dang",
      "Anh-Duc Hoang Gia",
      "Duc-Hai Nguyen",
      "Minh-Hoa Tien",
      "Xuan-Truong Ngo",
      "Quang-Trung Luu",
      "Quang-Lap Luu",
      "Tai-Hung Nguyen",
      "Thanh T. N. Nguyen",
      "Chanh Kieu"
    ],
    "published": "2025-10-07",
    "abstract": "This study presents a deep learning (DL) architecture based on residual\nconvolutional neural networks (ResNet) to reconstruct the climatology of\ntropical cyclogenesis (TCG) in the Western North Pacific (WNP) basin from\nclimate reanalysis datasets. Using different TCG data labeling strategies and\ndata enrichment windows for the NASA Modern-Era Retrospective analysis for\nResearch and Applications Version 2 (MERRA2) dataset during the 1980-2020\nperiod, we demonstrate that ResNet can reasonably reproduce the overall TCG\nclimatology in the WNP, capturing both its seasonality and spatial\ndistribution. Our sensitivity analyses and optimizations show that this TCG\nreconstruction depends on both the type of TCG climatology that one wishes to\nreconstruct and the strategies used to label TCG data. Of interest, analyses of\ndifferent input features reveal that DL-based reconstruction of TCG climatology\nneeds only a subset of channels rather than all available data, which is\nconsistent with previous modeling and observational studies of TCG. These\nresults not only enhance our understanding of the TCG process but also provide\na promising pathway for predicting or downscaling TCG climatology based on\nlarge-scale environments from global model forecasts or climate output.\nOverall, our study demonstrates that DL can offer an effective approach for\nstudying TC climatology beyond the traditional physical-based simulations and\nvortex-tracking algorithms used in current climate model analyses.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "ResNet"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "Developing a Sequential Deep Learning Pipeline to Model Alaskan Permafrost Thaw Under Climate Change",
    "url": "http://arxiv.org/abs/2510.06258v1",
    "authors": [
      "Addina Rahaman"
    ],
    "published": "2025-10-05",
    "abstract": "Changing climate conditions threaten the natural permafrost thaw-freeze\ncycle, leading to year-round soil temperatures above 0{\\deg}C. In Alaska, the\nwarming of the topmost permafrost layer, known as the active layer, signals\nelevated greenhouse gas release due to high carbon storage. Accurate soil\ntemperature prediction is therefore essential for risk mitigation and stability\nassessment; however, many existing approaches overlook the numerous factors\ndriving soil thermal dynamics. This study presents a proof-of-concept\nlatitude-based deep learning pipeline for modeling yearly soil temperatures\nacross multiple depths. The framework employs dynamic reanalysis feature data\nfrom the ERA5-Land dataset, static geologic and lithological features,\nsliding-window sequences for seasonal context, a derived scenario signal\nfeature for long-term climate forcing, and latitude band embeddings for spatial\nsensitivity. Five deep learning models were tested: a Temporal Convolutional\nNetwork (TCN), a Transformer, a 1-Dimensional Convolutional Long-Short Term\nMemory (Conv1DLSTM), a Gated-Recurrent Unit (GRU), and a Bidirectional\nLong-Short Term Memory (BiLSTM). Results showed solid recognition of\nlatitudinal and depth-wise temperature discrepancies, with the GRU performing\nbest in sequential temperature pattern detection. Bias-corrected CMIP5 RCP data\nenabled recognition of sinusoidal temperature trends, though limited divergence\nbetween scenarios were observed. This study establishes an end-to-end framework\nfor adopting deep learning in active layer temperature modeling, offering\nseasonal, spatial, and vertical temperature context without intrinsic\nrestrictions on feature selection.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Recognition",
      "Forecast"
    ]
  },
  {
    "title": "Deep learning the sources of MJO predictability: a spectral view of learned features",
    "url": "http://arxiv.org/abs/2510.03582v1",
    "authors": [
      "Lin Yao",
      "Da Yang",
      "James P. C. Duncan",
      "Ashesh Chattopadhyay",
      "Pedram Hassanzadeh",
      "Wahid Bhimji",
      "Bin Yu"
    ],
    "published": "2025-10-04",
    "abstract": "The Madden-Julian oscillation (MJO) is a planetary-scale, intraseasonal\ntropical rainfall phenomenon crucial for global weather and climate; however,\nits dynamics and predictability remain poorly understood. Here, we leverage\ndeep learning (DL) to investigate the sources of MJO predictability, motivated\nby a central difference in MJO theories: which spatial scales are essential for\ndriving the MJO? We first develop a deep convolutional neural network (DCNN) to\nforecast the MJO indices (RMM and ROMI). Our model predicts RMM and ROMI up to\n21 and 33 days, respectively, achieving skills comparable to leading\nsubseasonal-to-seasonal models such as NCEP. To identify the spatial scales\nmost relevant for MJO forecasting, we conduct spectral analysis of the latent\nfeature space and find that large-scale patterns dominate the learned signals.\nAdditional experiments show that models using only large-scale signals as the\ninput have the same skills as those using all the scales, supporting the\nlarge-scale view of the MJO. Meanwhile, we find that small-scale signals remain\ninformative: surprisingly, models using only small-scale input can still\nproduce skillful forecasts up to 1-2 weeks ahead. We show that this is achieved\nby reconstructing the large-scale envelope of the small-scale activities, which\naligns with the multi-scale view of the MJO. Altogether, our findings support\nthat large-scale patterns--whether directly included or reconstructed--may be\nthe primary source of MJO predictability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Beyond the Training Data: Confidence-Guided Mixing of Parameterizations in a Hybrid AI-Climate Model",
    "url": "http://arxiv.org/abs/2510.08107v1",
    "authors": [
      "Helge Heuer",
      "Tom Beucler",
      "Mierk Schwabe",
      "Julien Savre",
      "Manuel Schlund",
      "Veronika Eyring"
    ],
    "published": "2025-10-09",
    "abstract": "Persistent systematic errors in Earth system models (ESMs) arise from\ndifficulties in representing the full diversity of subgrid, multiscale\natmospheric convection and turbulence. Machine learning (ML) parameterizations\ntrained on short high-resolution simulations show strong potential to reduce\nthese errors. However, stable long-term atmospheric simulations with hybrid\n(physics + ML) ESMs remain difficult, as neural networks (NNs) trained offline\noften destabilize online runs. Training convection parameterizations directly\non coarse-grained data is challenging, notably because scales cannot be cleanly\nseparated. This issue is mitigated using data from superparameterized\nsimulations, which provide clearer scale separation. Yet, transferring a\nparameterization from one ESM to another remains difficult due to distribution\nshifts that induce large inference errors. Here, we present a proof-of-concept\nwhere a ClimSim-trained, physics-informed NN convection parameterization is\nsuccessfully transferred to ICON-A. The scheme is (a) trained on adjusted\nClimSim data with subtracted radiative tendencies, and (b) integrated into\nICON-A. The NN parameterization predicts its own error, enabling mixing with a\nconventional convection scheme when confidence is low, thus making the hybrid\nAI-physics model tunable with respect to observations and reanalysis through\nmixing parameters. This improves process understanding by constraining\nconvective tendencies across column water vapor, lower-tropospheric stability,\nand geographical conditions, yielding interpretable regime behavior. In\nAMIP-style setups, several hybrid configurations outperform the default\nconvection scheme (e.g., improved precipitation statistics). With additive\ninput noise during training, both hybrid and pure-ML schemes lead to stable\nsimulations and remain physically consistent for at least 20 years.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Climate Knowledge in Large Language Models",
    "url": "http://arxiv.org/abs/2510.08043v1",
    "authors": [
      "Ivan Kuznetsov",
      "Jacopo Grassi",
      "Dmitrii Pantiukhin",
      "Boris Shapkin",
      "Thomas Jung",
      "Nikolay Koldunov"
    ],
    "published": "2025-10-09",
    "abstract": "Large language models (LLMs) are increasingly deployed for climate-related\napplications, where understanding internal climatological knowledge is crucial\nfor reliability and misinformation risk assessment. Despite growing adoption,\nthe capacity of LLMs to recall climate normals from parametric knowledge\nremains largely uncharacterized. We investigate the capacity of contemporary\nLLMs to recall climate normals without external retrieval, focusing on a\nprototypical query: mean July 2-m air temperature 1991-2020 at specified\nlocations. We construct a global grid of queries at 1{\\deg} resolution land\npoints, providing coordinates and location descriptors, and validate responses\nagainst ERA5 reanalysis. Results show that LLMs encode non-trivial climate\nstructure, capturing latitudinal and topographic patterns, with\nroot-mean-square errors of 3-6 {\\deg}C and biases of $\\pm$1 {\\deg}C. However,\nspatially coherent errors remain, particularly in mountains and high latitudes.\nPerformance degrades sharply above 1500 m, where RMSE reaches 5-13 {\\deg}C\ncompared to 2-4 {\\deg}C at lower elevations. We find that including geographic\ncontext (country, city, region) reduces errors by 27% on average, with larger\nmodels being most sensitive to location descriptors. While models capture the\nglobal mean magnitude of observed warming between 1950-1974 and 2000-2024, they\nfail to reproduce spatial patterns of temperature change, which directly relate\nto assessing climate change. This limitation highlights that while LLMs may\ncapture present-day climate distributions, they struggle to represent the\nregional and local expression of long-term shifts in temperature essential for\nunderstanding climate dynamics. Our evaluation framework provides a\nreproducible benchmark for quantifying parametric climate knowledge in LLMs and\ncomplements existing climate communication assessments.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Climate Model Tuning with Online Synchronization-Based Parameter Estimation",
    "url": "http://arxiv.org/abs/2510.06180v1",
    "authors": [
      "Jordan Seneca",
      "Suzanne Bintanja",
      "Frank M. Selten"
    ],
    "published": "2025-10-07",
    "abstract": "In climate science, the tuning of climate models is a computationally\nintensive problem due to the combination of the high-dimensionality of the\nsystem state and long integration times. Here we demonstrate the potential of a\nparameter estimation algorithm which makes use of synchronization to tune a\nglobal atmospheric model at modest computational costs. We first use it to\ndirectly optimize internal model parameters. We then apply the algorithm to the\nweights of each member of a supermodel ensemble to optimize the overall\npredictions. In both cases, the algorithm is able to find parameters which\nresult in reduced errors in the climatology of the model. Finally, we introduce\na novel approach which combines both methods called adaptive supermodeling,\nwhere the internal parameters of the members of a supermodel are tuned\nsimultaneously with the model weights such that the supermodel predictions are\noptimized. For a case designed to challenge the two previous methods, adaptive\nsupermodeling achieves a performance similar to a perfect model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Representing Subgrid-Scale Cloud Effects in a Radiation Parameterization using Machine Learning: MLe-radiation v1.0",
    "url": "http://arxiv.org/abs/2510.05963v2",
    "authors": [
      "Katharina Hafner",
      "Sara Shamekh",
      "Guillaume Bertoli",
      "Axel Lauer",
      "Robert Pincus",
      "Julien Savre",
      "Veronika Eyring"
    ],
    "published": "2025-10-07",
    "abstract": "Improvements of Machine Learning (ML)-based radiation emulators remain\nconstrained by the underlying assumptions to represent horizontal and vertical\nsubgrid-scale cloud distributions, which continue to introduce substantial\nuncertainties. In this study, we introduce a method to represent the impact of\nsubgrid-scale clouds by applying ML to learn processes from high-resolution\nmodel output with a horizontal grid spacing of 5km. In global storm resolving\nmodels, clouds begin to be explicitly resolved. Coarse-graining these\nhigh-resolution simulations to the resolution of coarser Earth System Models\nyields radiative heating rates that implicitly include subgrid-scale cloud\neffects, without assumptions about their horizontal or vertical distributions.\nWe define the cloud radiative impact as the difference between all-sky and\nclear-sky radiative fluxes, and train the ML component solely on this\ncloud-induced contribution to heating rates. The clear-sky tendencies remain\nbeing computed with a conventional physics-based radiation scheme. This hybrid\ndesign enhances generalization, since the machine-learned part addresses only\nsubgrid-scale cloud effects, while the clear-sky component remains responsive\nto changes in greenhouse gas or aerosol concentrations. Applied to\ncoarse-grained data offline, the ML-enhanced radiation scheme reduces errors by\na factor of 4-10 compared with a conventional coarse-scale radiation scheme.\nThis shows the potential of representing subgrid-scale cloud effects in\nradiation schemes with ML for the next generation of Earth System Models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Mass Conservation on Rails -- Rethinking Physics-Informed Learning of Ice Flow Vector Fields",
    "url": "http://arxiv.org/abs/2510.06286v1",
    "authors": [
      "Kim Bente",
      "Roman Marchant",
      "Fabio Ramos"
    ],
    "published": "2025-10-07",
    "abstract": "To reliably project future sea level rise, ice sheet models require inputs\nthat respect physics. Embedding physical principles like mass conservation into\nmodels that interpolate Antarctic ice flow vector fields from sparse & noisy\nmeasurements not only promotes physical adherence but can also improve accuracy\nand robustness. While physics-informed neural networks (PINNs) impose physics\nas soft penalties, offering flexibility but no physical guarantees, we instead\npropose divergence-free neural networks (dfNNs), which enforce local mass\nconservation exactly via a vector calculus trick. Our comparison of dfNNs,\nPINNs, and unconstrained NNs on ice flux interpolation over Byrd Glacier\nsuggests that \"mass conservation on rails\" yields more reliable estimates, and\nthat directional guidance, a learning strategy leveraging continent-wide\nsatellite velocity data, boosts performance across models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "Benchmarking atmospheric circulation variability in an AI emulator, ACE2, and a hybrid model, NeuralGCM",
    "url": "http://arxiv.org/abs/2510.04466v1",
    "authors": [
      "Ian Baxter",
      "Hamid Pahlavan",
      "Pedram Hassanzadeh",
      "Katharine Rucker",
      "Tiffany Shaw"
    ],
    "published": "2025-10-06",
    "abstract": "Physics-based atmosphere-land models with prescribed sea surface temperature\nhave notable successes but also biases in their ability to represent\natmospheric variability compared to observations. Recently, AI emulators and\nhybrid models have emerged with the potential to overcome these biases, but\nstill require systematic evaluation against metrics grounded in fundamental\natmospheric dynamics. Here, we evaluate the representation of four atmospheric\nvariability benchmarking metrics in a fully data-driven AI emulator (ACE2-ERA5)\nand hybrid model (NeuralGCM). The hybrid model and emulator can capture the\nspectra of large-scale tropical waves and extratropical eddy-mean flow\ninteractions, including critical levels. However, both struggle to capture the\ntimescales associated with quasi-biennial oscillation (QBO, $\\sim 28$ months)\nand Southern annular mode propagation ($\\sim 150$ days). These dynamical\nmetrics serve as an initial benchmarking tool to inform AI model development\nand understand their limitations, which may be essential for\nout-of-distribution applications (e.g., extrapolating to unseen climates).",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Score-based generative emulation of impact-relevant Earth system model outputs",
    "url": "http://arxiv.org/abs/2510.04358v1",
    "authors": [
      "Shahine Bouabid",
      "Andre Nogueira Souza",
      "Raffaele Ferrari"
    ],
    "published": "2025-10-05",
    "abstract": "Policy targets evolve faster than the Couple Model Intercomparison Project\ncycles, complicating adaptation and mitigation planning that must often contend\nwith outdated projections. Climate model output emulators address this gap by\noffering inexpensive surrogates that can rapidly explore alternative futures\nwhile staying close to Earth System Model (ESM) behavior. We focus on emulators\ndesigned to provide inputs to impact models. Using monthly ESM fields of\nnear-surface temperature, precipitation, relative humidity, and wind speed, we\nshow that deep generative models have the potential to model jointly the\ndistribution of variables relevant for impacts. The specific model we propose\nuses score-based diffusion on a spherical mesh and runs on a single mid-range\ngraphical processing unit. We introduce a thorough suite of diagnostics to\ncompare emulator outputs with their parent ESMs, including their probability\ndensities, cross-variable correlations, time of emergence, or tail behavior. We\nevaluate performance across three distinct ESMs in both pre-industrial and\nforced regimes. The results show that the emulator produces distributions that\nclosely match the ESM outputs and captures key forced responses. They also\nreveal important failure cases, notably for variables with a strong regime\nshift in the seasonal cycle. Although not a perfect match to the ESM, the\ninaccuracies of the emulator are small relative to the scale of internal\nvariability in ESM projections. We therefore argue that it shows potential to\nbe useful in supporting impact assessment. We discuss priorities for future\ndevelopment toward daily resolution, finer spatial scales, and bias-aware\ntraining. Code is made available at https://github.com/shahineb/climemu.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints",
    "url": "http://arxiv.org/abs/2510.04006v1",
    "authors": [
      "Hang Fan",
      "Yi Xiao",
      "Yongquan Qu",
      "Fenghua Ling",
      "Ben Fei",
      "Lei Bai",
      "Pierre Gentine"
    ],
    "published": "2025-10-05",
    "abstract": "Data-driven machine learning (ML) models have recently shown promise in\nsurpassing traditional physics-based approaches for weather forecasting,\nleading to a so-called second revolution in weather forecasting. However, most\nML-based forecast models treat reanalysis as the truth and are trained under\nvariable-specific loss weighting, ignoring their physical coupling and spatial\nstructure. Over long time horizons, the forecasts become blurry and physically\nunrealistic under rollout training. To address this, we reinterpret model\ntraining as a weak-constraint four-dimensional variational data assimilation\n(WC-4DVar) problem, treating reanalysis data as imperfect observations. This\nallows the loss function to incorporate reanalysis error covariance and capture\nmultivariate dependencies. In practice, we compute the loss in a latent space\nlearned by an autoencoder (AE), where the reanalysis error covariance becomes\napproximately diagonal, thus avoiding the need to explicitly model it in the\nhigh-dimensional model space. We show that rollout training with latent-space\nconstraints improves long-term forecast skill and better preserves fine-scale\nstructures and physical realism compared to training with model-space loss.\nFinally, we extend this framework to accommodate heterogeneous data sources,\nenabling the forecast model to be trained jointly on reanalysis and\nmulti-source observations within a unified theoretical formulation.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations",
    "url": "http://arxiv.org/abs/2510.13774v1",
    "authors": [
      "Dominik J. M\u00fchlematter",
      "Lin Che",
      "Ye Hong",
      "Martin Raubal",
      "Nina Wiedemann"
    ],
    "published": "2025-10-15",
    "abstract": "Forecasting urban phenomena such as housing prices and public health\nindicators requires the effective integration of various geospatial data.\nCurrent methods primarily utilize task-specific models, while recent foundation\nmodels for spatial representations often support only limited modalities and\nlack multimodal fusion capabilities. To overcome these challenges, we present\nUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal\nFusion (SMF). The framework employs modality-specific encoders to process\ndifferent types of inputs, including street view imagery, remote sensing data,\ncartographic maps, and points of interest (POIs) data. These multimodal inputs\nare integrated via a Transformer-based fusion module that learns unified\nrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwide\ndemonstrates UrbanFusion's strong generalization and predictive performance\ncompared to state-of-the-art GeoAI models. Specifically, it 1) outperforms\nprior foundation models on location-encoding, 2) allows multimodal input during\ninference, and 3) generalizes well to regions unseen during training.\nUrbanFusion can flexibly utilize any subset of available modalities for a given\nlocation during both pretraining and inference, enabling broad applicability\nacross diverse data availability scenarios. All source code is available at\nhttps://github.com/DominikM198/UrbanFusion.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping",
    "url": "http://arxiv.org/abs/2510.11576v2",
    "authors": [
      "Walid Elbarz",
      "Mohamed Bourriz",
      "Hicham Hajji",
      "Hamd Ait Abdelali",
      "Fran\u00e7ois Bourzeix"
    ],
    "published": "2025-10-13",
    "abstract": "Foundation models are transforming Earth observation, but their potential for\nhyperspectral crop mapping remains underexplored. This study benchmarks three\nfoundation models for cereal crop mapping using hyperspectral imagery:\nHyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth\ndataset (a large multitemporal hyperspectral archive). Models were fine-tuned\non manually labeled data from a training region and evaluated on an independent\ntest region. Performance was measured with overall accuracy (OA), average\naccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),\nDOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of\n93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved\n91%, highlighting the importance of model architecture for strong\ngeneralization across geographic regions and sensor platforms. These results\nprovide a systematic evaluation of foundation models for operational\nhyperspectral crop mapping and outline directions for future model development.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework",
    "url": "http://arxiv.org/abs/2510.10084v1",
    "authors": [
      "Meijun Zhou",
      "Gang Mei",
      "Zhengjing Ma",
      "Nengxiong Xu",
      "Jianbing Peng"
    ],
    "published": "2025-10-11",
    "abstract": "Tracking the spatiotemporal evolution of large-scale landslide scars is\ncritical for understanding the evolution mechanisms and failure precursors,\nenabling effective early-warning. However, most existing studies have focused\non single-phase or pre- and post-failure dual-phase landslide identification.\nAlthough these approaches delineate post-failure landslide boundaries, it is\nchallenging to track the spatiotemporal evolution of landslide scars. To\naddress this problem, this study proposes a novel and universal framework for\ntracking the spatiotemporal evolution of large-scale landslide scars using a\nvision foundation model. The key idea behind the proposed framework is to\nreconstruct discrete optical remote sensing images into a continuous video\nsequence. This transformation enables a vision foundation model, which is\ndeveloped for video segmentation, to be used for tracking the evolution of\nlandslide scars. The proposed framework operates within a knowledge-guided,\nauto-propagation, and interactive refinement paradigm to ensure the continuous\nand accurate identification of landslide scars. The proposed framework was\nvalidated through application to two representative cases: the post-failure\nBaige landslide and the active Sela landslide (2017-2025). Results indicate\nthat the proposed framework enables continuous tracking of landslide scars,\ncapturing both failure precursors critical for early warning and post-failure\nevolution essential for assessing secondary hazards and long-term stability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Tracking"
    ]
  },
  {
    "title": "Rethinking deep learning: linear regression remains a key benchmark in predicting terrestrial water storage",
    "url": "http://arxiv.org/abs/2510.10799v1",
    "authors": [
      "Wanshu Nie",
      "Sujay V. Kumar",
      "Junyu Chen",
      "Long Zhao",
      "Olya Skulovich",
      "Jinwoong Yoo",
      "Justin Pflug",
      "Shahryar Khalique Ahmad",
      "Goutam Konapala"
    ],
    "published": "2025-10-12",
    "abstract": "Recent advances in machine learning such as Long Short-Term Memory (LSTM)\nmodels and Transformers have been widely adopted in hydrological applications,\ndemonstrating impressive performance amongst deep learning models and\noutperforming physical models in various tasks. However, their superiority in\npredicting land surface states such as terrestrial water storage (TWS) that are\ndominated by many factors such as natural variability and human driven\nmodifications remains unclear. Here, using the open-access, globally\nrepresentative HydroGlobe dataset - comprising a baseline version derived\nsolely from a land surface model simulation and an advanced version\nincorporating multi-source remote sensing data assimilation - we show that\nlinear regression is a robust benchmark, outperforming the more complex LSTM\nand Temporal Fusion Transformer for TWS prediction. Our findings highlight the\nimportance of including traditional statistical models as benchmarks when\ndeveloping and evaluating deep learning models. Additionally, we emphasize the\ncritical need to establish globally representative benchmark datasets that\ncapture the combined impact of natural variability and human interventions.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery",
    "url": "http://arxiv.org/abs/2510.14709v1",
    "authors": [
      "Caleb Robinson",
      "Kimberly T. Goetz",
      "Christin B. Khan",
      "Meredith Sackett",
      "Kathleen Leonard",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ],
    "published": "2025-10-16",
    "abstract": "Effective monitoring of whale populations is critical for conservation, but\ntraditional survey methods are expensive and difficult to scale. While prior\nwork has shown that whales can be identified in very high-resolution (VHR)\nsatellite imagery, large-scale automated detection remains challenging due to a\nlack of annotated imagery, variability in image quality and environmental\nconditions, and the cost of building robust machine learning pipelines over\nmassive remote sensing archives. We present a semi-automated approach for\nsurfacing possible whale detections in VHR imagery using a statistical anomaly\ndetection method that flags spatial outliers, i.e. \"interesting points\". We\npair this detector with a web-based labeling interface designed to enable\nexperts to quickly annotate the interesting points. We evaluate our system on\nthree benchmark scenes with known whale annotations and achieve recalls of\n90.3% to 96.4%, while reducing the area requiring expert inspection by up to\n99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method\ndoes not rely on labeled training data and offers a scalable first step toward\nfuture machine-assisted marine mammal monitoring from space. We have open\nsourced this pipeline at https://github.com/microsoft/whales.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models",
    "url": "http://arxiv.org/abs/2510.13993v1",
    "authors": [
      "Jia Yun Chua",
      "Argyrios Zolotas",
      "Miguel Arana-Catania"
    ],
    "published": "2025-10-15",
    "abstract": "Remote sensing has become a vital tool across sectors such as urban planning,\nenvironmental monitoring, and disaster response. While the volume of data\ngenerated has increased significantly, traditional vision models are often\nconstrained by the requirement for extensive domain-specific labelled data and\ntheir limited ability to understand the context within complex environments.\nVision Language Models offer a complementary approach by integrating visual and\ntextual data; however, their application to remote sensing remains\nunderexplored, particularly given their generalist nature. This work\ninvestigates the combination of vision models and VLMs to enhance image\nanalysis in remote sensing, with a focus on aircraft detection and scene\nunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and\nGemini aims to achieve more accurate and contextually aware image\ninterpretation. Performance is evaluated on both labelled and unlabelled remote\nsensing data, as well as degraded image scenarios which are crucial for remote\nsensing. The findings show an average MAE improvement of 48.46% across models\nin the accuracy of aircraft detection and counting, especially in challenging\nconditions, in both raw and degraded scenarios. A 6.17% improvement in\nCLIPScore for comprehensive understanding of remote sensing images is obtained.\nThe proposed approach combining traditional vision models and VLMs paves the\nway for more advanced and efficient remote sensing image analysis, especially\nin few-shot learning scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Knowledge-Guided Machine Learning Models to Upscale Evapotranspiration in the U.S. Midwest",
    "url": "http://arxiv.org/abs/2510.11505v1",
    "authors": [
      "Aleksei Rozanov",
      "Samikshya Subedi",
      "Vasudha Sharma",
      "Bryan C. Runck"
    ],
    "published": "2025-10-13",
    "abstract": "Evapotranspiration (ET) plays a critical role in the land-atmosphere\ninteractions, yet its accurate quantification across various spatiotemporal\nscales remains a challenge. In situ measurement approaches, like eddy\ncovariance (EC) or weather station-based ET estimation, allow for measuring ET\nat a single location. Agricultural uses of ET require estimates for each field\nover broad areas, making it infeasible to deploy sensing systems at each\nlocation. This study integrates tree-based and knowledge-guided machine\nlearning (ML) techniques with multispectral remote sensing data, griddled\nmeteorology and EC data to upscale ET across the Midwest United States. We\ncompare four tree-based models - Random Forest, CatBoost, XGBoost, LightGBM -\nand a simple feed-forward artificial neural network in combination with\nfeatures engineered using knowledge-guided ML principles. Models were trained\nand tested on EC towers located in the Midwest of the United States using\nk-fold cross validation with k=5 and site-year, biome stratified train-test\nsplit to avoid data leakage. Results show that LightGBM with knowledge-guided\nfeatures outperformed other methods with an R2=0.86, MSE=14.99 W m^-2 and MAE =\n8.82 W m^-2 according to grouped k-fold validation (k=5). Feature importance\nanalysis shows that knowledge-guided features were most important for\npredicting evapotranspiration. Using the best performing model, we provide a\ndata product at 500 m spatial and one-day temporal resolution for gridded ET\nfor the period of 2019-2024. Intercomparison between the new gridded product\nand state-level weather station-based ET estimates show best-in-class\ncorrespondence.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters",
    "url": "http://arxiv.org/abs/2510.14250v1",
    "authors": [
      "Lianzi Jiang",
      "Jianxin Zhang",
      "Xinyu Han",
      "Huanhe Dong",
      "Xiangrong Wang"
    ],
    "published": "2025-10-16",
    "abstract": "Accurate motion response prediction for elastic Bragg breakwaters is critical\nfor their structural safety and operational integrity in marine environments.\nHowever, conventional deep learning models often exhibit limited generalization\ncapabilities when presented with unseen sea states. These deficiencies stem\nfrom the neglect of natural decay observed in marine systems and inadequate\nmodeling of wave-structure interaction (WSI). To overcome these challenges,\nthis study proposes a novel Physics Prior-Guided Dual-Stream Attention Network\n(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module\nincorporates a learnable temporal decay to assign higher weights to recent\nstates, aiming to emulate the natural decay phenomenon. Meanwhile, the phase\ndifferences guided bidirectional cross-attention (PDG-BCA) module explicitly\ncaptures the bidirectional interaction and phase relationship between waves and\nthe structure using a cosine-based bias within a bidirectional\ncross-computation paradigm. These streams are synergistically integrated\nthrough a global context fusion (GCF) module. Finally, PhysAttnNet is trained\nwith a hybrid time-frequency loss that jointly minimizes time-domain prediction\nerrors and frequency-domain spectral discrepancies. Comprehensive experiments\non wave flume datasets demonstrate that PhysAttnNet significantly outperforms\nmainstream models. Furthermore,cross-scenario generalization tests validate the\nmodel's robustness and adaptability to unseen environments, highlighting its\npotential as a framework to develop predictive models for complex systems in\nocean engineering.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting",
    "url": "http://arxiv.org/abs/2510.13050v1",
    "authors": [
      "Shreya Agrawal",
      "Mohammed Alewi Hassen",
      "Emmanuel Asiedu Brempong",
      "Boris Babenko",
      "Fred Zyda",
      "Olivia Graham",
      "Di Li",
      "Samier Merchant",
      "Santiago Hincapie Potes",
      "Tyler Russell",
      "Danny Cheresnick",
      "Aditya Prakash Kakkirala",
      "Stephan Rasp",
      "Avinatan Hassidim",
      "Yossi Matias",
      "Nal Kalchbrenner",
      "Pramod Gupta",
      "Jason Hickey",
      "Aaron Bell"
    ],
    "published": "2025-10-15",
    "abstract": "Precipitation nowcasting, which predicts rainfall up to a few hours ahead, is\na critical tool for vulnerable communities in the Global South frequently\nexposed to intense, rapidly developing storms. Timely forecasts provide a\ncrucial window to protect lives and livelihoods. Traditional numerical weather\nprediction (NWP) methods suffer from high latency, low spatial and temporal\nresolution, and significant gaps in accuracy across the world. Recent machine\nlearning-based nowcasting methods, common in the Global North, cannot be\nextended to the Global South due to extremely sparse radar coverage. We present\nGlobal MetNet, an operational global machine learning nowcasting model. It\nleverages the Global Precipitation Mission's CORRA dataset, geostationary\nsatellite data, and global NWP data to predict precipitation for the next 12\nhours. The model operates at a high resolution of approximately 0.05{\\deg}\n(~5km) spatially and 15 minutes temporally. Global MetNet significantly\noutperforms industry-standard hourly forecasts and achieves significantly\nhigher skill, making forecasts useful over a much larger area of the world than\npreviously available. Our model demonstrates better skill in data-sparse\nregions than even the best high-resolution NWP models achieve in the US.\nValidated using ground radar and satellite data, it shows significant\nimprovements across key metrics like the critical success index and fractions\nskill score for all precipitation rates and lead times. Crucially, our model\ngenerates forecasts in under a minute, making it readily deployable for\nreal-time applications. It is already deployed for millions of users on Google\nSearch. This work represents a key step in reducing global disparities in\nforecast quality and integrating sparse, high-resolution satellite observations\ninto weather forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Probabilistic Super-Resolution for Urban Micrometeorology via a Schr\u00f6dinger Bridge",
    "url": "http://arxiv.org/abs/2510.12148v1",
    "authors": [
      "Yuki Yasuda",
      "Ryo Onishi"
    ],
    "published": "2025-10-14",
    "abstract": "This study employs a neural network that represents the solution to a\nSchr\\\"odinger bridge problem to perform super-resolution of 2-m temperature in\nan urban area. Schr\\\"odinger bridges generally describe transformations between\ntwo data distributions based on diffusion processes. We use a specific\nSchr\\\"odinger-bridge model (SM) that directly transforms low-resolution data\ninto high-resolution data, unlike denoising diffusion probabilistic models\n(simply, diffusion models; DMs) that generate high-resolution data from\nGaussian noise. Low-resolution and high-resolution data were obtained from\nseparate numerical simulations with a physics-based model under common initial\nand boundary conditions. Compared with a DM, the SM attains comparable accuracy\nat one-fifth the computational cost, requiring 50 neural-network evaluations\nper datum for the DM and only 10 for the SM. Furthermore, high-resolution\nsamples generated by the SM exhibit larger variance, implying superior\nuncertainty quantification relative to the DM. Owing to the reduced\ncomputational cost of the SM, our results suggest the feasibility of real-time\nensemble micrometeorological prediction using SM-based super-resolution.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "Deployment and Development of a Cognitive Teleoreactive Framework for Deep Sea Autonomy",
    "url": "http://arxiv.org/abs/2510.10716v1",
    "authors": [
      "Christopher Thierauf"
    ],
    "published": "2025-10-12",
    "abstract": "A new AUV mission planning and execution software has been tested on AUV\nSentry. Dubbed DINOS-R, it draws inspiration from cognitive architectures and\nAUV control systems to replace the legacy MC architecture. Unlike these\nexisting architectures, however, DINOS-R is built from the ground-up to unify\nsymbolic decision making (for understandable, repeatable, provable behavior)\nwith machine learning techniques and reactive behaviors, for field-readiness\nacross oceanographic platforms. Implemented primarily in Python3, DINOS-R is\nextensible, modular, and reusable, with an emphasis on non-expert use as well\nas growth for future research in oceanography and robot algorithms. Mission\nspecification is flexible, and can be specified declaratively. Behavior\nspecification is similarly flexible, supporting simultaneous use of real-time\ntask planning and hard-coded user specified plans. These features were\ndemonstrated in the field on Sentry, in addition to a variety of simulated\ncases. These results are discussed, and future work is outlined.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Interactive Atmospheric Composition Emulation for Next-Generation Earth System Models",
    "url": "http://arxiv.org/abs/2510.10654v1",
    "authors": [
      "Seyed Mohammad Hassan Erfani",
      "Kara Lamb",
      "Susanne Bauer",
      "Kostas Tsigaridis",
      "Marcus van Lier-Walqui",
      "Gavin Schmidt"
    ],
    "published": "2025-10-12",
    "abstract": "Interactive composition simulations in Earth System Models (ESMs) are\ncomputationally expensive as they transport numerous gaseous and aerosol\ntracers at each timestep. This limits higher-resolution transient climate\nsimulations with current computational resources. ESMs like NASA GISS-ModelE3\n(ModelE) often use pre-computed monthly-averaged atmospheric composition\nconcentrations (Non-Interactive Tracers or NINT) to reduce computational costs.\nWhile NINT significantly cuts computations, it fails to capture real-time\nfeedback between aerosols and other climate processes by relying on\npre-calculated fields. We extended the ModelE NINT version using machine\nlearning (ML) to create Smart NINT, which emulates interactive emissions. Smart\nNINT interactively calculates concentrations using ML with surface emissions\nand meteorological data as inputs, avoiding full physics parameterizations. Our\napproach utilizes a spatiotemporal architecture that possesses a well-matched\ninductive bias to effectively capture the spatial and temporal dependencies in\ntracer evolution. Input data processed through the first 20 vertical levels\n(from the surface up to 656 hPa) using the ModelE OMA scheme. This vertical\nrange covers nearly the entire BCB concentration distribution in the\ntroposphere, where significant variation on short time horizons due to\nsurface-level emissions is observed. Our evaluation shows excellent model\nperformance with R-squared values of 0.92 and Pearson-r of 0.96 at the first\npressure level. This high performance continues through level 15 (808.5 hPa),\nthen gradually decreases as BCB concentrations drop significantly. The model\nmaintains acceptable performance even when tested on data from entirely\ndifferent periods outside the training domain, which is a crucial capability\nfor climate modeling applications requiring reliable long-term projections.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Generative Modeling of Aerosol State Representations",
    "url": "http://arxiv.org/abs/2510.10361v1",
    "authors": [
      "Ehsan Saleh",
      "Saba Ghaffari",
      "Jeffrey H. Curtis",
      "Lekha Patel",
      "Peter A. Bosler",
      "Nicole Riemer",
      "Matthew West"
    ],
    "published": "2025-10-11",
    "abstract": "Aerosol-cloud--radiation interactions remain among the most uncertain\ncomponents of the Earth's climate system, in partdue to the high dimensionality\nof aerosol state representations and the difficulty of obtaining complete\n\\textit{in situ} measurements. Addressing these challenges requires methods\nthat distill complex aerosol properties into compact yet physically meaningful\nforms. Generative autoencoder models provide such a pathway. We present a\nframework for learning deep variational autoencoder (VAE) models of speciated\nmass and number concentration distributions, which capture detailed aerosol\nsize-composition characteristics. By compressing hundreds of original\ndimensions into ten latent variables, the approach enables efficient storage\nand processing while preserving the fidelity of key diagnostics, including\ncloud condensation nuclei (CCN) spectra, optical scattering and absorption\ncoefficients, and ice nucleation properties. Results show that CCN spectra are\neasiest to reconstruct accurately, optical properties are moderately difficult,\nand ice nucleation properties are the most challenging. To improve performance,\nwe introduce a preprocessing optimization strategy that avoids repeated\nretraining and yields latent representations resilient to high-magnitude\nGaussian noise, boosting accuracy for CCN spectra, optical coefficients, and\nfrozen fraction spectra. Finally, we propose a novel realism metric -- based on\nthe sliced Wasserstein distance between generated samples and a held-out test\nset -- for optimizing the KL divergence weight in VAEs. Together, these\ncontributions enable compact, robust, and physically meaningful representations\nof aerosol states for large-scale climate applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": []
  },
  {
    "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration",
    "url": "http://arxiv.org/abs/2510.17670v1",
    "authors": [
      "Yehonathan Refael",
      "Amit Aides",
      "Aviad Barzilai",
      "George Leifman",
      "Genady Beryozkin",
      "Vered Silverman",
      "Bolous Jaber",
      "Tomer Shekel"
    ],
    "published": "2025-10-20",
    "abstract": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Do Satellite Tasks Need Special Pretraining?",
    "url": "http://arxiv.org/abs/2510.17014v1",
    "authors": [
      "Ani Vanyan",
      "Alvard Barseghyan",
      "Hakob Tamazyan",
      "Tigran Galstyan",
      "Vahan Huroyan",
      "Naira Hovakimyan",
      "Hrant Khachatrian"
    ],
    "published": "2025-10-19",
    "abstract": "Foundation models have advanced machine learning across various modalities,\nincluding images. Recently multiple teams trained foundation models specialized\nfor remote sensing applications. This line of research is motivated by the\ndistinct characteristics of remote sensing imagery, specific applications and\ntypes of robustness useful for satellite image analysis. In this work we\nsystematically challenge the idea that specific foundation models are more\nuseful than general-purpose vision foundation models, at least in the small\nscale. First, we design a simple benchmark that measures generalization of\nremote sensing models towards images with lower resolution for two downstream\ntasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,\nan ImageNet-scale satellite imagery dataset, with several modifications\nspecific to remote sensing. We show that none of those pretrained models bring\nconsistent improvements upon general-purpose baselines at the ViT-B scale.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction",
    "url": "http://arxiv.org/abs/2510.18773v1",
    "authors": [
      "Jannis Fleckenstein",
      "David Kreismann",
      "Tamara Rosemary Govindasamy",
      "Thomas Brunschwiler",
      "Etienne Vos",
      "Mattia Rigotti"
    ],
    "published": "2025-10-21",
    "abstract": "As urbanization and climate change progress, urban heat island effects are\nbecoming more frequent and severe. To formulate effective mitigation plans,\ncities require detailed air temperature data, yet conventional machine learning\nmodels with limited data often produce inaccurate predictions, particularly in\nunderserved areas. Geospatial foundation models trained on global unstructured\ndata offer a promising alternative by demonstrating strong generalization and\nrequiring only minimal fine-tuning. In this study, an empirical ground truth of\nurban heat patterns is established by quantifying cooling effects from green\nspaces and benchmarking them against model predictions to evaluate the model's\naccuracy. The foundation model is subsequently fine-tuned to predict land\nsurface temperatures under future climate scenarios, and its practical value is\ndemonstrated through a simulated inpainting that highlights its role for\nmitigation support. The results indicate that foundation models offer a\npowerful way for evaluating urban heat island mitigation strategies in\ndata-scarce regions to support more climate-resilient cities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning",
    "url": "http://arxiv.org/abs/2510.18318v1",
    "authors": [
      "Aaron Bell",
      "Amit Aides",
      "Amr Helmy",
      "Arbaaz Muslim",
      "Aviad Barzilai",
      "Aviv Slobodkin",
      "Bolous Jaber",
      "David Schottlander",
      "George Leifman",
      "Joydeep Paul",
      "Mimi Sun",
      "Nadav Sherman",
      "Natalie Williams",
      "Per Bjornsson",
      "Roy Lee",
      "Ruth Alcantara",
      "Thomas Turnbull",
      "Tomer Shekel",
      "Vered Silverman",
      "Yotam Gigi",
      "Adam Boulanger",
      "Alex Ottenwess",
      "Ali Ahmadalipour",
      "Anna Carter",
      "Charles Elliott",
      "David Andre",
      "Elad Aharoni",
      "Gia Jung",
      "Hassler Thurston",
      "Jacob Bien",
      "Jamie McPike",
      "Juliet Rothenberg",
      "Kartik Hegde",
      "Kel Markert",
      "Kim Philipp Jablonski",
      "Luc Houriez",
      "Monica Bharel",
      "Phing VanLee",
      "Reuven Sayag",
      "Sebastian Pilarski",
      "Shelley Cazares",
      "Shlomi Pasternak",
      "Siduo Jiang",
      "Stone Jiang",
      "Thomas Colthurst",
      "Yang Chen",
      "Yehonathan Refael",
      "Yochai Blau",
      "Yuval Carny",
      "Yael Maguire",
      "Avinatan Hassidim",
      "James Manyika",
      "Tim Thelin",
      "Genady Beryozkin",
      "Gautam Prasad",
      "Luke Barrington",
      "Yossi Matias",
      "Niv Efron",
      "Shravya Shetty"
    ],
    "published": "2025-10-21",
    "abstract": "Geospatial data offers immense potential for understanding our planet.\nHowever, the sheer volume and diversity of this data along with its varied\nresolutions, timescales, and sparsity pose significant challenges for thorough\nanalysis and interpretation. This paper introduces Earth AI, a family of\ngeospatial AI models and agentic reasoning that enables significant advances in\nour ability to unlock novel and profound insights into our planet. This\napproach is built upon foundation models across three key domains--Planet-scale\nImagery, Population, and Environment--and an intelligent Gemini-powered\nreasoning engine. We present rigorous benchmarks showcasing the power and novel\ncapabilities of our foundation models and validate that when used together,\nthey provide complementary value for geospatial inference and their synergies\nunlock superior predictive capabilities. To handle complex, multi-step queries,\nwe developed a Gemini-powered agent that jointly reasons over our multiple\nfoundation models along with large geospatial data sources and tools. On a new\nbenchmark of real-world crisis scenarios, our agent demonstrates the ability to\ndeliver critical and timely insights, effectively bridging the gap between raw\ngeospatial data and actionable understanding.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence",
    "url": "http://arxiv.org/abs/2510.16555v1",
    "authors": [
      "Qiongyan Wang",
      "Xingchen Zou",
      "Yutian Jiang",
      "Haomin Wen",
      "Jiaheng Wei",
      "Qingsong Wen",
      "Yuxuan Liang"
    ],
    "published": "2025-10-18",
    "abstract": "Rapid urbanization intensifies the demand for Urban General Intelligence\n(UGI), referring to AI systems that can understand and reason about complex\nurban environments. Recent studies have built urban foundation models using\nsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit\npersistent geospatial bias, producing regionally skewed predictions and limited\ngeneralization. To this end, we propose Urban-R1, a reinforcement\nlearning-based post-training framework that aligns MLLMs with the objectives of\nUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize\nreasoning across geographic groups and employs urban region profiling as a\nproxy task to provide measurable rewards from multimodal urban data. Extensive\nexperiments across diverse regions and tasks show that Urban-R1 effectively\nmitigates geo-bias and improves cross-region generalization, outperforming both\nSFT-trained and closed-source models. Our results highlight reinforcement\nlearning alignment as a promising pathway toward equitable and trustworthy\nurban intelligence.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters",
    "url": "http://arxiv.org/abs/2510.19329v1",
    "authors": [
      "Panagiotis Agrafiotis",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-10-22",
    "abstract": "Accurate, detailed, and regularly updated bathymetry, coupled with complex\nsemantic content, is essential for under-mapped shallow-water environments\nfacing increasing climatological and anthropogenic pressures. However, existing\napproaches that derive either depth or seabed classes from remote sensing\nimagery treat these tasks in isolation, forfeiting the mutual benefits of their\ninteraction and hindering the broader adoption of deep learning methods. To\naddress these limitations, we introduce Seabed-Net, a unified multi-task\nframework that simultaneously predicts bathymetry and pixel-based seabed\nclassification from remote sensing imagery of various resolutions. Seabed-Net\nemploys dual-branch encoders for bathymetry estimation and pixel-based seabed\nclassification, integrates cross-task features via an Attention Feature Fusion\nmodule and a windowed Swin-Transformer fusion block, and balances objectives\nthrough dynamic task uncertainty weighting. In extensive evaluations at two\nheterogeneous coastal sites, it consistently outperforms traditional empirical\nmodels and traditional machine learning regression methods, achieving up to\n75\\% lower RMSE. It also reduces bathymetric RMSE by 10-30\\% compared to\nstate-of-the-art single-task and multi-task baselines and improves seabed\nclassification accuracy up to 8\\%. Qualitative analyses further demonstrate\nenhanced spatial consistency, sharper habitat boundaries, and corrected depth\nbiases in low-contrast regions. These results confirm that jointly modeling\ndepth with both substrate and seabed habitats yields synergistic gains,\noffering a robust, open solution for integrated shallow-water mapping. Code and\npretrained weights are available at https://github.com/pagraf/Seabed-Net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance",
    "url": "http://arxiv.org/abs/2510.16445v1",
    "authors": [
      "Chien Thai",
      "Mai Xuan Trang",
      "Huong Ninh",
      "Hoang Hiep Ly",
      "Anh Son Le"
    ],
    "published": "2025-10-18",
    "abstract": "Detecting rotated objects accurately and efficiently is a significant\nchallenge in computer vision, particularly in applications such as aerial\nimagery, remote sensing, and autonomous driving. Although traditional object\ndetection frameworks are effective for axis-aligned objects, they often\nunderperform in scenarios involving rotated objects due to their limitations in\ncapturing orientation variations. This paper introduces an improved loss\nfunction aimed at enhancing detection accuracy and robustness by leveraging the\nGaussian bounding box representation and Bhattacharyya distance. In addition,\nwe advocate for the use of an anisotropic Gaussian representation to address\nthe issues associated with isotropic variance in square-like objects. Our\nproposed method addresses these challenges by incorporating a\nrotation-invariant loss function that effectively captures the geometric\nproperties of rotated objects. We integrate this proposed loss function into\nstate-of-the-art deep learning-based rotated object detection detectors, and\nextensive experiments demonstrated significant improvements in mean Average\nPrecision metrics compared to existing methods. The results highlight the\npotential of our approach to establish new benchmark in rotated object\ndetection, with implications for a wide range of applications requiring precise\nand reliable object localization irrespective of orientation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval",
    "url": "http://arxiv.org/abs/2510.20486v1",
    "authors": [
      "Fangjian Zhang",
      "Xiaoyong Zhuge",
      "Wenlan Wang",
      "Haixia Xiao",
      "Yuying Zhu",
      "Siyang Cheng"
    ],
    "published": "2025-10-23",
    "abstract": "Artificial intelligence has advanced quantitative remote sensing, yet its\neffectiveness is constrained by imbalanced label distribution. This imbalance\nleads conventionally trained models to favor common samples, which in turn\ndegrades retrieval performance for rare ones. Rainfall retrieval exemplifies\nthis issue, with performance particularly compromised for heavy rain. This\nstudy proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.\nFollowing a divide-and-conquer strategy, imbalance in the rain distribution is\ndecomposed into two components: zero inflation, defined by the predominance of\nnon-rain samples; and long tail, defined by the disproportionate abundance of\nlight-rain samples relative to heavy-rain samples. A hurdle model is adopted to\nhandle the zero inflation, while IMDL is proposed to address the long tail by\ntransforming the learning object into an unbiased ideal inverse model.\nComprehensive evaluation via statistical metrics and case studies investigating\nrainy weather in eastern China confirms Hurdle-IMDL's superiority over\nconventional, cost-sensitive, generative, and multi-task learning methods. Its\nkey advancements include effective mitigation of systematic underestimation and\na marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a\ngeneralizable approach for addressing imbalance in distributions of\nenvironmental variables, enabling enhanced retrieval of rare yet high-impact\nevents.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals",
    "url": "http://arxiv.org/abs/2510.19917v1",
    "authors": [
      "Trajan Murphy",
      "Akshunna S. Dogra",
      "Hanfeng Gu",
      "Caleb Meredith",
      "Mark Kon",
      "Julio Enrique Castrillion-Candas"
    ],
    "published": "2025-10-22",
    "abstract": "''Noisy'' datasets (regimes with low signal to noise ratios, small sample\nsizes, faulty data collection, etc) remain a key research frontier for\nclassification methods with both theoretical and practical implications. We\nintroduce FINDER, a rigorous framework for analyzing generic classification\nproblems, with tailored algorithms for noisy datasets. FINDER incorporates\nfundamental stochastic analysis ideas into the feature learning and inference\nstages to optimally account for the randomness inherent to all empirical\ndatasets. We construct ''stochastic features'' by first viewing empirical\ndatasets as realizations from an underlying random field (without assumptions\non its exact distribution) and then mapping them to appropriate Hilbert spaces.\nThe Kosambi-Karhunen-Lo\\'eve expansion (KLE) breaks these stochastic features\ninto computable irreducible components, which allow classification over noisy\ndatasets via an eigen-decomposition: data from different classes resides in\ndistinct regions, identified by analyzing the spectrum of the associated\noperators. We validate FINDER on several challenging, data-deficient scientific\ndomains, producing state of the art breakthroughs in: (i) Alzheimer's Disease\nstage classification, (ii) Remote sensing detection of deforestation. We end\nwith a discussion on when FINDER is expected to outperform existing methods,\nits failure modes, and other limitations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Uncertainty evaluation of segmentation models for Earth observation",
    "url": "http://arxiv.org/abs/2510.19586v1",
    "authors": [
      "Melanie Rey",
      "Andriy Mnih",
      "Maxim Neumann",
      "Matt Overlan",
      "Drew Purves"
    ],
    "published": "2025-10-22",
    "abstract": "This paper investigates methods for estimating uncertainty in semantic\nsegmentation predictions derived from satellite imagery. Estimating uncertainty\nfor segmentation presents unique challenges compared to standard image\nclassification, requiring scalable methods producing per-pixel estimates. While\nmost research on this topic has focused on scene understanding or medical\nimaging, this work benchmarks existing methods specifically for remote sensing\nand Earth observation applications. Our evaluation focuses on the practical\nutility of uncertainty measures, testing their ability to identify prediction\nerrors and noise-corrupted input image regions. Experiments are conducted on\ntwo remote sensing datasets, PASTIS and ForTy, selected for their differences\nin scale, geographic coverage, and label confidence. We perform an extensive\nevaluation featuring several models, such as Stochastic Segmentation Networks\nand ensembles, in combination with a number of neural architectures and\nuncertainty metrics. We make a number of practical recommendations based on our\nfindings.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration",
    "url": "http://arxiv.org/abs/2510.19579v1",
    "authors": [
      "Francisco Mena",
      "Dino Ienco",
      "Cassio F. Dantas",
      "Roberto Interdonato",
      "Andreas Dengel"
    ],
    "published": "2025-10-22",
    "abstract": "Multi-modal co-learning is emerging as an effective paradigm in machine\nlearning, enabling models to collaboratively learn from different modalities to\nenhance single-modality predictions. Earth Observation (EO) represents a\nquintessential domain for multi-modal data analysis, wherein diverse remote\nsensors collect data to sense our planet. This unprecedented volume of data\nintroduces novel challenges. Specifically, the access to the same sensor\nmodalities at both training and inference stages becomes increasingly complex\nbased on real-world constraints affecting remote sensing platforms. In this\ncontext, multi-modal co-learning presents a promising strategy to leverage the\nvast amount of sensor-derived data available at the training stage to improve\nsingle-modality models for inference-time deployment. Most current research\nefforts focus on designing customized solutions for either particular\ndownstream tasks or specific modalities available at the inference stage. To\naddress this, we propose a novel multi-modal co-learning framework capable of\ngeneralizing across various tasks without targeting a specific modality for\ninference. Our approach combines contrastive and modality discriminative\nlearning together to guide single-modality models to structure the internal\nmodel manifold into modality-shared and modality-specific information. We\nevaluate our framework on four EO benchmarks spanning classification and\nregression tasks across different sensor modalities, where only one of the\nmodalities available during training is accessible at inference time. Our\nresults demonstrate consistent predictive improvements over state-of-the-art\napproaches from the recent machine learning and computer vision literature, as\nwell as EO-specific methods. The obtained findings validate our framework in\nthe single-modality inference scenarios across a diverse range of EO\napplications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Dimensionality Reduction for Remote Sensing Data Analysis: A Systematic Review of Methods and Applications",
    "url": "http://arxiv.org/abs/2510.18935v1",
    "authors": [
      "Nathan Mankovich",
      "Kai-Hendrik Cohrs",
      "Homer Durand",
      "Vasileios Sitokonstantinou",
      "Tristan Williams",
      "Gustau Camps-Valls"
    ],
    "published": "2025-10-21",
    "abstract": "Earth observation involves collecting, analyzing, and processing an\never-growing mass of data. Automatically harvesting information is crucial for\naddressing significant societal, economic, and environmental challenges,\nranging from environmental monitoring to urban planning and disaster\nmanagement. However, the high dimensionality of these data poses challenges in\nterms of sparsity, inefficiency, and the curse of dimensionality, which limits\nthe effectiveness of machine learning models. Dimensionality reduction (DR)\ntechniques, specifically feature extraction, address these challenges by\npreserving essential data properties while reducing complexity and enhancing\ntasks such as data compression, cleaning, fusion, visualization, anomaly\ndetection, and prediction. This review provides a handbook for leveraging DR\nacross the RS data value chain and identifies opportunities for under-explored\nDR algorithms and their application in future research.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network",
    "url": "http://arxiv.org/abs/2510.17756v1",
    "authors": [
      "Younghyun Koo",
      "Maryam Rahnemoonfar"
    ],
    "published": "2025-10-20",
    "abstract": "As an increasing amount of remote sensing data becomes available in the\nArctic Ocean, data-driven machine learning (ML) techniques are becoming widely\nused to predict sea ice velocity (SIV) and sea ice concentration (SIC).\nHowever, fully data-driven ML models have limitations in generalizability and\nphysical consistency due to their excessive reliance on the quantity and\nquality of training data. In particular, as Arctic sea ice entered a new phase\nwith thinner ice and accelerated melting, there is a possibility that an ML\nmodel trained with historical sea ice data cannot fully represent the\ndynamically changing sea ice conditions in the future. In this study, we\ndevelop physics-informed neural network (PINN) strategies to integrate physical\nknowledge of sea ice into the ML model. Based on the Hierarchical\nInformation-sharing U-net (HIS-Unet) architecture, we incorporate the physics\nloss function and the activation function to produce physically plausible SIV\nand SIC outputs. Our PINN model outperforms the fully data-driven model in the\ndaily predictions of SIV and SIC, even when trained with a small number of\nsamples. The PINN approach particularly improves SIC predictions in melting and\nearly freezing seasons and near fast-moving ice regions.",
    "categories": [
      "remote_sensing",
      "ocean"
    ],
    "architectures": [
      "UNET",
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory",
    "url": "http://arxiv.org/abs/2510.16676v1",
    "authors": [
      "Anindya Sarkar",
      "Binglin Ji",
      "Yevgeniy Vorobeychik"
    ],
    "published": "2025-10-19",
    "abstract": "In many scientific and engineering fields, where acquiring high-quality data\nis expensive--such as medical imaging, environmental monitoring, and remote\nsensing--strategic sampling of unobserved regions based on prior observations\nis crucial for maximizing discovery rates within a constrained budget. The rise\nof powerful generative models, such as diffusion models, has enabled active\ntarget discovery in partially observable environments by leveraging learned\npriors--probabilistic representations that capture underlying structure from\ndata. With guidance from sequentially gathered task-specific observations,\nthese models can progressively refine exploration and efficiently direct\nqueries toward promising regions. However, in domains where learning a strong\nprior is infeasible due to extremely limited data or high sampling cost (such\nas rare species discovery, diagnostics for emerging diseases, etc.), these\nmethods struggle to generalize. To overcome this limitation, we propose a novel\napproach that enables effective active target discovery even in settings with\nuninformative priors, ensuring robust exploration and adaptability in complex\nreal-world scenarios. Our framework is theoretically principled and draws\ninspiration from neuroscience to guide its design. Unlike black-box policies,\nour approach is inherently interpretable, providing clear insights into\ndecision-making. Furthermore, it guarantees a strong, monotonic improvement in\nprior estimates with each new observation, leading to increasingly accurate\nsampling and reinforcing both reliability and adaptability in dynamic settings.\nThrough comprehensive experiments and ablation studies across various domains,\nincluding species distribution modeling and remote sensing, we demonstrate that\nour method substantially outperforms baseline approaches.",
    "categories": [
      "remote_sensing",
      "fish_plankton"
    ],
    "architectures": [
      "Diffusion Models",
      "SDM"
    ],
    "applications": []
  },
  {
    "title": "CSU-PCAST: A Dual-Branch Transformer Framework for medium-range ensemble Precipitation Forecasting",
    "url": "http://arxiv.org/abs/2510.20769v1",
    "authors": [
      "Tianyi Xiong",
      "Haonan Chen"
    ],
    "published": "2025-10-23",
    "abstract": "Accurate medium-range precipitation forecasting is crucial for\nhydrometeorological risk management and disaster mitigation, yet remains\nchallenging for current numerical weather prediction (NWP) systems. Traditional\nensemble systems such as the Global Ensemble Forecast System (GEFS) struggle to\nmaintain high skill, especially for moderate and heavy rainfall at extended\nlead times. This study develops a deep learning-based ensemble framework for\nmulti-step precipitation prediction through joint modeling of a comprehensive\nset of atmospheric variables. The model is trained on ERA5 reanalysis data at\n0.25$^{\\circ}$ spatial resolution, with precipitation labels from NASA's\nIntegrated Multi-satellite Retrievals for Global Precipitation Measurement\n(GPM) constellation (IMERG), incorporating 57 input variables, including\nupper-air and surface predictors. The architecture employs a patch-based Swin\nTransformer backbone with periodic convolutions to handle longitudinal\ncontinuity and integrates time and noise embeddings through conditional layer\nnormalization. A dual-branch decoder predicts total precipitation and other\nvariables, with targeted freezing of encoder-decoder pathways for specialized\ntraining. Training minimizes a hybrid loss combining the Continuous Ranked\nProbability Score (CRPS) and weighted log1p mean squared error (log1pMSE),\nbalancing probabilistic accuracy and magnitude fidelity. During inference, the\nmodel ingests real-time Global Forecast System (GFS) initial conditions to\ngenerate 15-day forecasts autoregressively. Evaluation against GEFS using IMERG\ndata demonstrates higher Critical Success Index (CSI) scores at precipitation\nthresholds of 0.1 mm, 1 mm, 10 mm, and 20 mm, highlighting improved performance\nfor moderate to heavy rainfall.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Learning Coupled Earth System Dynamics with GraphDOP",
    "url": "http://arxiv.org/abs/2510.20416v1",
    "authors": [
      "Eulalie Boucher",
      "Mihai Alexe",
      "Peter Lean",
      "Ewan Pinnington",
      "Simon Lang",
      "Patrick Laloyaux",
      "Lorenzo Zampieri",
      "Patricia de Rosnay",
      "Niels Bormann",
      "Anthony McNally"
    ],
    "published": "2025-10-23",
    "abstract": "Interactions between different components of the Earth System (e.g. ocean,\natmosphere, land and cryosphere) are a crucial driver of global weather\npatterns. Modern Numerical Weather Prediction (NWP) systems typically run\nseparate models of the different components, explicitly coupled across their\ninterfaces to additionally model exchanges between the different components.\nAccurately representing these coupled interactions remains a major scientific\nand technical challenge of weather forecasting. GraphDOP is a graph-based\nmachine learning model that learns to forecast weather directly from raw\nsatellite and in-situ observations, without reliance on reanalysis products or\ntraditional physics-based NWP models. GraphDOP simultaneously embeds\ninformation from diverse observation sources spanning the full Earth system\ninto a shared latent space. This enables predictions that implicitly capture\ncross-domain interactions in a single model without the need for any explicit\ncoupling. Here we present a selection of case studies which illustrate the\ncapability of GraphDOP to forecast events where coupled processes play a\nparticularly key role. These include rapid sea-ice freezing in the Arctic,\nmixing-induced ocean surface cooling during Hurricane Ian and the severe\nEuropean heat wave of 2022. The results suggest that learning directly from\nEarth System observations can successfully characterise and propagate\ncross-component interactions, offering a promising path towards physically\nconsistent end-to-end data-driven Earth System prediction with a single model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Discovering How Ice Crystals Grow Using Neural ODE's and Symbolic Regression",
    "url": "http://arxiv.org/abs/2510.17935v1",
    "authors": [
      "Kara D. Lamb",
      "Jerry Y. Harrington",
      "Alfred M. Moyle",
      "Gwenore F. Pokrifka",
      "Benjamin W. Clouser",
      "Volker Ebert",
      "Ottmar M\u00f6hler",
      "Harald Saathoff"
    ],
    "published": "2025-10-20",
    "abstract": "Depositional ice growth is an important process for cirrus cloud evolution,\nbut the physics of ice growth in atmospheric conditions is still poorly\nunderstood. One major challenge in constraining depositional ice growth models\nagainst observations is that the early growth rates of ice crystals cannot be\ndirectly observed, and proposed models require assumptions about the functional\ndependence of physical processes that are still highly uncertain. Neural\nordinary differential equations (NODE's) are a recently developed machine\nlearning method that can be used to learn the derivative of an unknown\nfunction. Here we use NODE's to learn the functional dependence of unknown\nphysics in the depositional ice growth model by optimizing against experimental\nmeasurements of ice crystal mass. We find a functional form for the\ndepositional ice growth model that best fits 290 mass time series of ice\ncrystals grown in a levitation diffusion chamber. We use symbolic regression to\nderive an equation for the function learned by the NODE model, which includes\nadditional terms proportional to ice crystal mass in the capacitance growth\nmodel. We evaluate this functional form against experimental data sets from the\nAIDA Aerosol and Cloud Chamber, finding that our new proposed model for\ndepositional ice growth accurately reproduces experimental results in the early\nstages of ice crystal growth.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Optical turbulence forecast for ground-based astronomy and free-space optical communication",
    "url": "http://arxiv.org/abs/2510.16845v1",
    "authors": [
      "Elena Masciadria",
      "Alessio Turchia",
      "Camilo Weinbergera",
      "Marlene De Sepibusa",
      "Luca Finia"
    ],
    "published": "2025-10-19",
    "abstract": "Forecasting optical turbulence in the Earth's atmosphere has been an\nambitious challenge for the astronomical scientific community for several\ndecades. While earlier research primarily focused on whether it was possible to\npredict optical turbulence and its vertical distribution, current efforts are\nmore concentrated on the accuracy achievable at different timescales, the\nefficiency of various forecasting methods and the contributions of new\nstatistical approaches, such as auto-regression and machine learning to this\nfield. In this contribution, I will present the state of the art of the\nresearch conducted by our group, positioned within the international research\nscenery. Most of our past activity has been primarily focused on ground-based\nastronomy but recent advancements in space research opened new opportunities\nfor applications in the free-space optical communication.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models",
    "url": "http://arxiv.org/abs/2510.19749v1",
    "authors": [
      "Catherine Villeneuve",
      "Benjamin Akera",
      "M\u00e9lisande Teng",
      "David Rolnick"
    ],
    "published": "2025-10-22",
    "abstract": "Species distribution models (SDMs), which aim to predict species occurrence\nbased on environmental variables, are widely used to monitor and respond to\nbiodiversity change. Recent deep learning advances for SDMs have been shown to\nperform well on complex and heterogeneous datasets, but their effectiveness\nremains limited by spatial biases in the data. In this paper, we revisit deep\nSDMs from a Bayesian perspective and introduce BATIS, a novel and practical\nframework wherein prior predictions are updated iteratively using limited\nobservational data. Models must appropriately capture both aleatoric and\nepistemic uncertainty to effectively combine fine-grained local insights with\nbroader ecological patterns. We benchmark an extensive set of uncertainty\nquantification approaches on a novel dataset including citizen science\nobservations from the eBird platform. Our empirical study shows how Bayesian\ndeep learning approaches can greatly improve the reliability of SDMs in\ndata-scarce locations, which can contribute to ecological understanding and\nconservation efforts.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation",
    "url": "http://arxiv.org/abs/2510.19305v1",
    "authors": [
      "Chirag Padubidri",
      "Pranesh Velmurugan",
      "Andreas Lanitis",
      "Andreas Kamilaris"
    ],
    "published": "2025-10-22",
    "abstract": "Monitoring species distribution is vital for conservation efforts, enabling\nthe assessment of environmental impacts and the development of effective\npreservation strategies. Traditional data collection methods, including citizen\nscience, offer valuable insights but remain limited in coverage and\ncompleteness. Species Distribution Modelling (SDM) helps address these gaps by\nusing occurrence data and environmental variables to predict species presence\nacross large regions. In this study, we enhance SDM accuracy for frogs (Anura)\nby applying deep learning and data imputation techniques using data from the\n\"EY - 2022 Biodiversity Challenge.\" Our experiments show that data balancing\nsignificantly improved model performance, reducing the Mean Absolute Error\n(MAE) from 189 to 29 in frog counting tasks. Feature selection identified key\nenvironmental factors influencing occurrence, optimizing inputs while\nmaintaining predictive accuracy. The multimodal ensemble model, integrating\nland cover, NDVI, and other environmental inputs, outperformed individual\nmodels and showed robust generalization across unseen regions. The fusion of\nimage and tabular data improved both frog counting and habitat classification,\nachieving 84.9% accuracy with an AUC of 0.90. This study highlights the\npotential of multimodal learning and data preprocessing techniques such as\nbalancing and imputation to improve predictive ecological modeling when data\nare sparse or incomplete, contributing to more precise and scalable\nbiodiversity monitoring.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges",
    "url": "http://arxiv.org/abs/2510.22964v1",
    "authors": [
      "Liling Yang",
      "Ning Chen",
      "Jun Yue",
      "Yidan Liu",
      "Jiayi Ma",
      "Pedram Ghamisi",
      "Antonio Plaza",
      "Leyuan Fang"
    ],
    "published": "2025-10-27",
    "abstract": "Foundation models have transformed natural language processing and computer\nvision, and their impact is now reshaping remote sensing image analysis. With\npowerful generalization and transfer learning capabilities, they align\nnaturally with the multimodal, multi-resolution, and multi-temporal\ncharacteristics of remote sensing data. To address unique challenges in the\nfield, multimodal geospatial foundation models (GFMs) have emerged as a\ndedicated research frontier. This survey delivers a comprehensive review of\nmultimodal GFMs from a modality-driven perspective, covering five core visual\nand vision-language modalities. We examine how differences in imaging physics\nand data representation shape interaction design, and we analyze key techniques\nfor alignment, integration, and knowledge transfer to tackle modality\nheterogeneity, distribution shifts, and semantic gaps. Advances in training\nparadigms, architectures, and task-specific adaptation strategies are\nsystematically assessed alongside a wealth of emerging benchmarks.\nRepresentative multimodal visual and vision-language GFMs are evaluated across\nten downstream tasks, with insights into their architectures, performance, and\napplication scenarios. Real-world case studies, spanning land cover mapping,\nagricultural monitoring, disaster response, climate studies, and geospatial\nintelligence, demonstrate the practical potential of GFMs. Finally, we outline\npressing challenges in domain generalization, interpretability, efficiency, and\nprivacy, and chart promising avenues for future research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "WaveMAE: Wavelet decomposition Masked Auto-Encoder for Remote Sensing",
    "url": "http://arxiv.org/abs/2510.22697v1",
    "authors": [
      "Vittorio Bernuzzi",
      "Leonardo Rossi",
      "Tomaso Fontanini",
      "Massimo Bertozzi",
      "Andrea Prati"
    ],
    "published": "2025-10-26",
    "abstract": "Self-supervised learning (SSL) has recently emerged as a key strategy for\nbuilding foundation models in remote sensing, where the scarcity of annotated\ndata limits the applicability of fully supervised approaches. In this work, we\nintroduce WaveMAE, a masked autoencoding framework tailored for multispectral\nsatellite imagery. Unlike conventional pixel-based reconstruction, WaveMAE\nleverages a multi-level Discrete Wavelet Transform (DWT) to disentangle\nfrequency components and guide the encoder toward learning scale-aware\nhigh-frequency representations. We further propose a Geo-conditioned Positional\nEncoding (GPE), which incorporates geographical priors via Spherical Harmonics,\nencouraging embeddings that respect both semantic and geospatial structure. To\nensure fairness in evaluation, all methods are pretrained on the same dataset\n(fMoW-S2) and systematically evaluated on the diverse downstream tasks of the\nPANGAEA benchmark, spanning semantic segmentation, regression, change\ndetection, and multilabel classification. Extensive experiments demonstrate\nthat WaveMAE achieves consistent improvements over prior state-of-the-art\napproaches, with substantial gains on segmentation and regression benchmarks.\nThe effectiveness of WaveMAE pretraining is further demonstrated by showing\nthat even a lightweight variant, containing only 26.4% of the parameters,\nachieves state-of-the-art performance. Our results establish WaveMAE as a\nstrong and geographically informed foundation model for multispectral remote\nsensing imagery.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing",
    "url": "http://arxiv.org/abs/2510.26609v1",
    "authors": [
      "Shayan Nejadshamsi",
      "Yuanyuan Zhang",
      "Shadi Zaki",
      "Brock Porth",
      "Lysa Porth",
      "Vahab Khoshdel"
    ],
    "published": "2025-10-30",
    "abstract": "Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi",
    "url": "http://arxiv.org/abs/2510.25954v1",
    "authors": [
      "Lynn Metz",
      "Rachel Haggard",
      "Michael Moszczynski",
      "Samer Asbah",
      "Chris Mwase",
      "Patricia Khomani",
      "Tyler Smith",
      "Hannah Cooper",
      "Annie Mwale",
      "Arbaaz Muslim",
      "Gautam Prasad",
      "Mimi Sun",
      "Tomer Shekel",
      "Joydeep Paul",
      "Anna Carter",
      "Shravya Shetty",
      "Dylan Green"
    ],
    "published": "2025-10-29",
    "abstract": "The reliability of routine health data in low and middle-income countries\n(LMICs) is often constrained by reporting delays and incomplete coverage,\nnecessitating the exploration of novel data sources and analytics. Geospatial\nFoundation Models (GeoFMs) offer a promising avenue by synthesizing diverse\nspatial, temporal, and behavioral data into mathematical embeddings that can be\nefficiently used for downstream prediction tasks. This study evaluated the\npredictive performance of three GeoFM embedding sources - Google Population\nDynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite\nimagery), and mobile phone call detail records (CDR) - for modeling 15 routine\nhealth programmatic outputs in Malawi, and compared their utility to\ntraditional geospatial interpolation methods. We used XGBoost models on data\nfrom 552 health catchment areas (January 2021-May 2023), assessing performance\nwith R2, and using an 80/20 training and test data split with 5-fold\ncross-validation used in training. While predictive performance was mixed, the\nembedding-based approaches improved upon baseline geostatistical methods in 13\nof 15 (87%) indicators tested. A Multi-GeoFM model integrating all three\nembedding sources produced the most robust predictions, achieving average\n5-fold cross validated R2 values for indicators like population density (0.63),\nnew HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,\n0.68, and 0.55, respectively. Prediction was poor for prediction targets with\nlow primary data availability, such as TB and malnutrition cases. These results\ndemonstrate that GeoFM embeddings imbue a modest predictive improvement for\nselect health and demographic outcomes in an LMIC context. We conclude that the\nintegration of multiple GeoFM sources is an efficient and valuable tool for\nsupplementing and strengthening constrained routine health information systems.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Global Chlorophyll-\\textit{a} Retrieval algorithm from Sentinel 2 Using Residual Deep Learning and Novel Machine Learning Water Classification",
    "url": "http://arxiv.org/abs/2510.24124v1",
    "authors": [
      "Yotam Sherf",
      "Bar Efrati",
      "Gabriel Rozman",
      "Moshe Harel"
    ],
    "published": "2025-10-28",
    "abstract": "We present the Global Water Classifier (GWC), a supervised, geospatially\nextensive Machine Learning (ML) classifier trained on Sen2Cor corrected\nSentinel-2 surface reflectance data. Using nearly 100 globally distributed\ninland water bodies, GWC distinguishes water across Chlorophyll-a (Chla) levels\nfrom non-water spectra (clouds, sun glint, snow, ice, aquatic vegetation, land\nand sediments) and shows geographically stable performance.\n  Building on this foundation model, we perform Chla retrieval based on a\nmatchup Sentinel-2 reflectance data with the United States Geological Survey\n(USGS) AquaMatch in-situ dataset, covering diverse geographical and\nhydrological conditions.\n  We train an XGBoost regressor on 13626 matchup points. The positive labeled\nscenes by the GWC consistently outperform the negatives and produce more\naccurate Chla retrieval values, which confirms the classifiers advantage in\nreducing various interferences.\n  Next, residual analysis of the regression predictions revealed structured\nerrors, motivating a residual CNN (RCNN) correction stage. We add a CNN\nresidual stage trained on normalized residuals, which yield substantial\nimprovement. Our algorithm was tested on 867 water bodies with over 2,000\npredictions and Chla values up to 1000~mg$/m^{3}$, achieving $R^2$ = 0.79, MAE\n= 13.52~mg$/m^{3}$, and slope = 0.91, demonstrating robust, scalable, and\nglobally transferable performance without additional tuning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping",
    "url": "http://arxiv.org/abs/2510.23364v1",
    "authors": [
      "Hyeongkyun Kim",
      "Orestis Oikonomou"
    ],
    "published": "2025-10-27",
    "abstract": "Flood susceptibility mapping (FSM) is vital for disaster prevention but\nremains challenging in data-scarce regions where hydrodynamic models require\ndense geophysical inputs. This work introduces ZeroFlood, a geospatial\nfoundation model framework for data-efficient FSM. The approach fine-tunes\nGeospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning,\nenabling flood prediction from basic Earth observation data such as Sentinel-1\nor Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich\nregions, ZeroFlood bridges data availability gaps through cross-modal\nrepresentation learning. Experiments with TerraMind and Prithvi GFMs show that\nTiM enhances model robustness, with the TerraMind-Large configuration achieving\nan F1 score of 67.21. The results demonstrate the feasibility of\nfoundation-model-based FSM as a scalable and data-efficient solution for flood\nrisk management.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting",
    "url": "http://arxiv.org/abs/2510.25563v1",
    "authors": [
      "V\u00edctor Medina",
      "Giovanny A. Cuervo-Londo\u00f1o",
      "Javier S\u00e1nchez"
    ],
    "published": "2025-10-29",
    "abstract": "The accurate prediction of oceanographic variables is crucial for\nunderstanding climate change, managing marine resources, and optimizing\nmaritime activities. Traditional ocean forecasting relies on numerical models;\nhowever, these approaches face limitations in terms of computational cost and\nscalability. In this study, we adapt Aurora, a foundational deep learning model\noriginally designed for atmospheric forecasting, to predict sea surface\ntemperature (SST) in the Canary Upwelling System. By fine-tuning this model\nwith high-resolution oceanographic reanalysis data, we demonstrate its ability\nto capture complex spatiotemporal patterns while reducing computational\ndemands. Our methodology involves a staged fine-tuning process, incorporating\nlatitude-weighted error metrics and optimizing hyperparameters for efficient\nlearning. The experimental results show that the model achieves a low RMSE of\n0.119K, maintaining high anomaly correlation coefficients (ACC $\\approx\n0.997$). The model successfully reproduces large-scale SST structures but faces\nchallenges in capturing finer details in coastal regions. This work contributes\nto the field of data-driven ocean forecasting by demonstrating the feasibility\nof using deep learning models pre-trained in different domains for oceanic\napplications. Future improvements include integrating additional oceanographic\nvariables, increasing spatial resolution, and exploring physics-informed neural\nnetworks to enhance interpretability and understanding. These advancements can\nimprove climate modeling and ocean prediction accuracy, supporting\ndecision-making in environmental and economic sectors.",
    "categories": [
      "foundation_model",
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2",
    "url": "http://arxiv.org/abs/2510.26653v1",
    "authors": [
      "Daniela Martin",
      "Joseph Gallego"
    ],
    "published": "2025-10-30",
    "abstract": "Accurate estimation of sea ice drift is critical for Arctic navigation,\nclimate research, and operational forecasting. While optical flow, a computer\nvision technique for estimating pixel wise motion between consecutive images,\nhas advanced rapidly in computer vision, its applicability to geophysical\nproblems and to satellite SAR imagery remains underexplored. Classical optical\nflow methods rely on mathematical models and strong assumptions about motion,\nwhich limit their accuracy in complex scenarios. Recent deep learning based\napproaches have substantially improved performance and are now the standard in\ncomputer vision, motivating their application to sea ice drift estimation. We\npresent the first large scale benchmark of 48 deep learning optical flow models\non RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and\nFl all metrics against GNSS tracked buoys. Several models achieve sub kilometer\naccuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the\nspatial scales of sea ice motion and typical navigation requirements in the\nArctic. Our results demonstrate that the models are capable of capturing\nconsistent regional drift patterns and that recent deep learning based optical\nflow methods, which have substantially improved motion estimation accuracy\ncompared to classical methods, can be effectively transferred to polar remote\nsensing. Optical flow produces spatially continuous drift fields, providing\nmotion estimates for every image pixel rather than at sparse buoy locations,\noffering new opportunities for navigation and climate modeling.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning",
    "url": "http://arxiv.org/abs/2510.24321v1",
    "authors": [
      "Ivica Dimitrovski",
      "Vlatko Spasev",
      "Ivan Kitanovski"
    ],
    "published": "2025-10-28",
    "abstract": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM",
      "CLIP"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "A Review of End-to-End Precipitation Prediction Using Remote Sensing Data: from Divination to Machine Learning",
    "url": "http://arxiv.org/abs/2510.22855v1",
    "authors": [
      "Yugong Zeng",
      "Jonathan Wu"
    ],
    "published": "2025-10-26",
    "abstract": "Precipitation prediction has undergone a profound transformation -- from\nearly symbolic and empirical methods rooted in divination and observation, to\nmodern technologies based on atmospheric physics and artificial intelligence.\nThis review traces the historical and technological evolution of precipitation\nforecasting, presenting a survey about end-to-end precipitation prediction\ntechnologies that spans ancient practices, the foundations of meteorological\nscience, the rise of numerical weather prediction (NWP), and the emergence of\nmachine learning (ML) and deep learning (DL) models. We first explore\ntraditional and indigenous forecasting methods, then describe the development\nof physical modeling and statistical frameworks that underpin contemporary\noperational forecasting. Particular emphasis is placed on recent advances in\nneural network-based approaches, including automated deep learning,\ninterpretability-driven design, and hybrid physical-data models. By compositing\nresearch across multiple eras and paradigms, this review not only depicts the\nhistory of end-to-end precipitation prediction but also outlines future\ndirections in next generation forecasting systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enpowering Your Pansharpening Models with Generalizability: Unified Distribution is All You Need",
    "url": "http://arxiv.org/abs/2510.22217v1",
    "authors": [
      "Yongchuan Cui",
      "Peng Liu",
      "Hui Zhang"
    ],
    "published": "2025-10-25",
    "abstract": "Existing deep learning-based models for remote sensing pansharpening exhibit\nexceptional performance on training datasets. However, due to sensor-specific\ncharacteristics and varying imaging conditions, these models suffer from\nsubstantial performance degradation when applied to unseen satellite data,\nlacking generalizability and thus limiting their applicability. We argue that\nthe performance drops stem primarily from distributional discrepancies from\ndifferent sources and the key to addressing this challenge lies in bridging the\ngap between training and testing distributions. To validate the idea and\nfurther achieve a \"train once, deploy forever\" capability, this paper\nintroduces a novel and intuitive approach to enpower any pansharpening models\nwith generalizability by employing a unified distribution strategy (UniPAN).\nSpecifically, we construct a distribution transformation function that\nnormalizes the pixels sampled from different sources to conform to an identical\ndistribution. The deep models are trained on the transformed domain, and during\ntesting on new datasets, the new data are also transformed to match the\ntraining distribution. UniPAN aims to train and test the model on a unified and\nconsistent distribution, thereby enhancing its generalizability. Extensive\nexperiments validate the efficacy of UniPAN, demonstrating its potential to\nsignificantly enhance the performance of deep pansharpening models across\ndiverse satellite sensors. Codes: https://github.com/yc-cui/UniPAN.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Robust variable selection for spatial point processes observed with noise",
    "url": "http://arxiv.org/abs/2510.25550v1",
    "authors": [
      "Dominik Sturm",
      "Ivo F. Sbalzarini"
    ],
    "published": "2025-10-29",
    "abstract": "We propose a method for variable selection in the intensity function of\nspatial point processes that combines sparsity-promoting estimation with\nnoise-robust model selection. As high-resolution spatial data becomes\nincreasingly available through remote sensing and automated image analysis,\nidentifying spatial covariates that influence the localization of events is\ncrucial to understand the underlying mechanism. However, results from automated\nacquisition techniques are often noisy, for example due to measurement\nuncertainties or detection errors, which leads to spurious displacements and\nmissed events. We study the impact of such noise on sparse point-process\nestimation across different models, including Poisson and Thomas processes. To\nimprove noise robustness, we propose to use stability selection based on\npoint-process subsampling and to incorporate a non-convex best-subset penalty\nto enhance model-selection performance. In extensive simulations, we\ndemonstrate that such an approach reliably recovers true covariates under\ndiverse noise scenarios and improves both selection accuracy and stability. We\nthen apply the proposed method to a forestry data set, analyzing the\ndistribution of trees in relation to elevation and soil nutrients in a tropical\nrain forest. This shows the practical utility of the method, which provides a\nsystematic framework for robust variable selection in spatial point-process\nmodels under noise, without requiring additional knowledge of the process.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2510.24242v1",
    "authors": [
      "Zihan Li",
      "Jiahao Yang",
      "Yuxin Zhang",
      "Zhe Chen",
      "Yue Gao"
    ],
    "published": "2025-10-28",
    "abstract": "Large vision-language models (LVLMs) have recently demonstrated great\npotential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by\nlow Earth orbit (LEO) satellites. However, their deployment in real-world LEO\nsatellite systems remains largely unexplored, hindered by limited onboard\ncomputing resources and brief satellite-ground contacts. We propose Grace, a\nsatellite-ground collaborative system designed for near-realtime LVLM inference\nin RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime\ninference, but larger ones on ground stations (GSs) to guarantee end-to-end\nperformance. Grace is comprised of two main phases that are asynchronous\nsatellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch\nalgorithm. Firstly, we still the knowledge archive of GS RAG to satellite\narchive with tailored adaptive update algorithm during limited satellite-ground\ndata exchange period. Secondly, propose a confidence-based test algorithm that\neither processes the task onboard the satellite or offloads it to the GS.\nExtensive experiments based on real-world satellite orbital data show that\nGrace reduces the average latency by 76-95% compared to state-of-the-art\nmethods, without compromising inference accuracy.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "A Framework for Hybrid Physics-AI Coupled Ocean Models",
    "url": "http://arxiv.org/abs/2510.22676v1",
    "authors": [
      "Laure Zanna",
      "William Gregory",
      "Pavel Perezhogin",
      "Aakash Sane",
      "Cheng Zhang",
      "Alistair Adcroft",
      "Mitch Bushuk",
      "Carlos Fernandez-Granda",
      "Brandon Reichl",
      "Dhruv Balwada",
      "Julius Busecke",
      "William Chapman",
      "Alex Connolly",
      "Danni Du",
      "Kelsey Everard",
      "Fabrizio Falasca",
      "Renaud Falga",
      "David Kamm",
      "Etienne Meunier",
      "Qi Liu",
      "Antoine Nasser",
      "Matthew Pudig",
      "Andrew Shao",
      "Julia L. Simpson",
      "Linus Vogt",
      "Jiarong Wu"
    ],
    "published": "2025-10-26",
    "abstract": "Climate simulations, at all grid resolutions, rely on approximations that\nencapsulate the forcing due to unresolved processes on resolved variables,\nknown as parameterizations. Parameterizations often lead to inaccuracies in\nclimate models, with significant biases in the physics of key climate\nphenomena. Advances in artificial intelligence (AI) are now directly enabling\nthe learning of unresolved processes from data to improve the physics of\nclimate simulations. Here, we introduce a flexible framework for developing and\nimplementing physics- and scale-aware machine learning parameterizations within\nclimate models. We focus on the ocean and sea-ice components of a\nstate-of-the-art climate model by implementing a spectrum of data-driven\nparameterizations, ranging from complex deep learning models to more\ninterpretable equation-based models. Our results showcase the viability of\nAI-driven parameterizations in operational models, advancing the capabilities\nof a new generation of hybrid simulations, and include prototypes of fully\ncoupled atmosphere-ocean-sea-ice hybrid simulations. The tools developed are\nopen source, accessible, and available to all.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Curly Flow Matching for Learning Non-gradient Field Dynamics",
    "url": "http://arxiv.org/abs/2510.26645v1",
    "authors": [
      "Katarina Petrovi\u0107",
      "Lazar Atanackovic",
      "Viggo Moro",
      "Kacper Kapu\u015bniak",
      "\u0130smail \u0130lkan Ceylan",
      "Michael Bronstein",
      "Avishek Joey Bose",
      "Alexander Tong"
    ],
    "published": "2025-10-30",
    "abstract": "Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Predictability of Storms in an Idealized Climate Revealed by Machine Learning",
    "url": "http://arxiv.org/abs/2510.25633v1",
    "authors": [
      "Wuqiushi Yao",
      "Or Hadas",
      "Yohai Kaspi"
    ],
    "published": "2025-10-29",
    "abstract": "The midlatitude climate and weather are shaped by storms, yet the factors\ngoverning their predictability remain insufficiently understood. Here, we use a\nConvolutional Neural Network (CNN) to predict and quantify uncertainty in the\nintensity growth and trajectory of over 200,000 storms simulated in a 200-year\naquaplanet GCM. This idealized framework provides a controlled climate\nbackground for isolating factors that govern predictability. Results show that\nstorm intensity is less predictable than trajectory. Strong baroclinicity\naccelerates storm intensification and reduces its predictability, consistent\nwith theory. Crucially, enhanced jet meanders further degrade forecast skill,\nrevealing a synoptic source of uncertainty. Using sensitivity maps from\nexplainable AI, we find that the error growth rate is nearly doubled by the\nmore meandering structure. These findings highlight the potential of machine\nlearning for advancing understanding of predictability and its governing\nmechanisms.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Forecasting precipitation in the Arctic using probabilistic machine learning informed by causal climate drivers",
    "url": "http://arxiv.org/abs/2510.24254v1",
    "authors": [
      "Madhurima Panja",
      "Dhiman Das",
      "Tanujit Chakraborty",
      "Arnob Ray",
      "R. Athulya",
      "Chittaranjan Hens",
      "Syamal K. Dana",
      "Nuncio Murukesh",
      "Dibakar Ghosh"
    ],
    "published": "2025-10-28",
    "abstract": "Understanding and forecasting precipitation events in the Arctic maritime\nenvironments, such as Bear Island and Ny-{\\AA}lesund, is crucial for assessing\nclimate risk and developing early warning systems in vulnerable marine regions.\nThis study proposes a probabilistic machine learning framework for modeling and\npredicting the dynamics and severity of precipitation. We begin by analyzing\nthe scale-dependent relationships between precipitation and key atmospheric\ndrivers (e.g., temperature, relative humidity, cloud cover, and air pressure)\nusing wavelet coherence, which captures localized dependencies across time and\nfrequency domains. To assess joint causal influences, we employ\nSynergistic-Unique-Redundant Decomposition, which quantifies the impact of\ninteraction effects among each variable on future precipitation dynamics. These\ninsights inform the development of data-driven forecasting models that\nincorporate both historical precipitation and causal climate drivers. To\naccount for uncertainty, we employ the conformal prediction method, which\nenables the generation of calibrated non-parametric prediction intervals. Our\nresults underscore the importance of utilizing a comprehensive framework that\ncombines causal analysis with probabilistic forecasting to enhance the\nreliability and interpretability of precipitation predictions in Arctic marine\nenvironments.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training",
    "url": "http://arxiv.org/abs/2510.22094v2",
    "authors": [
      "Thomas Bailie",
      "S. Karthik Mukkavilli",
      "Varvara Vetrova",
      "Yun Sing Koh"
    ],
    "published": "2025-10-25",
    "abstract": "Climate events arise from intricate, multivariate dynamics governed by\nglobal-scale drivers, profoundly impacting food, energy, and infrastructure.\nYet, accurate weather prediction remains elusive due to physical processes\nunfolding across diverse spatio-temporal scales, which fixed-resolution methods\ncannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale\nrepresentation, but nonlinear downward mappings often erase global trends,\nweakening the integration of physics into forecasts. We introduce HiFlowCast\nand its ensemble variant HiAntFlow, HGNNs that embed physics within a\nmultiscale prediction framework. Two innovations underpin their design: a\nLatent-Memory-Retention mechanism that preserves global trends during downward\ntraversal, and a Latent-to-Physics branch that integrates PDE solution fields\nacross diverse scales. Our Flow models cut errors by over 5% at 13-day lead\ntimes and by 5-8% under 1st and 99th quantile extremes, improving reliability\nfor rare events. Leveraging pretrained model weights, they converge within a\nsingle epoch, reducing training cost and their carbon footprint. Such\nefficiency is vital as the growing scale of machine learning challenges\nsustainability and limits research accessibility. Code and model weights are in\nthe supplementary materials.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept",
    "url": "http://arxiv.org/abs/2510.23994v1",
    "authors": [
      "Geoffery Agorku",
      "Sarah Hernandez",
      "Hayley Hames",
      "Cade Wagner"
    ],
    "published": "2025-10-28",
    "abstract": "Accurate, real-time estimation of barge quantity on inland waterways remains\na critical challenge due to the non-self-propelled nature of barges and the\nlimitations of existing monitoring systems. This study introduces a novel\nmethod to use Automatic Identification System (AIS) vessel tracking data to\npredict the number of barges in tow using Machine Learning (ML). To train and\ntest the model, barge instances were manually annotated from satellite scenes\nacross the Lower Mississippi River. Labeled images were matched to AIS vessel\ntracks using a spatiotemporal matching procedure. A comprehensive set of 30\nAIS-derived features capturing vessel geometry, dynamic movement, and\ntrajectory patterns were created and evaluated using Recursive Feature\nElimination (RFE) to identify the most predictive variables. Six regression\nmodels, including ensemble, kernel-based, and generalized linear approaches,\nwere trained and evaluated. The Poisson Regressor model yielded the best\nperformance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of\nthe 30 features. The feature importance analysis revealed that metrics\ncapturing vessel maneuverability such as course entropy, speed variability and\ntrip length were most predictive of barge count. The proposed approach provides\na scalable, readily implementable method for enhancing Maritime Domain\nAwareness (MDA), with strong potential applications in lock scheduling, port\nmanagement, and freight planning. Future work will expand the proof of concept\npresented here to explore model transferability to other inland rivers with\ndiffering operational and environmental conditions.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Tracking"
    ]
  },
  {
    "title": "GeoCrossBench: Cross-Band Generalization for Remote Sensing",
    "url": "http://arxiv.org/abs/2511.02831v1",
    "authors": [
      "Hakob Tamazyan",
      "Ani Vanyan",
      "Alvard Barseghyan",
      "Anna Khosrovyan",
      "Evan Shelhamer",
      "Hrant Khachatrian"
    ],
    "published": "2025-11-04",
    "abstract": "The number and diversity of remote sensing satellites grows over time, while\nthe vast majority of labeled data comes from older satellites. As the\nfoundation models for Earth observation scale up, the cost of (re-)training to\nsupport new satellites grows too, so the generalization capabilities of the\nmodels towards new satellites become increasingly important. In this work we\nintroduce GeoCrossBench, an extension of the popular GeoBench benchmark with a\nnew evaluation protocol: it tests the in-distribution performance;\ngeneralization to satellites with no band overlap; and generalization to\nsatellites with additional bands with respect to the training set. We also\ndevelop a self-supervised extension of ChannelViT, ChiViT, to improve its\ncross-satellite performance. First, we show that even the best foundation\nmodels for remote sensing (DOFA, TerraFM) do not outperform general purpose\nmodels like DINOv3 in the in-distribution setting. Second, when generalizing to\nnew satellites with no band overlap, all models suffer 2-4x drop in\nperformance, and ChiViT significantly outperforms the runner-up DINOv3. Third,\nthe performance of all tested models drops on average by 5-25\\% when given\nadditional bands during test time. Finally, we show that fine-tuning just the\nlast linear layer of these models using oracle labels from all bands can get\nrelatively consistent performance across all satellites, highlighting that the\nbenchmark is far from being saturated. We publicly release the code and the\ndatasets to encourage the development of more future-proof remote sensing\nmodels with stronger cross-satellite generalization.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability",
    "url": "http://arxiv.org/abs/2511.04474v1",
    "authors": [
      "Wenwen Li",
      "Sizhe Wang",
      "Hyunho Lee",
      "Chenyan Lu",
      "Sujit Roy",
      "Rahul Ramachandran",
      "Chia-Yu Hsu"
    ],
    "published": "2025-11-06",
    "abstract": "Landslides cause severe damage to lives, infrastructure, and the environment,\nmaking accurate and timely mapping essential for disaster preparedness and\nresponse. However, conventional deep learning models often struggle when\napplied across different sensors, regions, or under conditions of limited\ntraining data. To address these challenges, we present a three-axis analytical\nframework of sensor, label, and domain for adapting geospatial foundation\nmodels (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a\nseries of experiments, we show that it consistently outperforms task-specific\nCNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other\nGeoFMs (TerraMind, SatMAE). The model, built on global pretraining,\nself-supervision, and adaptable fine-tuning, proved resilient to spectral\nvariation, maintained accuracy under label scarcity, and generalized more\nreliably across diverse datasets and geographic settings. Alongside these\nstrengths, we also highlight remaining challenges such as computational cost\nand the limited availability of reusable AI-ready training data for landslide\nresearch. Overall, our study positions GeoFMs as a step toward more robust and\nscalable approaches for landslide risk reduction and environmental monitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Cropland Mapping using Geospatial Embeddings",
    "url": "http://arxiv.org/abs/2511.02923v1",
    "authors": [
      "Ivan Zvonkov",
      "Gabriel Tseng",
      "Inbal Becker-Reshef",
      "Hannah Kerner"
    ],
    "published": "2025-11-04",
    "abstract": "Accurate and up-to-date land cover maps are essential for understanding land\nuse change, a key driver of climate change. Geospatial embeddings offer a more\nefficient and accessible way to map landscape features, yet their use in\nreal-world mapping applications remains underexplored. In this work, we\nevaluated the utility of geospatial embeddings for cropland mapping in Togo. We\nproduced cropland maps using embeddings from Presto and AlphaEarth. Our\nfindings show that geospatial embeddings can simplify workflows, achieve\nhigh-accuracy cropland classification and ultimately support better assessments\nof land use change and its climate impacts.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data",
    "url": "http://arxiv.org/abs/2511.04304v1",
    "authors": [
      "Robin Spanier",
      "Thorsten Hoeser",
      "Claudia Kuenzer"
    ],
    "published": "2025-11-06",
    "abstract": "The recent and ongoing expansion of marine infrastructure, including offshore\nwind farms, oil and gas platforms, artificial islands, and aquaculture\nfacilities, highlights the need for effective monitoring systems. The\ndevelopment of robust models for offshore infrastructure detection relies on\ncomprehensive, balanced datasets, but falls short when samples are scarce,\nparticularly for underrepresented object classes, shapes, and sizes. By\ntraining deep learning-based YOLOv10 object detection models with a combination\nof synthetic and real Sentinel-1 satellite imagery acquired in the fourth\nquarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of\nGuinea, and Coast of Brazil), this study investigates the use of synthetic\ntraining data to enhance model performance. We evaluated this approach by\napplying the model to detect offshore platforms in three unseen regions (Gulf\nof Mexico, North Sea, Persian Gulf) and thereby assess geographic\ntransferability. This region-holdout evaluation demonstrated that the model\ngeneralises beyond the training areas. In total, 3,529 offshore platforms were\ndetected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and\n1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which\nimproved to 0.90 upon incorporating synthetic data. We analysed how synthetic\ndata enhances the representation of unbalanced classes and overall model\nperformance, taking a first step toward globally transferable detection of\noffshore infrastructure. This study underscores the importance of balanced\ndatasets and highlights synthetic data generation as an effective strategy to\naddress common challenges in remote sensing, demonstrating the potential of\ndeep learning for scalable, global offshore infrastructure monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning",
    "url": "http://arxiv.org/abs/2511.03004v1",
    "authors": [
      "Dakota Hester",
      "Vitor S. Martins",
      "Lucas B. Ferreira",
      "Thainara M. A. Lima"
    ],
    "published": "2025-11-04",
    "abstract": "Deep learning semantic segmentation methods have shown promising performance\nfor very high 1-m resolution land cover classification, but the challenge of\ncollecting large volumes of representative training data creates a significant\nbarrier to widespread adoption of such models for meter-scale land cover\nmapping over large areas. In this study, we present a novel label-efficient\napproach for statewide 1-m land cover classification using only 1,000 annotated\nreference image patches with self-supervised deep learning. We use the\n\"Bootstrap Your Own Latent\" pre-training strategy with a large amount of\nunlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to\npre-train a ResNet-101 convolutional encoder. The learned encoder weights were\nsubsequently transferred into multiple deep semantic segmentation architectures\n(FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then\nfine-tuned using very small training dataset sizes with cross-validation (250,\n500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall\naccuracy and 75.58% macro F1 score using an ensemble of the best performing\nU-Net models for comprehensive 1-m, 8-class land cover mapping, covering more\nthan 123 billion pixels over the state of Mississippi, USA. Detailed\nqualitative and quantitative analysis revealed accurate mapping of open water\nand forested areas, while highlighting challenges in accurate delineation\nbetween cropland, herbaceous, and barren land cover types. These results show\nthat self-supervised learning is an effective strategy for reducing the need\nfor large volumes of manually annotated data, directly addressing a major\nlimitation to high spatial resolution land cover mapping at scale.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "ResNet"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ]
  },
  {
    "title": "Spectral-Convergent Decentralized Machine Learning: Theory and Application in Space Networks",
    "url": "http://arxiv.org/abs/2511.03291v1",
    "authors": [
      "Zhiyuan Zhai",
      "Shuyan Hu",
      "Wei Ni",
      "Xiaojun Yuan",
      "Xin Wang"
    ],
    "published": "2025-11-05",
    "abstract": "Decentralized machine learning (DML) supports collaborative training in\nlarge-scale networks with no central server. It is sensitive to the quality and\nreliability of inter-device communications that result in time-varying and\nstochastic topologies. This paper studies the impact of unreliable\ncommunication on the convergence of DML and establishes a direct connection\nbetween the spectral properties of the mixing process and the global\nperformance. We provide rigorous convergence guarantees under random topologies\nand derive bounds that characterize the impact of the expected mixing matrix's\nspectral properties on learning. We formulate a spectral optimization problem\nthat minimizes the spectral radius of the expected second-order mixing matrix\nto enhance the convergence rate under probabilistic link failures. To solve\nthis non-smooth spectral problem in a fully decentralized manner, we design an\nefficient subgradient-based algorithm that integrates Chebyshev-accelerated\neigenvector estimation with local update and aggregation weight adjustment,\nwhile ensuring symmetry and stochasticity constraints without central\ncoordination. Experiments on a realistic low Earth orbit (LEO) satellite\nconstellation with time-varying inter-satellite link models and real-world\nremote sensing data demonstrate the feasibility and effectiveness of the\nproposed method. The method significantly improves classification accuracy and\nconvergence efficiency compared to existing baselines, validating its\napplicability in satellite and other decentralized systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "SAAIPAA: Optimizing aspect-angles-invariant physical adversarial attacks on SAR target recognition models",
    "url": "http://arxiv.org/abs/2511.03192v1",
    "authors": [
      "Isar Lemeire",
      "Yee Wei Law",
      "Sang-Heon Lee",
      "Will Meakin",
      "Tat-Jun Chin"
    ],
    "published": "2025-11-05",
    "abstract": "Synthetic aperture radar (SAR) enables versatile, all-time, all-weather\nremote sensing. Coupled with automatic target recognition (ATR) leveraging\nmachine learning (ML), SAR is empowering a wide range of Earth observation and\nsurveillance applications. However, the surge of attacks based on adversarial\nperturbations against the ML algorithms underpinning SAR ATR is prompting the\nneed for systematic research into adversarial perturbation mechanisms. Research\nin this area began in the digital (image) domain and evolved into the physical\n(signal) domain, resulting in physical adversarial attacks (PAAs) that\nstrategically exploit corner reflectors as attack vectors to evade ML-based\nATR. This paper proposes a novel framework called SAR Aspect-Angles-Invariant\nPhysical Adversarial Attack (SAAIPAA) for physics-based modelling of\nreflector-actuated adversarial perturbations, which improves on the rigor of\nprior work. A unique feature of SAAIPAA is its ability to remain effective even\nwhen the attacker lacks knowledge of the SAR platform's aspect angles, by\ndeploying at least one reflector in each azimuthal quadrant and optimizing\nreflector orientations. The resultant physical evasion attacks are efficiently\nrealizable and optimal over the considered range of aspect angles between a SAR\nplatform and a target, achieving state-of-the-art fooling rates (over 80% for\nDenseNet-121 and ResNet50) in the white-box setting. When aspect angles are\nknown to the attacker, an average fooling rate of 99.2% is attainable. In\nblack-box settings, although the attack efficacy of SAAIPAA transfers well\nbetween some models (e.g., from ResNet50 to DenseNet121), the transferability\nto some models (e.g., MobileNetV2) can be improved. A useful outcome of using\nthe MSTAR dataset for the experiments in this article, a method for generating\nbounding boxes for densely sampled azimuthal SAR datasets is introduced.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies",
    "url": "http://arxiv.org/abs/2511.03173v1",
    "authors": [
      "Arsalan Muhammad",
      "Wasiu Akande Ahmed",
      "Omada Friday Ojonugwa",
      "Paul Puspendu Biswas"
    ],
    "published": "2025-11-05",
    "abstract": "The rapid growth of cislunar activities, including lunar landings, the Lunar\nGateway, and in-space refueling stations, requires advances in cost-efficient\ntrajectory design and reliable integration of navigation and remote sensing.\nTraditional Earth-Moon transfers suffer from rigid launch windows and high\npropellant demands, while Earth-based GNSS systems provide little to no\ncoverage beyond geostationary orbit. This limits autonomy and environmental\nawareness in cislunar space. This review compares four major transfer\nstrategies by evaluating velocity requirements, flight durations, and fuel\nefficiency, and by identifying their suitability for both crewed and robotic\nmissions. The emerging role of artificial intelligence and machine learning is\nhighlighted: convolutional neural networks support automated crater recognition\nand digital terrain model generation, while deep reinforcement learning enables\nadaptive trajectory refinement during descent and landing to reduce risk and\ndecision latency. The study also examines how GNSS-Reflectometry and advanced\nPositioning, Navigation, and Timing architectures can extend navigation\ncapabilities beyond current limits. GNSS-R can act as a bistatic radar for\nmapping lunar ice, soil properties, and surface topography, while PNT systems\nsupport autonomous rendezvous, Lagrange point station-keeping, and coordinated\nsatellite swarm operations. Combining these developments establishes a scalable\nframework for sustainable cislunar exploration and long-term human and robotic\npresence.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Recognition",
=======
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "Reinforcement Learning"
    ]
  },
  {
<<<<<<< HEAD
    "title": "Nowcast3D: Reliable precipitation nowcasting via gray-box learning",
    "url": "http://arxiv.org/abs/2511.04659v1",
    "authors": [
      "Huaguan Chen",
      "Wei Han",
      "Haofei Sun",
      "Ning Lin",
      "Xingtao Song",
      "Yunfan Yang",
      "Jie Tian",
      "Yang Liu",
      "Ji-Rong Wen",
      "Xiaoye Zhang",
      "Xueshun Shen",
      "Hao Sun"
    ],
    "published": "2025-11-06",
    "abstract": "Extreme precipitation nowcasting demands high spatiotemporal fidelity and\nextended lead times, yet existing approaches remain limited. Numerical Weather\nPrediction (NWP) and its deep-learning emulations are too slow and coarse for\nrapidly evolving convection, while extrapolation and purely data-driven models\nsuffer from error accumulation and excessive smoothing. Hybrid 2D radar-based\nmethods discard crucial vertical information, preventing accurate\nreconstruction of height-dependent dynamics. We introduce a gray-box, fully\nthree-dimensional nowcasting framework that directly processes volumetric radar\nreflectivity and couples physically constrained neural operators with\ndatadriven learning. The model learns vertically varying 3D advection fields\nunder a conservative advection operator, parameterizes spatially varying\ndiffusion, and introduces a Brownian-motion--inspired stochastic term to\nrepresent unresolved motions. A residual branch captures small-scale convective\ninitiation and microphysical variability, while a diffusion-based stochastic\nmodule estimates uncertainty. The framework achieves more accurate forecasts up\nto three-hour lead time across precipitation regimes and ranked first in 57\\%\nof cases in a blind evaluation by 160 meteorologists. By restoring full 3D\ndynamics with physical consistency, it offers a scalable and robust pathway for\nskillful and reliable nowcasting of extreme precipitation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Deep Learning-Driven Downscaling for Climate Risk Assessment of Projected Temperature Extremes in the Nordic Region",
    "url": "http://arxiv.org/abs/2511.03770v1",
    "authors": [
      "Parthiban Loganathan",
      "Elias Zea",
      "Ricardo Vinuesa",
      "Evelyn Otero"
    ],
    "published": "2025-11-05",
    "abstract": "Rapid changes and increasing climatic variability across the widely varied\nKoppen-Geiger regions of northern Europe generate significant needs for\nadaptation. Regional planning needs high-resolution projected temperatures.\nThis work presents an integrative downscaling framework that incorporates\nVision Transformer (ViT), Convolutional Long Short-Term Memory (ConvLSTM), and\nGeospatial Spatiotemporal Transformer with Attention and Imbalance-Aware\nNetwork (GeoStaNet) models. The framework is evaluated with a multicriteria\ndecision system, Deep Learning-TOPSIS (DL-TOPSIS), for ten strategically chosen\nmeteorological stations encompassing the temperate oceanic (Cfb), subpolar\noceanic (Cfc), warm-summer continental (Dfb), and subarctic (Dfc) climate\nregions. Norwegian Earth System Model (NorESM2-LM) Coupled Model\nIntercomparison Project Phase 6 (CMIP6) outputs were bias-corrected during the\n1951-2014 period and subsequently validated against earlier observations of\nday-to-day temperature metrics and diurnal range statistics. The ViT showed\nimproved performance (Root Mean Squared Error (RMSE): 1.01 degrees C; R^2:\n0.92), allowing for production of credible downscaled projections. Under the\nSSP5-8.5 scenario, the Dfc and Dfb climate zones are projected to warm by 4.8\ndegrees C and 3.9 degrees C, respectively, by 2100, with expansion in the\ndiurnal temperature range by more than 1.5 degrees C. The Time of Emergence\nsignal first appears in subarctic winter seasons (Dfc: approximately 2032),\nsignifying an urgent need for adaptation measures. The presented framework\noffers station-based, high-resolution estimates of uncertainties and extremes,\nwith direct uses for adaptation policy over high-latitude regions with fast\nenvironmental change.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Disentangling Internal Tides from Balanced Motions with Deep Learning and Surface Field Synergy",
    "url": "http://arxiv.org/abs/2511.03614v1",
    "authors": [
      "Han Wang",
      "Jeffrey Uncu",
      "Kaushik Srinivasan",
      "Nicolas Grisouard"
    ],
    "published": "2025-11-05",
    "abstract": "A fundamental challenge in ocean dynamics is the disentanglement of balanced\nmotions and internal waves. Extracting internal tidal (IT) imprints on surface\ndata is a central part of this challenge. For IT extraction, traditional\nharmonic analysis fails in the presence of strong incoherence and poor temporal\nsampling, as is common in global satellite observations. The advent of new\nwide-swath satellites, which provide two-dimensional spatial coverage, allows\nIT extraction to be reformulated as an image translation problem. Building on\nrecent work where we developed a deep learning approach to extract IT\nsignatures from sea surface height (SSH) in an idealized turbulent simulation,\nwe show here that a simpler and computationally cheaper algorithm can perform\nequally well if the learning rate is annealed during training. Using this new,\nconvenient algorithm, we experiment with different combinations of input\nsurface fields -- SSH, surface temperature, and surface velocity. All fields\ncontribute synergistically to disentanglement, with surface velocity by far the\nmost informative. These findings underscore the value of coordinated\nmulti-platform observational campaigns and highlight the critical importance of\nsurface velocity observations for separating balanced motions and internal\nwaves. Additional insights into the behavior of deep learning algorithm emerge:\nboth wave signature and scattering medium aids IT extraction, and to exploit\nlarge-scale information in the scattering medium, the algorithm must be highly\nnon-local. Residual errors of our algorithm concentrate at small spatial scales\nnear mode-2 tidal wavelengths, likely arising from artifacts introduced during\ndata preparation (e.g., Doppler shifts) as well as imperfections in the deep\nlearning architecture.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets",
    "url": "http://arxiv.org/abs/2511.02887v1",
    "authors": [
      "Chaitanya Rele",
      "Aditya Rathod",
      "Kaustubh Natu",
      "Saurabh Kulkarni",
      "Ajay Koli",
      "Swapnali Makdey"
    ],
    "published": "2025-11-04",
    "abstract": "The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,\nrepresents a vital source of livelihood for coastal communities, yet fishermen\noften face uncertainty in locating productive fishing grounds. To address this\nchallenge, we present an AI-assisted framework for predicting Potential Fishing\nZones (PFZs) using oceanographic parameters such as sea surface temperature and\nchlorophyll concentration. The approach is designed to enhance the accuracy of\nPFZ identification and provide region-specific insights for sustainable fishing\npractices. Preliminary results indicate that the framework can support\nfishermen by reducing search time, lowering fuel consumption, and promoting\nefficient resource utilization.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics",
    "url": "http://arxiv.org/abs/2511.04534v1",
    "authors": [
      "Jonas E. Katona",
      "Emily K. de Jong",
      "Nipun Gunawardena"
    ],
    "published": "2025-11-06",
    "abstract": "Reduced-order models (ROMs) can efficiently simulate high-dimensional\nphysical systems, but lack robust uncertainty quantification methods. Existing\napproaches are frequently architecture- or training-specific, which limits\nflexibility and generalization. We introduce a post hoc, model-agnostic\nframework for predictive uncertainty quantification in latent space ROMs that\nrequires no modification to the underlying architecture or training procedure.\nUsing conformal prediction, our approach estimates statistical prediction\nintervals for multiple components of the ROM pipeline: latent dynamics,\nreconstruction, and end-to-end predictions. We demonstrate the method on a\nlatent space dynamical model for cloud microphysics, where it accurately\npredicts the evolution of droplet-size distributions and quantifies uncertainty\nacross the ROM pipeline.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Probabilistic U-Net Approach to Downscaling Climate Simulations",
    "url": "http://arxiv.org/abs/2511.03197v1",
    "authors": [
      "Maryam Alipourhajiagha",
      "Pierre-Louis Lemaire",
      "Youssef Diouane",
      "Julie Carreau"
    ],
    "published": "2025-11-05",
    "abstract": "Climate models are limited by heavy computational costs, often producing\noutputs at coarse spatial resolutions, while many climate change impact studies\nrequire finer scales. Statistical downscaling bridges this gap, and we adapt\nthe probabilistic U-Net for this task, combining a deterministic U-Net backbone\nwith a variational latent space to capture aleatoric uncertainty. We evaluate\nfour training objectives, afCRPS and WMSE-MS-SSIM with three settings for\ndownscaling precipitation and temperature from $16\\times$ coarser resolution.\nOur main finding is that WMSE-MS-SSIM performs well for extremes under certain\nsettings, whereas afCRPS better captures spatial variability across scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Towards a Unified Data-Driven Boundary Layer Momentum Flux Parameterization for Ocean and Atmosphere",
    "url": "http://arxiv.org/abs/2511.01766v1",
    "authors": [
      "Renaud Falga",
      "Sara Shamekh",
      "Laure Zanna"
    ],
    "published": "2025-11-03",
    "abstract": "Boundary layer turbulence, particularly the vertical fluxes of momentum,\nshapes the evolution of winds and currents and plays a critical role in\nweather, climate, and biogeochemical processes. In this work, a unified,\ndata-driven parameterization of turbulent momentum fluxes is introduced for\nboth the oceanic and atmospheric convective boundary layers. An artificial\nneural network (ANN) is trained offline on coarse-grained large-eddy simulation\n(LES) data representing a wide range of turbulent regimes in both fluids. By\nnormalizing momentum flux profiles with their surface values, we exploit a\nself-similar structure across regimes and fluids, enabling joint training. The\nANN learns to predict vertical profiles of subgrid momentum fluxes from mean\nwind or current profiles, capturing key physical features such as upgradient\nfluxes that are inaccessible to traditional first-order closure schemes. When\nimplemented online in the Single Column Atmospheric Model (SCAM), the ANN\nparameterization consistently outperforms the SCAM baseline parameterization in\nreplicating the evolution of the boundary layer wind profiles from the LES,\nespecially under convective conditions, with errors reduced by a factor of 2-3\nacross regimes. ANN performance remains robust even when the surface momentum\nflux is biased up to 30\\%, and generalization is confirmed by testing on LES\ncases excluded from the training dataset. This work demonstrates the potential\nof machine learning to create unified and physically consistent\nparameterizations across boundary layer systems in climate models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",
    "url": "http://arxiv.org/abs/2511.01019v2",
    "authors": [
      "Bowen Chen",
      "Jayesh Gajbhar",
      "Gregory Dusek",
      "Rob Redmon",
      "Patrick Hogan",
      "Paul Liu",
      "DelWayne Bohnenstiehl",
      "Dongkuan Xu",
      "Ruoying He"
    ],
    "published": "2025-11-02",
    "abstract": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "The Future Orchid Diversity of Great Britain and Ireland using an SDM Approach",
    "url": "http://arxiv.org/abs/2511.01122v1",
    "authors": [
      "Sofoklis Mouratidis",
      "Konstantinos Kougioumoutzis",
      "Martha Charitonidou",
      "John M. Halley"
    ],
    "published": "2025-11-03",
    "abstract": "In this paper we use Species Distribution Models (SDMs) to forecast the\nfuture diversity and distribution of orchids in Great Britain and Ireland under\nscenarios of climate and land-use change. The study analyzes occurrence data\nfor native orchid taxa in the BSBI database at a fine spatial resolution (1\nkm^2, monads) and incorporates multiple environmental variables including\nclimate, land use, topography, and soil. These SDMs project significant losses\nin orchid species richness by 2050 and 2070, especially under severe climate\nand land-use scenarios, with declines expected across most species and regions,\nincluding Ireland where historical data previously indicated gains. The models\nreveal vulnerable species likely to face extinction by 2070, emphasizing the\nimpact of both climate warming and habitat modifications. This approach differs\nfrom previous trend-based analyses by integrating future projections,\nhigh-resolution spatial data, and dynamic land-use scenarios, thereby providing\nhigher-resolution estimates of orchid range contractions and diversity losses.\nWhile current observed orchid trends show some regional increases, particularly\nin Ireland, the SDM forecasts indicate substantial future risks. The study also\ndiscusses uncertainties due to niche truncation from geographic data limits and\nhighlights the need for broader-scale modeling for more robust predictions.\nOverall, the paper anticipates conservation challenges for orchid biodiversity\nin response to ongoing environmental changes.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "CORONA-Fields: Leveraging Foundation Models for Classification of Solar Wind Phenomena",
    "url": "http://arxiv.org/abs/2511.09843v1",
    "authors": [
      "Daniela Martin",
      "Jinsu Hong",
      "Connor O'Brien",
      "Valmir P Moraes Filho",
      "Jasmine R. Kobayashi",
      "Evangelia Samara",
      "Joseph Gallego"
    ],
    "published": "2025-11-13",
    "abstract": "Space weather at Earth, driven by the solar activity, poses growing risks to satellites around our planet as well as to critical ground-based technological infrastructure. Major space weather contributors are the solar wind and coronal mass ejections whose variable density, speed, temperature, and magnetic field make the automated classification of those structures challenging. In this work, we adapt a foundation model for solar physics, originally trained on Solar Dynamics Observatory imagery, to create embeddings suitable for solar wind structure analysis. These embeddings are concatenated with the spacecraft position and solar magnetic connectivity encoded using Fourier features which generates a neural field-based model. The full deep learning architecture is fine-tuned bridging the gap between remote sensing and in situ observations. Labels are derived from Parker Solar Probe measurements, forming a downstream classification task that maps plasma properties to solar wind structures. Although overall classification performance is modest, likely due to coarse labeling, class imbalance, and limited transferability of the pretrained model, this study demonstrates the feasibility of leveraging foundation model embeddings for in situ solar wind tasks. As a first proof-of-concept, it lays the groundwork for future improvements toward more reliable space weather predictions. The code and configuration files used in this study are publicly available to support reproducibility.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping",
    "url": "http://arxiv.org/abs/2511.08156v1",
    "authors": [
      "Chenying Liu",
      "Wei Huang",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-11",
    "abstract": "Land Use and Land Cover (LULC) mapping is a fundamental task in Earth Observation (EO). However, current LULC models are typically developed for a specific modality and a fixed class taxonomy, limiting their generability and broader applicability. Recent advances in foundation models (FMs) offer promising opportunities for building universal models. Yet, task-agnostic FMs often require fine-tuning for downstream applications, whereas task-specific FMs rely on massive amounts of labeled data for training, which is costly and impractical in the remote sensing (RS) domain. To address these challenges, we propose LandSegmenter, an LULC FM framework that resolves three-stage challenges at the input, model, and output levels. From the input side, to alleviate the heavy demand on labeled data for FM training, we introduce LAnd Segment (LAS), a large-scale, multi-modal, multi-source dataset built primarily with globally sampled weak labels from existing LULC products. LAS provides a scalable, cost-effective alternative to manual annotation, enabling large-scale FM training across diverse LULC domains. For model architecture, LandSegmenter integrates an RS-specific adapter for cross-modal feature extraction and a text encoder for semantic awareness enhancement. At the output stage, we introduce a class-wise confidence-guided fusion strategy to mitigate semantic omissions and further improve LandSegmenter's zero-shot performance. We evaluate LandSegmenter on six precisely annotated LULC datasets spanning diverse modalities and class taxonomies. Extensive transfer learning and zero-shot experiments demonstrate that LandSegmenter achieves competitive or superior performance, particularly in zero-shot settings when transferred to unseen datasets. These results highlight the efficacy of our proposed framework and the utility of weak supervision for building task-specific FMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DiffRegCD: Integrated Registration and Change Detection with Diffusion Features",
    "url": "http://arxiv.org/abs/2511.07935v2",
    "authors": [
      "Seyedehanita Madani",
      "Rama Chellappa",
      "Vishal M. Patel"
    ],
    "published": "2025-11-11",
    "abstract": "Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation",
    "url": "http://arxiv.org/abs/2511.10370v1",
    "authors": [
      "Kai-Hendrik Cohrs",
      "Zuzanna Osika",
      "Maria Gonzalez-Calabuig",
      "Vishal Nedungadi",
      "Ruben Cartuyvels",
      "Steffen Knoblauch",
      "Joppe Massant",
      "Shruti Nath",
      "Patrick Ebel",
      "Vasileios Sitokonstantinou"
    ],
    "published": "2025-11-13",
    "abstract": "Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands",
    "url": "http://arxiv.org/abs/2511.10177v1",
    "authors": [
      "Tishya Chhabra",
      "Manisha Bajpai",
      "Walter Zesk",
      "Skylar Tibbits"
    ],
    "published": "2025-11-13",
    "abstract": "We present an initial evaluation of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on shoreline delineation of small sandy islands using satellite images. We curated and labeled a dataset of 225 multispectral images of two Maldivian islands, which we publicly release, and fine-tuned both the 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Our experiments show that even with as few as 5 training images, the models achieve high performance (F1 of 0.94, IoU of 0.79). Our results demonstrate the strong transfer learning capability of Prithvi, underscoring the potential of such models to support coastal monitoring in data-poor regions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning",
    "url": "http://arxiv.org/abs/2511.06316v1",
    "authors": [
      "MD Thamed Bin Zaman Chowdhury",
      "Moazzem Hossain"
    ],
    "published": "2025-11-09",
    "abstract": "Reliable geospatial information on road accidents is vital for safety analysis and infrastructure planning, yet most low- and middle-income countries continue to face a critical shortage of accurate, location-specific crash data. Existing text-based geocoding tools perform poorly in multilingual and unstructured news environments, where incomplete place descriptions and mixed Bangla-English scripts obscure spatial context. To address these limitations, this study introduces ALIGN (Accident Location Inference through Geo-Spatial Neural Reasoning)- a vision-language framework that emulates human spatial reasoning to infer accident coordinates directly from textual and map-based cues. ALIGN integrates large language and vision-language models within a multi-stage pipeline that performs optical character recognition, linguistic reasoning, and map-level verification through grid-based spatial scanning. The framework systematically evaluates each predicted location against contextual and visual evidence, ensuring interpretable, fine-grained geolocation outcomes without requiring model retraining. Applied to Bangla-language news data, ALIGN demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district and sub-district-level crash sites. Beyond its technical contribution, the framework establishes a high accuracy foundation for automated crash mapping in data-scarce regions, supporting evidence-driven road-safety policymaking and the broader integration of multimodal artificial intelligence in transportation analytics. The code for this paper is open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery",
    "url": "http://arxiv.org/abs/2511.10387v1",
    "authors": [
      "Prince Mensah",
      "Pelumi Victor Aderinto",
      "Ibrahim Salihu Yusuf",
      "Arnu Pretorius"
    ],
    "published": "2025-11-13",
    "abstract": "Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.",
    "categories": [
      "remote_sensing",
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder",
      "PINN"
    ],
    "applications": []
  },
  {
    "title": "PGDM: Physically guided diffusion model for land surface temperature downscaling",
    "url": "http://arxiv.org/abs/2511.05964v1",
    "authors": [
      "Huanyu Zhang",
      "Bo-Hui Tang",
      "Tian Hu",
      "Yun Jiang",
      "Zhao-Liang Li"
    ],
    "published": "2025-11-08",
    "abstract": "Land surface temperature (LST) is a fundamental parameter in thermal infrared remote sensing, while current LST products are often constrained by the trade-off between spatial and temporal resolutions. To mitigate this limitation, numerous studies have been conducted to enhance the resolutions of LST data, with a particular emphasis on the spatial dimension (commonly known as LST downscaling). Nevertheless, a comprehensive benchmark dataset tailored for this task remains scarce. In addition, existing downscaling models face challenges related to accuracy, practical usability, and the capability to self-evaluate their uncertainties. To overcome these challenges, this study first compiled three representative datasets, including one dataset over mainland China containing 22,909 image patches for model training and evaluation, as well as two datasets covering 40 heterogeneous regions worldwide for external evaluation. Subsequently, grounded in the surface energy balance (SEB)-based geophysical reasoning, we proposed the physically guided diffusion model (PGDM) for LST downscaling. In this framework, the downscaling task was formulated as an inference problem, aiming to sample from the posterior distribution of high-spatial-resolution (HR) LST conditioned on low-spatial-resolution (LR) LST observations and a suite of HR geophysical priors. Comprehensive evaluations demonstrate the effectiveness of PGDM, which generates high-quality downscaling results and outperforms existing representative interpolation, kernel-driven, hybrid, and deep learning approaches. Finally, by exploiting the inherent stochasticity of PGDM, the scene-level standard deviation of multiple generations was computed, revealing a strong positive linear correlation with the actual downscaling error...",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data",
    "url": "http://arxiv.org/abs/2511.10461v1",
    "authors": [
      "Simon Donike",
      "Cesar Aybar",
      "Julio Contreras",
      "Luis G\u00f3mez-Chova"
    ],
    "published": "2025-11-13",
    "abstract": "We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Super-Resolution"
    ]
  },
  {
    "title": "Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps with Sub-Meter Imagery",
    "url": "http://arxiv.org/abs/2511.07231v2",
    "authors": [
      "Kyeongjin Ahn",
      "YongHun Suh",
      "Sungwon Han",
      "Jeasurk Yang",
      "Hannes Taubenb\u00f6ck",
      "Meeyoung Cha"
    ],
    "published": "2025-11-10",
    "abstract": "Access to Water, Sanitation, and Hygiene (WASH) services remains a major public health concern in refugee camps. This study introduces a remote sensing-driven framework to quantify WASH accessibility-specifically to water pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one of the world's most densely populated displacement settings. Detecting refugee shelters in such emergent camps presents substantial challenges, primarily due to their dense spatial configuration and irregular geometric patterns. Using sub-meter satellite images, we develop a semi-supervised segmentation framework that achieves an F1-score of 76.4% in detecting individual refugee shelters. Applying the framework across multi-year data reveals declining WASH accessibility, driven by rapid refugee population growth and reduced facility availability, rising from 25 people per facility in 2022 to 29.4 in 2025. Gender-disaggregated analysis further shows that women and girls experience reduced accessibility, in scenarios with inadequate safety-related segregation in WASH facilities. These findings suggest the importance of demand-responsive allocation strategies that can identify areas with under-served populations-such as women and girls-and ensure that limited infrastructure serves the greatest number of people in settings with fixed or shrinking budgets. We also discuss the value of high-resolution remote sensing and machine learning to detect inequality and inform equitable resource planning in complex humanitarian environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Advancing Ocean State Estimation with efficient and scalable AI",
    "url": "http://arxiv.org/abs/2511.06041v1",
    "authors": [
      "Yanfei Xiang",
      "Yuan Gao",
      "Hao Wu",
      "Quan Zhang",
      "Ruiqi Shu",
      "Xiao Zhou",
      "Xi Wu",
      "Xiaomeng Huang"
    ],
    "published": "2025-11-08",
    "abstract": "Accurate and efficient global ocean state estimation remains a grand challenge for Earth system science, hindered by the dual bottlenecks of computational scalability and degraded data fidelity in traditional data assimilation (DA) and deep learning (DL) approaches. Here we present an AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly assimilates multi-source and multi-scale observations, ranging from sparse in-situ measurements to 4 km satellite swaths, without any interpolation or data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous mapping from heterogeneous inputs to ocean states, preserving native data fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\\circ$ mesoscale dynamics from coarse 1$^\\circ$ fields, which ensures both efficiency and scalability, with just 3.7\\% more parameters than the 1$^\\circ$ configuration. When coupled with a DL forecasting system, ADAF-Ocean extends global forecast skill by up to 20 days compared to baselines without assimilation. This framework establishes a computationally viable and scientifically rigorous pathway toward real-time, high-resolution Earth system monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ]
  },
  {
    "title": "The Role of Deep Mesoscale Eddies in Ensemble Forecast Performance",
    "url": "http://arxiv.org/abs/2511.09747v1",
    "authors": [
      "Justin Cooke",
      "Kathleen Donohue",
      "Clark D Rowley",
      "Prasad G Thoppil",
      "D Randolph Watts"
    ],
    "published": "2025-11-12",
    "abstract": "Present forecasting efforts rely on assimilation of observational data captured in the upper ocean (< 1000 m depth). These observations constrain the upper ocean and minimally influence the deep ocean. Nevertheless, development of the full water column circulation critically depends upon the dynamical interactions between upper and deep fields. Forecasts demonstrate that the initialization of the deep field is influential for the development and evolution of the surface in the forecast. Deep initial conditions that better agree with observations have lower upper ocean uncertainty as the forecast progresses. Here, best and worst ensemble members in two 92-day forecasts are identified and contrasted in order to determine how the deep ocean differs between these groups. The forecasts cover the duration of the Loop Current Eddy Thor separation event, which coincides with available deep observations in the Gulf. Model member performance is assessed by comparing surface variables against verifying analysis and satellite altimeter data during the forecast time-period. Deep cyclonic and anticyclonic features are reviewed, and compared against deep observations, indicating subtle differences in locations of deep eddies at relevant times. These results highlight both the importance of deep circulation in the dynamics of the Loop Current system and more broadly motivate efforts to assimilate deep observations to better constrain the deep initial fields and improve surface predictions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "General Circulation Models of Hycean Worlds",
    "url": "http://arxiv.org/abs/2511.07546v1",
    "authors": [
      "Edouard Barrier",
      "Nikku Madhusudhan"
    ],
    "published": "2025-11-10",
    "abstract": "Sub-Neptunes represent the current frontier of exoplanet atmospheric characterisation. A proposed subset, Hycean planets, would have liquid water oceans and be potentially habitable, but there are many unanswered questions about their atmospheric dynamics and 3D climate states. To explore such climates in detail, we report a General Circulation Model (GCM) for Hycean worlds, building on a modified version of the ExoCAM GCM. Considering the temperate sub-Neptune K2-18 b as a Hycean candidate, we implement GCMs with different surface pressures and albedos. We find dynamical structures similar to those of tidally-locked terrestrial planets as `slow rotators' with either one equatorial or twin mid-latitude zonal jets. We see moist convective inhibition that matches high resolution models, although in hotter cases the inhibited zone is subsaturated. When imposing a top-of-the-atmosphere (TOA) Bond albedo ($A_b$) by modifying the incident stellar flux, we find that the threshold for K2-18~b to not enter a runaway greenhouse state is $A_b \\geq 0.55$ for a 1 bar atmosphere, consistent with previous studies, and $A_b \\geq 0.8$ for a 5 bar atmosphere. However, a more realistic treatment of the albedo, by modelling scattering within the atmosphere using an enhanced Rayleigh parametrisation, leads to lower lapse rates and stronger thermal inversions. We find that 1 bar atmospheres are stable for an albedo of $A_b \\geq 0.27$, 5 bar atmospheres for $A_b \\geq 0.35$, and 10 bar atmospheres for $A_b \\geq 0.48$. Moderate albedos such as these are typical of the solar system planets and the required scattering is consistent with observational constraints for K2-18~b, supporting its plausibility as a Hycean world.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Global Distribution of the Key Species on the Surface of Europa",
    "url": "http://arxiv.org/abs/2511.09489v1",
    "authors": [
      "Jiazheng Li",
      "Yinsi Shou",
      "Cheng Li",
      "Xianzhe Jia"
    ],
    "published": "2025-11-12",
    "abstract": "The icy surface of Europa is continuously bombarded by ions and electrons from Jupiter's magnetosphere. The bombardment of the particles dissociates water molecules on the surface of Europa and introduces impurities to the icy surface. Such processes lead to the generation of the non-water species on the surface of Europa. Such processes lead to the generation of the nonwater species on the surface of Europa. These chemical species are closely related to the chemistry of the icy crust and the subsurface ocean, as well as Europa's habitability. However, our knowledge of the global distribution of these species is limited due to the sparse satellite and telescope observations on Europa. In this study, we combine a Europa plasma model and a chemical-transport model to simulate the global distribution of the key nonwater species on the surface of Europa. The initial results from our model agree well with the existing observations on the distributions of H2SO4 and SO2 but they show a significant discrepancy with the observed distribution of H2O2. Sensitivity tests on the reaction rate coefficients indicate that the simulated global distribution of all three species fit the observations well if the reaction rate coefficients in the ice are reduced by one order of magnitude. This finding provides a useful constraint on the rate coefficient of the chemical reactions in the ice. Furthermore, our model predicts that the O2 on the surface ice of Europa is concentrated on the leading hemisphere. The simulated global distribution of the key species on Europa may provide useful guidance for future missions to Europa, such as Europa Clipper and JUICE.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Backcasting biodiversity at high spatiotemporal resolution using flexible site-occupancy models for opportunistically sampled citizen science data",
    "url": "http://arxiv.org/abs/2511.08802v1",
    "authors": [
      "Maxime Fajgenblat",
      "Marc Herremans",
      "Pieter Vanormelingen",
      "Kristijn Swinnen",
      "Dirk Maes",
      "Robby Stoks",
      "Luc De Meester",
      "Christel Faes",
      "Thomas Neyens"
    ],
    "published": "2025-11-11",
    "abstract": "For many taxonomic groups, online biodiversity portals used by naturalists and citizen scientists constitute the primary source of distributional information. Over the last decade, site-occupancy models have been advanced as a promising framework to analyse such loosely structured, opportunistically collected datasets. Current approaches often ignore important aspects of the detection process and do not fully capitalise on the information present in these datasets, leaving opportunities for fine-grained spatiotemporal backcasting untouched. We propose a flexible Bayesian spatiotemporal site-occupancy model that aims to mimic the data-generating process that underlies common citizen science datasets sourced from public biodiversity portals, and yields rich biological output. We illustrate the use of the model to a dataset containing over 3M butterfly records in Belgium, collected through the citizen science data portal Observations.be. We show that the proposed approach enables retrospective predictions on the occupancy of species through time and space at high resolution, as well as inference on inter-annual distributional trends, range dynamics, habitat preferences, phenological patterns, detection patterns and observer heterogeneity. The proposed model can be used to increase the value of opportunistically collected data by naturalists and citizen scientists, and can aid the understanding of spatiotemporal dynamics of species for which rigorously collected data are absent or too costly to collect.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "The MATISSE view of the inner region of the RY Tau protoplanetary disk",
    "url": "http://arxiv.org/abs/2511.08309v1",
    "authors": [
      "J. S. Martin",
      "J. Kobus",
      "J. Varga",
      "A. Matter",
      "S. Wolf",
      "M. Abello",
      "F. Allouche",
      "J. -C. Augereau",
      "P. Berio",
      "F. Bettonvil",
      "R. van Boekel",
      "P. A. Boley",
      "P. Cruzal\u00e8bes",
      "W. C. Danchi",
      "J. Drevon",
      "C. Dominik",
      "V. Fleury",
      "V. G\u00e1mez Rosas",
      "A. Glindemann",
      "L. N. A. van Haastere",
      "M. Heininger",
      "Th. Henning",
      "K. -H. Hofmann",
      "M. Hogerheijde",
      "M. Houll\u00e9",
      "J. W. Isbell",
      "W. Jaffe",
      "L. Labadie",
      "S. Lagarde",
      "J. H. Leftley",
      "M. Lehmitz",
      "M. Letessier",
      "B. Lopez",
      "F. Lykou",
      "J. Ma",
      "A. Meilland",
      "F. Millour",
      "C. Paladini",
      "E. Pantin",
      "R. G. Petrov",
      "P. Priolet",
      "S. Robbe-Dubois",
      "D. Schertl",
      "M. Scheuck",
      "J. Scigliuto",
      "G. Weigelt",
      "J. Woillez"
    ],
    "published": "2025-11-11",
    "abstract": "The T-Tauri type young stellar object RY Tau exhibits a dust depleted inner cavity characteristic of a transition disk. We constrain the spatial distribution and mineralogy of dust in the RY Tau protoplanetary disk in the inner few astronomical units using spectrally resolved interferometric observations in the L, M, and N bands obtained with VLTI/MATISSE.\n  Employing a 2D temperature gradient model we estimate the orientation of the inner disk finding no evidence of significant misalignment between the inner and outer disk of RY Tau. Successively, we analyze the chemical composition of silicates depending on spatial region in the disk and identify several silicate species commonly found in protoplanetary disks. Additionally, a depletion of amorphous dust grains toward the central protostar is observed. Monte Carlo radiative transfer simulations show that hot dust close to the protostar and in the line of sight to the observer, either in the uppermost disk layers of a strongly flared disk or in a dusty envelope, is necessary to model the observations. The shadow cast by a dense innermost disk midplane on the dust further out explains the observed closure phases in the L band and to some extent in the M band. However, the closure phases in the N band are underestimated by our model, hinting at an additional asymmetry in the flux density distribution not visible at shorter wavelengths.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Uncertainty Calibration of Multi-Label Bird Sound Classifiers",
    "url": "http://arxiv.org/abs/2511.08261v1",
    "authors": [
      "Raphael Schwinger",
      "Ben McEwen",
      "Vincent S. Kather",
      "Ren\u00e9 Heinrich",
      "Lukas Rauch",
      "Sven Tomforde"
    ],
    "published": "2025-11-11",
    "abstract": "Passive acoustic monitoring enables large-scale biodiversity assessment, but reliable classification of bioacoustic sounds requires not only high accuracy but also well-calibrated uncertainty estimates to ground decision-making. In bioacoustics, calibration is challenged by overlapping vocalisations, long-tailed species distributions, and distribution shifts between training and deployment data. The calibration of multi-label deep learning classifiers within the domain of bioacoustics has not yet been assessed. We systematically benchmark the calibration of four state-of-the-art multi-label bird sound classifiers on the BirdSet benchmark, evaluating both global, per-dataset and per-class calibration using threshold-free calibration metrics (ECE, MCS) alongside discrimination metrics (cmAP). Model calibration varies significantly across datasets and classes. While Perch v2 and ConvNeXt$_{BS}$ show better global calibration, results vary between datasets. Both models indicate consistent underconfidence, while AudioProtoPNet and BirdMAE are mostly overconfident. Surprisingly, calibration seems to be better for less frequent classes. Using simple post hoc calibration methods we demonstrate a straightforward way to improve calibration. A small labelled calibration set is sufficient to significantly improve calibration with Platt scaling, while global calibration parameters suffer from dataset variability. Our findings highlight the importance of evaluating and improving uncertainty calibration in bioacoustic classifiers.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
    "url": "http://arxiv.org/abs/2511.15658v1",
    "authors": [
      "Naomi Simumba",
      "Nils Lehmann",
      "Paolo Fraccaro",
      "Hamed Alemohammad",
      "Geeth De Mel",
      "Salman Khan",
      "Manil Maskey",
      "Nicolas Longepe",
      "Xiao Xiang Zhu",
      "Hannah Kerner",
      "Juan Bernabe-Moreno",
      "Alexander Lacoste"
    ],
    "published": "2025-11-19",
    "abstract": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.\n  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving",
    "url": "http://arxiv.org/abs/2511.16049v1",
    "authors": [
      "Pei Liu",
      "Songtao Wang",
      "Lang Zhang",
      "Xingyue Peng",
      "Yuandong Lyu",
      "Jiaxin Deng",
      "Songxin Lu",
      "Weiliang Ma",
      "Xueyang Zhang",
      "Yifei Zhan",
      "XianPeng Lang",
      "Jun Ma"
    ],
    "published": "2025-11-20",
    "abstract": "Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2511.16322v1",
    "authors": [
      "Ching-Heng Cheng",
      "Chih-Chung Hsu"
    ],
    "published": "2025-11-20",
    "abstract": "Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection",
    "url": "http://arxiv.org/abs/2511.16143v1",
    "authors": [
      "Quanqing Ma",
      "Jiaen Chen",
      "Peng Wang",
      "Yao Zheng",
      "Qingzhan Zhao",
      "Yuchen Zheng"
    ],
    "published": "2025-11-20",
    "abstract": "Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "SpectralTrain: A Universal Framework for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2511.16084v1",
    "authors": [
      "Meihua Zhou",
      "Liping Yu",
      "Jiawei Cai",
      "Wai Kin Fung",
      "Ruiguo Hu",
      "Jiarui Zhao",
      "Wenzhuo Liu",
      "Nan Wan"
    ],
    "published": "2025-11-20",
    "abstract": "Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images",
    "url": "http://arxiv.org/abs/2511.13552v1",
    "authors": [
      "Sining Chen",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-17",
    "abstract": "Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "Mapping the Vanishing and Transformation of Urban Villages in China",
    "url": "http://arxiv.org/abs/2511.13507v1",
    "authors": [
      "Wenyu Zhang",
      "Yao Tong",
      "Yiqiu Liu",
      "Rui Cao"
    ],
    "published": "2025-11-17",
    "abstract": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests",
    "url": "http://arxiv.org/abs/2511.12740v1",
    "authors": [
      "Amirhossein Hassanzadeh",
      "Bartosz Krawczyk",
      "Michael Saunders",
      "Rob Wible",
      "Keith Krause",
      "Dimah Dera",
      "Jan van Aardt"
    ],
    "published": "2025-11-16",
    "abstract": "Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Regression",
      "Image Generation"
    ]
  },
  {
    "title": "Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models",
    "url": "http://arxiv.org/abs/2511.13891v1",
    "authors": [
      "Seyed Mohamad Ali Tousi",
      "John A. Lory",
      "G. N. DeSouza"
    ],
    "published": "2025-11-17",
    "abstract": "Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "An Active Learning Interatomic Potential For Defect-Engineered CoCrFeMnNi High-Entropy Alloy",
    "url": "http://arxiv.org/abs/2511.12514v1",
    "authors": [
      "Manish Sahoo",
      "Akash Deshmukh",
      "Yash Kokane",
      "Jayaprakash H M",
      "Raghavan Ranganathan"
    ],
    "published": "2025-11-16",
    "abstract": "High-entropy alloys (HEAs) exhibit exceptional properties arising from a combination of thermodynamic, kinetic and structural factors and have found applications in numerous fields such as aerospace, energy, chemical industries, hydrogen storage, and ocean engineering. However, a large compositional space remains to be explored. Unlike conventional approaches, computational methods have shown accelerated discovery of novel alloys in a short time. However, the lack of interatomic potentials have posed a challenge in discovering new alloy compositions and property measurements. In the present work, we have developed a Moment Tensor Potential (MTP) trained by Machine Learning based approach using the BFGS unconstrained optimization algorithm for the CoCrFeMnNi High-entropy alloy. Our training set consists of various defects induced configurations such as vacancies, dislocations and stacking-faults. An active learning scheme to re-train the potential was undertaken to dynamically to add training data upon encountering extrapolative configurations during non-equilibrium simulations. A thorough investigation of the error metrics, equation of state, uniaxial tensile deformation, nano-indentation and solid-liquid interface stability for this alloy was carried out, and it is seen that the MTP potential outperforms the popular Modified Embedded Atom Method (MEAM) potential on physical properties prediction. The accuracy and high computational speed are discussed using scaling performance. The potential is prepared for public use by embedding it into the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) code.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Current effects on wind generated waves near an Ocean Eddy Dipole",
    "url": "http://arxiv.org/abs/2511.12711v1",
    "authors": [
      "Nelson Violante-Carvalho",
      "Thiago de Paula",
      "Leandro Calado",
      "Felipe Marques dos Santos",
      "Luiz Mariano Carvalho",
      "Andre Luiz Cordeiro dos Santos",
      "Wilton Z. Arruda",
      "Leandro Farina"
    ],
    "published": "2025-11-16",
    "abstract": "Ocean eddy dipoles are among the most common mesoscale features and may be ubiquitous across the global oceans. However, wave-current interactions in their proximity have not been extensively studied. Here we examine the impact of surface currents on the wave field near an ocean eddy dipole. Using the WW3 wave model, we conducted idealized numerical simulations to assess the influence of different configurations on the spatial variability of Significant Wave Height ($H_s$). Additionally, a two-month hindcast of a strong dipole event in the Southwestern Atlantic Ocean was performed using three distinct surface current products: SSalto/Duacs, HYCOM NCODA and GlobCurrent. Among these, HYCOM, which incorporates ageostrophic effects, provided a more detailed representation of oceanic energy compared to GlobCurrent and SSalto/Duacs, which primarily reflect geostrophic components. The hindcast assessment employed denoised altimeter-derived $H_s$ data, with a spatial resolution of approximately 6~km. The greatest increase in wave energy occurs in the region between the peak values of positive and negative vorticity, where the opposing surface currents reach their maximum intensity. Therefore, dipoles act as converging lenses for surface waves, channeling their refraction towards the central jet. Despite its poorer spatial and temporal resolutions, SSalto-Duacs surface current data provides more reliable $H_s$ fields, in the study region where geostrophic dynamics are expected to be significant or even dominant.\n  HYCOM captures a broader range of dynamical processes, essential for accurately representing the total energy, though discrepancies with SSalto/Duacs data may arise from assimilation inaccuracies and model limitations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Integrating Deep Learning and Spatial Statistics in Marine Ecosystem Monitoring",
    "url": "http://arxiv.org/abs/2511.16447v1",
    "authors": [
      "Gian Mario Sangiovanni",
      "Gianluca Mastrantonio",
      "Daniele Ventura",
      "Alessio Pollice",
      "Giovanna Jona Lasinio"
    ],
    "published": "2025-11-20",
    "abstract": "In ecology, photogrammetry is a crucial method for efficiently collecting non-destructive samples of natural environments. When estimating the spatial distribution of animals, detecting objects in large-scale images becomes crucial. Object detection models enable large-scale analysis but introduce uncertainty because detection probability depends on various factors. To address detection bias, we model the distribution of a species of benthic animals (holothurians) in an area of the Italian Tyrrhenian coast near Giglio Island using a Thinned Log-Gaussian Cox Process (LGCP). We assume that a \"true\" intensity function accurately describes the distribution, while the observed process, resulting from independent thinning, is represented by a degraded intensity. The detection function controls the thinning mechanism, influenced by the object's location and other detection-related features. We use manual identification of holothurians as our benchmark. We compare automatic detection with this benchmark, an unthinned LGCP, and the thinned model to highlight the improvements gained from the proposed approach.Our method allows researchers to use photogrammetry, automatically identify objects of interest, and correct biases and approximations caused by the observation process.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Explainable AI for Diabetic Retinopathy Detection Using Deep Learning with Attention Mechanisms and Fuzzy Logic-Based Interpretability",
    "url": "http://arxiv.org/abs/2511.16294v1",
    "authors": [
      "Abishek Karthik",
      "Pandiyaraju V",
      "Sreya Mynampati"
    ],
    "published": "2025-11-20",
    "abstract": "The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment of edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "CNN",
      "Transformer",
      "GAN",
      "GNN"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture",
    "url": "http://arxiv.org/abs/2511.15535v1",
    "authors": [
      "Pandiyaraju V",
      "Abishek Karthik",
      "Sreya Mynampati",
      "Poovarasan L",
      "D. Saraswathi"
    ],
    "published": "2025-11-19",
    "abstract": "The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "CNN",
      "Transformer",
      "GAN",
      "GNN"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Experimental and Theoretical Aspects of the Fragmentation of Carbon's Single and Multi-Walled Nanotubes",
    "url": "http://arxiv.org/abs/2511.15467v1",
    "authors": [
      "Sumera Javeed",
      "Shoaib Ahmad"
    ],
    "published": "2025-11-19",
    "abstract": "Energetic ion irradiation is an effective method for studying how single and multi-shelled carbon nanotubes break apart. The energy from ions is dissipated through both linear and nonlinear processes in the nanotubes, leading to defect formation. Fragmentation occurs via atomic collision cascades and thermal spikes, each described by different theoretical models. Experiments with Cs-irradiated nanotubes support these models, and an information-theoretic approach further explains the fragmentation mechanisms. Sputtered species yield probability distributions, which are analyzed using Shannon entropy and fractal dimension to assess spatial characteristics. Kullback-Leibler divergence helps identify the diversity of emission mechanisms. Together, thermal and information-theoretic models clarify and distinguish the roles of collision cascades and thermal spikes in nanotube fragmentation.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Sense and Sensitivity - I. Uncertainty analysis of the gas-phase chemistry in AGB outflows",
    "url": "http://arxiv.org/abs/2511.13638v1",
    "authors": [
      "M. Van de Sande",
      "M. Gueguen",
      "T. Danilovich",
      "T. J. Millar"
    ],
    "published": "2025-11-17",
    "abstract": "Chemical reaction networks are central to all chemical models. Each rate coefficient has an associated uncertainty, which is generally not taken into account when calculating the chemistry. We performed the first uncertainty analysis of a chemical model of C-rich and O-rich AGB outflows using the Rate22 reaction network. Quantifying the error on the model predictions enables us to determine the need for adding complexity to the model. Using a Monte Carlo sampling method, we quantified the impact of the uncertainties on the chemical kinetic data on the predicted fractional abundances and column densities. The errors are caused by a complex interplay of reactions forming and destroying each species. Parent species show an error on their envelope sizes, which is not caused by the uncertainty on their photodissociation rate, but rather the chemistry reforming the parent after its photodissociation. Using photodissociation models to estimate the envelope size might be an oversimplification. The error on the CO envelope impacts retrieved mass-loss rates by up to a factor of two. For daughter species, the error on the peak fractional abundance ranges from a factor of a few to three orders of magnitude, and is on average about 10\\% of its value. This error is positively correlated with the error on the column density. The standard model suffices for many species, e.g., the radial distribution of cyanopolyynes and hydrocarbon radicals around IRC +10216. However, including spherical asymmetries, dust-gas chemistry, and photochemistry induced by a close-by stellar companion are still necessary to explain certain observations.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GeoPl@ntNet: A Platform for Exploring Essential Biodiversity Variables",
    "url": "http://arxiv.org/abs/2511.13790v1",
    "authors": [
      "Lukas Picek",
      "C\u00e9sar Leblanc",
      "Alexis Joly",
      "Pierre Bonnet",
      "R\u00e9mi Palard",
      "Maximilien Servajean"
    ],
    "published": "2025-11-16",
    "abstract": "This paper describes GeoPl@ntNet, an interactive web application designed to make Essential Biodiversity Variables accessible and understandable to everyone through dynamic maps and fact sheets. Its core purpose is to allow users to explore high-resolution AI-generated maps of species distributions, habitat types, and biodiversity indicators across Europe. These maps, developed through a cascading pipeline involving convolutional neural networks and large language models, provide an intuitive yet information-rich interface to better understand biodiversity, with resolutions as precise as 50x50 meters. The website also enables exploration of specific regions, allowing users to select areas of interest on the map (e.g., urban green spaces, protected areas, or riverbanks) to view local species and their coverage. Additionally, GeoPl@ntNet generates comprehensive reports for selected regions, including insights into the number of protected species, invasive species, and endemic species.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "CNN",
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Axiomatic Foundations of Chemical Systems as Ternary Gamma-Semirings",
    "url": "http://arxiv.org/abs/2511.12699v1",
    "authors": [
      "Chandrasekhar Gokavarapu",
      "Venkata Rao Kaviti",
      "Srinivasa Rao Thirunagari",
      "D. Madhusudhana Rao"
    ],
    "published": "2025-11-16",
    "abstract": "Chemical transformations depend not only on the identities of the reacting species but also on the catalytic, environmental, and intermediate conditions under which they occur. Classical binary reaction formalisms usually treat such condi- tions as external annotations, which obscures the genuinely multi-state and multi- parameter character of real chemical processes. In this paper we introduce an axiomatic framework in which a chemical system is modelled by a ternary \u0393-semiring. The elements of the state set represent chemical states, while the parameter set encodes catalytic and environmental conditions. A \u0393- dependent ternary operation is used to describe mediated transformations, treating reactants, intermediates, and mediators as intrinsic arguments of the transformation law. We develop the algebraic axioms governing these mediated interactions and in- terpret their associativity, distributivity, and \u0393-linearity in terms of multi-step path- ways, parallel processes, and controlled environmental dependence. We introduce chemical ideals and \u0393-ideals as algebraic structures modelling reaction-closed sub- systems and pathway-stable domains, and study their prime and semiprime forms. Homomorphisms between TGS-chemical systems are shown to preserve reaction pathways and describe consistent changes of chemical environment. Abstract examples from catalysis, thermodynamic phase control, and field- induced quantum transitions illustrate how familiar chemical phenomena fit within this framework. The resulting theory provides a unified algebraic foundation for multi-parameter chemical behaviour and establishes the structural basis for subse- quent developments involving kinetics, geometric methods, and computational or AI-assisted models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2511.21606v1",
    "authors": [
      "M. Naseer Subhani"
    ],
    "published": "2025-11-26",
    "abstract": "Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?",
    "url": "http://arxiv.org/abs/2511.21523v1",
    "authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "St\u00e9phane May",
      "S\u00e9bastien Lef\u00e8vre"
    ],
    "published": "2025-11-26",
    "abstract": "Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning",
    "url": "http://arxiv.org/abs/2511.21420v1",
    "authors": [
      "Futian Wang",
      "Mengqi Wang",
      "Xiao Wang",
      "Haowen Wang",
      "Jin Tang"
    ],
    "published": "2025-11-26",
    "abstract": "Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search",
    "url": "http://arxiv.org/abs/2511.20460v1",
    "authors": [
      "Yunqi Zhou",
      "Chengjie Jiang",
      "Chun Yuan",
      "Jing Li"
    ],
    "published": "2025-11-25",
    "abstract": "With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8\\% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2511.20302v2",
    "authors": [
      "Shilei Cao",
      "Ziyang Gong",
      "Hehai Lin",
      "Yang Liu",
      "Jiashun Cheng",
      "Xiaoxing Hu",
      "Haoyuan Liang",
      "Guowen Li",
      "Chengwei Qin",
      "Hong Cheng",
      "Xue Yang",
      "Juepeng Zheng",
      "Haohuan Fu"
    ],
    "published": "2025-11-25",
    "abstract": "In Remote Sensing (RS), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key approach to activate the generalizable representation ability of foundation models for downstream tasks. However, existing specialized PEFT methods often fail when applied to large-scale Earth observation tasks, as they are unable to fully handle the multifaceted and unpredictable domain gaps (\\eg, spatial, semantic, and frequency shifts) inherent in RS data. To overcome this, we propose CrossEarth-Gate, which introduces two primary contributions. First, we establish a comprehensive RS module toolbox to address multifaceted domain gaps, comprising spatial, semantic, and frequency modules. Second, we develop a Fisher-guided adaptive selection mechanism that operates on this toolbox. This selection is guided by Fisher Information to quantify each module's importance by measuring its contribution to the task-specific gradient flow. It dynamically activates only the most critical modules at the appropriate layers, guiding the gradient flow to maximize adaptation effectiveness and efficiency. Comprehensive experiments validate the efficacy and generalizability of our method, where CrossEarth-Gate achieves state-of-the-art performance across 16 cross-domain benchmarks for RS semantic segmentation. The code of the work will be released.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting",
    "url": "http://arxiv.org/abs/2511.20004v1",
    "authors": [
      "Peining Zhang",
      "Hongchen Qin",
      "Haochen Zhang",
      "Ziqi Guo",
      "Guiling Wang",
      "Jinbo Bi"
    ],
    "published": "2025-11-25",
    "abstract": "This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors",
    "url": "http://arxiv.org/abs/2511.18264v2",
    "authors": [
      "Ruijie Fan",
      "Junyan Ye",
      "Huan Chen",
      "Zilong Huang",
      "Xiaolei Wang",
      "Weijia Li"
    ],
    "published": "2025-11-23",
    "abstract": "Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ]
  },
  {
    "title": "SX-GeoTree: Self-eXplaining Geospatial Regression Tree Incorporating the Spatial Similarity of Feature Attributions",
    "url": "http://arxiv.org/abs/2511.19845v1",
    "authors": [
      "Chaogui Kang",
      "Lijian Luo",
      "Qingfeng Guan",
      "Yu Liu"
    ],
    "published": "2025-11-25",
    "abstract": "Decision trees remain central for tabular prediction but struggle with (i) capturing spatial dependence and (ii) producing locally stable (robust) explanations. We present SX-GeoTree, a self-explaining geospatial regression tree that integrates three coupled objectives during recursive splitting: impurity reduction (MSE), spatial residual control (global Moran's I), and explanation robustness via modularity maximization on a consensus similarity network formed from (a) geographically weighted regression (GWR) coefficient distances (stimulus-response similarity) and (b) SHAP attribution distances (explanatory similarity). We recast local Lipschitz continuity of feature attributions as a network community preservation problem, enabling scalable enforcement of spatially coherent explanations without per-sample neighborhood searches. Experiments on two exemplar tasks (county-level GDP in Fujian, n=83; point-wise housing prices in Seattle, n=21,613) show SX-GeoTree maintains competitive predictive accuracy (within 0.01 $R^{2}$ of decision trees) while improving residual spatial evenness and doubling attribution consensus (modularity: Fujian 0.19 vs 0.09; Seattle 0.10 vs 0.05). Ablation confirms Moran's I and modularity terms are complementary; removing either degrades both spatial residual structure and explanation stability. The framework demonstrates how spatial similarity - extended beyond geometric proximity through GWR-derived local relationships - can be embedded in interpretable models, advancing trustworthy geospatial machine learning and offering a transferable template for domain-aware explainability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "Hierarchical Semi-Supervised Active Learning for Remote Sensing",
    "url": "http://arxiv.org/abs/2511.18058v1",
    "authors": [
      "Wei Huang",
      "Zhitong Xiong",
      "Chenying Liu",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-22",
    "abstract": "The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Extratropical Atmospheric Circulation Response to ENSO in Deep Learning Pacific Pacemaker Experiments",
    "url": "http://arxiv.org/abs/2511.20899v1",
    "authors": [
      "Zhanxiang Hua",
      "Christina Karamperidou",
      "Zilu Meng"
    ],
    "published": "2025-11-25",
    "abstract": "Coupled atmosphere-ocean deep learning (DL) climate emulators are a new frontier but are known to exhibit weak ENSO variability, raising questions about their ability to simulate teleconnections. Here, we present the first Pacific pacemaker (PACE) experiments using a coupled DL emulator (DLESyM) to bypass this weak variability and isolate the atmospheric response to observed ENSO forcing. We find that while the emulator realistically captures internal atmospheric variability, it produces a significantly amplified forced teleconnection response to ENSO. This amplified response leads to biases in simulating extremes, notably an overestimation of atmospheric blocking frequency and duration with the underestimation of peak intensity. Our findings underscore that coupled DL climate models require in-depth and physically-grounded validation, analogous to traditional numerical models, to build confidence in their use for physical climate analysis.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting",
    "url": "http://arxiv.org/abs/2511.18732v1",
    "authors": [
      "Haoming Jia",
      "Yi Han",
      "Xiang Wang",
      "Huizan Wang",
      "Wei Wu",
      "Jianming Zheng",
      "Peikun Xiao"
    ],
    "published": "2025-11-24",
    "abstract": "Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition",
    "url": "http://arxiv.org/abs/2511.20612v1",
    "authors": [
      "Yujin Kim",
      "Sarah Dean"
    ],
    "published": "2025-11-25",
    "abstract": "Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Icy worlds: Moons and Dwarf Planets",
    "url": "http://arxiv.org/abs/2511.18776v1",
    "authors": [
      "Jun Kimura"
    ],
    "published": "2025-11-24",
    "abstract": "In the outer solar system beyond Jupiter, water ice is a dominant component of planetary bodies, and most solid objects in this region are classified as icy bodies. Icy bodies display a remarkable diversity of geological, geophysical, and atmospheric processes, which differ fundamentally from those of the rocky terrestrial planets. Evidence from past and ongoing spacecraft missions has revealed subsurface oceans, cryovolcanic activity, and tenuous but persistent atmospheres, showing that icy bodies are active and evolving worlds. At the same time, major questions remain unresolved, including the chemical properties of icy materials, the geological histories of their surfaces, and the coupling between internal evolution and orbital dynamics. Current knowledge of the surfaces, interiors, and atmospheres of the principal icy bodies is built on spacecraft measurements, telescopic observations, laboratory experiments, and theoretical modeling. Recent contributions from Juno, JWST, and stellar occultation studies have added valuable constraints on atmospheric composition, interior structure, and surface activity. Looking ahead, missions such as JUICE, Europa Clipper, Dragonfly, and the Uranus Orbiter and Probe are expected to deliver substantial progress in the study of icy bodies. Their findings, combined with continued Earth- and space-based observations and laboratory studies, will be critical for assessing the potential habitability of these environments and for placing them within a broader framework of planetary system formation and evolution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration",
    "url": "http://arxiv.org/abs/2512.03345v1",
    "authors": [
      "Seunghoi Kim",
      "Henry F. J. Tregidgo",
      "Chen Jin",
      "Matteo Figini",
      "Daniel C. Alexander"
    ],
    "published": "2025-12-03",
    "abstract": "Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening",
    "url": "http://arxiv.org/abs/2512.02643v1",
    "authors": [
      "Yongchuan Cui",
      "Peng Liu",
      "Yi Zeng"
    ],
    "published": "2025-12-02",
    "abstract": "Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.",
    "categories": [
      "remote_sensing",
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts",
    "url": "http://arxiv.org/abs/2512.02517v1",
    "authors": [
      "Jiaqi Liu",
      "Ronghao Fu",
      "Lang Sun",
      "Haoran Liu",
      "Xiao Yang",
      "Weipeng Zhang",
      "Xu Na",
      "Zhuoran Duan",
      "Bo Yang"
    ],
    "published": "2025-12-02",
    "abstract": "The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2512.01665v1",
    "authors": [
      "Zhicheng Zhao",
      "Yin Huang",
      "Lingma Sun",
      "Chenglong Li",
      "Jin Tang"
    ],
    "published": "2025-12-01",
    "abstract": "Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images",
    "url": "http://arxiv.org/abs/2512.00718v1",
    "authors": [
      "Deliang Wang",
      "Peng Liu"
    ],
    "published": "2025-11-30",
    "abstract": "Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "First On-Orbit Demonstration of a Geospatial Foundation Model",
    "url": "http://arxiv.org/abs/2512.01181v1",
    "authors": [
      "Andrew Du",
      "Roberto Del Prete",
      "Alejandro Mousist",
      "Nick Manser",
      "Fabrice Marre",
      "Andrew Barton",
      "Carl Seubert",
      "Gabriele Meoni",
      "Tat-Jun Chin"
    ],
    "published": "2025-12-01",
    "abstract": "Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "A 3D virtual geographic environment for flood representation towards risk communication",
    "url": "http://arxiv.org/abs/2512.03839v1",
    "authors": [
      "Weilian Li",
      "Jun Zhu",
      "Saied Pirasteh",
      "Qing Zhu",
      "Yukun Guo",
      "Lan Luo",
      "Youness Dehbi"
    ],
    "published": "2025-12-03",
    "abstract": "Risk communication seeks to develop a shared understanding of disaster among stakeholders, thereby amplifying public awareness and empowering them to respond more effectively to emergencies. However, existing studies have overemphasized specialized numerical modelling, making the professional output challenging to understand and use by non-research stakeholders. In this context, this article proposes a 3D virtual geographic environment for flood representation towards risk communication, which integrates flood modelling, parallel computation, and 3D representation in a pipeline. Finally, a section of the Rhine River in Bonn, Germany, is selected for experiment analysis. The experimental results show that the proposed approach is capable of flood modelling and 3D representation within a few hours, the parallel speedup ratio reached 6.45. The intuitive flood scene with 3D city models is beneficial for promoting flood risk communication and is particularly helpful for participants without direct experience of floods to understand its spatiotemporal process. It also can be embedded in the Geospatial Infrastructure Management Ecosystem (GeoIME) cloud application for intelligent flood systems.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
=======
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    "title": "CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding",
    "url": "http://arxiv.org/abs/2512.03558v1",
    "authors": [
      "Huy Quang Ung",
      "Guillaume Habault",
      "Yasutaka Nishimura",
      "Hao Niu",
      "Roberto Legaspi",
      "Tomoki Oya",
      "Ryoichi Kojima",
      "Masato Taya",
      "Chihiro Ono",
      "Atsunori Minamikawa",
      "Yan Liu"
    ],
    "published": "2025-12-03",
    "abstract": "The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git",
    "categories": [
<<<<<<< HEAD
      "foundation_model"
=======
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [],
    "applications": [
      "Recognition"
    ]
  },
  {
<<<<<<< HEAD
    "title": "PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2512.03257v1",
    "authors": [
      "Mark Moussa",
      "Andre Williams",
      "Seth Roffe",
      "Douglas Morton"
    ],
    "published": "2025-12-02",
    "abstract": "Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.\n  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.\n  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ]
  },
  {
    "title": "FluxLab: Creating 3D Printable Shape-Changing Devices with Integrated Deformation Sensing",
    "url": "http://arxiv.org/abs/2512.02911v1",
    "authors": [
      "Hsuanling Lee",
      "Jiakun Yu",
      "Shurui Zheng",
      "Te-Yan Wu",
      "Liang He"
    ],
    "published": "2025-12-02",
    "abstract": "We present FluxLab, a system comprising interactive tools for creating custom 3D-printable shape-changing devices with integrated deformation sensing. To achieve this, we propose a 3D printable nesting structure, consisting of a central SMA channel for sensing and actuation, lattice-based padding in the middle for structural support and controllable elasticity, and parallel helix-based surface wires that preserve the overall form and provide anchoring struts for guided deformation. We developed a design editor to embed these structures into custom 3D models for printing with elastic silicone resin on a consumer-grade SLA 3D printer and minimal post-printing assembly. A deformation authoring tool was also developed for users to build a machine learning-based classifier that distinguishes desired deformation behaviors using inductive sensing. Finally, we demonstrate the potential of our system through example applications, including a self-deformable steamer bowl clip, a remotely controllable gripper, and an interactive desk lamp.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": []
  },
  {
    "title": "Observation-driven correction of numerical weather prediction for marine winds",
    "url": "http://arxiv.org/abs/2512.03606v1",
    "authors": [
      "Matteo Peduto",
      "Qidong Yang",
      "Jonathan Giezendanner",
      "Devis Tuia",
      "Sherrie Wang"
    ],
    "published": "2025-12-03",
    "abstract": "Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
=======
    "title": "GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding",
    "url": "http://arxiv.org/abs/2512.02715v1",
    "authors": [
      "Peirong Zhang",
      "Yidan Zhang",
      "Luxiao Xu",
      "Jinliang Lin",
      "Zonghao Guo",
      "Fengxiang Wang",
      "Xue Yang",
      "Kaiwen Wei",
      "Lei Wang"
    ],
    "published": "2025-12-02",
    "abstract": "Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes",
    "url": "http://arxiv.org/abs/2511.23332v1",
    "authors": [
      "Shuo Ni",
      "Di Wang",
      "He Chen",
      "Haonan Guo",
      "Ning Zhang",
      "Jing Zhang"
    ],
    "published": "2025-11-28",
    "abstract": "Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "GeoZero: Incentivizing Reasoning from Scratch on Geospatial Scenes",
    "url": "http://arxiv.org/abs/2511.22645v1",
    "authors": [
      "Di Wang",
      "Shunyu Liu",
      "Wentao Jiang",
      "Fengxiang Wang",
      "Yi Liu",
      "Xiaolei Qin",
      "Zhiming Luo",
      "Chaoyang Zhou",
      "Haonan Guo",
      "Jing Zhang",
      "Bo Du",
      "Dacheng Tao",
      "Liangpei Zhang"
    ],
    "published": "2025-11-27",
    "abstract": "Multimodal large language models (MLLMs) have undergone rapid development in advancing geospatial scene understanding. Recent studies have sought to enhance the reasoning capabilities of remote sensing MLLMs, typically through cold-start training with elaborately curated chain-of-thought (CoT) data. However, this approach not only incurs substantial annotation costs but also introduces human biases that may limit the diversity of model reasoning. To address these challenges, we propose GeoZero, a framework that enables MLLMs to perform geospatial reasoning without any predefined CoT supervision. Specifically, we construct two datasets, GeoZero-Instruct and GeoZero-Hard. GeoZero-Instruct allows the model to acquire preliminary geospatial knowledge through supervised fine-tuning, while GeoZero-Hard stimulates deep reasoning during the subsequent reinforcement learning stage. Furthermore, we introduce Answer-Anchored Group Relative Policy Optimization (A$^2$GRPO), where the reasoning process is regularized by the model's own answers, encouraging diverse yet accurate thinking. Extensive experiments on multiple remote sensing vision-language benchmarks demonstrate that GeoZero not only surpasses existing state-of-the-art methods but also fosters universal emergent reasoning capabilities across diverse geospatial tasks. Code,data,and models will be publicly available at https://github.com/MiliLab/GeoZero.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Towards Unified Vision Language Models for Forest Ecological Analysis in Earth Observation",
    "url": "http://arxiv.org/abs/2511.16853v1",
    "authors": [
      "Xizhe Xue",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-20",
    "abstract": "Recent progress in vision language models (VLMs) has enabled remarkable perception and reasoning capabilities, yet their potential for scientific regression in Earth Observation (EO) remains largely unexplored. Existing EO datasets mainly emphasize semantic understanding tasks such as captioning or classification, lacking benchmarks that align multimodal perception with measurable biophysical variables. To fill this gap, we present REO-Instruct, the first unified benchmark designed for both descriptive and regression tasks in EO. REO-Instruct establishes a cognitively interpretable logic chain in forest ecological scenario (human activity,land-cover classification, ecological patch counting, above-ground biomass (AGB) regression), bridging qualitative understanding and quantitative prediction. The dataset integrates co-registered Sentinel-2 and ALOS-2 imagery with structured textual annotations generated and validated through a hybrid human AI pipeline. Comprehensive evaluation protocols and baseline results across generic VLMs reveal that current models struggle with numeric reasoning, highlighting an essential challenge for scientific VLMs. REO-Instruct offers a standardized foundation for developing and assessing next-generation geospatial models capable of both description and scientific inference. The project page are publicly available at \\href{https://github.com/zhu-xlab/REO-Instruct}{REO-Instruct}.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness",
    "url": "http://arxiv.org/abs/2511.13712v1",
    "authors": [
      "Kiana Vu",
      "\u0130smet Sel\u00e7uk \u00d6zer",
      "Phung Lai",
      "Zheng Wu",
      "Thilanka Munasinghe",
      "Jennifer Wei"
    ],
    "published": "2025-11-17",
    "abstract": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Geospatial Chain of Thought Reasoning for Enhanced Visual Question Answering on Satellite Imagery",
    "url": "http://arxiv.org/abs/2511.11198v1",
    "authors": [
      "Shambhavi Shanker",
      "Manikandan Padmanaban",
      "Jagabondhu Hazra"
    ],
    "published": "2025-11-14",
    "abstract": "Geospatial chain of thought (CoT) reasoning is essential for advancing Visual Question Answering (VQA) on satellite imagery, particularly in climate related applications such as disaster monitoring, infrastructure risk assessment, urban resilience planning, and policy support. Existing VQA models enable scalable interpretation of remote sensing data but often lack the structured reasoning required for complex geospatial queries. We propose a VQA framework that integrates CoT reasoning with Direct Preference Optimization (DPO) to improve interpretability, robustness, and accuracy. By generating intermediate rationales, the model better handles tasks involving detection, classification, spatial relations, and comparative analysis, which are critical for reliable decision support in high stakes climate domains. Experiments show that CoT supervision improves accuracy by 34.9\\% over direct baselines, while DPO yields additional gains in accuracy and reasoning quality. The resulting system advances VQA for multispectral Earth observation by enabling richer geospatial reasoning and more effective climate use cases.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning",
    "url": "http://arxiv.org/abs/2511.06316v1",
    "authors": [
      "MD Thamed Bin Zaman Chowdhury",
      "Moazzem Hossain"
    ],
    "published": "2025-11-09",
    "abstract": "Reliable geospatial information on road accidents is vital for safety analysis and infrastructure planning, yet most low- and middle-income countries continue to face a critical shortage of accurate, location-specific crash data. Existing text-based geocoding tools perform poorly in multilingual and unstructured news environments, where incomplete place descriptions and mixed Bangla-English scripts obscure spatial context. To address these limitations, this study introduces ALIGN (Accident Location Inference through Geo-Spatial Neural Reasoning)- a vision-language framework that emulates human spatial reasoning to infer accident coordinates directly from textual and map-based cues. ALIGN integrates large language and vision-language models within a multi-stage pipeline that performs optical character recognition, linguistic reasoning, and map-level verification through grid-based spatial scanning. The framework systematically evaluates each predicted location against contextual and visual evidence, ensuring interpretable, fine-grained geolocation outcomes without requiring model retraining. Applied to Bangla-language news data, ALIGN demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district and sub-district-level crash sites. Beyond its technical contribution, the framework establishes a high accuracy foundation for automated crash mapping in data-scarce regions, supporting evidence-driven road-safety policymaking and the broader integration of multimodal artificial intelligence in transportation analytics. The code for this paper is open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation",
    "url": "http://arxiv.org/abs/2510.18751v2",
    "authors": [
      "Patterson Hsieh",
      "Jerry Yeh",
      "Mao-Chi He",
      "Wen-Han Hsieh",
      "Elvis Hsieh"
    ],
    "published": "2025-10-21",
    "abstract": "Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation",
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "Forecast"
    ]
  },
  {
<<<<<<< HEAD
    "title": "NORi: An ML-Augmented Ocean Boundary Layer Parameterization",
    "url": "http://arxiv.org/abs/2512.04452v1",
    "authors": [
      "Xin Kai Lee",
      "Ali Ramadhan",
      "Andre Souza",
      "Gregory LeClaire Wagner",
      "Simone Silvestri",
      "John Marshall",
      "Raffaele Ferrari"
    ],
    "published": "2025-12-04",
    "abstract": "NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an \"a posteriori\" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Isolating Balanced Ocean Dynamics in SWOT Data",
    "url": "http://arxiv.org/abs/2512.03258v1",
    "authors": [
      "Jack William Skinner",
      "J\u00f6rn Callies",
      "Albion Lawrence",
      "Xihan Zhang"
    ],
    "published": "2025-12-02",
    "abstract": "The Surface Water and Ocean Topography (SWOT) mission provides two-dimensional sea surface height (SSH) maps at unprecedented resolution, but its signal is a combination of balanced meso- and submesoscale turbulence, unbalanced internal waves, and small-scale noise. Interpreting the meso- and submesoscale flow features captured by SWOT requires a careful isolation of the balanced signal. We present a statistical method to do so in regions where internal-wave signals are negligible, such as western boundary current regions and the Southern Ocean. Our method assumes Gaussian statistics for both the balanced flow and the noise, which we infer by fitting parametric models to the observed SSH wavenumber spectrum. Using these inferred parameters, we perform a Bayesian inversion to reconstruct swath-aligned SSH maps that fill the nadir gap. We evaluate the method using synthetic data from a high-resolution simulation with realistic SWOT-like noise added. Comparisons with the underlying model data show that our reconstruction successfully removes small-scale noise while preserving meso- and submesoscale eddies, fronts, and filaments down to a feature scale of 10km. The comparison also demonstrates that the posterior uncertainty is a reliable estimate of the error.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "ELG$\\times$LRG distribution through dark matter halo dynamics",
    "url": "http://arxiv.org/abs/2512.04362v1",
    "authors": [
      "Ginevra Favole",
      "Francisco-Shu Kitaura",
      "Boryana Hadzhiyska",
      "Daniel J. Eisenstein",
      "Lehman H. Garrison",
      "Sownak Bose"
    ],
    "published": "2025-12-04",
    "abstract": "We investigate the clustering and halo occupation distribution (HOD) of DESI Y1 emission-line (ELGs) and luminous red (LRGs) galaxies at $0.8<z<1.1$, including their cross-correlation (ELG$\\times$LRG), using the AbacusSummit suite and a new Halo Occupation Model (HOMe) for galaxy multi-tracers. This integrates intra-halo dynamics, halo exclusion, and quenching, bridging insights from hydrodynamical, HOD, abundance-matching, and semi-analytic studies. Leveraging full phase-space information from the Uchuu N-body simulation, and sampling satellites from dark-matter particle positions via physically motivated prescriptions, HOMe reproduces the anisotropic clustering down to $s=200\\,h^{-1}$kpc with unprecedented accuracy. Model parameters are inferred solely from two-point statistics using a two-level Bayesian framework, yielding high-fidelity ELG, LRG and cross-reference catalogs. We find that satellite ELGs behave as incoherent flows within their parent halos, dominating the clustering below $4\\,h^{-1}$Mpc. The HOD from the best-fit HOMe has the following properties: (i) 90.50% (85.91%) of ELGs (LRGs) are central galaxies without satellites, residing in halos of $M_{\\rm vir}\\sim6.6\\times10^{11}\\,(1.2\\times10^{13})\\,h^{-1}{\\rm M}_\\odot$; (ii) the ELG$\\times$LRG cross-correlation is governed by central-central pairs and shaped by halo exclusion on $2-5\\,h^{-1}$Mpc scales; (iii) 9.50% (14.09%) of ELGs (LRGs) are satellites, of which 1.09% (3.52%) inhabit halos with a central galaxy of the same species in a maximally conformal configuration, 7.02% (0.005%) orbit complementary hosts in a minimally conformal state, and 0.58% (10.57%) are orphans. HOMe high sensitivity precisely captures the dynamics of satellites in different host environments, opening a promising avenue for understanding systematics, the dynamical nature of dark matter, potentially distinguishing gravity models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "EcoCast: A Spatio-Temporal Model for Continual Biodiversity and Climate Risk Forecasting",
    "url": "http://arxiv.org/abs/2512.02260v1",
    "authors": [
      "Hammed A. Akande",
      "Abdulrauf A. Gidado"
    ],
    "published": "2025-12-01",
    "abstract": "Increasing climate change and habitat loss are driving unprecedented shifts in species distributions. Conservation professionals urgently need timely, high-resolution predictions of biodiversity risks, especially in ecologically diverse regions like Africa. We propose EcoCast, a spatio-temporal model designed for continual biodiversity and climate risk forecasting. Utilizing multisource satellite imagery, climate data, and citizen science occurrence records, EcoCast predicts near-term (monthly to seasonal) shifts in species distributions through sequence-based transformers that model spatio-temporal environmental dependencies. The architecture is designed with support for continual learning to enable future operational deployment with new data streams. Our pilot study in Africa shows promising improvements in forecasting distributions of selected bird species compared to a Random Forest baseline, highlighting EcoCast's potential to inform targeted conservation policies. By demonstrating an end-to-end pipeline from multi-modal data ingestion to operational forecasting, EcoCast bridges the gap between cutting-edge machine learning and biodiversity management, ultimately guiding data-driven strategies for climate resilience and ecosystem conservation throughout Africa.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Global dynamics in a reaction-diffusion competition model with edge behavior",
    "url": "http://arxiv.org/abs/2512.00339v1",
    "authors": [
      "Kuiyue Liu",
      "Shanshan Chen"
    ],
    "published": "2025-11-29",
    "abstract": "In this paper, we investigate a two-species competition model in a landscape consisting of a finite number of adjacent patches. For the two-patch scenario, by treating edge behavior at the interface as a strategy, it has been shown that there exists an ideal free distribution (IFD) strategy, which is a globally evolutionarily stable strategy. Specifically, when the resident species follows the IFD strategy and the mutant species does not, the mutant species is unable to invade the resident population. Building on this foundation, our work focuses on exploring the dynamics of the system when neither species can adopt the IFD strategy. We demonstrate that if the strategies of both species either exceed or fall below the IFD strategy, the mutant species can outcompete and eliminate the resident species, provided that its strategy is closer to the IFD strategy and its diffusion rates are equal to or slower than those of the resident species. Furthermore, if the strategies of the two species lie on opposite sides of the IFD strategy, the two species can coexist. This result is further extended to the case of an arbitrary but finite number of patches.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis",
    "url": "http://arxiv.org/abs/2512.09576v1",
    "authors": [
      "David Seu",
      "Nicolas Longepe",
      "Gabriel Cioltea",
      "Erik Maidik",
      "Calin Andrei"
    ],
    "published": "2025-12-10",
    "abstract": "Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "PINN"
=======
    "title": "Coordinates from Context: Using LLMs to Ground Complex Location References",
    "url": "http://arxiv.org/abs/2510.08741v1",
    "authors": [
      "Tessa Masis",
      "Brendan O'Connor"
    ],
    "published": "2025-10-09",
    "abstract": "Geocoding is the task of linking a location reference to an actual geographic location and is essential for many downstream analyses of unstructured text. In this paper, we explore the challenging setting of geocoding compositional location references. Building on recent work demonstrating LLMs' abilities to reason over geospatial data, we evaluate LLMs' geospatial knowledge versus reasoning skills relevant to our task. Based on these insights, we propose an LLM-based strategy for geocoding compositional location references. We show that our approach improves performance for the task and that a relatively small fine-tuned LLM can achieve comparable performance with much larger off-the-shelf models.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective",
    "url": "http://arxiv.org/abs/2510.01639v1",
    "authors": [
      "Thinh Hung Truong",
      "Jey Han Lau",
      "Jianzhong Qi"
    ],
    "published": "2025-10-02",
    "abstract": "We explore the geospatial reasoning capabilities of Large Language Models (LLMs), specifically, whether LLMs can read road network maps and perform navigation. We frame trajectory recovery as a proxy task, which requires models to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with over 4,000 real-world trajectories across diverse regions and transportation modes. Using road network as context, our prompting framework enables LLMs to generate valid paths without accessing any external navigation tools. Experiments show that LLMs outperform off-the-shelf baselines and specialized trajectory recovery models, with strong zero-shot generalization. Fine-grained analysis shows that LLMs have strong comprehension of the road network and coordinate systems, but also pose systematic biases with respect to regions and transportation modes. Finally, we demonstrate how LLMs can enhance navigation experiences by reasoning over maps in flexible ways to incorporate user preferences.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning",
    "url": "http://arxiv.org/abs/2510.00072v1",
    "authors": [
      "Chenhui Xu",
      "Fuxun Yu",
      "Michael J. Bianco",
      "Jacob Kovarskiy",
      "Raphael Tang",
      "Qi Zhang",
      "Zirui Xu",
      "Will LeVine",
      "Brandon Dubbs",
      "Heming Liao",
      "Cassandra Burgess",
      "Suvam Bag",
      "Jay Patravali",
      "Rupanjali Kukal",
      "Mikael Figueroa",
      "Rishi Madhok",
      "Nikolaos Karianakis",
      "Jinjun Xiong"
    ],
    "published": "2025-09-29",
    "abstract": "We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm\" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at https://huggingface.co/miniHui/Geo-R1.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Forecast",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models",
    "url": "http://arxiv.org/abs/2509.22221v1",
    "authors": [
      "Jiaqi Liu",
      "Lang Sun",
      "Ronghao Fu",
      "Bo Yang"
    ],
    "published": "2025-09-26",
    "abstract": "Vision-Language Models (VLMs) in remote sensing often fail at complex analytical tasks, a limitation stemming from their end-to-end training paradigm that bypasses crucial reasoning steps and leads to unverifiable outputs. To address this limitation, we introduce the Perceptually-Grounded Geospatial Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as a verifiable, multi-step process. We instill this analytical process through a two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale dataset of structured Geo-CoT rationales. This strategy first employs supervised fine-tuning (SFT) to instill the foundational cognitive architecture, then leverages Group Reward Policy Optimization (GRPO) to refine the model's reasoning policy towards factual correctness. The resulting model, RSThinker, outputs both a final answer and its justifying, verifiable analytical trace. This capability yields dominant performance, significantly outperforming state-of-the-art models across a comprehensive range of tasks. The public release of our Geo-CoT380k dataset and RSThinker model upon publication serves as a concrete pathway from opaque perception towards structured, verifiable reasoning for Earth Observation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": []
  },
  {
    "title": "Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2509.21976v2",
    "authors": [
      "Zilun Zhang",
      "Zian Guan",
      "Tiancheng Zhao",
      "Haozhan Shen",
      "Tianyu Li",
      "Yuxiang Cai",
      "Zhonggen Su",
      "Zhaojun Liu",
      "Jianwei Yin",
      "Xiang Li"
    ],
    "published": "2025-09-26",
    "abstract": "Referring expression understanding in remote sensing poses unique challenges, as it requires reasoning over complex object-context relationships. While supervised fine-tuning (SFT) on multimodal large language models achieves strong performance with massive labeled datasets, they struggle in data-scarce scenarios, leading to poor generalization. To address this limitation, we propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm for few-shot geospatial referring. Geo-R1 enforces the model to first generate explicit, interpretable reasoning chains that decompose referring expressions, and then leverage these rationales to localize target objects. This \"reason first, then act\" process enables the model to make more effective use of limited annotations, enhances generalization, and provides interpretability. We validate Geo-R1 on three carefully designed few-shot geospatial referring benchmarks, where our model consistently and substantially outperforms SFT baselines. It also demonstrates strong cross-dataset generalization, highlighting its robustness. Code and data will be released at: https://github.com/Geo-R1/geo-r1.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery",
    "url": "http://arxiv.org/abs/2509.20628v1",
    "authors": [
      "Yiming Xiao",
      "Archit Gupta",
      "Miguel Esparza",
      "Yu-Hsuan Ho",
      "Antonia Sebastian",
      "Hannah Weas",
      "Rose Houck",
      "Ali Mostafavi"
    ],
    "published": "2025-09-25",
    "abstract": "Building-level occupancy after disasters is vital for triage, inspections, utility re-energization, and equitable resource allocation. Overhead imagery provides rapid coverage but often misses facade and access cues that determine habitability, while street-view imagery captures those details but is sparse and difficult to align with parcels. We present FacadeTrack, a street-level, language-guided framework that links panoramic video to parcels, rectifies views to facades, and elicits interpretable attributes (for example, entry blockage, temporary coverings, localized debris) that drive two decision strategies: a transparent one-stage rule and a two-stage design that separates perception from conservative reasoning. Evaluated across two post-Hurricane Helene surveys, the two-stage approach achieves a precision of 0.927, a recall of 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a precision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond accuracy, intermediate attributes and spatial diagnostics reveal where and why residual errors occur, enabling targeted quality control. The pipeline provides auditable, scalable occupancy assessments suitable for integration into geospatial and emergency-management workflows.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2509.19087v1",
    "authors": [
      "Ganesh Mallya",
      "Yotam Gigi",
      "Dahun Kim",
      "Maxim Neumann",
      "Genady Beryozkin",
      "Tomer Shekel",
      "Anelia Angelova"
    ],
    "published": "2025-09-23",
    "abstract": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models' understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "RoadMind: Towards a Geospatial AI Expert for Disaster Response",
    "url": "http://arxiv.org/abs/2509.19354v1",
    "authors": [
      "Ahmed El Fekih Zguir",
      "Ferda Ofli",
      "Muhammad Imran"
    ],
    "published": "2025-09-18",
    "abstract": "Large Language Models (LLMs) have shown impressive performance across a range of natural language tasks, but remain limited in their ability to reason about geospatial data, particularly road networks, distances, and directions. This gap poses challenges in disaster scenarios, where spatial understanding is critical for tasks such as evacuation planning and resource allocation. In this work, we present RoadMind, a self-supervised framework that enhances the geospatial reasoning capabilities of LLMs using structured data from OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data for a given city and converts it into multiple supervision formats tailored to key spatial tasks. We pretrain and fine-tune LLMs on these representations using QLoRA adapters and 4-bit quantized models. We evaluate our approach on three disaster-prone cities with varying global representation, Los Angeles, Christchurch, and Manila, across tasks such as road segment identification, nearest road retrieval, and distance/direction estimation. Our results show that models trained via RoadMind significantly outperform strong baselines, including state-of-the-art LLMs equipped with advanced prompt engineering. This demonstrates the potential of structured geospatial data to enhance language models with robust spatial reasoning, enabling more effective offline AI systems for disaster response.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "applications": []
  },
  {
<<<<<<< HEAD
    "title": "GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model",
    "url": "http://arxiv.org/abs/2512.09251v1",
    "authors": [
      "Lalit Maurya",
      "Saurabh Kaushik",
      "Beth Tellman"
    ],
    "published": "2025-12-10",
    "abstract": "Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\\textbf{G}lacial \\textbf{LA}ke segmentation with \\textbf{C}ontextual \\textbf{I}nstance \\textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer",
=======
    "title": "GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation",
    "url": "http://arxiv.org/abs/2509.05881v1",
    "authors": [
      "Qianheng Zhang",
      "Song Gao",
      "Chen Wei",
      "Yibo Zhao",
      "Ying Nie",
      "Ziru Chen",
      "Shijie Chen",
      "Yu Su",
      "Huan Sun"
    ],
    "published": "2025-09-07",
    "abstract": "Recent advances in large language models (LLMs) have fueled growing interest in automating geospatial analysis and GIS workflows, yet their actual capabilities remain uncertain. In this work, we call for rigorous evaluation of LLMs on well-defined geoprocessing tasks before making claims about full GIS automation. To this end, we present GeoAnalystBench, a benchmark of 50 Python-based tasks derived from real-world geospatial problems and carefully validated by GIS experts. Each task is paired with a minimum deliverable product, and evaluation covers workflow validity, structural alignment, semantic similarity, and code quality (CodeBLEU). Using this benchmark, we assess both proprietary and open source models. Results reveal a clear gap: proprietary models such as ChatGPT-4o-mini achieve high validity 95% and stronger code alignment (CodeBLEU 0.39), while smaller open source models like DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5% validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as spatial relationship detection or optimal site selection, remain the most challenging across all models. These findings demonstrate both the promise and limitations of current LLMs in GIS automation and provide a reproducible framework to advance GeoAI research with human-in-the-loop support.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "GRASP: Geospatial pixel Reasoning viA Structured Policy learning",
    "url": "http://arxiv.org/abs/2508.17102v2",
    "authors": [
      "Chengjie Jiang",
      "Yunqi Zhou",
      "Jiafeng Yan",
      "Jing Li",
      "Jiayang Li",
      "Yue Zhou",
      "Hongjie He",
      "Jonathan Li"
    ],
    "published": "2025-08-23",
    "abstract": "Geospatial pixel reasoning aims to generate segmentation masks in remote sensing imagery directly from natural-language instructions. Most existing approaches follow a paradigm that fine-tunes multimodal large language models under supervision with dense pixel-level masks as ground truth. While effective within the training data distribution, this design suffers from two main drawbacks: (1) the high cost of large-scale dense mask annotation, and (2) the limited generalization capability of supervised fine-tuning in out-of-domain scenarios. To address these issues, we propose GRASP, a structured policy-learning framework that integrates a multimodal large language model with a pretrained segmentation model in a cascaded manner. To enhance generalization, we introduce PRIME, a training paradigm that replaces supervised fine-tuning with reinforcement learning to better align reasoning and grounding behaviors with task objectives. To reduce annotation costs, we design BoP-Rewards, which substitutes dense mask labels with bounding box and positive points. It further verifies outputs through two complementary signals: format, which constrains the reasoning and grounding structure to remain syntactically parsable, and accuracy, which evaluates the quality of predicted boxes and points. For evaluation, we train our method and all baselines on EarthReason and GeoPixInstruct, constructing an in-domain benchmark by merging their test sets. We further release GRASP-1k, a fully out-of-domain benchmark with reasoning-intensive queries, reasoning traces, and fine-grained masks. Experimental results demonstrate state-of-the-art (SOTA) in-domain performance and up to 54\\% improvement in out-of-domain scenarios, confirming that reinforcement learning with cost-aware rewards provides a robust and scalable paradigm for geospatial pixel reasoning. All code and datasets will be released publicly.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "LLM"
    ],
    "applications": [
      "Segmentation",
<<<<<<< HEAD
=======
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges",
    "url": "http://arxiv.org/abs/2508.06832v1",
    "authors": [
      "Haifeng Li",
      "Wang Guo",
      "Haiyang Wu",
      "Mengwei Wu",
      "Jipeng Zhang",
      "Qing Zhu",
      "Yu Liu",
      "Xin Huang",
      "Chao Tao"
    ],
    "published": "2025-08-09",
    "abstract": "The mainstream paradigm of remote sensing image interpretation has long been dominated by vision-centered models, which rely on visual features for semantic understanding. However, these models face inherent limitations in handling multi-modal reasoning, semantic abstraction, and interactive decision-making. While recent advances have introduced Large Language Models (LLMs) into remote sensing workflows, existing studies primarily focus on downstream applications, lacking a unified theoretical framework that explains the cognitive role of language. This review advocates a paradigm shift from vision-centered to language-centered remote sensing interpretation. Drawing inspiration from the Global Workspace Theory (GWT) of human cognition, We propose a language-centered framework for remote sensing interpretation that treats LLMs as the cognitive central hub integrating perceptual, task, knowledge and action spaces to enable unified understanding, reasoning, and decision-making. We first explore the potential of LLMs as the central cognitive component in remote sensing interpretation, and then summarize core technical challenges, including unified multimodal representation, knowledge association, and reasoning and decision-making. Furthermore, we construct a global workspace-driven interpretation mechanism and review how language-centered solutions address each challenge. Finally, we outline future research directions from four perspectives: adaptive alignment of multimodal data, task understanding under dynamic knowledge constraints, trustworthy reasoning, and autonomous interaction. This work aims to provide a conceptual foundation for the next generation of remote sensing interpretation systems and establish a roadmap toward cognition-driven intelligent geospatial analysis.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence",
    "url": "http://arxiv.org/abs/2508.04852v2",
    "authors": [
      "Chenhui Qiang",
      "Zhaoyang Wei",
      "Xumeng Han",
      "Zipeng Wang",
      "Siyao Li",
      "Xiangyuan Lan",
      "Jianbin Jiao",
      "Zhenjun Han"
    ],
    "published": "2025-08-06",
    "abstract": "With the rapid development of MLLMs, evaluating their visual capabilities has become increasingly crucial. Current benchmarks primarily fall into two main types: basic perception benchmarks, which focus on local details but lack deep reasoning (e.g., \"what is in the image?\"), and mainstream reasoning benchmarks, which concentrate on prominent image elements but may fail to assess subtle clues requiring intricate analysis. However, profound visual understanding and complex reasoning depend more on interpreting subtle, inconspicuous local details than on perceiving salient, macro-level objects. These details, though occupying minimal image area, often contain richer, more critical information for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel framework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues, often occupying on average just 0.25% of the image area; 2) integrate these clues with world knowledge for complex reasoning. Comprising 374 carefully designed questions across Geospatial, Temporal, Situational, Intent, System State, and Symbolic reasoning, each question in VER-Bench is accompanied by structured evidence: visual clues and question-related reasoning derived from them. VER-Bench reveals current models' limitations in extracting subtle visual evidence and constructing evidence-based arguments, highlighting the need to enhance models's capabilities in fine-grained visual evidence extraction, integration, and reasoning for genuine visual understanding and human-like analysis. Dataset and additional materials are available https://github.com/verbta/ACMMM-25-Materials.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models",
    "url": "http://arxiv.org/abs/2508.01608v1",
    "authors": [
      "Lingyao Li",
      "Runlong Yu",
      "Qikai Hu",
      "Bowei Li",
      "Min Deng",
      "Yang Zhou",
      "Xiaowei Jia"
    ],
    "published": "2025-08-03",
    "abstract": "Image geolocalization, the task of identifying the geographic location depicted in an image, is important for applications in crisis response, digital forensics, and location-based intelligence. While recent advances in large language models (LLMs) offer new opportunities for visual reasoning, their ability to perform image geolocalization remains underexplored. In this study, we introduce a benchmark called IMAGEO-Bench that systematically evaluates accuracy, distance error, geospatial bias, and reasoning process. Our benchmark includes three diverse datasets covering global street scenes, points of interest (POIs) in the United States, and a private collection of unseen images. Through experiments on 10 state-of-the-art LLMs, including both open- and closed-source models, we reveal clear performance disparities, with closed-source models generally showing stronger reasoning. Importantly, we uncover geospatial biases as LLMs tend to perform better in high-resource regions (e.g., North America, Western Europe, and California) while exhibiting degraded performance in underrepresented areas. Regression diagnostics demonstrate that successful geolocalization is primarily dependent on recognizing urban settings, outdoor environments, street-level imagery, and identifiable landmarks. Overall, IMAGEO-Bench provides a rigorous lens into the spatial reasoning capabilities of LLMs and offers implications for building geolocation-aware AI systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning",
    "url": "http://arxiv.org/abs/2507.19586v1",
    "authors": [
      "Shengyuan Wang",
      "Jie Feng",
      "Tianhui Liu",
      "Dan Pei",
      "Yong Li"
    ],
    "published": "2025-07-25",
    "abstract": "Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations of geospatial information) that compromise their reliability. While the phenomenon of general knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark. Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "Forecast"
    ]
  },
  {
<<<<<<< HEAD
    "title": "OSM+: Billion-Level Open Street Map Data Processing System for City-wide Experiments",
    "url": "http://arxiv.org/abs/2512.06743v1",
    "authors": [
      "Guanjie Zheng",
      "Ziyang Su",
      "Yiheng Wang",
      "Yuhang Luo",
      "Hongwei Zhang",
      "Xuanhe Zhou",
      "Linghe Kong",
      "Fan Wu",
      "Wen Ling"
    ],
    "published": "2025-12-07",
    "abstract": "Road network data can provide rich information about cities and thus become the base for various urban research. However, processing large volume world-wide road network data requires intensive computing resources and the processed results might be different to be unified for testing downstream tasks. Therefore, in this paper, we process the OpenStreetMap data via a distributed computing of 5,000 cores on cloud services and release a structured world-wide 1-billion-vertex road network graph dataset with high accessibility (opensource and downloadable to the whole world) and usability (open-box graph structure and easy spatial query interface). To demonstrate how this dataset can be utilized easily, we present three illustrative use cases, including traffic prediction, city boundary detection and traffic policy control, and conduct extensive experiments for these three tasks. (1) For the well-investigated traffic prediction tasks, we release a new benchmark with 31 cities (traffic data processed and combined with our released OSM+ road network dataset), to provide much larger spatial coverage and more comprehensive evaluation of compared algorithms than the previously frequently-used datasets. This new benchmark will push the algorithms on their scalability from hundreds of road network intersections to thousands of intersections. (2) While for the more advanced traffic policy control task which requires interaction with the road network, we release a new 6 city datasets with much larger scale than the previous datasets. This brings new challenge for thousand-scale multi-agent coordination. (3) Along with the OSM+ dataset, the release of data converters facilitates the integration of multimodal spatial-temporal data for geospatial foundation model training, thereby expediting the process of uncovering compelling scientific insights. PVLDB Reference Forma",
    "categories": [
      "foundation_model"
=======
    "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow",
    "url": "http://arxiv.org/abs/2507.19280v2",
    "authors": [
      "Liang Yao",
      "Fan Liu",
      "Hongbo Lu",
      "Chuanyi Zhang",
      "Rui Min",
      "Shengxiang Xu",
      "Shimin Di",
      "Pai Peng"
    ],
    "published": "2025-07-25",
    "abstract": "Remote sensing imagery presents vast, inherently unstructured spatial data, necessitating sophisticated reasoning to interpret complex user intents and contextual relationships beyond simple recognition tasks. In this paper, we aim to construct an Earth observation workflow to handle complex queries by reasoning about spatial context and user intent. As a reasoning workflow, it should autonomously explore and construct its own inference paths, rather than being confined to predefined ground-truth sequences. Ideally, its architecture ought to be unified yet generalized, possessing capabilities to perform diverse reasoning tasks through one model without requiring additional fine-tuning. Existing remote sensing approaches rely on supervised fine-tuning paradigms and task-specific heads, limiting both autonomous reasoning and unified generalization. To this end, we propose RemoteReasoner, a unified workflow for geospatial reasoning. The design of RemoteReasoner integrates a multi-modal large language model (MLLM) for interpreting user instructions and localizing targets, together with task transformation strategies that enable multi-granularity tasks, including object-, region-, and pixel-level. In contrast to existing methods, our framework is trained with reinforcement learning (RL) to endow the MLLM sufficient reasoning autonomy. At the inference stage, our transformation strategies enable diverse task output formats without requiring task-specific decoders or further fine-tuning. Experiments demonstrated that RemoteReasoner achieves state-of-the-art (SOTA) performance across multi-granularity reasoning tasks. Furthermore, it retains the MLLM's inherent generalization capability, demonstrating robust performance on unseen tasks and out-of-distribution categories.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Recognition",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "TrajSceneLLM: A Multimodal Perspective on Semantic GPS Trajectory Analysis",
    "url": "http://arxiv.org/abs/2506.16401v1",
    "authors": [
      "Chunhou Ji",
      "Qiumeng Li"
    ],
    "published": "2025-06-19",
    "abstract": "GPS trajectory data reveals valuable patterns of human mobility and urban dynamics, supporting a variety of spatial applications. However, traditional methods often struggle to extract deep semantic representations and incorporate contextual map information. We propose TrajSceneLLM, a multimodal perspective for enhancing semantic understanding of GPS trajectories. The framework integrates visualized map images (encoding spatial context) and textual descriptions generated through LLM reasoning (capturing temporal sequences and movement dynamics). Separate embeddings are generated for each modality and then concatenated to produce trajectory scene embeddings with rich semantic content which are further paired with a simple MLP classifier. We validate the proposed framework on Travel Mode Identification (TMI), a critical task for analyzing travel choices and understanding mobility behavior. Our experiments show that these embeddings achieve significant performance improvement, highlighting the advantage of our LLM-driven method in capturing deep spatio-temporal dependencies and reducing reliance on handcrafted features. This semantic enhancement promises significant potential for diverse downstream applications and future research in geospatial artificial intelligence. The source code and dataset are publicly available at: https://github.com/februarysea/TrajSceneLLM.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine",
    "url": "http://arxiv.org/abs/2506.10365v1",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Huayi Wu",
      "Haoyue Jiao",
      "Ziqi Liu",
      "Lutong Xie",
      "Chang Liu",
      "Jianyuan Liang",
      "Yaxian Qing",
      "Xiaopu Zhang",
      "Dehua Peng",
      "Zhipeng Gui",
      "Xuefeng Guan"
    ],
    "published": "2025-06-12",
    "abstract": "Geospatial code generation is becoming a key frontier in integrating artificial intelligence with geo-scientific analysis, yet standardised automated evaluation tools for this task remain absent. This study presents AutoGEEval++, an enhanced framework building on AutoGEEval, and the first automated assessment system for large language models (LLMs) generating geospatial code on Google Earth Engine (GEE). It supports diverse data modalities and varying task complexities. Built on the GEE Python API, AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test cases across 26 data types and three task categories: unit, combo, and theme tests. It includes a submission programme and a judge module to realise an end-to-end automated evaluation pipeline from code generation to execution-based validation. The framework adopts multi-dimensional metrics-accuracy, resource usage, run-time efficiency, and error types-balancing hallucination control and efficiency, and enabling boundary testing and error pattern analysis. Using AutoGEEval++, we evaluate 24 state-of-the-art LLMs (as of June 2025), including general-purpose, reasoning-enhanced, code-centric, and geoscience-specific models. Results reveal clear performance, stability, and error differences across task types, model designs, and deployment settings, confirming AutoGEEval++'s practical value and scalability in vertical-domain code generation. This work establishes the first standardised evaluation protocol and foundational benchmark for GEE-based LLM code generation, providing a unified basis for performance comparison and a methodological framework for systematic, domain-specific code evaluation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data",
    "url": "http://arxiv.org/abs/2505.17116v1",
    "authors": [
      "Akash Dhruv",
      "Yangxinyu Xie",
      "Jordan Branham",
      "Tanwi Mallick"
    ],
    "published": "2025-05-21",
    "abstract": "This paper presents a comparative study of large language models (LLMs) in interpreting grid-structured geospatial data. We evaluate the performance of a base model through structured prompting and contrast it with a fine-tuned variant trained on a dataset of user-assistant interactions. Our results highlight the strengths and limitations of zero-shot prompting and demonstrate the benefits of fine-tuning for structured geospatial and temporal reasoning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models",
    "url": "http://arxiv.org/abs/2505.12900v1",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Huayi Wu",
      "Jianyuan Liang",
      "Haoyue Jiao",
      "Yaxian Qing",
      "Xiaopu Zhang",
      "Xu Li",
      "Zhipeng Gui",
      "Xuefeng Guan",
      "Longgang Xiang"
    ],
    "published": "2025-05-19",
    "abstract": "Geospatial code generation is emerging as a key direction in the integration of artificial intelligence and geoscientific analysis. However, there remains a lack of standardized tools for automatic evaluation in this domain. To address this gap, we propose AutoGEEval, the first multimodal, unit-level automated evaluation framework for geospatial code generation tasks on the Google Earth Engine (GEE) platform powered by large language models (LLMs). Built upon the GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench) comprising 1325 test cases that span 26 GEE data types. The framework integrates both question generation and answer verification components to enable an end-to-end automated evaluation pipeline-from function invocation to execution validation. AutoGEEval supports multidimensional quantitative analysis of model outputs in terms of accuracy, resource consumption, execution efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including general-purpose, reasoning-augmented, code-centric, and geoscience-specialized models-revealing their performance characteristics and potential optimization pathways in GEE code generation. This work provides a unified protocol and foundational resource for the development and assessment of geospatial code generation models, advancing the frontier of automated natural language to domain-specific code translation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark",
    "url": "http://arxiv.org/abs/2505.12254v1",
    "authors": [
      "Yiwei Ou",
      "Xiaobin Ren",
      "Ronggui Sun",
      "Guansong Gao",
      "Ziyi Jiang",
      "Kaiqi Zhao",
      "Manfredo Manfredini"
    ],
    "published": "2025-05-18",
    "abstract": "Existing visual place recognition (VPR) datasets predominantly rely on vehicle-mounted imagery, lack multimodal diversity and underrepresent dense, mixed-use street-level spaces, especially in non-Western urban contexts. To address these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for street-level place recognition in complex, pedestrian-only environments. The dataset comprises 78,575 annotated images and 2,512 video clips captured across 207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in Chengdu, China. Each image is labeled with precise GPS coordinates, timestamp, and textual metadata, and covers varied lighting conditions, viewpoints, and timeframes. MMS-VPR follows a systematic and replicable data collection protocol with minimal device requirements, lowering the barrier for scalable dataset creation. Importantly, the dataset forms an inherent spatial graph with 125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place recognition. We further define two application-specific subsets -- Dataset_Edges and Dataset_Points -- to support fine-grained and graph-based evaluation tasks. Extensive benchmarks using conventional VPR models, graph neural networks, and multimodal baselines show substantial improvements when leveraging multimodal and structural cues. MMS-VPR facilitates future research at the intersection of computer vision, geospatial understanding, and multimodal reasoning. The dataset is publicly available at https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "CLIP",
      "GNN"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era",
    "url": "http://arxiv.org/abs/2505.09651v1",
    "authors": [
      "Xixuan Hao",
      "Yutian Jiang",
      "Xingchen Zou",
      "Jiabo Liu",
      "Yifang Yin",
      "Yuxuan Liang"
    ],
    "published": "2025-05-13",
    "abstract": "Location Intelligence (LI), the science of transforming location-centric geospatial data into actionable knowledge, has become a cornerstone of modern spatial decision-making. The rapid evolution of Geospatial Representation Learning is fundamentally reshaping LI development through two successive technological revolutions: the deep learning breakthrough and the emerging large language model (LLM) paradigm. While deep neural networks (DNNs) have demonstrated remarkable success in automated feature extraction from structured geospatial data (e.g., satellite imagery, GPS trajectories), the recent integration of LLMs introduces transformative capabilities for cross-modal geospatial reasoning and unstructured geo-textual data processing. This survey presents a comprehensive review of geospatial representation learning across both technological eras, organizing them into a structured taxonomy based on the complete pipeline comprising: (1) data perspective, (2) methodological perspective and (3) application perspective. We also highlight current advancements, discuss existing limitations, and propose potential future research directions in the LLM era. This work offers a thorough exploration of the field and providing a roadmap for further innovation in LI. The summary of the up-to-date paper list can be found in https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo continuous updates.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Deep Neural Network",
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model",
    "url": "http://arxiv.org/abs/2504.09644v1",
    "authors": [
      "Kaiyu Li",
      "Zepeng Xin",
      "Li Pang",
      "Chao Pang",
      "Yupeng Deng",
      "Jing Yao",
      "Guisong Xia",
      "Deyu Meng",
      "Zhi Wang",
      "Xiangyong Cao"
    ],
    "published": "2025-04-13",
    "abstract": "Remote sensing has become critical for understanding environmental dynamics, urban planning, and disaster management. However, traditional remote sensing workflows often rely on explicit segmentation or detection methods, which struggle to handle complex, implicit queries that require reasoning over spatial context, domain knowledge, and implicit user intent. Motivated by this, we introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit querying and reasoning and generates the mask of the target region. To advance this task, we construct and release the first large-scale benchmark dataset called EarthReason, which comprises 5,434 manually annotated image masks with over 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a simple yet effective language-guided segmentation baseline that integrates a hierarchical visual encoder, a large language model (LLM) for instruction parsing, and a tailored mask generator for spatial correlation. The design of SegEarth-R1 incorporates domain-specific adaptations, including aggressive visual token compression to handle ultra-high-resolution remote sensing images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. Extensive experiments demonstrate that SegEarth-R1 achieves state-of-the-art performance on both reasoning and referring segmentation tasks, significantly outperforming traditional and LLM-based segmentation methods. Our data and code will be released at https://github.com/earth-insights/SegEarth-R1.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ]
  },
  {
    "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence",
    "url": "http://arxiv.org/abs/2503.16326v1",
    "authors": [
      "Long Yuan",
      "Fengran Mo",
      "Kaiyu Huang",
      "Wenjie Wang",
      "Wangyuxuan Zhai",
      "Xiaoyu Zhu",
      "You Li",
      "Jinan Xu",
      "Jian-Yun Nie"
    ],
    "published": "2025-03-20",
    "abstract": "The rapid advancement of multimodal large language models (LLMs) has opened new frontiers in artificial intelligence, enabling the integration of diverse large-scale data types such as text, images, and spatial information. In this paper, we explore the potential of multimodal LLMs (MLLM) for geospatial artificial intelligence (GeoAI), a field that leverages spatial data to address challenges in domains including Geospatial Semantics, Health Geography, Urban Geography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo) tailored to geospatial applications, capable of processing and analyzing heterogeneous data sources, including satellite imagery, geospatial metadata, and textual descriptions. By combining the strengths of natural language understanding and spatial reasoning, our model enhances the ability of instruction following and the accuracy of GeoAI systems. Results demonstrate that our model outperforms task-specific models and existing LLMs on diverse geospatial tasks, effectively addressing the multimodality nature while achieving competitive results on the zero-shot geospatial tasks. Our code will be released after publication.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning",
    "url": "http://arxiv.org/abs/2503.13517v2",
    "authors": [
      "Hao Cui",
      "Zahra Shamsi",
      "Gowoon Cheon",
      "Xuejian Ma",
      "Shutong Li",
      "Maria Tikhanovskaya",
      "Peter Norgaard",
      "Nayantara Mudur",
      "Martyna Plomecka",
      "Paul Raccuglia",
      "Yasaman Bahri",
      "Victor V. Albert",
      "Pranesh Srinivasan",
      "Haining Pan",
      "Philippe Faist",
      "Brian Rohr",
      "Ekin Dogus Cubuk",
      "Muratahan Aykol",
      "Amil Merchant",
      "Michael J. Statt",
      "Dan Morris",
      "Drew Purves",
      "Elise Kleeman",
      "Ruth Alcantara",
      "Matthew Abraham",
      "Muqthar Mohammad",
      "Ean Phing VanLee",
      "Chenfei Jiang",
      "Elizabeth Dorfman",
      "Eun-Ah Kim",
      "Michael P Brenner",
      "Viren Jain",
      "Sameera Ponda",
      "Subhashini Venugopalan"
    ],
    "published": "2025-03-14",
    "abstract": "Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "MapQA: Open-domain Geospatial Question Answering on Map Data",
    "url": "http://arxiv.org/abs/2503.07871v1",
    "authors": [
      "Zekun Li",
      "Malcolm Grossman",
      "Eric",
      "Qasemi",
      "Mihir Kulkarni",
      "Muhao Chen",
      "Yao-Yi Chiang"
    ],
    "published": "2025-03-10",
    "abstract": "Geospatial question answering (QA) is a fundamental task in navigation and point of interest (POI) searches. While existing geospatial QA datasets exist, they are limited in both scale and diversity, often relying solely on textual descriptions of geo-entities without considering their geometries. A major challenge in scaling geospatial QA datasets for reasoning lies in the complexity of geospatial relationships, which require integrating spatial structures, topological dependencies, and multi-hop reasoning capabilities that most text-based QA datasets lack. To address these limitations, we introduce MapQA, a novel dataset that not only provides question-answer pairs but also includes the geometries of geo-entities referenced in the questions. MapQA is constructed using SQL query templates to extract question-answer pairs from OpenStreetMap (OSM) for two study regions: Southern California and Illinois. It consists of 3,154 QA pairs spanning nine question types that require geospatial reasoning, such as neighborhood inference and geo-entity type identification. Compared to existing datasets, MapQA expands both the number and diversity of geospatial question types. We explore two approaches to tackle this challenge: (1) a retrieval-based language model that ranks candidate geo-entities by embedding similarity, and (2) a large language model (LLM) that generates SQL queries from natural language questions and geo-entity attributes, which are then executed against an OSM database. Our findings indicate that retrieval-based methods effectively capture concepts like closeness and direction but struggle with questions that require explicit computations (e.g., distance calculations). LLMs (e.g., GPT and Gemini) excel at generating SQL queries for one-hop reasoning but face challenges with multi-hop reasoning, highlighting a key bottleneck in advancing geospatial QA systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Geospatial Reasoning Questions",
    "url": "http://arxiv.org/abs/2502.18470v5",
    "authors": [
      "Dazhou Yu",
      "Riyang Bao",
      "Ruiyu Ning",
      "Jinghong Peng",
      "Gengchen Mai",
      "Liang Zhao"
    ],
    "published": "2025-02-04",
    "abstract": "Answering real-world geospatial questions--such as finding restaurants along a travel route or amenities near a landmark--requires reasoning over both geographic relationships and semantic user intent. However, existing large language models (LLMs) lack spatial computing capabilities and access to up-to-date, ubiquitous real-world geospatial data, while traditional geospatial systems fall short in interpreting natural language. To bridge this gap, we introduce Spatial-RAG, a Retrieval-Augmented Generation (RAG) framework designed for geospatial question answering. Spatial-RAG integrates structured spatial databases with LLMs via a hybrid spatial retriever that combines sparse spatial filtering and dense semantic matching. It formulates the answering process as a multi-objective optimization over spatial and semantic relevance, identifying Pareto-optimal candidates and dynamically selecting the best response based on user intent. Experiments across multiple tourism and map-based QA datasets show that Spatial-RAG significantly improves accuracy, precision, and ranking performance over strong baselines.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Neurosymbolic AI for Travel Demand Prediction: Integrating Decision Tree Rules into Neural Networks",
    "url": "http://arxiv.org/abs/2502.01680v1",
    "authors": [
      "Kamal Acharya",
      "Mehul Lad",
      "Liang Sun",
      "Houbing Song"
    ],
    "published": "2025-02-02",
    "abstract": "Travel demand prediction is crucial for optimizing transportation planning, resource allocation, and infrastructure development, ensuring efficient mobility and economic sustainability. This study introduces a Neurosymbolic Artificial Intelligence (Neurosymbolic AI) framework that integrates decision tree (DT)-based symbolic rules with neural networks (NNs) to predict travel demand, leveraging the interpretability of symbolic reasoning and the predictive power of neural learning. The framework utilizes data from diverse sources, including geospatial, economic, and mobility datasets, to build a comprehensive feature set. DTs are employed to extract interpretable if-then rules that capture key patterns, which are then incorporated as additional features into a NN to enhance its predictive capabilities. Experimental results show that the combined dataset, enriched with symbolic rules, consistently outperforms standalone datasets across multiple evaluation metrics, including Mean Absolute Error (MAE), \\(R^2\\), and Common Part of Commuters (CPC). Rules selected at finer variance thresholds (e.g., 0.0001) demonstrate superior effectiveness in capturing nuanced relationships, reducing prediction errors, and aligning with observed commuter patterns. By merging symbolic and neural learning paradigms, this Neurosymbolic approach achieves both interpretability and accuracy.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets",
    "url": "http://arxiv.org/abs/2412.21015v2",
    "authors": [
      "Mahir Labib Dihan",
      "Mohammed Eunus Ali",
      "Md Rizwan Parvez"
    ],
    "published": "2024-12-30",
    "abstract": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, an extensible open-source framework that streamlines the creation of reproducible, traceable map-based QA datasets. MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/bVv7-NYRsTw.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "StaR Maps: Unveiling Uncertainty in Geospatial Relations",
    "url": "http://arxiv.org/abs/2412.18356v1",
    "authors": [
      "Simon Kohaut",
      "Benedict Flade",
      "Julian Eggert",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "published": "2024-12-24",
    "abstract": "The growing complexity of intelligent transportation systems and their applications in public spaces has increased the demand for expressive and versatile knowledge representation. While various mapping efforts have achieved widespread coverage, including detailed annotation of features with semantic labels, it is essential to understand their inherent uncertainties, which are commonly underrepresented by the respective geographic information systems. Hence, it is critical to develop a representation that combines a statistical, probabilistic perspective with the relational nature of geospatial data. Further, such a representation should facilitate an honest view of the data's accuracy and provide an environment for high-level reasoning to obtain novel insights from task-dependent queries. Our work addresses this gap in two ways. First, we present Statistical Relational Maps (StaR Maps) as a representation of uncertain, semantic map data. Second, we demonstrate efficient computation of StaR Maps to scale the approach to wide urban spaces. Through experiments on real-world, crowd-sourced data, we underpin the application and utility of StaR Maps in terms of representing uncertain knowledge and reasoning for complex geospatial information.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Decoding Urban Industrial Complexity: Enhancing Knowledge-Driven Insights via IndustryScopeGPT",
    "url": "http://arxiv.org/abs/2411.15758v1",
    "authors": [
      "Siqi Wang",
      "Chao Liang",
      "Yunfan Gao",
      "Yang Liu",
      "Jing Li",
      "Haofen Wang"
    ],
    "published": "2024-11-24",
    "abstract": "Industrial parks are critical to urban economic growth. Yet, their development often encounters challenges stemming from imbalances between industrial requirements and urban services, underscoring the need for strategic planning and operations. This paper introduces IndustryScopeKG, a pioneering large-scale multi-modal, multi-level industrial park knowledge graph, which integrates diverse urban data including street views, corporate, socio-economic, and geospatial information, capturing the complex relationships and semantics within industrial parks. Alongside this, we present the IndustryScopeGPT framework, which leverages Large Language Models (LLMs) with Monte Carlo Tree Search to enhance tool-augmented reasoning and decision-making in Industrial Park Planning and Operation (IPPO). Our work significantly improves site recommendation and functional planning, demonstrating the potential of combining LLMs with structured datasets to advance industrial park management. This approach sets a new benchmark for intelligent IPPO research and lays a robust foundation for advancing urban industrial development. The dataset and related code are available at https://github.com/Tongji-KGLLM/IndustryScope.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model",
    "url": "http://arxiv.org/abs/2411.14476v1",
    "authors": [
      "Zongrong Li",
      "Junhao Xu",
      "Siqin Wang",
      "Yifan Wu",
      "Haiyang Li"
    ],
    "published": "2024-11-19",
    "abstract": "Geospatial predictions are crucial for diverse fields such as disaster management, urban planning, and public health. Traditional machine learning methods often face limitations when handling unstructured or multi-modal data like street view imagery. To address these challenges, we propose StreetViewLLM, a novel framework that integrates a large language model with the chain-of-thought reasoning and multimodal data sources. By combining street view imagery with geographic coordinates and textual data, StreetViewLLM improves the precision and granularity of geospatial predictions. Using retrieval-augmented generation techniques, our approach enhances geographic information extraction, enabling a detailed analysis of urban environments. The model has been applied to seven global cities, including Hong Kong, Tokyo, Singapore, Los Angeles, New York, London, and Paris, demonstrating superior performance in predicting urban indicators, including population density, accessibility to healthcare, normalized difference vegetation index, building height, and impervious surface. The results show that StreetViewLLM consistently outperforms baseline models, offering improved predictive accuracy and deeper insights into the built environment. This research opens new opportunities for integrating the large language model into urban analytics, decision-making in urban planning, infrastructure management, and environmental monitoring.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Geometric Feature Enhanced Knowledge Graph Embedding and Spatial Reasoning",
    "url": "http://arxiv.org/abs/2410.18345v1",
    "authors": [
      "Lei Hu",
      "Wenwen Li",
      "Yunqiang Zhu"
    ],
    "published": "2024-10-24",
    "abstract": "Geospatial Knowledge Graphs (GeoKGs) model geoentities (e.g., places and natural features) and spatial relationships in an interconnected manner, providing strong knowledge support for geographic applications, including data retrieval, question-answering, and spatial reasoning. However, existing methods for mining and reasoning from GeoKGs, such as popular knowledge graph embedding (KGE) techniques, lack geographic awareness. This study aims to enhance general-purpose KGE by developing new strategies and integrating geometric features of spatial relations, including topology, direction, and distance, to infuse the embedding process with geographic intuition. The new model is tested on downstream link prediction tasks, and the results show that the inclusion of geometric features, particularly topology and direction, improves prediction accuracy for both geoentities and spatial relations. Our research offers new perspectives for integrating spatial concepts and principles into the GeoKG mining process, providing customized GeoAI solutions for geospatial challenges.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Can Large Language Models Generate Geospatial Code?",
    "url": "http://arxiv.org/abs/2410.09738v2",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Jianyuan Liang",
      "Anqi Zhao",
      "Zhipeng Gui",
      "Rui Li",
      "Huayi Wu"
    ],
    "published": "2024-10-13",
    "abstract": "With the growing demand for spatiotemporal data processing and geospatial modeling, automating geospatial code generation has become essential for productivity. Large language models (LLMs) show promise in code generation but face challenges like domain-specific knowledge gaps and \"coding hallucinations.\" This paper introduces GeoCode-Eval (GCE), a framework for assessing LLMs' ability to generate geospatial code across three dimensions: \"Cognition and Memory,\" \"Comprehension and Interpretation,\" and \"Innovation and Creation,\" distributed across eight capability levels. We developed a benchmark dataset, GeoCode-Bench, consisting of 5,000 multiple-choice, 1,500 fill-in-the-blank, 1,500 true/false questions, and 1,000 subjective tasks covering code summarization, generation, completion, and correction. Using GeoCode-Bench, we evaluated three commercial closed-source LLMs, four open-source general-purpose LLMs, and 14 specialized code generation models. We also conducted experiments on few-shot and zero-shot learning, Chain of Thought reasoning, and multi-round majority voting to measure their impact on geospatial code generation. Additionally, we fine-tuned the Code LLaMA-7B model using Google Earth Engine-related JavaScript, creating GEECode-GPT, and evaluated it on subjective tasks. Results show that constructing pre-training and instruction datasets significantly improves code generation, offering insights for optimizing LLMs in specific domains.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Evaluation of Code LLMs on Geospatial Code Generation",
    "url": "http://arxiv.org/abs/2410.04617v2",
    "authors": [
      "Piotr Gramacki",
      "Bruno Martins",
      "Piotr Szyma\u0144ski"
    ],
    "published": "2024-10-06",
    "abstract": "Software development support tools have been studied for a long time, with recent approaches using Large Language Models (LLMs) for code generation. These models can generate Python code for data science and machine learning applications. LLMs are helpful for software engineers because they increase productivity in daily work. An LLM can also serve as a \"mentor\" for inexperienced software developers, and be a viable learning support. High-quality code generation with LLMs can also be beneficial in geospatial data science. However, this domain poses different challenges, and code generation LLMs are typically not evaluated on geospatial tasks. Here, we show how we constructed an evaluation benchmark for code generation models, based on a selection of geospatial tasks. We categorised geospatial tasks based on their complexity and required tools. Then, we created a dataset with tasks that test model capabilities in spatial reasoning, spatial data processing, and geospatial tools usage. The dataset consists of specific coding problems that were manually created for high quality. For every problem, we proposed a set of test scenarios that make it possible to automatically check the generated code for correctness. In addition, we tested a selection of existing code generation LLMs for code generation in the geospatial domain. We share our dataset and reproducible evaluation code on a public GitHub repository, arguing that this can serve as an evaluation benchmark for new LLMs in the future. Our dataset will hopefully contribute to the development new models capable of solving geospatial coding tasks with high accuracy. These models will enable the creation of coding assistants tailored for geospatial applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Urban context and delivery performance: Modelling service time for cargo bikes and vans across diverse urban environments",
    "url": "http://arxiv.org/abs/2409.06730v1",
    "authors": [
      "Maxwell Schrader",
      "Navish Kumar",
      "Esben S\u00f8rig",
      "Soonmyeong Yoon",
      "Akash Srivastava",
      "Kai Xu",
      "Maria Astefanoaei",
      "Nicolas Collignon"
    ],
    "published": "2024-08-27",
    "abstract": "Light goods vehicles (LGV) used extensively in the last mile of delivery are one of the leading polluters in cities. Cargo-bike logistics and Light Electric Vehicles (LEVs) have been put forward as a high impact candidate for replacing LGVs. Studies have estimated over half of urban van deliveries being replaceable by cargo-bikes, due to their faster speeds, shorter parking times and more efficient routes across cities. However, the logistics sector suffers from a lack of publicly available data, particularly pertaining to cargo-bike deliveries, thus limiting the understanding of their potential benefits. Specifically, service time (which includes cruising for parking, and walking to destination) is a major, but often overlooked component of delivery time modelling. The aim of this study is to establish a framework for measuring the performance of delivery vehicles, with an initial focus on modelling service times of vans and cargo-bikes across diverse urban environments. We introduce two datasets that allow for in-depth analysis and modelling of service times of cargo bikes and use existing datasets to reason about differences in delivery performance across vehicle types. We introduce a modelling framework to predict the service times of deliveries based on urban context. We employ Uber's H3 index to divide cities into hexagonal cells and aggregate OpenStreetMap tags for each cell, providing a detailed assessment of urban context. Leveraging this spatial grid, we use GeoVex to represent micro-regions as points in a continuous vector space, which then serve as input for predicting vehicle service times. We show that geospatial embeddings can effectively capture urban contexts and facilitate generalizations to new contexts and cities. Our methodology addresses the challenge of limited comparative data available for different vehicle types within the same urban settings.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "GeoReasoner: Reasoning On Geospatially Grounded Context For Natural Language Understanding",
    "url": "http://arxiv.org/abs/2408.11366v1",
    "authors": [
      "Yibo Yan",
      "Joey Lee"
    ],
    "published": "2024-08-21",
    "abstract": "In human reading and communication, individuals tend to engage in geospatial reasoning, which involves recognizing geographic entities and making informed inferences about their interrelationships. To mimic such cognitive process, current methods either utilize conventional natural language understanding toolkits, or directly apply models pretrained on geo-related natural language corpora. However, these methods face two significant challenges: i) they do not generalize well to unseen geospatial scenarios, and ii) they overlook the importance of integrating geospatial context from geographical databases with linguistic information from the Internet. To handle these challenges, we propose GeoReasoner, a language model capable of reasoning on geospatially grounded natural language. Specifically, it first leverages Large Language Models (LLMs) to generate a comprehensive location description based on linguistic and geospatial information. It also encodes direction and distance information into spatial embedding via treating them as pseudo-sentences. Consequently, the model is trained on both anchor-level and neighbor-level inputs to learn geo-entity representation. Extensive experimental results demonstrate GeoReasoner's superiority in three tasks: toponym recognition, toponym linking, and geo-entity typing, compared to the state-of-the-art baselines.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Recognition"
    ]
  },
  {
    "title": "Into the Unknown: Generating Geospatial Descriptions for New Environments",
    "url": "http://arxiv.org/abs/2406.19967v1",
    "authors": [
      "Tzuf Paz-Argaman",
      "John Palowitch",
      "Sayali Kulkarni",
      "Reut Tsarfaty",
      "Jason Baldridge"
    ],
    "published": "2024-06-28",
    "abstract": "Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships (independent of the observer's viewpoint) using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data. Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (`shop north of school') generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models",
    "url": "http://arxiv.org/abs/2406.13948v2",
    "authors": [
      "Jie Feng",
      "Tianhui Liu",
      "Yuwei Du",
      "Siqi Guo",
      "Yuming Lin",
      "Yong Li"
    ],
    "published": "2024-06-20",
    "abstract": "Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \\textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \\textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \\textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \\textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \\textit{CityInstruction} by \\textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \\textit{CityEval}.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "STAR: A First-Ever Dataset and A Large-Scale Benchmark for Scene Graph Generation in Large-Size Satellite Imagery",
    "url": "http://arxiv.org/abs/2406.09410v3",
    "authors": [
      "Yansheng Li",
      "Linlin Wang",
      "Tingzhu Wang",
      "Xue Yang",
      "Junwei Luo",
      "Qi Wang",
      "Youming Deng",
      "Wenbin Wang",
      "Xian Sun",
      "Haifeng Li",
      "Bo Dang",
      "Yongjun Zhang",
      "Yi Yu",
      "Junchi Yan"
    ],
    "published": "2024-06-13",
    "abstract": "Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting understanding of geospatial scenarios from perception to cognition. In SAI, objects exhibit great variations in scales and aspect ratios, and there exist rich relationships between objects (even between spatially disjoint objects), which makes it attractive to holistically conduct SGG in large-size very-high-resolution (VHR) SAI. However, there lack such SGG datasets. Due to the complexity of large-size SAI, mining triplets <subject, relationship, object> heavily relies on long-range contextual reasoning. Consequently, SGG models designed for small-size natural imagery are not directly applicable to large-size SAI. This paper constructs a large-scale dataset for SGG in large-size VHR SAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, named STAR (Scene graph generaTion in lArge-size satellite imageRy), encompassing over 210K objects and over 400K triplets. To realize SGG in large-size SAI, we propose a context-aware cascade cognition (CAC) framework to understand SAI regarding object detection (OBD), pair pruning and relationship prediction for SGG. We also release a SAI-oriented SGG toolkit with about 30 OBD and 10 SGG methods which need further adaptation by our devised modules on our challenging STAR dataset. The dataset and toolkit are available at: https://linlin-dev.github.io/project/STAR.",
    "categories": [
      "geo_reasoning"
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast"
    ]
  },
  {
<<<<<<< HEAD
    "title": "Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion",
    "url": "http://arxiv.org/abs/2512.06504v1",
    "authors": [
      "Andrii Lysyi",
      "Anatoliy Sachenko",
      "Pavlo Radiuk",
      "Mykola Lysyi",
      "Oleksandr Melnychenko",
      "Diana Zahorodnia"
    ],
    "published": "2025-12-06",
    "abstract": "The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening",
    "url": "http://arxiv.org/abs/2512.08331v1",
    "authors": [
      "Xianghong Xiao",
      "Zeyu Xia",
      "Zhou Fei",
      "Jinliang Xiao",
      "Haorui Chen",
      "Liangjian Deng"
    ],
    "published": "2025-12-09",
    "abstract": "Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Near-real time fires detection using satellite imagery in Sudan conflict",
    "url": "http://arxiv.org/abs/2512.07925v1",
    "authors": [
      "Kuldip Singh Atwal",
      "Dieter Pfoser",
      "Daniel Rothbart"
    ],
    "published": "2025-12-08",
    "abstract": "The challenges of ongoing war in Sudan highlight the need for rapid monitoring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitoring. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our results indicate that using 8-band imagery or time series of such imagery only result in marginal gains.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A Model-Guided Neural Network Method for the Inverse Scattering Problem",
    "url": "http://arxiv.org/abs/2512.10123v1",
    "authors": [
      "Olivia Tsang",
      "Owen Melia",
      "Vasileios Charisopoulos",
      "Jeremy Hoskins",
      "Yuehaw Khoo",
      "Rebecca Willett"
    ],
    "published": "2025-12-10",
    "abstract": "Inverse medium scattering is an ill-posed, nonlinear wave-based imaging problem arising in medical imaging, remote sensing, and non-destructive testing. Machine learning (ML) methods offer increased inference speed and flexibility in capturing prior knowledge of imaging targets relative to classical optimization-based approaches; however, they perform poorly in regimes where the scattering behavior is highly nonlinear. A key limitation is that ML methods struggle to incorporate the physics governing the scattering process, which are typically inferred implicitly from the training data or loosely enforced via architectural design. In this paper, we present a method that endows a machine learning framework with explicit knowledge of problem physics, in the form of a differentiable solver representing the forward model. The proposed method progressively refines reconstructions of the scattering potential using measurements at increasing wave frequencies, following a classical strategy to stabilize recovery. Empirically, we find that our method provides high-quality reconstructions at a fraction of the computational or sampling costs of competing approaches.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A roadmap of geospatial soil quality analysis systems",
    "url": "http://arxiv.org/abs/2512.09817v1",
    "authors": [
      "Habiba BEN ABDERRAHMANE",
      "Slimane Oulad-Naoui",
      "Benameur ZIANI"
    ],
    "published": "2025-12-10",
    "abstract": "Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection",
    "url": "http://arxiv.org/abs/2512.07078v1",
    "authors": [
      "Bo Gao",
      "Jingcheng Tong",
      "Xingsheng Chen",
      "Han Yu",
      "Zichen Li"
    ],
    "published": "2025-12-08",
    "abstract": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.\n  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.\n  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
=======
    "title": "Quantifying Geospatial in the Common Crawl Corpus",
    "url": "http://arxiv.org/abs/2406.04952v2",
    "authors": [
      "Ilya Ilyankou",
      "Meihui Wang",
      "Stefano Cavazzi",
      "James Haworth"
    ],
    "published": "2024-06-07",
    "abstract": "Large language models (LLMs) exhibit emerging geospatial capabilities, stemming from their pre-training on vast unlabelled text datasets that are often derived from the Common Crawl (CC) corpus. However, the geospatial content within CC remains largely unexplored, impacting our understanding of LLMs' spatial reasoning. This paper investigates the prevalence of geospatial data in recent Common Crawl releases using Gemini 1.5, a powerful language model. By analyzing a sample of documents and manually revising the results, we estimate that 18.7% of web documents in CC contain geospatial information such as coordinates and addresses. We find little difference in prevalence between Enlgish- and non-English-language documents. Our findings provide quantitative insights into the nature and extent of geospatial data in CC, and lay the groundwork for future studies of geospatial biases of LLMs.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Geospatial Knowledge Graphs",
    "url": "http://arxiv.org/abs/2405.07664v1",
    "authors": [
      "Rui Zhu"
    ],
    "published": "2024-05-13",
    "abstract": "Geospatial knowledge graphs have emerged as a novel paradigm for representing and reasoning over geospatial information. In this framework, entities such as places, people, events, and observations are depicted as nodes, while their relationships are represented as edges. This graph-based data format lays the foundation for creating a \"FAIR\" (Findable, Accessible, Interoperable, and Reusable) environment, facilitating the management and analysis of geographic information. This entry first introduces key concepts in knowledge graphs along with their associated standardization and tools. It then delves into the application of knowledge graphs in geography and environmental sciences, emphasizing their role in bridging symbolic and subsymbolic GeoAI to address cross-disciplinary geospatial challenges. At the end, new research directions related to geospatial knowledge graphs are outlined.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "GlobalBuildingMap -- Unveiling the Mystery of Global Buildings",
    "url": "http://arxiv.org/abs/2404.13911v2",
    "authors": [
      "Xiao Xiang Zhu",
      "Qingyu Li",
      "Yilei Shi",
      "Yuanyuan Wang",
      "Adam Stewart",
      "Jonathan Prexl"
    ],
    "published": "2024-04-22",
    "abstract": "Understanding how buildings are distributed globally is crucial to revealing the human footprint on our home planet. This built environment affects local climate, land surface albedo, resource distribution, and many other key factors that influence well-being and human health. Despite this, quantitative and comprehensive data on the distribution and properties of buildings worldwide is lacking. To this end, by using a big data analytics approach and nearly 800,000 satellite images, we generated the highest resolution and highest accuracy building map ever created: the GlobalBuildingMap (GBM). A joint analysis of building maps and solar potentials indicates that rooftop solar energy can supply the global energy consumption need at a reasonable cost. Specifically, if solar panels were placed on the roofs of all buildings, they could supply 1.1-3.3 times -- depending on the efficiency of the solar device -- the global energy consumption in 2020, which is the year with the highest consumption on record. We also identified a clear geospatial correlation between building areas and key socioeconomic variables, which indicates our global building map can serve as an important input to modeling global socioeconomic needs and drivers.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery",
    "url": "http://arxiv.org/abs/2401.06762v1",
    "authors": [
      "Caleb Robinson",
      "Isaac Corley",
      "Anthony Ortiz",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres",
      "Peyman Najafirad"
    ],
    "published": "2024-01-12",
    "abstract": "Fully understanding a complex high-resolution satellite or aerial imagery scene often requires spatial reasoning over a broad relevant context. The human object recognition system is able to understand object in a scene over a long-range relevant context. For example, if a human observes an aerial scene that shows sections of road broken up by tree canopy, then they will be unlikely to conclude that the road has actually been broken up into disjoint pieces by trees and instead think that the canopy of nearby trees is occluding the road. However, there is limited research being conducted to understand long-range context understanding of modern machine learning models. In this work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial Context (RSC), for evaluating the spatial long-range context understanding of geospatial machine learning models and show how commonly used semantic segmentation models can fail at this task. For example, we show that a U-Net trained to segment roads from background in aerial imagery achieves an 84% recall on unoccluded roads, but just 63.5% recall on roads covered by tree canopy despite being trained to model both the same way. We further analyze how the performance of models changes as the relevant context for a decision (unoccluded roads in our case) varies in distance. We release the code to reproduce our experiments and dataset of imagery and masks to encourage future research in this direction -- https://github.com/isaaccorley/ChesapeakeRSC.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Recognition"
    ]
  },
  {
    "title": "A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems",
    "url": "http://arxiv.org/abs/2401.10279v1",
    "authors": [
      "Sean Tucker"
    ],
    "published": "2024-01-12",
    "abstract": "Geospatial Location Embedding (GLE) helps a Large Language Model (LLM) assimilate and analyze spatial data. GLE emergence in Geospatial Artificial Intelligence (GeoAI) is precipitated by the need for deeper geospatial awareness in our complex contemporary spaces and the success of LLMs in extracting deep meaning in Generative AI. We searched Google Scholar, Science Direct, and arXiv for papers on geospatial location embedding and LLM and reviewed articles focused on gaining deeper spatial \"knowing\" through LLMs. We screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE themes - Entity Location Embedding (ELE), Document Location Embedding (DLE), Sequence Location Embedding (SLE), and Token Location Embedding (TLE). Synthesis is tabular and narrative, including a dialogic conversation between \"Space\" and \"LLM.\" Though GLEs aid spatial understanding by superimposing spatial data, they emphasize the need to advance in the intricacies of spatial modalities and generalized reasoning. GLEs signal the need for a Spatial Foundation/Language Model (SLM) that embeds spatial knowing within the model architecture. The SLM framework advances Spatial Artificial Intelligence Systems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to physical space. The resulting spatially imbued Language Model is unique. It simultaneously represents actual space and an AI-capable space, paving the way for AI native geo storage, analysis, and multi-modality as the basis for Spatial Artificial Intelligence Systems (SPAIS).",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries",
    "url": "http://arxiv.org/abs/2509.25264v2",
    "authors": [
      "Shuyang Hou",
      "Haoyue Jiao",
      "Ziqi Liu",
      "Lutong Xie",
      "Guanyu Chen",
      "Shaowen Wu",
      "Xuefeng Guan",
      "Huayi Wu"
    ],
    "published": "2025-09-28",
    "abstract": "Large language models (LLMs) have shown strong performance in natural language to SQL (NL2SQL) tasks within general databases. However, extending to GeoSQL introduces additional complexity from spatial data types, function invocation, and coordinate systems, which greatly increases generation and execution difficulty. Existing benchmarks mainly target general SQL, and a systematic evaluation framework for GeoSQL is still lacking. To fill this gap, we present GeoSQL-Eval, the first end-to-end automated evaluation framework for PostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing LLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task categories-conceptual understanding, syntax-level SQL generation, and schema retrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic databases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model, covering four cognitive dimensions, five capability levels, and twenty task types to establish a comprehensive process from knowledge acquisition and syntax generation to semantic alignment, execution accuracy, and robustness. We evaluate 24 representative models across six categories and apply the entropy weight method with statistical analyses to uncover performance differences, common error patterns, and resource usage. Finally, we release a public GeoSQL-Eval leaderboard platform for continuous testing and global comparison. This work extends the NL2GeoSQL paradigm and provides a standardized, interpretable, and extensible framework for evaluating LLMs in spatial database contexts, offering valuable references for geospatial information science and related applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data",
    "url": "http://arxiv.org/abs/2509.17544v2",
    "authors": [
      "Juan Ca\u00f1ada",
      "Ra\u00fal Alonso",
      "Julio Molleda",
      "Fidel D\u00edez"
    ],
    "published": "2025-09-22",
    "abstract": "The increasing availability of open Earth Observation (EO) and agricultural datasets holds great potential for supporting sustainable land management. However, their high technical entry barrier limits accessibility for non-expert users. This study presents an open-source conversational assistant that integrates multimodal retrieval and large language models (LLMs) to enable natural language interaction with heterogeneous agricultural and geospatial data. The proposed architecture combines orthophotos, Sentinel-2 vegetation indices, and user-provided documents through retrieval-augmented generation (RAG), allowing the system to flexibly determine whether to rely on multimodal evidence, textual knowledge, or both in formulating an answer. To assess response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional quantitative evaluation framework. Preliminary results show that the system is capable of generating clear, relevant, and context-aware responses to agricultural queries, while remaining reproducible and scalable across geographic regions. The primary contributions of this work include an architecture for fusing multimodal EO and textual knowledge sources, a demonstration of lowering the barrier to access specialized agricultural information through natural language interaction, and an open and reproducible design.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route",
    "url": "http://arxiv.org/abs/2509.18173v1",
    "authors": [
      "Hongyi Luo",
      "Qing Cheng",
      "Daniel Matos",
      "Hari Krishna Gadi",
      "Yanfeng Zhang",
      "Lu Liu",
      "Yongliang Wang",
      "Niclas Zeller",
      "Daniel Cremers",
      "Liqiu Meng"
    ],
    "published": "2025-09-17",
    "abstract": "Humans can interpret geospatial information through natural language, while the geospatial cognition capabilities of Large Language Models (LLMs) remain underexplored. Prior research in this domain has been constrained by non-quantifiable metrics, limited evaluation datasets and unclear research hierarchies. Therefore, we propose a large-scale benchmark and conduct a comprehensive evaluation of the geospatial route cognition of LLMs. We create a large-scale evaluation dataset comprised of 36000 routes from 12 metropolises worldwide. Then, we introduce PathBuilder, a novel tool for converting natural language instructions into navigation routes, and vice versa, bridging the gap between geospatial information and natural language. Finally, we propose a new evaluation framework and metrics to rigorously assess 11 state-of-the-art (SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs exhibit limitation to reverse routes: most reverse routes neither return to the starting point nor are similar to the optimal route. Additionally, LLMs face challenges such as low robustness in route generation and high confidence for their incorrect answers. Code\\ \\&\\ Data available here: \\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "An AI system to help scientists write expert-level empirical software",
    "url": "http://arxiv.org/abs/2509.06503v1",
    "authors": [
      "Eser Ayg\u00fcn",
      "Anastasiya Belyaeva",
      "Gheorghe Comanici",
      "Marc Coram",
      "Hao Cui",
      "Jake Garrison",
      "Renee Johnston Anton Kast",
      "Cory Y. McLean",
      "Peter Norgaard",
      "Zahra Shamsi",
      "David Smalling",
      "James Thompson",
      "Subhashini Venugopalan",
      "Brian P. Williams",
      "Chujun He",
      "Sarah Martinson",
      "Martyna Plomecka",
      "Lai Wei",
      "Yuchen Zhou",
      "Qian-Ze Zhu",
      "Matthew Abraham",
      "Erica Brand",
      "Anna Bulanova",
      "Jeffrey A. Cardille",
      "Chris Co",
      "Scott Ellsworth",
      "Grace Joseph",
      "Malcolm Kane",
      "Ryan Krueger",
      "Johan Kartiwa",
      "Dan Liebling",
      "Jan-Matthis Lueckmann",
      "Paul Raccuglia",
      "Xuefei",
      "Wang",
      "Katherine Chou",
      "James Manyika",
      "Yossi Matias",
      "John C. Platt",
      "Lizzie Dorfman",
      "Shibl Mourad",
      "Michael P. Brenner"
    ],
    "published": "2025-09-08",
    "abstract": "The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system that creates expert-level scientific software whose goal is to maximize a quality metric. The system uses a Large Language Model (LLM) and Tree Search (TS) to systematically improve the quality metric and intelligently navigate the large space of possible solutions. The system achieves expert-level results when it explores and integrates complex research ideas from external sources. The effectiveness of tree search is demonstrated across a wide range of benchmarks. In bioinformatics, it discovered 40 novel methods for single-cell data analysis that outperformed the top human-developed methods on a public leaderboard. In epidemiology, it generated 14 models that outperformed the CDC ensemble and all other individual models for forecasting COVID-19 hospitalizations. Our method also produced state-of-the-art software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting and numerical solution of integrals. By devising and implementing novel solutions to diverse tasks, the system represents a significant step towards accelerating scientific progress.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Geospatial Question Answering on Historical Maps Using Spatio-Temporal Knowledge Graphs and Large Language Models",
    "url": "http://arxiv.org/abs/2508.21491v1",
    "authors": [
      "Ziyi Liu",
      "Sidi Wu",
      "Lorenz Hurni"
    ],
    "published": "2025-08-29",
    "abstract": "Recent advances have enabled the extraction of vectorized features from digital historical maps. To fully leverage this information, however, the extracted features must be organized in a structured and meaningful way that supports efficient access and use. One promising approach is question answering (QA), which allows users -- especially those unfamiliar with database query languages -- to retrieve knowledge in a natural and intuitive manner. In this project, we developed a GeoQA system by integrating a spatio-temporal knowledge graph (KG) constructed from historical map data with large language models (LLMs). Specifically, we have defined the ontology to guide the construction of the spatio-temporal KG and investigated workflows of two different types of GeoQA: factual and descriptive. Additional data sources, such as historical map images and internet search results, are incorporated into our framework to provide extra context for descriptive GeoQA. Evaluation results demonstrate that the system can generate answers with a high delivery rate and a high semantic accuracy. To make the framework accessible, we further developed a web application that supports interactive querying and visualization.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "jXBW: Fast Substructure Search for Large-Scale JSONL Datasets with LLM Applications",
    "url": "http://arxiv.org/abs/2508.12536v2",
    "authors": [
      "Yasuo Tabei"
    ],
    "published": "2025-08-18",
    "abstract": "JSON Lines (JSONL) is widely used for managing large collections of semi-structured data, ranging from large language model (LLM) prompts to chemical compound records and geospatial datasets. A key operation is substructure search, which identifies all JSON objects containing a query pattern. This task underpins applications such as drug discovery (querying compounds for functional groups), prompt engineering (extracting prompts with schema fragments), and geospatial analytics (finding entities with nested attributes). However, existing methods are inefficient: traversal requires exhaustive tree matching, succinct JSON representations save space but do not accelerate search, and XML-based approaches incur conversion overhead and semantic mismatches. We present jXBW, a compressed index for efficient substructure search over JSONL. jXBW introduces three innovations: (i) a merged tree representation that consolidates repeated structures, (ii) a succinct tree index based on the eXtended Burrows--Wheeler Transform (XBW), and (iii) a three-phase algorithm for substructure search. These enable query-dependent complexity, where cost depends on query characteristics rather than dataset size, while retaining succinct space. This resolves a key bottleneck in retrieval-augmented generation (RAG) systems requiring structure-aware retrieval. Experiments on seven real datasets, including PubChem (1M compounds) and OSM geospatial data (6.6M objects), achieve up to 4,700$\\times$ speedup over tree-based methods and over $6\\times 10^6$ speedup relative to XML-based approaches. jXBW makes JSONL substructure search practical for the first time, opening opportunities for large-scale LLM-based analytics.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution",
    "url": "http://arxiv.org/abs/2508.06584v1",
    "authors": [
      "Kalana Wijegunarathna",
      "Kristin Stock",
      "Christopher B. Jones"
    ],
    "published": "2025-08-08",
    "abstract": "The development, integration, and maintenance of geospatial databases rely heavily on efficient and accurate matching procedures of Geospatial Entity Resolution (ER). While resolution of points-of-interest (POIs) has been widely addressed, resolution of entities with diverse geometries has been largely overlooked. This is partly due to the lack of a uniform technique for embedding heterogeneous geometries seamlessly into a neural network framework. Existing neural approaches simplify complex geometries to a single point, resulting in significant loss of spatial information. To address this limitation, we propose Omni, a geospatial ER model featuring an omni-geometry encoder. This encoder is capable of embedding point, line, polyline, polygon, and multi-polygon geometries, enabling the model to capture the complex geospatial intricacies of the places being compared. Furthermore, Omni leverages transformer-based pre-trained language models over individual textual attributes of place records in an Attribute Affinity mechanism. The model is rigorously tested on existing point-only datasets and a new diverse-geometry geospatial ER dataset. Omni produces up to 12% (F1) improvement over existing methods.\n  Furthermore, we test the potential of Large Language Models (LLMs) to conduct geospatial ER, experimenting with prompting strategies and learning scenarios, comparing the results of pre-trained language model-based methods with LLMs. Results indicate that LLMs show competitive results.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)",
    "url": "http://arxiv.org/abs/2508.04846v1",
    "authors": [
      "Mahdi Nazari Ashani",
      "Ali Asghar Alesheikh",
      "Saba Kazemi",
      "Kimya Kheirkhah",
      "Yasin Mohammadi",
      "Fatemeh Rezaie",
      "Amir Mahdi Manafi",
      "Hedieh Zarkesh"
    ],
    "published": "2025-08-06",
    "abstract": "Autonomous web-based geographical information systems (AWebGIS) aim to perform geospatial operations from natural language input, providing intuitive, intelligent, and hands-free interaction. However, most current solutions rely on cloud-based large language models (LLMs), which require continuous internet access and raise users' privacy and scalability issues due to centralized server processing. This study compares three approaches to enabling AWebGIS: (1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2) a semi-automated offline method using classical machine learning classifiers such as support vector machine and random forest; and (3) a fully autonomous offline (client-side) method based on a fine-tuned small language model (SLM), specifically T5-small model, executed in the client's web browser. The third approach, which leverages SLMs, achieved the highest accuracy among all methods, with an exact matching accuracy of 0.93, Levenshtein similarity of 0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L scores of 0.98. Crucially, this client-side computation strategy reduces the load on backend servers by offloading processing to the user's device, eliminating the need for server-based inference. These results highlight the feasibility of browser-executable models for AWebGIS solutions.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis",
    "url": "http://arxiv.org/abs/2508.01341v3",
    "authors": [
      "Markus B. Pettersson",
      "Connor T. Jerzak",
      "Adel Daoud"
    ],
    "published": "2025-08-02",
    "abstract": "Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a \"one map, many trials\" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GeoJSEval: An Automated Evaluation Framework for Large Language Models on JavaScript-Based Geospatial Computation and Visualization Code Generation",
    "url": "http://arxiv.org/abs/2507.20553v1",
    "authors": [
      "Guanyu Chen",
      "Haoyue Jiao",
      "Shuyang Hou",
      "Ziqi Liu",
      "Lutong Xie",
      "Shaowen Wu",
      "Huayi Wu",
      "Xuefeng Guan",
      "Zhipeng Gui"
    ],
    "published": "2025-07-28",
    "abstract": "With the widespread adoption of large language models (LLMs) in code generation tasks, geospatial code generation has emerged as a critical frontier in the integration of artificial intelligence and geoscientific analysis. This trend underscores the urgent need for systematic evaluation methodologies to assess LLMs generation capabilities in geospatial contexts. In particular, geospatial computation and visualization tasks in JavaScript environments rely heavily on orchestrating diverse frontend libraries and ecosystems, placing elevated demands on a model's semantic understanding and code synthesis abilities. To address this challenge, we propose GeoJSEval--the first multimodal, function-level automatic evaluation framework for LLMs in JavaScript-based geospatial code generation. GeoJSEval comprises three core components: a standardized test suite (GeoJSEval-Bench), a code submission engine, and an evaluation module. It includes 432 function-level tasks and 2,071 structured test cases spanning five widely used JavaScript geospatial libraries and 25 mainstream geospatial data types. GeoJSEval enables multidimensional quantitative evaluation across metrics such as accuracy, output stability, execution efficiency, resource consumption, and error type distribution, and integrates boundary testing mechanisms to enhance robustness and coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs using GeoJSEval, revealing significant performance disparities and bottlenecks in spatial semantic understanding, code reliability, and function invocation accuracy. GeoJSEval provides a foundational methodology, evaluation resource, and practical toolkit for the standardized assessment and optimization of geospatial code generation models, with strong extensibility and applicability in real-world scenarios.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic Estimation using LLM",
    "url": "http://arxiv.org/abs/2507.13323v1",
    "authors": [
      "Kyeongjin Ahn",
      "Sungwon Han",
      "Seungeon Lee",
      "Donghyun Ahn",
      "Hyoshin Kim",
      "Jungwon Kim",
      "Jihee Kim",
      "Sangyoon Park",
      "Meeyoung Cha"
    ],
    "published": "2025-07-17",
    "abstract": "Socio-economic indicators like regional GDP, population, and education levels, are crucial to shaping policy decisions and fostering sustainable development. This research introduces GeoReg a regression model that integrates diverse data sources, including satellite imagery and web-based geospatial information, to estimate these indicators even for data-scarce regions such as developing countries. Our approach leverages the prior knowledge of large language model (LLM) to address the scarcity of labeled data, with the LLM functioning as a data engineer by extracting informative features to enable effective estimation in few-shot settings. Specifically, our model obtains contextual relationships between data features and the target indicator, categorizing their correlations as positive, negative, mixed, or irrelevant. These features are then fed into the linear estimator with tailored weight constraints for each category. To capture nonlinear patterns, the model also identifies meaningful feature interactions and integrates them, along with nonlinear transformations. Experiments across three countries at different stages of development demonstrate that our model outperforms baselines in estimating socio-economic indicators, even for low-income countries with limited data availability.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Regression"
    ]
  },
  {
    "title": "SPOT: Bridging Natural Language and Geospatial Search for Investigative Journalists",
    "url": "http://arxiv.org/abs/2506.13188v1",
    "authors": [
      "Lynn Khellaf",
      "Ipek Baris Schlicht",
      "Tilman Mirass",
      "Julia Bayer",
      "Tilman Wagner",
      "Ruben Bouwmeester"
    ],
    "published": "2025-06-16",
    "abstract": "OpenStreetMap (OSM) is a vital resource for investigative journalists doing geolocation verification. However, existing tools to query OSM data such as Overpass Turbo require familiarity with complex query languages, creating barriers for non-technical users. We present SPOT, an open source natural language interface that makes OSM's rich, tag-based geographic data more accessible through intuitive scene descriptions. SPOT interprets user inputs as structured representations of geospatial object configurations using fine-tuned Large Language Models (LLMs), with results being displayed in an interactive map interface. While more general geospatial search tasks are conceivable, SPOT is specifically designed for use in investigative journalism, addressing real-world challenges such as hallucinations in model output, inconsistencies in OSM tagging, and the noisy nature of user input. It combines a novel synthetic data pipeline with a semantic bundling system to enable robust, accurate query generation. To our knowledge, SPOT is the first system to achieve reliable natural language access to OSM data at this level of accuracy. By lowering the technical barrier to geolocation verification, SPOT contributes a practical tool to the broader efforts to support fact-checking and combat disinformation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant",
    "url": "http://arxiv.org/abs/2506.11781v1",
    "authors": [
      "Gaspard Merten",
      "Gilles Dejaegere",
      "Mahmoud Sakr"
    ],
    "published": "2025-06-13",
    "abstract": "Geospatial data analysis plays a crucial role in tackling intricate societal challenges such as urban planning and climate modeling. However, employing tools like GeoPandas, a prominent Python library for geospatial data manipulation, necessitates expertise in complex domain-specific syntax and workflows. GeoPandas-AI addresses this gap by integrating LLMs directly into the GeoPandas workflow, transforming the GeoDataFrame class into an intelligent, stateful class for both data analysis and geospatial code development. This paper formalizes the design of such a smart class and provides an open-source implementation of GeoPandas-AI in PyPI package manager. Through its innovative combination of conversational interfaces and stateful exploitation of LLMs for code generation and data analysis, GeoPandas-AI introduces a new paradigm for code-copilots and instantiates it for geospatial development.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment",
    "url": "http://arxiv.org/abs/2506.06355v1",
    "authors": [
      "Lingyao Li",
      "Dawei Li",
      "Zhenhui Ou",
      "Xiaoran Xu",
      "Jingxiao Liu",
      "Zihui Ma",
      "Runlong Yu",
      "Min Deng"
    ],
    "published": "2025-06-02",
    "abstract": "Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes. Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios. This study examines multiple LLMs to proactively estimate perceived earthquake impacts. Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales. Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real reports at the zip code level. Techniques such as RAG and ICL can improve simulation performance, while visual inputs notably enhance accuracy compared to structured numerical data alone. These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale",
    "url": "http://arxiv.org/abs/2506.01297v3",
    "authors": [
      "Ya Wen",
      "Jixuan Cai",
      "Qiyao Ma",
      "Linyan Li",
      "Xinhua Chen",
      "Chris Webster",
      "Yulun Zhou"
    ],
    "published": "2025-06-02",
    "abstract": "Representation learning of geospatial locations remains a core challenge in achieving general geospatial intelligence. Current embedding methods often lack versatility, limiting their utility across diverse tasks in both human and natural domains. We present MobCLIP, the first nationwide general-purpose location encoder, integrating an unprecedented diversity of data modalities through effective and scalable multimodal fusion. Adopting a novel CLIP-based architecture, our framework aligns 100M+ POIs, nationwide remote sensing imagery, and structured demographic statistics with a billion-edge mobility graph. By tokenizing spatial locations into grid cells inspired by Vision Transformers, we establish a unified representation space bridging mobility patterns and multimodal features. To rigorously evaluate the general-purpose effectiveness of MobCLIP, we construct a benchmark dataset composed of 11 downstream prediction tasks across social, economic, and natural domains. Experiments show that MobCLIP, with four input modalities and a compact 128-dimensional representation space, achieves significantly superior general-purpose predictive performances than state-of-the-art models by an average of 35%. Thanks to the effective integration of human-centric modalities, the performance gain is particularly profound in human-centric tasks, such as energy consumption (+260%), offline retail consumption amount (+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we further demonstrate the scaling behavior in geospatial representation learning. We open-source code and pretrained models at: https://github.com/ylzhouchris/MobCLIP.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer",
      "LLM",
      "CLIP"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features",
    "url": "http://arxiv.org/abs/2506.00203v1",
    "authors": [
      "Omid Reza Abbasi",
      "Franz Welscher",
      "Georg Weinberger",
      "Johannes Scholz"
    ],
    "published": "2025-05-30",
    "abstract": "As large language models (LLMs) continue to evolve, questions about their trustworthiness in delivering factual information have become increasingly important. This concern also applies to their ability to accurately represent the geographic world. With recent advancements in this field, it is relevant to consider whether and to what extent LLMs' representations of the geographical world can be trusted. This study evaluates the performance of GPT-4o and Gemini 2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and reverse geocoding. In the geocoding task, both models exhibited systematic and random errors in estimating the coordinates of St. Anne's Column in Innsbruck, Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash demonstrating more precision but a significant systematic offset. For elevation estimation, both models tended to underestimate elevations across Austria, though they captured overall topographical trends, and Gemini 2.0 Flash performed better in eastern regions. The reverse geocoding task, which involved identifying Austrian federal states from coordinates, revealed that Gemini 2.0 Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating better consistency across regions. Despite these findings, neither model achieved an accurate reconstruction of Austria's federal states, highlighting persistent misclassifications. The study concludes that while LLMs can approximate geographic information, their accuracy and reliability are inconsistent, underscoring the need for fine-tuning with geographical information to enhance their utility in GIScience and Geoinformatics.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models",
    "url": "http://arxiv.org/abs/2505.24340v1",
    "authors": [
      "Gilles Quentin Hacheme",
      "Girmaw Abebe Tadesse",
      "Caleb Robinson",
      "Akram Zaytar",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ],
    "published": "2025-05-30",
    "abstract": "Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that claim zero-shot classification capabilities for satellite imagery nonetheless rely on task-specific pretraining and adaptation to reach competitive performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot classification framework: a vision Large Language Model (vLLM) generates rich, human-readable image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). This modular, and interpretable pipeline enables flexible image classification for a large range of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced, and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class classification tasks (UC Merced, RESISC45), we implemented a recursive LLM-driven clustering to form meta-classes at successive depths, followed by hierarchical classification-first resolving coarse groups, then finer distinctions-to deliver competitive zero-shot performance. GVL is open-sourced at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in real-world geospatial workflows.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM",
      "CLIP"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "The Role of Open-Source LLMs in Shaping the Future of GeoAI",
    "url": "http://arxiv.org/abs/2504.17833v2",
    "authors": [
      "Xiao Huang",
      "Zhengzhong Tu",
      "Xinyue Ye",
      "Michael Goodchild"
    ],
    "published": "2025-04-24",
    "abstract": "Large Language Models (LLMs) are transforming geospatial artificial intelligence (GeoAI), offering new capabilities in data processing, spatial analysis, and decision support. This paper examines the open-source paradigm's critical role in this transformation. While proprietary LLMs offer accessibility, they often limit the customization, interoperability, and transparency vital for specialized geospatial tasks. Conversely, open-source alternatives significantly advance Geographic Information Science (GIScience) by fostering greater adaptability, reproducibility, and community-driven innovation. Open frameworks empower researchers to tailor solutions, integrate cutting-edge methodologies (e.g., reinforcement learning, advanced spatial indexing), and align with FAIR (Findable, Accessible, Interoperable, and Reusable) principles. However, the growing reliance on any LLM necessitates careful consideration of security vulnerabilities, ethical risks, and robust governance for AI-generated geospatial outputs. This paper argues that GIScience advances best not through a single model type, but by cultivating a diverse, interoperable ecosystem combining open-source foundations for innovation, custom geospatial models, and interdisciplinary collaboration. By critically evaluating the opportunities and challenges of open-source LLMs within the broader GeoAI landscape, this work contributes to a thorough discourse on leveraging LLMs to effectively advance spatial research, policy, and decision-making in an equitable, sustainable, and scientifically rigorous manner.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction",
    "url": "http://arxiv.org/abs/2503.20981v1",
    "authors": [
      "Xiaoran Xu",
      "Zhaoqian Xue",
      "Chi Zhang",
      "Jhonatan Medri",
      "Junjie Xiong",
      "Jiayan Zhou",
      "Jin Jin",
      "Yongfeng Zhang",
      "Siyuan Ma",
      "Lingyao Li"
    ],
    "published": "2025-03-26",
    "abstract": "Investigating the public experience of urgent care facilities is essential for promoting community healthcare development. Traditional survey methods often fall short due to limited scope, time, and spatial coverage. Crowdsourcing through online reviews or social media offers a valuable approach to gaining such insights. With recent advancements in large language models (LLMs), extracting nuanced perceptions from reviews has become feasible. This study collects Google Maps reviews across the DMV and Florida areas and conducts prompt engineering with the GPT model to analyze the aspect-based sentiment of urgent care. We first analyze the geospatial patterns of various aspects, including interpersonal factors, operational efficiency, technical quality, finances, and facilities. Next, we determine Census Block Group(CBG)-level characteristics underpinning differences in public perception, including population density, median income, GINI Index, rent-to-income ratio, household below poverty rate, no insurance rate, and unemployment rate. Our results show that interpersonal factors and operational efficiency emerge as the strongest determinants of patient satisfaction in urgent care, while technical quality, finances, and facilities show no significant independent effects when adjusted for in multivariate models. Among socioeconomic and demographic factors, only population density demonstrates a significant but modest association with patient ratings, while the remaining factors exhibit no significant correlations. Overall, this study highlights the potential of crowdsourcing to uncover the key factors that matter to residents and provide valuable insights for stakeholders to improve public satisfaction with urgent care.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite Images by Poverty Level: Advancing Tools for Social Science Research",
    "url": "http://arxiv.org/abs/2501.14546v1",
    "authors": [
      "Hamid Sarmadi",
      "Ola Hall",
      "Thorsteinn R\u00f6gnvaldsson",
      "Mattias Ohlsson"
    ],
    "published": "2025-01-24",
    "abstract": "This paper investigates the novel application of Large Language Models (LLMs) with vision capabilities to analyze satellite imagery for village-level poverty prediction. Although LLMs were originally designed for natural language understanding, their adaptability to multimodal tasks, including geospatial analysis, has opened new frontiers in data-driven research. By leveraging advancements in vision-enabled LLMs, we assess their ability to provide interpretable, scalable, and reliable insights into human poverty from satellite images. Using a pairwise comparison approach, we demonstrate that ChatGPT can rank satellite images based on poverty levels with accuracy comparable to domain experts. These findings highlight both the promise and the limitations of LLMs in socioeconomic research, providing a foundation for their integration into poverty assessment workflows. This study contributes to the ongoing exploration of unconventional data sources for welfare analysis and opens pathways for cost-effective, large-scale poverty monitoring.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Augmenting a Large Language Model with a Combination of Text and Visual Data for Conversational Visualization of Global Geospatial Data",
    "url": "http://arxiv.org/abs/2501.09521v1",
    "authors": [
      "Omar Mena",
      "Alexandre Kouyoumdjian",
      "Lonni Besan\u00e7on",
      "Michael Gleicher",
      "Ivan Viola",
      "Anders Ynnerman"
    ],
    "published": "2025-01-16",
    "abstract": "We present a method for augmenting a Large Language Model (LLM) with a combination of text and visual data to enable accurate question answering in visualization of scientific data, making conversational visualization possible. LLMs struggle with tasks like visual data interaction, as they lack contextual visual information. We address this problem by merging a text description of a visualization and dataset with snapshots of the visualization. We extract their essential features into a structured text file, highly compact, yet descriptive enough to appropriately augment the LLM with contextual information, without any fine-tuning. This approach can be applied to any visualization that is already finally rendered, as long as it is associated with some textual description.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Comparative Performance of Advanced NLP Models and LLMs in Multilingual Geo-Entity Detection",
    "url": "http://arxiv.org/abs/2412.20414v1",
    "authors": [
      "Kalin Kopanov"
    ],
    "published": "2024-12-29",
    "abstract": "The integration of advanced Natural Language Processing (NLP) methodologies and Large Language Models (LLMs) has significantly enhanced the extraction and analysis of geospatial data from multilingual texts, impacting sectors such as national and international security. This paper presents a comprehensive evaluation of leading NLP models -- SpaCy, XLM-RoBERTa, mLUKE, GeoLM -- and LLMs, specifically OpenAI's GPT 3.5 and GPT 4, within the context of multilingual geo-entity detection. Utilizing datasets from Telegram channels in English, Russian, and Arabic, we examine the performance of these models through metrics such as accuracy, precision, recall, and F1 scores, to assess their effectiveness in accurately identifying geospatial references. The analysis exposes each model's distinct advantages and challenges, underscoring the complexities involved in achieving precise geo-entity identification across varied linguistic landscapes. The conclusions drawn from this experiment aim to direct the enhancement and creation of more advanced and inclusive NLP tools, thus advancing the field of geospatial analysis and its application to global security.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on the Google Earth Engine Platform Powered by Large Language Models",
    "url": "http://arxiv.org/abs/2412.05587v2",
    "authors": [
      "Shuyang Hou",
      "Jianyuan Liang",
      "Anqi Zhao",
      "Huayi Wu"
    ],
    "published": "2024-12-07",
    "abstract": "As the scale and complexity of spatiotemporal data continue to grow rapidly, the use of geospatial modeling on the Google Earth Engine (GEE) platform presents dual challenges: improving the coding efficiency of domain experts and enhancing the coding capabilities of interdisciplinary users. To address these challenges and improve the performance of large language models (LLMs) in geospatial code generation tasks, we propose a framework for building a geospatial operator knowledge base tailored to the GEE JavaScript API. This framework consists of an operator syntax knowledge table, an operator relationship frequency table, an operator frequent pattern knowledge table, and an operator relationship chain knowledge table. By leveraging Abstract Syntax Tree (AST) techniques and frequent itemset mining, we systematically extract operator knowledge from 185,236 real GEE scripts and syntax documentation, forming a structured knowledge base. Experimental results demonstrate that the framework achieves over 90% accuracy, recall, and F1 score in operator knowledge extraction. When integrated with the Retrieval-Augmented Generation (RAG) strategy for LLM-based geospatial code generation tasks, the knowledge base improves performance by 20-30%. Ablation studies further quantify the necessity of each knowledge table in the knowledge base construction. This work provides robust support for the advancement and application of geospatial code modeling techniques, offering an innovative approach to constructing domain-specific knowledge bases that enhance the code generation capabilities of LLMs, and fostering the deeper integration of generative AI technologies within the field of geoinformatics.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Advancing Large Language Models for Spatiotemporal and Semantic Association Mining of Similar Environmental Events",
    "url": "http://arxiv.org/abs/2411.12880v1",
    "authors": [
      "Yuanyuan Tian",
      "Wenwen Li",
      "Lei Hu",
      "Xiao Chen",
      "Michael Brook",
      "Michael Brubaker",
      "Fan Zhang",
      "Anna K. Liljedahl"
    ],
    "published": "2024-11-19",
    "abstract": "Retrieval and recommendation are two essential tasks in modern search tools. This paper introduces a novel retrieval-reranking framework leveraging Large Language Models (LLMs) to enhance the spatiotemporal and semantic associated mining and recommendation of relevant unusual climate and environmental events described in news articles and web posts. This framework uses advanced natural language processing techniques to address the limitations of traditional manual curation methods in terms of high labor cost and lack of scalability. Specifically, we explore an optimized solution to employ cutting-edge embedding models for semantically analyzing spatiotemporal events (news) and propose a Geo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria including spatial proximity, temporal association, semantic similarity, and category-instructed similarity to rank and identify similar spatiotemporal events. We apply the proposed framework to a dataset of four thousand Local Environmental Observer (LEO) Network events, achieving top performance in recommending similar events among multiple cutting-edge dense retrieval models. The search and recommendation pipeline can be applied to a wide range of similar data search tasks dealing with geospatial and temporal data. We hope that by linking relevant events, we can better aid the general public to gain an enhanced understanding of climate change and its impact on different communities.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Chain-of-Programming (CoP) : Empowering Large Language Models for Geospatial Code Generation",
    "url": "http://arxiv.org/abs/2411.10753v1",
    "authors": [
      "Shuyang Hou",
      "Haoyue Jiao",
      "Zhangxiao Shen",
      "Jianyuan Liang",
      "Anqi Zhao",
      "Xiaopu Zhang",
      "Jianxun Wang",
      "Huayi Wu"
    ],
    "published": "2024-11-16",
    "abstract": "With the rapid growth of interdisciplinary demands for geospatial modeling and the rise of large language models (LLMs), geospatial code generation technology has seen significant advancements. However, existing LLMs often face challenges in the geospatial code generation process due to incomplete or unclear user requirements and insufficient knowledge of specific platform syntax rules, leading to the generation of non-executable code, a phenomenon known as \"code hallucination.\" To address this issue, this paper proposes a Chain of Programming (CoP) framework, which decomposes the code generation process into five steps: requirement analysis, algorithm design, code implementation, code debugging, and code annotation. The framework incorporates a shared information pool, knowledge base retrieval, and user feedback mechanisms, forming an end-to-end code generation flow from requirements to code without the need for model fine-tuning. Based on a geospatial problem classification framework and evaluation benchmarks, the CoP strategy significantly improves the logical clarity, syntactical correctness, and executability of the generated code, with improvements ranging from 3.0% to 48.8%. Comparative and ablation experiments further validate the superiority of the CoP strategy over other optimization approaches and confirm the rationality and necessity of its key components. Through case studies on building data visualization and fire data analysis, this paper demonstrates the application and effectiveness of CoP in various geospatial scenarios. The CoP framework offers a systematic, step-by-step approach to LLM-based geospatial code generation tasks, significantly enhancing code generation performance in geospatial tasks and providing valuable insights for code generation in other vertical domains.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base for Geospatial Code Generation Tasks Using Large Language Models",
    "url": "http://arxiv.org/abs/2410.20975v1",
    "authors": [
      "Shuyang Hou",
      "Anqi Zhao",
      "Jianyuan Liang",
      "Zhangxiao Shen",
      "Huayi Wu"
    ],
    "published": "2024-10-28",
    "abstract": "The rise of spatiotemporal data and the need for efficient geospatial modeling have spurred interest in automating these tasks with large language models (LLMs). However, general LLMs often generate errors in geospatial code due to a lack of domain-specific knowledge on functions and operators. To address this, a retrieval-augmented generation (RAG) approach, utilizing an external knowledge base of geospatial functions and operators, is proposed. This study introduces a framework to construct such a knowledge base, leveraging geospatial script semantics. The framework includes: Function Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination Statistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like Chain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and align geospatial functions. An example knowledge base, Geo-FuB, built from 154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics show a high accuracy, reaching 88.89% overall, with structural and semantic accuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize geospatial code generation through the RAG and fine-tuning paradigms is highlighted.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks",
    "url": "http://arxiv.org/abs/2410.17031v2",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Anqi Zhao",
      "Jianyuan Liang",
      "Zhipeng Gui",
      "Xuefeng Guan",
      "Rui Li",
      "Huayi Wu"
    ],
    "published": "2024-10-22",
    "abstract": "The increasing demand for spatiotemporal data and modeling tasks in geosciences has made geospatial code generation technology a critical factor in enhancing productivity. Although large language models (LLMs) have demonstrated potential in code generation tasks, they often encounter issues such as refusal to code or hallucination in geospatial code generation due to a lack of domain-specific knowledge and code corpora. To address these challenges, this paper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along with the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and LoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first LLM focused on geospatial code generation, fine-tuned from Code Llama-7B. Furthermore, we establish a comprehensive geospatial code evaluation framework, incorporating option matching, expert validation, and prompt engineering scoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the GeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms other models in multiple-choice accuracy by 9.1% to 32.1%, in code summarization ability by 1.7% to 25.4%, and in code generation capability by 1.2% to 25.1%. This paper provides a solution and empirical validation for enhancing LLMs' performance in geospatial code generation, extends the boundaries of domain-specific model applications, and offers valuable insights into unlocking their potential in geospatial code generation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Beyond Words: Evaluating Large Language Models in Transportation Planning",
    "url": "http://arxiv.org/abs/2409.14516v1",
    "authors": [
      "Shaowei Ying",
      "Zhenlong Li",
      "Manzhu Yu"
    ],
    "published": "2024-09-22",
    "abstract": "The resurgence and rapid advancement of Generative Artificial Intelligence (GenAI) in 2023 has catalyzed transformative shifts across numerous industry sectors, including urban transportation and logistics. This study investigates the evaluation of Large Language Models (LLMs), specifically GPT-4 and Phi-3-mini, to enhance transportation planning. The study assesses the performance and spatial comprehension of these models through a transportation-informed evaluation framework that includes general geospatial skills, general transportation domain skills, and real-world transportation problem-solving. Utilizing a mixed-methods approach, the research encompasses an evaluation of the LLMs' general Geographic Information System (GIS) skills, general transportation domain knowledge as well as abilities to support human decision-making in the real-world transportation planning scenarios of congestion pricing. Results indicate that GPT-4 demonstrates superior accuracy and reliability across various GIS and transportation-specific tasks compared to Phi-3-mini, highlighting its potential as a robust tool for transportation planners. Nonetheless, Phi-3-mini exhibits competence in specific analytical scenarios, suggesting its utility in resource-constrained environments. The findings underscore the transformative potential of GenAI technologies in urban transportation planning. Future work could explore the application of newer LLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a broader set of real-world transportation planning and operations challenges, to deepen the integration of advanced AI models in transportation management practices.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large Language Model Empowered Graph-based Learning Approach",
    "url": "http://arxiv.org/abs/2408.17258v3",
    "authors": [
      "Tong Nie",
      "Junlin He",
      "Yuewen Mei",
      "Guoyang Qin",
      "Guilong Li",
      "Jian Sun",
      "Wei Ma"
    ],
    "published": "2024-08-30",
    "abstract": "The proliferation of e-commerce and urbanization has significantly intensified delivery operations in urban areas, boosting the volume and complexity of delivery demand. Data-driven predictive methods, especially those utilizing machine learning techniques, have emerged to handle these complexities in urban delivery demand management problems. One particularly pressing issue that has yet to be sufficiently addressed is the joint estimation and prediction of city-wide delivery demand, as well as the generalization of the model to new cities. To this end, we formulate this problem as a transferable graph-based spatiotemporal learning task. First, an individual-collective message-passing neural network model is formalized to capture the interaction between demand patterns of associated regions. Second, by exploiting recent advances in large language models (LLMs), we extract general geospatial knowledge encodings from the unstructured locational data using the embedding generated by LLMs. Last, to encourage the cross-city generalization of the model, we integrate the encoding into the demand predictor in a transferable way. Comprehensive empirical evaluation results on two real-world delivery datasets, including eight cities in China and the US, demonstrate that our model significantly outperforms state-of-the-art baselines in accuracy, efficiency, and transferability.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning",
    "url": "http://arxiv.org/abs/2408.12116v2",
    "authors": [
      "Junlin He",
      "Tong Nie",
      "Wei Ma"
    ],
    "published": "2024-08-22",
    "abstract": "In the geospatial domain, universal representation models are significantly less prevalent than their extensive use in natural language processing and computer vision. This discrepancy arises primarily from the high costs associated with the input of existing representation models, which often require street views and mobility data. To address this, we develop a novel, training-free method that leverages large language models (LLMs) and auxiliary map data from OpenStreetMap to derive geolocation representations (LLMGeovec). LLMGeovec can represent the geographic semantics of city, country, and global scales, which acts as a generic enhancer for spatio-temporal learning. Specifically, by direct feature concatenation, we introduce a simple yet effective paradigm for enhancing multiple spatio-temporal tasks including geographic prediction (GP), long-term time series forecasting (LTSF), and graph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly integrate into a wide spectrum of spatio-temporal learning models, providing immediate enhancements. Experimental results demonstrate that LLMGeovec achieves global coverage and significantly boosts the performance of leading GP, LTSF, and GSTF models. Our codes are available at \\url{https://github.com/Umaruchain/LLMGeovec}.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "A Unified Framework for Next-Gen Urban Forecasting via LLM-driven Dependency Retrieval and GeoTransformer",
    "url": "http://arxiv.org/abs/2408.08852v4",
    "authors": [
      "Yuhao Jia",
      "Zile Wu",
      "Shengao Yi",
      "Yifei Sun",
      "Xiao Huang"
    ],
    "published": "2024-08-16",
    "abstract": "Urban forecasting has increasingly benefited from high-dimensional spatial data through two primary approaches: graph-based methods that rely on predefined spatial structures, and region-based methods that focus on learning expressive urban representations. Although these methods have laid a strong foundation, they either rely heavily on structured spatial data, struggle to adapt to task-specific dependencies, or fail to integrate holistic urban context. Moreover, no existing framework systematically integrates these two paradigms and overcomes their respective limitations. To address this gap, we propose a novel, unified framework for high-dimensional urban forecasting, composed of three key components: (1) the Urban Region Representation Module that organizes latent embeddings and semantic descriptions for each region, (2) the Task-aware Dependency Retrieval module that selects relevant context regions based on natural language prompts, and (3) the Prediction Module, exemplified by our proposed GeoTransformer architecture, which adopts a novel geospatial attention mechanism to incorporate spatial proximity and information entropy as priors. Our framework is modular, supports diverse representation methods and forecasting models, and can operate even with minimal input. Quantitative experiments and qualitative analysis across six urban forecasting tasks demonstrate strong task generalization and validate the framework's effectiveness.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks",
    "url": "http://arxiv.org/abs/2406.13945v3",
    "authors": [
      "Jie Feng",
      "Jun Zhang",
      "Tianhui Liu",
      "Xin Zhang",
      "Tianjian Ouyang",
      "Junbo Yan",
      "Yuwei Du",
      "Siqi Guo",
      "Yong Li"
    ],
    "published": "2024-06-20",
    "abstract": "As large language models (LLMs) continue to advance and gain widespread use, establishing systematic and reliable evaluation methodologies for LLMs and vision-language models (VLMs) has become essential to ensure their real-world effectiveness and reliability. There have been some early explorations about the usability of LLMs for limited urban tasks, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for urban research lies in the diversity of urban data, the complexity of application scenarios and the highly dynamic nature of the urban environment. In this paper, we design \\textit{CityBench}, an interactive simulator based evaluation platform, as the first systematic benchmark for evaluating the capabilities of LLMs for diverse tasks in urban research. First, we build \\textit{CityData} to integrate the diverse urban data and \\textit{CitySimu} to simulate fine-grained urban dynamics. Based on \\textit{CityData} and \\textit{CitySimu}, we design 8 representative urban tasks in 2 categories of perception-understanding and decision-making as the \\textit{CityBench}. With extensive results from 30 well-known LLMs and VLMs in 13 cities around the world, we find that advanced LLMs and VLMs can achieve competitive performance in diverse urban tasks requiring commonsense and semantic understanding abilities, e.g., understanding the human dynamics and semantic inference of urban images. Meanwhile, they fail to solve the challenging urban tasks requiring professional knowledge and high-level numerical abilities, e.g., geospatial prediction and traffic control task.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Geospatial Big Data: Survey and Challenges",
    "url": "http://arxiv.org/abs/2404.18428v1",
    "authors": [
      "Jiayang Wu",
      "Wensheng Gan",
      "Han-Chieh Chao",
      "Philip S. Yu"
    ],
    "published": "2024-04-29",
    "abstract": "In recent years, geospatial big data (GBD) has obtained attention across various disciplines, categorized into big earth observation data and big human behavior data. Identifying geospatial patterns from GBD has been a vital research focus in the fields of urban management and environmental sustainability. This paper reviews the evolution of GBD mining and its integration with advanced artificial intelligence (AI) techniques. GBD consists of data generated by satellites, sensors, mobile devices, and geographical information systems, and we categorize geospatial data based on different perspectives. We outline the process of GBD mining and demonstrate how it can be incorporated into a unified framework. Additionally, we explore new technologies like large language models (LLM), the Metaverse, and knowledge graphs, and how they could make GBD even more useful. We also share examples of GBD helping with city management and protecting the environment. Finally, we discuss the real challenges that come up when working with GBD, such as issues with data retrieval and security. Our goal is to give readers a clear view of where GBD mining stands today and where it might go next.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "LAMP: A Language Model on the Map",
    "url": "http://arxiv.org/abs/2403.09059v2",
    "authors": [
      "Pasquale Balsebre",
      "Weiming Huang",
      "Gao Cong"
    ],
    "published": "2024-03-14",
    "abstract": "Large Language Models (LLMs) are poised to play an increasingly important role in our lives, providing assistance across a wide array of tasks. In the geospatial domain, LLMs have demonstrated the ability to answer generic questions, such as identifying a country's capital; nonetheless, their utility is hindered when it comes to answering fine-grained questions about specific places, such as grocery stores or restaurants, which constitute essential aspects of people's everyday lives. This is mainly because the places in our cities haven't been systematically fed into LLMs, so as to understand and memorize them. This study introduces a novel framework for fine-tuning a pre-trained model on city-specific data, to enable it to provide accurate recommendations, while minimizing hallucinations. We share our model, LAMP, and the data used to train it. We conduct experiments to analyze its ability to correctly retrieving spatial objects, and compare it to well-known open- and closed- source language models, such as GPT-4. Finally, we explore its emerging capabilities through a case study on day planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series",
    "url": "http://arxiv.org/abs/2506.14786v1",
    "authors": [
      "Haobo Li",
      "Eunseo Jung",
      "Zixin Chen",
      "Zhaowei Wang",
      "Yueya Wang",
      "Huamin Qu",
      "Alexis Kai Hon Lau"
    ],
    "published": "2025-05-27",
    "abstract": "Multimodal time series forecasting is foundational in various fields, such as utilizing satellite imagery and numerical data for predicting typhoons in climate science. However, existing multimodal approaches primarily focus on utilizing text data to help time series forecasting, leaving the visual data in existing time series datasets untouched. Furthermore, it is challenging for models to effectively capture the physical information embedded in visual data, such as satellite imagery's temporal and geospatial context, which extends beyond images themselves. To address this gap, we propose physics-informed positional encoding (PIPE), a lightweight method that embeds physical information into vision language models (VLMs). PIPE introduces two key innovations: (1) a physics-informed positional indexing scheme for mapping physics to positional IDs, and (2) a variant-frequency positional encoding mechanism for encoding frequency information of physical variables and sequential order of tokens within the embedding space. By preserving both the physical information and sequential order information, PIPE significantly improves multimodal alignment and forecasting accuracy. Through the experiments on the most representative and the largest open-sourced satellite image dataset, PIPE achieves state-of-the-art performance in both deep learning forecasting and climate domain methods, demonstrating superiority across benchmarks, including a 12% improvement in typhoon intensity forecasting over prior works. Our code is provided in the supplementary material.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM",
      "PINN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
    "url": "http://arxiv.org/abs/2411.19325v2",
    "authors": [
      "Muhammad Sohail Danish",
      "Muhammad Akhtar Munir",
      "Syed Roshaan Ali Shah",
      "Kartik Kuckreja",
      "Fahad Shahbaz Khan",
      "Paolo Fraccaro",
      "Alexandre Lacoste",
      "Salman Khan"
    ],
    "published": "2024-11-28",
    "abstract": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management. Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery. To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales. We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark is publicly available at https://github.com/The-AI-Alliance/GEO-Bench-VLM .",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ]
  },
  {
    "title": "Zero-shot Building Age Classification from Facade Image Using GPT-4",
    "url": "http://arxiv.org/abs/2404.09921v1",
    "authors": [
      "Zichao Zeng",
      "June Moh Goo",
      "Xinglei Wang",
      "Bin Chi",
      "Meihui Wang",
      "Jan Boehm"
    ],
    "published": "2024-04-15",
    "abstract": "A building's age of construction is crucial for supporting many geospatial applications. Much current research focuses on estimating building age from facade images using deep learning. However, building an accurate deep learning model requires a considerable amount of labelled training data, and the trained models often have geographical constraints. Recently, large pre-trained vision language models (VLMs) such as GPT-4 Vision, which demonstrate significant generalisation capabilities, have emerged as potential training-free tools for dealing with specific vision tasks, but their applicability and reliability for building information remain unexplored. In this study, a zero-shot building age classifier for facade images is developed using prompts that include logical instructions. Taking London as a test case, we introduce a new dataset, FI-London, comprising facade images and building age epochs. Although the training-free classifier achieved a modest accuracy of 39.69%, the mean absolute error of 0.85 decades indicates that the model can predict building age epochs successfully albeit with a small bias. The ensuing discussion reveals that the classifier struggles to predict the age of very old buildings and is challenged by fine-grained predictions within 2 decades. Overall, the classifier utilising GPT-4 Vision is capable of predicting the rough age epoch of a building from a single facade image without any training.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Trust in foundation models and GenAI: A geographic perspective",
    "url": "http://arxiv.org/abs/2510.17942v1",
    "authors": [
      "Grant McKenzie",
      "Krzysztof Janowicz",
      "Carsten Kessler"
    ],
    "published": "2025-10-20",
    "abstract": "Large-scale pre-trained machine learning models have reshaped our understanding of artificial intelligence across numerous domains, including our own field of geography. As with any new technology, trust has taken on an important role in this discussion. In this chapter, we examine the multifaceted concept of trust in foundation models, particularly within a geographic context. As reliance on these models increases and they become relied upon for critical decision-making, trust, while essential, has become a fractured concept. Here we categorize trust into three types: epistemic trust in the training data, operational trust in the model's functionality, and interpersonal trust in the model developers. Each type of trust brings with it unique implications for geographic applications. Topics such as cultural context, data heterogeneity, and spatial relationships are fundamental to the spatial sciences and play an important role in developing trust. The chapter continues with a discussion of the challenges posed by different forms of biases, the importance of transparency and explainability, and ethical responsibilities in model development. Finally, the novel perspective of geographic information scientists is emphasized with a call for further transparency, bias mitigation, and regionally-informed policies. Simply put, this chapter aims to provide a conceptual starting point for researchers, practitioners, and policy-makers to better understand trust in (generative) GeoAI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety",
    "url": "http://arxiv.org/abs/2510.15434v1",
    "authors": [
      "Huan Chen",
      "Ting Han",
      "Siyu Chen",
      "Zhihao Guo",
      "Yiping Chen",
      "Meiliu Wu"
    ],
    "published": "2025-10-17",
    "abstract": "Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two fundamental challenges persist: (1) how to construct street-level indicators that capture accident-related features, and (2) how to quantify their causal impacts across different accident types. To address these challenges, we propose Semantic4Safety, a framework that applies zero-shot semantic segmentation to SVIs to derive 11 interpretable streetscape indicators, and integrates road type as contextual information to analyze approximately 30,000 accident records in Austin. Specifically, we train an eXtreme Gradient Boosting (XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP) to interpret both global and local feature contributions, and then apply Generalized Propensity Score (GPS) weighting and Average Treatment Effect (ATE) estimation to control confounding and quantify causal effects. Results uncover heterogeneous, accident-type-specific causal patterns: features capturing scene complexity, exposure, and roadway geometry dominate predictive power; larger drivable area and emergency space reduce risk, whereas excessive visual openness can increase it. By bridging predictive modeling with causal inference, Semantic4Safety supports targeted interventions and high-risk corridor diagnosis, offering a scalable, data-informed tool for urban road safety planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)",
    "url": "http://arxiv.org/abs/2510.14661v1",
    "authors": [
      "Weikang Yu",
      "Vincent Nwazelibe",
      "Xianping Ma",
      "Xiaokang Zhang",
      "Richard Gloaguen",
      "Xiao Xiang Zhu",
      "Pedram Ghamisi"
    ],
    "published": "2025-10-16",
    "abstract": "Mining activities are essential for industrial and economic development, but remain a leading source of environmental degradation, contributing to deforestation, soil erosion, and water contamination. Sustainable resource management and environmental governance require consistent, long-term monitoring of mining-induced land surface changes, yet existing datasets are often limited in temporal depth or geographic scope. To address this gap, we present EuroMineNet, the first comprehensive multitemporal benchmark for mining footprint mapping and monitoring based on Sentinel-2 multispectral imagery. Spanning 133 mining sites across the European Union, EuroMineNet provides annual observations and expert-verified annotations from 2015 to 2024, enabling GeoAI-based models to analyze environmental dynamics at a continental scale. It supports two sustainability-driven tasks: (1) multitemporal mining footprint mapping for consistent annual land-use delineation, evaluated with a novel Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change detection to capture both gradual and abrupt surface transformations. Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI methods effectively identify long-term environmental changes, challenges remain in detecting short-term dynamics critical for timely mitigation. By advancing temporally consistent and explainable mining monitoring, EuroMineNet contributes to sustainable land-use management, environmental resilience, and the broader goal of applying GeoAI for social and environmental good. We release the codes and datasets by aligning with FAIR and the open science paradigm at https://github.com/EricYu97/EuroMineNet.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Deep Learning to Identify the Spatio-Temporal Cascading Effects of Train Delays in a High-Density Network",
    "url": "http://arxiv.org/abs/2510.09350v1",
    "authors": [
      "Vu Duc Anh Nguyen",
      "Ziyue Li"
    ],
    "published": "2025-10-10",
    "abstract": "The operational efficiency of railway networks, a cornerstone of modern economies, is persistently undermined by the cascading effects of train delays. Accurately forecasting this delay propagation is a critical challenge for real-time traffic management. While recent research has leveraged Graph Neural Networks (GNNs) to model the network structure of railways, a significant gap remains in developing frameworks that provide multi-step autoregressive forecasts at a network-wide scale, while simultaneously offering the live, interpretable explanations needed for decision support. This paper addresses this gap by developing and evaluating a novel XGeoAI framework for live, explainable, multi-step train delay forecasting. The core of this work is a two-stage, autoregressive Graph Attention Network (GAT) model, trained on a real-world dataset covering over 40% of the Dutch railway network. The model represents the system as a spatio-temporal graph of operational events (arrivals and departures) and is enriched with granular features, including platform and station congestion. To test its viability for live deployment, the model is rigorously evaluated using a sequential, k-step-ahead forecasting protocol that simulates real-world conditions where prediction errors can compound. The results demonstrate that while the proposed GATv2 model is challenged on pure error metrics (MAE) by a simpler Persistence baseline, it achieves consistently higher precision in classifying delay events -- a crucial advantage for a reliable decision support tool.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "OBSR: Open Benchmark for Spatial Representations",
    "url": "http://arxiv.org/abs/2510.05879v2",
    "authors": [
      "Julia Moska",
      "Oleksii Furman",
      "Kacper Kozaczko",
      "Szymon Leszkiewicz",
      "Jakub Polczyk",
      "Piotr Gramacki",
      "Piotr Szyma\u0144ski"
    ],
    "published": "2025-10-07",
    "abstract": "GeoAI is evolving rapidly, fueled by diverse geospatial datasets like traffic patterns, environmental data, and crowdsourced OpenStreetMap (OSM) information. While sophisticated AI models are being developed, existing benchmarks are often concentrated on single tasks and restricted to a single modality. As such, progress in GeoAI is limited by the lack of a standardized, multi-task, modality-agnostic benchmark for their systematic evaluation. This paper introduces a novel benchmark designed to assess the performance, accuracy, and efficiency of geospatial embedders. Our benchmark is modality-agnostic and comprises 7 distinct datasets from diverse cities across three continents, ensuring generalizability and mitigating demographic biases. It allows for the evaluation of GeoAI embedders on various phenomena that exhibit underlying geographic processes. Furthermore, we establish a simple and intuitive task-oriented model baselines, providing a crucial reference point for comparing more complex solutions.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models",
    "url": "http://arxiv.org/abs/2509.23482v1",
    "authors": [
      "Zhangyu Wang",
      "Nemin Wu",
      "Qian Cao",
      "Jiangnan Xia",
      "Zeping Liu",
      "Yiqun Xie",
      "Akshay Nambi",
      "Tanuja Ganu",
      "Ni Lao",
      "Ninghao Liu",
      "Gengchen Mai"
    ],
    "published": "2025-09-27",
    "abstract": "The widespread adoption of AI models, especially foundation models (FMs), has made a profound impact on numerous domains. However, it also raises significant ethical concerns, including bias issues. Although numerous efforts have been made to quantify and mitigate social bias in AI models, geographic bias (in short, geo-bias) receives much less attention, which presents unique challenges. While previous work has explored ways to quantify geo-bias, these measures are model-specific (e.g., mean absolute deviation of LLM ratings) or spatially implicit (e.g., average fairness scores of all spatial partitions). We lack a model-agnostic, universally applicable, and spatially explicit geo-bias evaluation framework that allows researchers to fairly compare the geo-bias of different AI models and to understand what spatial factors contribute to the geo-bias. In this paper, we establish an information-theoretic framework for geo-bias evaluation, called GeoBS (Geo-Bias Scores). We demonstrate the generalizability of the proposed framework by showing how to interpret and analyze existing geo-bias measures under this framework. Then, we propose three novel geo-bias scores that explicitly take intricate spatial factors (multi-scalability, distance decay, and anisotropy) into consideration. Finally, we conduct extensive experiments on 3 tasks, 8 datasets, and 8 models to demonstrate that both task-specific GeoAI models and general-purpose foundation models may suffer from various types of geo-bias. This framework will not only advance the technical understanding of geographic bias but will also establish a foundation for integrating spatial fairness into the design, deployment, and evaluation of AI systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025",
    "url": "http://arxiv.org/abs/2509.03191v1",
    "authors": [
      "Taiga Saito",
      "Yu Otake",
      "Stephen Wu"
    ],
    "published": "2025-09-03",
    "abstract": "This paper presents a novel application of the Tabular Prior-Data Fitted Network (TabPFN) - a transformer-based foundation model for tabular data - to geotechnical site characterization problems defined in the GEOAI benchmark BM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the spatial variation of undrained shear strength (su) across borehole depth profiles, and (2) imputing missing mechanical parameters in a dense-site dataset. We apply TabPFN in a zero-training, few-shot, in-context learning setting - without hyper-parameter tuning - and provide it with additional context from the big indirect database (BID). The study demonstrates that TabPFN, as a general-purpose foundation model, achieved superior accuracy and well-calibrated predictive distributions compared to a conventional hierarchical Bayesian model (HBM) baseline, while also offering significant gains in inference efficiency. In Benchmark Problem #1 (spatial su prediction), TabPFN outperformed the HBM in prediction accuracy and delivered an order-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical parameter imputation), TabPFN likewise achieved lower RMSE for all target parameters with well-quantified uncertainties, though its cumulative computation cost was higher than HBM's due to its one-variable-at-a-time inference. These results mark the first successful use of a tabular foundation model in geotechnical modeling, suggesting a potential paradigm shift in probabilistic site characterization.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation",
    "url": "http://arxiv.org/abs/2509.01341v1",
    "authors": [
      "Yunus Serhat Bicakci",
      "Joseph Shingleton",
      "Anahid Basiri"
    ],
    "published": "2025-09-01",
    "abstract": "Street-level geolocalization from images is crucial for a wide range of essential applications and services, such as navigation, location-based recommendations, and urban planning. With the growing popularity of social media data and cameras embedded in smartphones, applying traditional computer vision techniques to localize images has become increasingly challenging, yet highly valuable. This paper introduces a novel approach that integrates open-weight and publicly accessible multimodal large language models with retrieval-augmented generation. The method constructs a vector database using the SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query images are augmented with prompts containing both similar and dissimilar geolocation information retrieved from this database before being processed by the multimodal large language models. Our approach has demonstrated state-of-the-art performance, achieving higher accuracy compared against three widely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our solution eliminates the need for expensive fine-tuning or retraining and scales seamlessly to incorporate new data sources. The effectiveness of retrieval-augmented generation-based multimodal large language models in geolocation estimation demonstrated by this paper suggests an alternative path to the traditional methods which rely on the training models from scratch, opening new possibilities for more accessible and scalable solutions in GeoAI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": []
  },
  {
    "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities",
    "url": "http://arxiv.org/abs/2508.19305v1",
    "authors": [
      "Chen Chu",
      "Cyrus Shahabi"
    ],
    "published": "2025-08-26",
    "abstract": "Spatial representation learning is essential for GeoAI applications such as urban analytics, enabling the encoding of shapes, locations, and spatial relationships (topological and distance-based) of geo-entities like points, polylines, and polygons. Existing methods either target a single geo-entity type or, like Poly2Vec, decompose entities into simpler components to enable Fourier transformation, introducing high computational cost. Moreover, since the transformed space lacks geometric alignment, these methods rely on uniform, non-adaptive sampling, which blurs fine-grained features like edges and boundaries. To address these limitations, we introduce Geo2Vec, a novel method inspired by signed distance fields (SDF) that operates directly in the original space. Geo2Vec adaptively samples points and encodes their signed distances (positive outside, negative inside), capturing geometry without decomposition. A neural network trained to approximate the SDF produces compact, geometry-aware, and unified representations for all geo-entity types. Additionally, we propose a rotation-invariant positional encoding to model high-frequency spatial variations and construct a structured and robust embedding space for downstream GeoAI models. Empirical results show that Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieving greater efficiency in real-world GeoAI applications. Code and Data can be found at: https://github.com/chuchen2017/GeoNeuralRepresentation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision",
    "url": "http://arxiv.org/abs/2508.03745v1",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Maosheng Hu"
    ],
    "published": "2025-08-01",
    "abstract": "Recent interest in geospatial artificial intelligence (GeoAI) has fostered a wide range of applications using artificial intelligence (AI), especially deep learning, for geospatial problem solving. However, major challenges such as a lack of training data and the neglect of spatial principles and spatial effects in AI model design remain, significantly hindering the in-depth integration of AI with geospatial research. This paper reports our work in developing a deep learning model that enables object detection, particularly of natural features, in a weakly supervised manner. Our work makes three contributions: First, we present a method of object detection using only weak labels. This is achieved by developing a spatially explicit model based on Tobler's first law of geography. Second, we incorporate attention maps into the object detection pipeline and develop a multistage training strategy to improve performance. Third, we apply this model to detect impact craters on Mars, a task that previously required extensive manual effort. The model generalizes to both natural and human-made features on the surfaces of Earth and other planets. This research advances the theoretical and methodological foundations of GeoAI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Partitioning of Eddy Covariance Footprint Evapotranspiration Using Field Data, UAS Observations and GeoAI in the U.S. Chihuahuan Desert",
    "url": "http://arxiv.org/abs/2507.14829v1",
    "authors": [
      "Habibur R. Howlider",
      "Hernan A. Moreno",
      "Marguerite E. Mauritz",
      "Stephanie N. Marquez"
    ],
    "published": "2025-07-20",
    "abstract": "This study proposes a new method for computing transpiration across an eddy covariance footprint using field observations of plant sap flow, phytomorphology sampling, uncrewed aerial system (UAS), deep learning-based digital image processing, and eddy covariance micrometeorological measurements. The method is applied to the Jornada Experimental Range, New Mexico, where we address three key questions: (1) What are the daily summer transpiration rates of Mesquite (Prosopis glandulosa) and Creosote (Larrea tridentata) individuals, and how do these species contribute to footprint-scale evapotranspiration? (2) How can the plant-level measurements be integrated for terrain-wide transpiration estimates? (3) What is the contribution of transpiration to total evapotranspiration within the eddy covariance footprint? Data collected from June to October 2022, during the North American Monsoon season, include hourly evapotranspiration and precipitation rates from the Ameriflux eddy covariance system (US Jo-1 Bajada site) and sap flux rates from heat-balance sensors. We used plant biometric measurements and supervised classification of multispectral imagery to upscale from the patch to footprint-scale estimations. A proportional relationship between the plant's horizontal projected area and the estimated number of water flow conduits was extended to the eddy covariance footprint via UAS data. Our results show that Mesquite's average daily summer transpiration is 2.84 mm/d, while Creosote's is 1.78 mm/d (a ratio of 1.6:1). The summer footprint integrated transpiration to evapotranspiration ratio (T/ET) was 0.50, decreasing to 0.44 during dry spells and increasing to 0.63 following significant precipitation. Further testing of this method is needed in different regions to validate its applicability. With appropriate adjustments, it could be relevant for other areas with similar ecological conditions.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "CLIP the Landscape: Automated Tagging of Crowdsourced Landscape Images",
    "url": "http://arxiv.org/abs/2506.12214v1",
    "authors": [
      "Ilya Ilyankou",
      "Natchapon Jongwiriyanurak",
      "Tao Cheng",
      "James Haworth"
    ],
    "published": "2025-06-13",
    "abstract": "We present a CLIP-based, multi-modal, multi-label classifier for predicting geographical context tags from landscape photos in the Geograph dataset--a crowdsourced image archive spanning the British Isles, including remote regions lacking POIs and street-level imagery. Our approach addresses a Kaggle competition\\footnote{https://www.kaggle.com/competitions/predict-geographic-context-from-landscape-photos} task based on a subset of Geograph's 8M images, with strict evaluation: exact match accuracy is required across 49 possible tags. We show that combining location and title embeddings with image features improves accuracy over using image embeddings alone. We release a lightweight pipeline\\footnote{https://github.com/SpaceTimeLab/ClipTheLandscape} that trains on a modest laptop, using pre-trained CLIP image and text embeddings and a simple classification head. Predicted tags can support downstream tasks such as building location embedders for GeoAI applications, enriching spatial understanding in data-sparse regions.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "From Bias to Accountability: How the EU AI Act Confronts Challenges in European GeoAI Auditing",
    "url": "http://arxiv.org/abs/2505.18236v1",
    "authors": [
      "Natalia Matuszczyk",
      "Craig R. Barnes",
      "Rohit Gupta",
      "Bulent Ozel",
      "Aniket Mitra"
    ],
    "published": "2025-05-23",
    "abstract": "Bias in geospatial artificial intelligence (GeoAI) models has been documented, yet the evidence is scattered across narrowly focused studies. We synthesize this fragmented literature to provide a concise overview of bias in GeoAI and examine how the EU's Artificial Intelligence Act (EU AI Act) shapes audit obligations. We discuss recurring bias mechanisms, including representation, algorithmic and aggregation bias, and map them to specific provisions of the EU AI Act. By applying the Act's high-risk criteria, we demonstrate that widely deployed GeoAI applications qualify as high-risk systems. We then present examples of recent audits along with an outline of practical methods for detecting bias. As far as we know, this study represents the first integration of GeoAI bias evidence into the EU AI Act context, by identifying high-risk GeoAI systems and mapping bias mechanisms to the Act's Articles. Although the analysis is exploratory, it suggests that even well-curated European datasets should employ routine bias audits before 2027, when the AI Act's high-risk provisions take full effect.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw",
    "url": "http://arxiv.org/abs/2504.17822v1",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Sizhe Wang",
      "Zhining Gu",
      "Yili Yang",
      "Brendan M. Rogers",
      "Anna Liljedahl"
    ],
    "published": "2025-04-23",
    "abstract": "Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost landforms with significant environmental impacts. Mapping these RTS is crucial because their appearance serves as a clear indication of permafrost thaw. However, their small scale compared to other landform features, vague boundaries, and spatiotemporal variation pose significant challenges for accurate detection. In this paper, we employed a state-of-the-art deep learning model, the Cascade Mask R-CNN with a multi-scale vision transformer-based backbone, to delineate RTS features across the Arctic. Two new strategies were introduced to optimize multimodal learning and enhance the model's predictive performance: (1) a feature-level, residual cross-modality attention fusion strategy, which effectively integrates feature maps from multiple modalities to capture complementary information and improve the model's ability to understand complex patterns and relationships within the data; (2) pre-trained unimodal learning followed by multimodal fine-tuning to alleviate high computing demand while achieving strong model performance. Experimental results demonstrated that our approach outperformed existing models adopting data-level fusion, feature-level convolutional fusion, and various attention fusion strategies, providing valuable insights into the efficient utilization of multimodal data for RTS mapping. This research contributes to our understanding of permafrost landforms and their environmental implications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "CNN",
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
      "Transformer"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
<<<<<<< HEAD
    "title": "Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain",
    "url": "http://arxiv.org/abs/2512.08657v1",
    "authors": [
      "Renato Cordeiro Ferreira",
      "Aditya Dhinavahi",
      "Rowanne Trapmann",
      "Willem-Jan van den Heuvel"
    ],
    "published": "2025-12-09",
    "abstract": "ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ]
  },
  {
    "title": "Khalasi: Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields",
    "url": "http://arxiv.org/abs/2512.06912v2",
    "authors": [
      "Rushiraj Gadhvi",
      "Sandeep Manjanna"
    ],
    "published": "2025-12-07",
    "abstract": "For centuries, khalasi (Gujarati for sailor) have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ]
  },
  {
    "title": "An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence",
    "url": "http://arxiv.org/abs/2512.09670v1",
    "authors": [
      "Gil Weissman",
      "Amir Ivry",
      "Israel Cohen"
    ],
    "published": "2025-12-10",
    "abstract": "The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ]
  },
  {
    "title": "Spatio-temporal Shared-Field Modeling of Beluga and Bowhead Whale Sightings Using a Joint Marked Log-Gaussian Cox Process",
    "url": "http://arxiv.org/abs/2512.06450v1",
    "authors": [
      "Mauli Pant",
      "Linda Fernandez",
      "Indranil Sahoo"
    ],
    "published": "2025-12-06",
    "abstract": "We analyze a decade of aerial survey whale sighting data (2010-2019) to model the spatio-temporal distributions and group sizes of beluga (Delphinapterus leucas) and bowhead (Balaena mysticetus) whales in the United States Arctic. To jointly model these species, we develop a multi-species Log-Gaussian Cox Process (LGCP) in which species specific intensity surfaces are linked through a shared latent spatial Gaussian field. This structure allows the model to capture broad spatial patterns common to both species while still accommodating species level responses to environmental covariates and seasonal variation. The latent field is represented using the Stochastic Partial Differential Equation (SPDE) approach with an anisotropic Matern covariance, implemented on an ocean constrained triangulated mesh so that spatial dependence aligns with marine geography. Whale group size is incorporated through a marked point process extension with species specific negative binomial marks, allowing occurrence and group sizes to be jointly analyzed within a unified framework. Inference is carried out using the Integrated Nested Laplace Approximation (INLA), enabling efficient model fitting over a decade of survey effort. The results highlight persistent multi-species hotspots and distinct environmental associations for each species, demonstrating the value of shared field LGCPs for joint species distribution modeling in data sparse and heterogeneous survey settings.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": []
=======
    "title": "Geospatial Artificial Intelligence for Satellite-Based Flood Extent Mapping: Concepts, Advances, and Future Perspectives",
    "url": "http://arxiv.org/abs/2504.02214v3",
    "authors": [
      "Hyunho Lee",
      "Wenwen Li"
    ],
    "published": "2025-04-03",
    "abstract": "Geospatial Artificial Intelligence (GeoAI) for satellite-based flood extent mapping systematically integrates artificial intelligence techniques with satellite data to identify flood events and assess their impacts, for disaster management and spatial decision-making. The primary output often includes flood extent maps, which delineate the affected areas, along with additional analytical outputs such as uncertainty estimation and change detection.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "RegionGCN: Spatial-Heterogeneity-Aware Graph Convolutional Networks",
    "url": "http://arxiv.org/abs/2501.17599v2",
    "authors": [
      "Hao Guo",
      "Han Wang",
      "Di Zhu",
      "Lun Wu",
      "A. Stewart Fotheringham",
      "Yu Liu"
    ],
    "published": "2025-01-29",
    "abstract": "Modeling spatial heterogeneity in the data generation process is essential for understanding and predicting geographical phenomena. Despite their prevalence in geospatial tasks, neural network models usually assume spatial stationarity, which could limit their performance in the presence of spatial process heterogeneity. By allowing model parameters to vary over space, several approaches have been proposed to incorporate spatial heterogeneity into neural networks. However, current geographically weighting approaches are ineffective on graph neural networks, yielding no significant improvement in prediction accuracy. We assume the crux lies in the over-fitting risk brought by a large number of local parameters. Accordingly, we propose to model spatial process heterogeneity at the regional level rather than at the individual level, which largely reduces the number of spatially varying parameters. We further develop a heuristic optimization procedure to learn the region partition adaptively in the process of model training. Our proposed spatial-heterogeneity-aware graph convolutional network, named RegionGCN, is applied to the spatial prediction of county-level vote share in the 2016 US presidential election based on socioeconomic attributes. Results show that RegionGCN achieves significant improvement over the basic and geographically weighted GCNs. We also offer an exploratory analysis tool for the spatial variation of non-linear relationships through ensemble learning of regional partitions from RegionGCN. Our work contributes to the practice of Geospatial Artificial Intelligence (GeoAI) in tackling spatial heterogeneity.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Human-centered Geospatial Data Science",
    "url": "http://arxiv.org/abs/2501.05595v1",
    "authors": [
      "Yuhao Kang"
    ],
    "published": "2025-01-09",
    "abstract": "This entry provides an overview of Human-centered Geospatial Data Science, highlighting the gaps it aims to bridge, its significance, and its key topics and research. Geospatial Data Science, which derives geographic knowledge and insights from large volumes of geospatial big data using advanced Geospatial Artificial Intelligence (GeoAI), has been widely used to tackle a wide range of geographic problems. However, it often overlooks the subjective human experiences that fundamentally influence human-environment interactions, and few strategies have been developed to ensure that these technologies follow ethical guidelines and prioritize human values. Human-centered Geospatial Data Science advocates for two primary focuses. First, it advances our understanding of human-environment interactions by leveraging Geospatial Data Science to measure and analyze human subjective experiences at place including emotion, perception, cognition, and creativity. Second, it advocates for the development of responsible and ethical Geospatial Data Science methods that protect geoprivacy, enhance fairness and reduce bias, and improve the explainability and transparency of geospatial technologies. With these two missions, Human-centered Geospatial Data Sciences brings a fresh perspective to develop and utilize geospatial technologies that positively impact society and benefit human well-being and the humanities.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "A comprehensive GeoAI review: Progress, Challenges and Outlooks",
    "url": "http://arxiv.org/abs/2412.11643v1",
    "authors": [
      "Anasse Boutayeb",
      "Iyad Lahsen-cherif",
      "Ahmed El Khadimi"
    ],
    "published": "2024-12-16",
    "abstract": "In recent years, Geospatial Artificial Intelligence (GeoAI) has gained traction in the most relevant research works and industrial applications, while also becoming involved in various fields of use. This paper offers a comprehensive review of GeoAI as a synergistic concept applying Artificial Intelligence (AI) methods and models to geospatial data. A preliminary study is carried out, identifying the methodology of the work, the research motivations, the issues and the directions to be tracked, followed by exploring how GeoAI can be used in various interesting fields of application, such as precision agriculture, environmental monitoring, disaster management and urban planning. Next, a statistical and semantic analysis is carried out, followed by a clear and precise presentation of the challenges facing GeoAI. Then, a concrete exploration of the future prospects is provided, based on several informations gathered during the census. To sum up, this paper provides a complete overview of the correlation between AI and the geospatial domain, while mentioning the researches conducted in this context, and emphasizing the close relationship linking GeoAI with other advanced concepts such as geographic information systems (GIS) and large-scale geospatial data, known as big geodata. This will enable researchers and scientific community to assess the state of progress in this promising field, and will help other interested parties to gain a better understanding of the issues involved.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "GeoConformal prediction: a model-agnostic framework of measuring the uncertainty of spatial prediction",
    "url": "http://arxiv.org/abs/2412.08661v3",
    "authors": [
      "Xiayin Lou",
      "Peng Luo",
      "Liqiu Meng"
    ],
    "published": "2024-12-05",
    "abstract": "Spatial prediction is a fundamental task in geography. In recent years, with advances in geospatial artificial intelligence (GeoAI), numerous models have been developed to improve the accuracy of geographic variable predictions. Beyond achieving higher accuracy, it is equally important to obtain predictions with uncertainty measures to enhance model credibility and support responsible spatial prediction. Although geostatistic methods like Kriging offer some level of uncertainty assessment, such as Kriging variance, these measurements are not always accurate and lack general applicability to other spatial models. To address this issue, we propose a model-agnostic uncertainty assessment method called GeoConformal Prediction, which incorporates geographical weighting into conformal prediction. We applied it to two classic spatial prediction cases, spatial regression and spatial interpolation, to evaluate its reliability. First, in the spatial regression case, we used XGBoost to predict housing prices, followed by GeoConformal to calculate uncertainty. Our results show that GeoConformal achieved a coverage rate of 93.67%, while Bootstrap methods only reached a maximum coverage of 81.00% after 2000 runs. Next, we applied GeoConformal to spatial interpolation models. We found that the uncertainty obtained from GeoConformal aligned closely with the variance in Kriging. Finally, using GeoConformal, we analyzed the sources of uncertainty in spatial prediction. We found that explicitly including local features in AI models can significantly reduce prediction uncertainty, especially in areas with strong local dependence. Our findings suggest that GeoConformal holds potential not only for geographic knowledge discovery but also for guiding the design of future GeoAI models, paving the way for more reliable and interpretable spatial prediction frameworks.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ]
  },
  {
    "title": "GeoAI-Enhanced Community Detection on Spatial Networks with Graph Deep Learning",
    "url": "http://arxiv.org/abs/2411.15428v1",
    "authors": [
      "Yunlei Liang",
      "Jiawei Zhu",
      "Wen Ye",
      "Song Gao"
    ],
    "published": "2024-11-23",
    "abstract": "Spatial networks are useful for modeling geographic phenomena where spatial interaction plays an important role. To analyze the spatial networks and their internal structures, graph-based methods such as community detection have been widely used. Community detection aims to extract strongly connected components from the network and reveal the hidden relationships between nodes, but they usually do not involve the attribute information. To consider edge-based interactions and node attributes together, this study proposed a family of GeoAI-enhanced unsupervised community detection methods called region2vec based on Graph Attention Networks (GAT) and Graph Convolutional Networks (GCN). The region2vec methods generate node neural embeddings based on attribute similarity, geographic adjacency and spatial interactions, and then extract network communities based on node embeddings using agglomerative clustering. The proposed GeoAI-based methods are compared with multiple baselines and perform the best when one wants to maximize node attribute similarity and spatial interaction intensity simultaneously within the spatial network communities. It is further applied in the shortage area delineation problem in public health and demonstrates its promise in regionalization problems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "Enhancing GeoAI and location encoding with spatial point pattern statistics: A Case Study of Terrain Feature Classification",
    "url": "http://arxiv.org/abs/2411.14560v1",
    "authors": [
      "Sizhe Wang",
      "Wenwen Li"
    ],
    "published": "2024-11-21",
    "abstract": "This study introduces a novel approach to terrain feature classification by incorporating spatial point pattern statistics into deep learning models. Inspired by the concept of location encoding, which aims to capture location characteristics to enhance GeoAI decision-making capabilities, we improve the GeoAI model by a knowledge driven approach to integrate both first-order and second-order effects of point patterns. This paper investigates how these spatial contexts impact the accuracy of terrain feature predictions. The results show that incorporating spatial point pattern statistics notably enhances model performance by leveraging different representations of spatial relationships.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "GeOT: A spatially explicit framework for evaluating spatio-temporal predictions",
    "url": "http://arxiv.org/abs/2410.11709v3",
    "authors": [
      "Nina Wiedemann",
      "Th\u00e9o Uscidda",
      "Martin Raubal"
    ],
    "published": "2024-10-15",
    "abstract": "When predicting observations across space and time, the spatial layout of errors impacts a model's real-world utility. For instance, in bike sharing demand prediction, error patterns translate to relocation costs. However, commonly used error metrics in GeoAI evaluate predictions point-wise, neglecting effects such as spatial heterogeneity, autocorrelation, and the Modifiable Areal Unit Problem. We put forward Optimal Transport (OT) as a spatial evaluation metric and loss function. The proposed framework, called GeOT, assesses the performance of prediction models by quantifying the transport costs associated with their prediction errors. Through experiments on real and synthetic data, we demonstrate that 1) the spatial distribution of prediction errors relates to real-world costs in many applications, 2) OT captures these spatial costs more accurately than existing metrics, and 3) OT enhances comparability across spatial and temporal scales. Finally, we advocate for leveraging OT as a loss function in neural networks to improve the spatial accuracy of predictions. Experiments with bike sharing, charging station, and traffic datasets show that spatial costs are significantly reduced with only marginal changes to non-spatial error metrics. Thus, this approach not only offers a spatially explicit tool for model evaluation and selection, but also integrates spatial considerations into model training. All code is available at https://github.com/mie-lab/geospatialOT.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Encoding Agent Trajectories as Representations with Sequence Transformers",
    "url": "http://arxiv.org/abs/2410.09204v1",
    "authors": [
      "Athanasios Tsiligkaridis",
      "Nicholas Kalinowski",
      "Zhongheng Li",
      "Elizabeth Hou"
    ],
    "published": "2024-10-11",
    "abstract": "Spatiotemporal data faces many analogous challenges to natural language text including the ordering of locations (words) in a sequence, long range dependencies between locations, and locations having multiple meanings. In this work, we propose a novel model for representing high dimensional spatiotemporal trajectories as sequences of discrete locations and encoding them with a Transformer-based neural network architecture. Similar to language models, our Sequence Transformer for Agent Representation Encodings (STARE) model can learn representations and structure in trajectory data through both supervisory tasks (e.g., classification), and self-supervisory tasks (e.g., masked modelling). We present experimental results on various synthetic and real trajectory datasets and show that our proposed model can learn meaningful encodings that are useful for many downstream tasks including discriminating between labels and indicating similarity between locations. Using these encodings, we also learn relationships between agents and locations present in spatiotemporal data.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "GeoAI in resource-constrained environments",
    "url": "http://arxiv.org/abs/2408.17361v1",
    "authors": [
      "Marc B\u00f6hlen",
      "Gede Sughiarta",
      "Atiek Kurnianingsih",
      "Srikar Reddy Gopaladinne",
      "Sujay Shrivastava",
      "Hemanth Kumar Reddy Gorla"
    ],
    "published": "2024-08-30",
    "abstract": "This paper describes spatially aware Artificial Intelligence, GeoAI, tailored for small organizations such as NGOs in resource constrained contexts where access to large datasets, expensive compute infrastructure and AI expertise may be restricted. We furthermore consider future scenarios in which resource-intensive, large geospatial models may homogenize the representation of complex landscapes, and suggest strategies to prepare for this condition.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications",
    "url": "http://arxiv.org/abs/2408.14806v2",
    "authors": [
      "Maria Despoina Siampou",
      "Jialiang Li",
      "John Krumm",
      "Cyrus Shahabi",
      "Hua Lu"
    ],
    "published": "2024-08-27",
    "abstract": "Encoding geospatial objects is fundamental for geospatial artificial intelligence (GeoAI) applications, which leverage machine learning (ML) models to analyze spatial information. Common approaches transform each object into known formats, like image and text, for compatibility with ML models. However, this process often discards crucial spatial information, such as the object's position relative to the entire space, reducing downstream task effectiveness. Alternative encoding methods that preserve some spatial properties are often devised for specific data objects (e.g., point encoders), making them unsuitable for tasks that involve different data types (i.e., points, polylines, and polygons). To this end, we propose Poly2Vec, a polymorphic Fourier-based encoding approach that unifies the representation of geospatial objects, while preserving the essential spatial properties. Poly2Vec incorporates a learned fusion module that adaptively integrates the magnitude and phase of the Fourier transform for different tasks and geometries. We evaluate Poly2Vec on five diverse tasks, organized into two categories. The first empirically demonstrates that Poly2Vec consistently outperforms object-specific baselines in preserving three key spatial relationships: topology, direction, and distance. The second shows that integrating Poly2Vec into a state-of-the-art GeoAI workflow improves the performance in two popular tasks: population prediction and land use inference.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Cross-View Geolocalization and Disaster Mapping with Street-View and VHR Satellite Imagery: A Case Study of Hurricane IAN",
    "url": "http://arxiv.org/abs/2408.06761v1",
    "authors": [
      "Hao Li",
      "Fabian Deuser",
      "Wenping Yina",
      "Xuanshu Luo",
      "Paul Walther",
      "Gengchen Mai",
      "Wei Huang",
      "Martin Werner"
    ],
    "published": "2024-08-13",
    "abstract": "Nature disasters play a key role in shaping human-urban infrastructure interactions. Effective and efficient response to natural disasters is essential for building resilience and a sustainable urban environment. Two types of information are usually the most necessary and difficult to gather in disaster response. The first information is about disaster damage perception, which shows how badly people think that urban infrastructure has been damaged. The second information is geolocation awareness, which means how people whereabouts are made available. In this paper, we proposed a novel disaster mapping framework, namely CVDisaster, aiming at simultaneously addressing geolocalization and damage perception estimation using cross-view Street-View Imagery (SVI) and Very High-Resolution satellite imagery. CVDisaster consists of two cross-view models, where CVDisaster-Geoloc refers to a cross-view geolocalization model based on a contrastive learning objective with a Siamese ConvNeXt image encoder, and CVDisaster-Est is a cross-view classification model based on a Couple Global Context Vision Transformer (CGCViT). Taking Hurricane IAN as a case study, we evaluate the CVDisaster framework by creating a novel cross-view dataset (CVIAN) and conducting extensive experiments. As a result, we show that CVDisaster can achieve highly competitive performance (over 80% for geolocalization and 75% for damage perception estimation) with even limited fine-tuning efforts, which largely motivates future cross-view models and applications within a broader GeoAI research community. The data and code are publicly available at: https://github.com/tum-bgd/CVDisaster.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "CMAB: A First National-Scale Multi-Attribute Building Dataset in China Derived from Open Source Data and GeoAI",
    "url": "http://arxiv.org/abs/2408.05891v3",
    "authors": [
      "Yecheng Zhang",
      "Huimin Zhao",
      "Ying Long"
    ],
    "published": "2024-08-12",
    "abstract": "Rapidly acquiring three-dimensional (3D) building data, including geometric attributes like rooftop, height and orientations, as well as indicative attributes like function, quality, and age, is essential for accurate urban analysis, simulations, and policy updates. Current building datasets suffer from incomplete coverage of building multi-attributes. This paper introduces a geospatial artificial intelligence (GeoAI) framework for large-scale building modeling, presenting the first national-scale Multi-Attribute Building dataset (CMAB), covering 3,667 spatial cities, 29 million buildings, and 21.3 billion square meters of rooftops with an F1-Score of 89.93% in OCRNet-based extraction, totaling 337.7 billion cubic meters of building stock. We trained bootstrap aggregated XGBoost models with city administrative classifications, incorporating features such as morphology, location, and function. Using multi-source data, including billions of high-resolution Google Earth images and 60 million street view images (SVIs), we generated rooftop, height, function, age, and quality attributes for each building. Accuracy was validated through model benchmarks, existing similar products, and manual SVI validation, mostly above 80%. Our dataset and results are crucial for global SDGs and urban planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "Partial Label Learning with Focal Loss for Sea Ice Classification Based on Ice Charts",
    "url": "http://arxiv.org/abs/2406.03645v2",
    "authors": [
      "Behzad Vahedi",
      "Benjamin Lucas",
      "Farnoush Banaei-Kashani",
      "Andrew P. Barrett",
      "Walter N. Meier",
      "Siri Jodha Khalsa",
      "Morteza Karimzadeh"
    ],
    "published": "2024-06-05",
    "abstract": "Sea ice, crucial to the Arctic and Earth's climate, requires consistent monitoring and high-resolution mapping. Manual sea ice mapping, however, is time-consuming and subjective, prompting the need for automated deep learning-based classification approaches. However, training these algorithms is challenging because expert-generated ice charts, commonly used as training data, do not map single ice types but instead map polygons with multiple ice types. Moreover, the distribution of various ice types in these charts is frequently imbalanced, resulting in a performance bias towards the dominant class. In this paper, we present a novel GeoAI approach to training sea ice classification by formalizing it as a partial label learning task with explicit confidence scores to address multiple labels and class imbalance. We treat the polygon-level labels as candidate partial labels, assign the corresponding ice concentrations as confidence scores to each candidate label, and integrate them with focal loss to train a Convolutional Neural Network (CNN). Our proposed approach leads to enhanced performance for sea ice classification in Sentinel-1 dual-polarized SAR images, improving classification accuracy (from 87% to 92%) and weighted average F-1 score (from 90% to 93%) compared to the conventional training approach of using one-hot encoded labels and Categorical Cross-Entropy loss. It also improves the F-1 score in 4 out of the 6 sea ice classes.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "GeoAI Reproducibility and Replicability: a computational and spatial perspective",
    "url": "http://arxiv.org/abs/2404.10108v2",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Sizhe Wang",
      "Peter Kedron"
    ],
    "published": "2024-04-15",
    "abstract": "GeoAI has emerged as an exciting interdisciplinary research area that combines spatial theories and data with cutting-edge AI models to address geospatial problems in a novel, data-driven manner. While GeoAI research has flourished in the GIScience literature, its reproducibility and replicability (R&R), fundamental principles that determine the reusability, reliability, and scientific rigor of research findings, have rarely been discussed. This paper aims to provide an in-depth analysis of this topic from both computational and spatial perspectives. We first categorize the major goals for reproducing GeoAI research, namely, validation (repeatability), learning and adapting the method for solving a similar or new problem (reproducibility), and examining the generalizability of the research findings (replicability). Each of these goals requires different levels of understanding of GeoAI, as well as different methods to ensure its success. We then discuss the factors that may cause the lack of R&R in GeoAI research, with an emphasis on (1) the selection and use of training data; (2) the uncertainty that resides in the GeoAI model design, training, deployment, and inference processes; and more importantly (3) the inherent spatial heterogeneity of geospatial data and processes. We use a deep learning-based image analysis task as an example to demonstrate the results' uncertainty and spatial variance caused by different factors. The findings reiterate the importance of knowledge sharing, as well as the generation of a \"replicability map\" that incorporates spatial autocorrelation and spatial heterogeneity into consideration in quantifying the spatial replicability of GeoAI research.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Cross-Modal Learning of Housing Quality in Amsterdam",
    "url": "http://arxiv.org/abs/2403.08915v1",
    "authors": [
      "Alex Levering",
      "Diego Marcos",
      "Devis Tuia"
    ],
    "published": "2024-03-13",
    "abstract": "In our research we test data and models for the recognition of housing quality in the city of Amsterdam from ground-level and aerial imagery. For ground-level images we compare Google StreetView (GSV) to Flickr images. Our results show that GSV predicts the most accurate building quality scores, approximately 30% better than using only aerial images. However, we find that through careful filtering and by using the right pre-trained model, Flickr image features combined with aerial image features are able to halve the performance gap to GSV features from 30% to 15%. Our results indicate that there are viable alternatives to GSV for liveability factor prediction, which is encouraging as GSV images are more difficult to acquire and not always available.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Recognition",
      "Forecast"
    ]
>>>>>>> 6b6cbdc (Improve scraper: sorting, recent papers flag)
  }
]