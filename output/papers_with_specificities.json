[
  {
    "title": "Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis",
    "url": "http://arxiv.org/abs/2512.09576v1",
    "authors": [
      "David Seu",
      "Nicolas Longepe",
      "Gabriel Cioltea",
      "Erik Maidik",
      "Calin Andrei"
    ],
    "published": "2025-12-10",
    "abstract": "Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model",
    "url": "http://arxiv.org/abs/2512.09251v1",
    "authors": [
      "Lalit Maurya",
      "Saurabh Kaushik",
      "Beth Tellman"
    ],
    "published": "2025-12-10",
    "abstract": "Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\\textbf{G}lacial \\textbf{LA}ke segmentation with \\textbf{C}ontextual \\textbf{I}nstance \\textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening",
    "url": "http://arxiv.org/abs/2512.02643v1",
    "authors": [
      "Yongchuan Cui",
      "Peng Liu",
      "Yi Zeng"
    ],
    "published": "2025-12-02",
    "abstract": "Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts",
    "url": "http://arxiv.org/abs/2512.02517v1",
    "authors": [
      "Jiaqi Liu",
      "Ronghao Fu",
      "Lang Sun",
      "Haoran Liu",
      "Xiao Yang",
      "Weipeng Zhang",
      "Xu Na",
      "Zhuoran Duan",
      "Bo Yang"
    ],
    "published": "2025-12-02",
    "abstract": "The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2512.01665v1",
    "authors": [
      "Zhicheng Zhao",
      "Yin Huang",
      "Lingma Sun",
      "Chenglong Li",
      "Jin Tang"
    ],
    "published": "2025-12-01",
    "abstract": "Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "VFM-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images",
    "url": "http://arxiv.org/abs/2512.00718v2",
    "authors": [
      "Deliang Wang",
      "Peng Liu",
      "Yan Ma",
      "Rongkai Zhuang",
      "Lajiao Chen",
      "Bing Li",
      "Yi Zeng"
    ],
    "published": "2025-11-30",
    "abstract": "Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary accuracy. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios. The codes are available at https://github.com/wondelyan/VFM-ISRefiner .",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?",
    "url": "http://arxiv.org/abs/2511.21523v2",
    "authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "St\u00e9phane May",
      "S\u00e9bastien Lef\u00e8vre"
    ],
    "published": "2025-11-26",
    "abstract": "Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs. All codes and pretrained models are available at https://github.com/pierreadorni/EoS-FM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning",
    "url": "http://arxiv.org/abs/2511.21420v1",
    "authors": [
      "Futian Wang",
      "Mengqi Wang",
      "Xiao Wang",
      "Haowen Wang",
      "Jin Tang"
    ],
    "published": "2025-11-26",
    "abstract": "Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search",
    "url": "http://arxiv.org/abs/2511.20460v2",
    "authors": [
      "Yunqi Zhou",
      "Chengjie Jiang",
      "Chun Yuan",
      "Jing Li"
    ],
    "published": "2025-11-25",
    "abstract": "With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2511.20302v2",
    "authors": [
      "Shilei Cao",
      "Ziyang Gong",
      "Hehai Lin",
      "Yang Liu",
      "Jiashun Cheng",
      "Xiaoxing Hu",
      "Haoyuan Liang",
      "Guowen Li",
      "Chengwei Qin",
      "Hong Cheng",
      "Xue Yang",
      "Juepeng Zheng",
      "Haohuan Fu"
    ],
    "published": "2025-11-25",
    "abstract": "In Remote Sensing (RS), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key approach to activate the generalizable representation ability of foundation models for downstream tasks. However, existing specialized PEFT methods often fail when applied to large-scale Earth observation tasks, as they are unable to fully handle the multifaceted and unpredictable domain gaps (\\eg, spatial, semantic, and frequency shifts) inherent in RS data. To overcome this, we propose CrossEarth-Gate, which introduces two primary contributions. First, we establish a comprehensive RS module toolbox to address multifaceted domain gaps, comprising spatial, semantic, and frequency modules. Second, we develop a Fisher-guided adaptive selection mechanism that operates on this toolbox. This selection is guided by Fisher Information to quantify each module's importance by measuring its contribution to the task-specific gradient flow. It dynamically activates only the most critical modules at the appropriate layers, guiding the gradient flow to maximize adaptation effectiveness and efficiency. Comprehensive experiments validate the efficacy and generalizability of our method, where CrossEarth-Gate achieves state-of-the-art performance across 16 cross-domain benchmarks for RS semantic segmentation. The code of the work will be released.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting",
    "url": "http://arxiv.org/abs/2511.20004v1",
    "authors": [
      "Peining Zhang",
      "Hongchen Qin",
      "Haochen Zhang",
      "Ziqi Guo",
      "Guiling Wang",
      "Jinbo Bi"
    ],
    "published": "2025-11-25",
    "abstract": "This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors",
    "url": "http://arxiv.org/abs/2511.18264v2",
    "authors": [
      "Ruijie Fan",
      "Junyan Ye",
      "Huan Chen",
      "Zilong Huang",
      "Xiaolei Wang",
      "Weijia Li"
    ],
    "published": "2025-11-23",
    "abstract": "Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
    "url": "http://arxiv.org/abs/2511.17442v1",
    "authors": [
      "Binger Chen",
      "Tacettin Emre B\u00f6k",
      "Behnood Rasti",
      "Volker Markl",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-11-21",
    "abstract": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "CORONA-Fields: Leveraging Foundation Models for Classification of Solar Wind Phenomena",
    "url": "http://arxiv.org/abs/2511.09843v1",
    "authors": [
      "Daniela Martin",
      "Jinsu Hong",
      "Connor O'Brien",
      "Valmir P Moraes Filho",
      "Jasmine R. Kobayashi",
      "Evangelia Samara",
      "Joseph Gallego"
    ],
    "published": "2025-11-13",
    "abstract": "Space weather at Earth, driven by the solar activity, poses growing risks to satellites around our planet as well as to critical ground-based technological infrastructure. Major space weather contributors are the solar wind and coronal mass ejections whose variable density, speed, temperature, and magnetic field make the automated classification of those structures challenging. In this work, we adapt a foundation model for solar physics, originally trained on Solar Dynamics Observatory imagery, to create embeddings suitable for solar wind structure analysis. These embeddings are concatenated with the spacecraft position and solar magnetic connectivity encoded using Fourier features which generates a neural field-based model. The full deep learning architecture is fine-tuned bridging the gap between remote sensing and in situ observations. Labels are derived from Parker Solar Probe measurements, forming a downstream classification task that maps plasma properties to solar wind structures. Although overall classification performance is modest, likely due to coarse labeling, class imbalance, and limited transferability of the pretrained model, this study demonstrates the feasibility of leveraging foundation model embeddings for in situ solar wind tasks. As a first proof-of-concept, it lays the groundwork for future improvements toward more reliable space weather predictions. The code and configuration files used in this study are publicly available to support reproducibility.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental Modelling",
    "url": "http://arxiv.org/abs/2511.11706v3",
    "authors": [
      "Julia Peters",
      "Karin Mora",
      "Miguel D. Mahecha",
      "Chaonan Ji",
      "David Montero",
      "Clemens Mosig",
      "Guido Kraemer"
    ],
    "published": "2025-11-12",
    "abstract": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping",
    "url": "http://arxiv.org/abs/2511.08156v1",
    "authors": [
      "Chenying Liu",
      "Wei Huang",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-11",
    "abstract": "Land Use and Land Cover (LULC) mapping is a fundamental task in Earth Observation (EO). However, current LULC models are typically developed for a specific modality and a fixed class taxonomy, limiting their generability and broader applicability. Recent advances in foundation models (FMs) offer promising opportunities for building universal models. Yet, task-agnostic FMs often require fine-tuning for downstream applications, whereas task-specific FMs rely on massive amounts of labeled data for training, which is costly and impractical in the remote sensing (RS) domain. To address these challenges, we propose LandSegmenter, an LULC FM framework that resolves three-stage challenges at the input, model, and output levels. From the input side, to alleviate the heavy demand on labeled data for FM training, we introduce LAnd Segment (LAS), a large-scale, multi-modal, multi-source dataset built primarily with globally sampled weak labels from existing LULC products. LAS provides a scalable, cost-effective alternative to manual annotation, enabling large-scale FM training across diverse LULC domains. For model architecture, LandSegmenter integrates an RS-specific adapter for cross-modal feature extraction and a text encoder for semantic awareness enhancement. At the output stage, we introduce a class-wise confidence-guided fusion strategy to mitigate semantic omissions and further improve LandSegmenter's zero-shot performance. We evaluate LandSegmenter on six precisely annotated LULC datasets spanning diverse modalities and class taxonomies. Extensive transfer learning and zero-shot experiments demonstrate that LandSegmenter achieves competitive or superior performance, particularly in zero-shot settings when transferred to unseen datasets. These results highlight the efficacy of our proposed framework and the utility of weak supervision for building task-specific FMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoCrossBench: Cross-Band Generalization for Remote Sensing",
    "url": "http://arxiv.org/abs/2511.02831v1",
    "authors": [
      "Hakob Tamazyan",
      "Ani Vanyan",
      "Alvard Barseghyan",
      "Anna Khosrovyan",
      "Evan Shelhamer",
      "Hrant Khachatrian"
    ],
    "published": "2025-11-04",
    "abstract": "The number and diversity of remote sensing satellites grows over time, while the vast majority of labeled data comes from older satellites. As the foundation models for Earth observation scale up, the cost of (re-)training to support new satellites grows too, so the generalization capabilities of the models towards new satellites become increasingly important. In this work we introduce GeoCrossBench, an extension of the popular GeoBench benchmark with a new evaluation protocol: it tests the in-distribution performance; generalization to satellites with no band overlap; and generalization to satellites with additional bands with respect to the training set. We also develop a self-supervised extension of ChannelViT, ChiViT, to improve its cross-satellite performance. First, we show that even the best foundation models for remote sensing (DOFA, TerraFM) do not outperform general purpose models like DINOv3 in the in-distribution setting. Second, when generalizing to new satellites with no band overlap, all models suffer 2-4x drop in performance, and ChiViT significantly outperforms the runner-up DINOv3. Third, the performance of all tested models drops on average by 5-25\\% when given additional bands during test time. Finally, we show that fine-tuning just the last linear layer of these models using oracle labels from all bands can get relatively consistent performance across all satellites, highlighting that the benchmark is far from being saturated. We publicly release the code and the datasets to encourage the development of more future-proof remote sensing models with stronger cross-satellite generalization.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping",
    "url": "http://arxiv.org/abs/2510.27219v1",
    "authors": [
      "Renjie Ji",
      "Xue Wang",
      "Chao Niu",
      "Wen Zhang",
      "Yong Mei",
      "Kun Tan"
    ],
    "published": "2025-10-31",
    "abstract": "Hyperspectral imaging (HSI) is a vital tool for fine-grained land-use and land-cover (LULC) mapping. However, the inherent heterogeneity of HSI data has long posed a major barrier to developing generalized models via joint training. Although HSI foundation models have shown promise for different downstream tasks, the existing approaches typically overlook the critical guiding role of sensor meta-attributes, and struggle with multi-sensor training, limiting their transferability. To address these challenges, we propose SpecAware, which is a novel hyperspectral spectral-content aware foundation model for unifying multi-sensor learning for HSI mapping. We also constructed the Hyper-400K dataset to facilitate this research, which is a new large-scale, high-quality benchmark dataset with over 400k image patches from diverse airborne AVIRIS sensors. The core of SpecAware is a two-step hypernetwork-driven encoding process for HSI data. Firstly, we designed a meta-content aware module to generate a unique conditional input for each HSI patch, tailored to each spectral band of every sample by fusing the sensor meta-attributes and its own image content. Secondly, we designed the HyperEmbedding module, where a sample-conditioned hypernetwork dynamically generates a pair of matrix factors for channel-wise encoding, consisting of adaptive spatial pattern extraction and latent semantic feature re-projection. Thus, SpecAware gains the ability to perceive and interpret spatial-spectral features across diverse scenes and sensors. This, in turn, allows SpecAware to adaptively process a variable number of spectral channels, establishing a unified framework for joint pre-training. Extensive experiments on six datasets demonstrate that SpecAware can learn superior feature representations, excelling in land-cover semantic segmentation classification, change detection, and scene classification.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges",
    "url": "http://arxiv.org/abs/2510.22964v1",
    "authors": [
      "Liling Yang",
      "Ning Chen",
      "Jun Yue",
      "Yidan Liu",
      "Jiayi Ma",
      "Pedram Ghamisi",
      "Antonio Plaza",
      "Leyuan Fang"
    ],
    "published": "2025-10-27",
    "abstract": "Foundation models have transformed natural language processing and computer vision, and their impact is now reshaping remote sensing image analysis. With powerful generalization and transfer learning capabilities, they align naturally with the multimodal, multi-resolution, and multi-temporal characteristics of remote sensing data. To address unique challenges in the field, multimodal geospatial foundation models (GFMs) have emerged as a dedicated research frontier. This survey delivers a comprehensive review of multimodal GFMs from a modality-driven perspective, covering five core visual and vision-language modalities. We examine how differences in imaging physics and data representation shape interaction design, and we analyze key techniques for alignment, integration, and knowledge transfer to tackle modality heterogeneity, distribution shifts, and semantic gaps. Advances in training paradigms, architectures, and task-specific adaptation strategies are systematically assessed alongside a wealth of emerging benchmarks. Representative multimodal visual and vision-language GFMs are evaluated across ten downstream tasks, with insights into their architectures, performance, and application scenarios. Real-world case studies, spanning land cover mapping, agricultural monitoring, disaster response, climate studies, and geospatial intelligence, demonstrate the practical potential of GFMs. Finally, we outline pressing challenges in domain generalization, interpretability, efficiency, and privacy, and chart promising avenues for future research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "WaveMAE: Wavelet decomposition Masked Auto-Encoder for Remote Sensing",
    "url": "http://arxiv.org/abs/2510.22697v1",
    "authors": [
      "Vittorio Bernuzzi",
      "Leonardo Rossi",
      "Tomaso Fontanini",
      "Massimo Bertozzi",
      "Andrea Prati"
    ],
    "published": "2025-10-26",
    "abstract": "Self-supervised learning (SSL) has recently emerged as a key strategy for building foundation models in remote sensing, where the scarcity of annotated data limits the applicability of fully supervised approaches. In this work, we introduce WaveMAE, a masked autoencoding framework tailored for multispectral satellite imagery. Unlike conventional pixel-based reconstruction, WaveMAE leverages a multi-level Discrete Wavelet Transform (DWT) to disentangle frequency components and guide the encoder toward learning scale-aware high-frequency representations. We further propose a Geo-conditioned Positional Encoding (GPE), which incorporates geographical priors via Spherical Harmonics, encouraging embeddings that respect both semantic and geospatial structure. To ensure fairness in evaluation, all methods are pretrained on the same dataset (fMoW-S2) and systematically evaluated on the diverse downstream tasks of the PANGAEA benchmark, spanning semantic segmentation, regression, change detection, and multilabel classification. Extensive experiments demonstrate that WaveMAE achieves consistent improvements over prior state-of-the-art approaches, with substantial gains on segmentation and regression benchmarks. The effectiveness of WaveMAE pretraining is further demonstrated by showing that even a lightweight variant, containing only 26.4% of the parameters, achieves state-of-the-art performance. Our results establish WaveMAE as a strong and geographically informed foundation model for multispectral remote sensing imagery.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration",
    "url": "http://arxiv.org/abs/2510.17670v2",
    "authors": [
      "Yehonathan Refael",
      "Amit Aides",
      "Aviad Barzilai",
      "George Leifman",
      "Genady Beryozkin",
      "Vered Silverman",
      "Bolous Jaber",
      "Tomer Shekel"
    ],
    "published": "2025-10-20",
    "abstract": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by detecting objects from arbitrary text queries. However, their zero-shot performance in specialized domains like Remote Sensing (RS) is often compromised by the inherent ambiguity of natural language, limiting critical downstream applications. For instance, an OVD model may struggle to distinguish between fine-grained classes such as \"fishing boat\" and \"yacht\" since their embeddings are similar and often inseparable. This can hamper specific user goals, such as monitoring illegal fishing, by producing irrelevant detections. To address this, we propose a cascaded approach that couples the broad generalization of a large pre-trained OVD model with a lightweight few-shot classifier. Our method first employs the zero-shot model to generate high-recall object proposals. These proposals are then refined for high precision by a compact classifier trained in real-time on only a handful of user-annotated examples - drastically reducing the high costs of RS imagery annotation.The core of our framework is FLAME, a one-step active learning strategy that selects the most informative samples for training. FLAME identifies, on the fly, uncertain marginal candidates near the decision boundary using density estimation, followed by clustering to ensure sample diversity. This efficient sampling technique achieves high accuracy without costly full-model fine-tuning and enables instant adaptation, within less then a minute, which is significantly faster than state-of-the-art alternatives.Our method consistently surpasses state-of-the-art performance on RS benchmarks, establishing a practical and resource-efficient framework for adapting foundation models to specific user needs.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Do Satellite Tasks Need Special Pretraining?",
    "url": "http://arxiv.org/abs/2510.17014v1",
    "authors": [
      "Ani Vanyan",
      "Alvard Barseghyan",
      "Hakob Tamazyan",
      "Tigran Galstyan",
      "Vahan Huroyan",
      "Naira Hovakimyan",
      "Hrant Khachatrian"
    ],
    "published": "2025-10-19",
    "abstract": "Foundation models have advanced machine learning across various modalities, including images. Recently multiple teams trained foundation models specialized for remote sensing applications. This line of research is motivated by the distinct characteristics of remote sensing imagery, specific applications and types of robustness useful for satellite image analysis. In this work we systematically challenge the idea that specific foundation models are more useful than general-purpose vision foundation models, at least in the small scale. First, we design a simple benchmark that measures generalization of remote sensing models towards images with lower resolution for two downstream tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID, an ImageNet-scale satellite imagery dataset, with several modifications specific to remote sensing. We show that none of those pretrained models bring consistent improvements upon general-purpose baselines at the ViT-B scale.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations",
    "url": "http://arxiv.org/abs/2510.13774v1",
    "authors": [
      "Dominik J. M\u00fchlematter",
      "Lin Che",
      "Ye Hong",
      "Martin Raubal",
      "Nina Wiedemann"
    ],
    "published": "2025-10-15",
    "abstract": "Forecasting urban phenomena such as housing prices and public health indicators requires the effective integration of various geospatial data. Current methods primarily utilize task-specific models, while recent foundation models for spatial representations often support only limited modalities and lack multimodal fusion capabilities. To overcome these challenges, we present UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal Fusion (SMF). The framework employs modality-specific encoders to process different types of inputs, including street view imagery, remote sensing data, cartographic maps, and points of interest (POIs) data. These multimodal inputs are integrated via a Transformer-based fusion module that learns unified representations. An extensive evaluation across 41 tasks in 56 cities worldwide demonstrates UrbanFusion's strong generalization and predictive performance compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms prior foundation models on location-encoding, 2) allows multimodal input during inference, and 3) generalizes well to regions unseen during training. UrbanFusion can flexibly utilize any subset of available modalities for a given location during both pretraining and inference, enabling broad applicability across diverse data availability scenarios. All source code is available at https://github.com/DominikM198/UrbanFusion.",
    "categories": [
      "geo_reasoning",
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping",
    "url": "http://arxiv.org/abs/2510.11576v2",
    "authors": [
      "Walid Elbarz",
      "Mohamed Bourriz",
      "Hicham Hajji",
      "Hamd Ait Abdelali",
      "Fran\u00e7ois Bourzeix"
    ],
    "published": "2025-10-13",
    "abstract": "Foundation models are transforming Earth observation, but their potential for hyperspectral crop mapping remains underexplored. This study benchmarks three foundation models for cereal crop mapping using hyperspectral imagery: HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth dataset (a large multitemporal hyperspectral archive). Models were fine-tuned on manually labeled data from a training region and evaluated on an independent test region. Performance was measured with overall accuracy (OA), average accuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%), DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of 93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved 91%, highlighting the importance of model architecture for strong generalization across geographic regions and sensor platforms. These results provide a systematic evaluation of foundation models for operational hyperspectral crop mapping and outline directions for future model development.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework",
    "url": "http://arxiv.org/abs/2510.10084v1",
    "authors": [
      "Meijun Zhou",
      "Gang Mei",
      "Zhengjing Ma",
      "Nengxiong Xu",
      "Jianbing Peng"
    ],
    "published": "2025-10-11",
    "abstract": "Tracking the spatiotemporal evolution of large-scale landslide scars is critical for understanding the evolution mechanisms and failure precursors, enabling effective early-warning. However, most existing studies have focused on single-phase or pre- and post-failure dual-phase landslide identification. Although these approaches delineate post-failure landslide boundaries, it is challenging to track the spatiotemporal evolution of landslide scars. To address this problem, this study proposes a novel and universal framework for tracking the spatiotemporal evolution of large-scale landslide scars using a vision foundation model. The key idea behind the proposed framework is to reconstruct discrete optical remote sensing images into a continuous video sequence. This transformation enables a vision foundation model, which is developed for video segmentation, to be used for tracking the evolution of landslide scars. The proposed framework operates within a knowledge-guided, auto-propagation, and interactive refinement paradigm to ensure the continuous and accurate identification of landslide scars. The proposed framework was validated through application to two representative cases: the post-failure Baige landslide and the active Sela landslide (2017-2025). Results indicate that the proposed framework enables continuous tracking of landslide scars, capturing both failure precursors critical for early warning and post-failure evolution essential for assessing secondary hazards and long-term stability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "The View From Space: Navigating Instrumentation Differences with EOFMs",
    "url": "http://arxiv.org/abs/2510.03316v1",
    "authors": [
      "Ryan P. Demilt",
      "Nicholas LaHaye",
      "Karis Tenneson"
    ],
    "published": "2025-10-01",
    "abstract": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as tools for processing the massive volumes of remotely sensed and other earth observation data, and for delivering impact on the many essential earth monitoring tasks. An emerging trend posits using the outputs of pre-trained models as 'embeddings' which summarize high dimensional data to be used for generic tasks such as similarity search and content-specific queries. However, most EOFM models are trained only on single modalities of data and then applied or benchmarked by matching bands across different modalities. It is not clear from existing work what impact diverse sensor architectures have on the internal representations of the present suite of EOFMs. We show in this work that the representation space of EOFMs is highly sensitive to sensor architecture and that understanding this difference gives a vital perspective on the pitfalls of current EOFM design and signals for how to move forward as model developers, users, and a community guided by robust remote-sensing science.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoLink: Empowering Remote Sensing Foundation Model with OpenStreetMap Data",
    "url": "http://arxiv.org/abs/2509.26016v1",
    "authors": [
      "Lubian Bai",
      "Xiuyuan Zhang",
      "Siqi Zhang",
      "Zepeng Zhang",
      "Haoyu Wang",
      "Wei Qin",
      "Shihong Du"
    ],
    "published": "2025-09-30",
    "abstract": "Integrating ground-level geospatial data with rich geographic context, like OpenStreetMap (OSM), into remote sensing (RS) foundation models (FMs) is essential for advancing geospatial intelligence and supporting a broad spectrum of tasks. However, modality gap between RS and OSM data, including differences in data structure, content, and spatial granularity, makes effective synergy highly challenging, and most existing RS FMs focus on imagery alone. To this end, this study presents GeoLink, a multimodal framework that leverages OSM data to enhance RS FM during both the pretraining and downstream task stages. Specifically, GeoLink enhances RS self-supervised pretraining using multi-granularity learning signals derived from OSM data, guided by cross-modal spatial correlations for information interaction and collaboration. It also introduces image mask-reconstruction to enable sparse input for efficient pretraining. For downstream tasks, GeoLink generates both unimodal and multimodal fine-grained encodings to support a wide range of applications, from common RS interpretation tasks like land cover classification to more comprehensive geographic tasks like urban function zone mapping. Extensive experiments show that incorporating OSM data during pretraining enhances the performance of the RS image encoder, while fusing RS and OSM data in downstream tasks improves the FM's adaptability to complex geographic scenarios. These results underscore the potential of multimodal synergy in advancing high-level geospatial artificial intelligence. Moreover, we find that spatial correlation plays a crucial role in enabling effective multimodal geospatial data integration. Code, checkpoints, and using examples are released at https://github.com/bailubin/GeoLink_NeurIPS2025",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Environment-Aware Satellite Image Generation with Diffusion Models",
    "url": "http://arxiv.org/abs/2509.24875v1",
    "authors": [
      "Nikos Kostagiolas",
      "Pantelis Georgiades",
      "Yannis Panagakis",
      "Mihalis A. Nicolaou"
    ],
    "published": "2025-09-29",
    "abstract": "Diffusion-based foundation models have recently garnered much attention in the field of generative modeling due to their ability to generate images of high quality and fidelity. Although not straightforward, their recent application to the field of remote sensing signaled the first successful trials towards harnessing the large volume of publicly available datasets containing multimodal information. Despite their success, existing methods face considerable limitations: they rely on limited environmental context, struggle with missing or corrupted data, and often fail to reliably reflect user intentions in generated outputs. In this work, we propose a novel diffusion model conditioned on environmental context, that is able to generate satellite images by conditioning from any combination of three different control signals: a) text, b) metadata, and c) visual data. In contrast to previous works, the proposed method is i) to our knowledge, the first of its kind to condition satellite image generation on dynamic environmental conditions as part of its control signals, and ii) incorporating a metadata fusion strategy that models attribute embedding interactions to account for partially corrupt and/or missing observations. Our method outperforms previous methods both qualitatively (robustness to missing metadata, higher responsiveness to control inputs) and quantitatively (higher fidelity, accuracy, and quality of generations measured using 6 different metrics) in the trials of single-image and temporal generation. The reported results support our hypothesis that conditioning on environmental context can improve the performance of foundation models for satellite imagery, and render our model a promising candidate for usage in downstream tasks. The collected 3-modal dataset is to our knowledge, the first publicly-available dataset to combine data from these three different mediums.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ],
    "is_recent": false
  },
  {
    "title": "FUSAR-KLIP: Towards Multimodal Foundation Models for Remote Sensing",
    "url": "http://arxiv.org/abs/2509.23927v2",
    "authors": [
      "Yi Yang",
      "Xiaokun Zhang",
      "Qingchen Fang",
      "Jing Liu",
      "Ziqi Ye",
      "Rui Li",
      "Li Liu",
      "Haipeng Wang"
    ],
    "published": "2025-09-28",
    "abstract": "Cross-modal artificial intelligence has garnered widespread attention in recent years, achieving significant progress in the study of natural images. However, existing methods are mostly designed for RGB imagery, leaving a significant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with its all-day, all-weather imaging capabilities, plays an irreplaceable role in remote sensing scene understanding. To address this gap, this paper proposes FUSAR-KLIP, the first universal SAR multimodal foundational model, along with reusable data and evaluation baselines. Specifically: (1) This work introduces the critical yet long-overlooked attribute of geographic information into remote sensing research, constructing FUSAR-GEOVL-1M (the first large-scale SAR dataset with complete geographic projection properties), covering multiple satellite platforms, 120,000 images, and 135 cities. (2) Aligned structured text is generated through a hierarchical cognitive chain-of-thought (HCoT), providing more than one million multi-dimensional semantic annotations of landforms, regional functions, target attributes, and spatial relationships. (3) We design a Self-Consistent Iterative Optimization mechanism that continuously enhances cross-modal alignment through a self-supervised closed loop of contrastive, matching, and reconstruction learning on a transferable multimodal encoder. (4) A unified evaluation benchmark is established across 11 representative downstream vision and vision-language tasks, with comparisons against 14 leading foundation models, where FUSAR-KLIP demonstrates leading performance, particularly in object counting and land-cover classification. We expect that FUSAR-KLIP's large-scale multimodal data, transferable model architecture, and comprehensive experimental benchmark will significantly advance the development of SAR multimodal baseline models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation",
    "url": "http://arxiv.org/abs/2509.21894v1",
    "authors": [
      "Yixiao Liu",
      "Yizhou Yang",
      "Jinwen Li",
      "Jun Tao",
      "Ruoyu Li",
      "Xiangkun Wang",
      "Min Zhu",
      "Junlong Cheng"
    ],
    "published": "2025-09-26",
    "abstract": "Remote Sensing Change Detection (RSCD) typically identifies changes in land cover or surface conditions by analyzing multi-temporal images. Currently, most deep learning-based methods primarily focus on learning unimodal visual information, while neglecting the rich semantic information provided by multimodal data such as text. To address this limitation, we propose a novel Language-Guided Change Detection model (LG-CD). This model leverages natural language prompts to direct the network's attention to regions of interest, significantly improving the accuracy and robustness of change detection. Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature extractor to capture multi-scale pyramid features from high-resolution to low-resolution across bi-temporal remote sensing images. Subsequently, multi-layer adapters are employed to fine-tune the model for downstream tasks, ensuring its effectiveness in remote sensing change detection. Additionally, we design a Text Fusion Attention Module (TFAM) to align visual and textual information, enabling the model to focus on target change regions using text prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented, which deeply integrates visual and semantic information through a cross-attention mechanism to produce highly accurate change detection masks. Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate that LG-CD consistently outperforms state-of-the-art change detection methods. Furthermore, our approach provides new insights into achieving generalized change detection by leveraging multimodal information.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "A Sentinel-3 foundation model for ocean colour",
    "url": "http://arxiv.org/abs/2509.21273v1",
    "authors": [
      "Geoffrey Dawson",
      "Remy Vandaele",
      "Andrew Taylor",
      "David Moffat",
      "Helen Tamura-Wicks",
      "Sarah Jackson",
      "Rosie Lickorish",
      "Paolo Fraccaro",
      "Hywel Williams",
      "Chunbo Luo",
      "Anne Jones"
    ],
    "published": "2025-09-25",
    "abstract": "Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive unlabelled datasets, have the potential to drastically change AI applications in ocean science, where labelled data are often sparse and expensive to collect. In this work, we describe a new foundation model using the Prithvi-EO Vision Transformer architecture which has been pre-trained to reconstruct data from the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the model by fine-tuning on two downstream marine earth observation tasks. We first assess model performance compared to current baseline models used to quantify chlorophyll concentration. We then evaluate the FMs ability to refine remote sensing-based estimates of ocean primary production. Our results demonstrate the utility of self-trained FMs for marine monitoring, in particular for making use of small amounts of high quality labelled data and in capturing detailed spatial patterns of ocean colour whilst matching point observations. We conclude that this new generation of geospatial AI models has the potential to provide more robust, data-driven insights into ocean ecosystems and their role in global climate processes.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2509.18711v2",
    "authors": [
      "Ke Li",
      "Di Wang",
      "Ting Wang",
      "Fuyu Dong",
      "Yiming Zhang",
      "Luyao Zhang",
      "Xiangyu Wang",
      "Shaofeng Li",
      "Quan Wang"
    ],
    "published": "2025-09-23",
    "abstract": "Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Visual Instruction Pretraining for Domain-Specific Foundation Models",
    "url": "http://arxiv.org/abs/2509.17562v2",
    "authors": [
      "Yuxuan Li",
      "Yicheng Zhang",
      "Wenhao Tang",
      "Yimian Dai",
      "Ming-Ming Cheng",
      "Xiang Li",
      "Jian Yang"
    ],
    "published": "2025-09-22",
    "abstract": "Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at https://github.com/zcablii/ViTP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation",
    "url": "http://arxiv.org/abs/2509.15795v1",
    "authors": [
      "Tianyang Wang",
      "Xi Xiao",
      "Gaofei Chen",
      "Hanzhang Chi",
      "Qi Zhang",
      "Guo Cheng",
      "Yingrui Ji"
    ],
    "published": "2025-09-19",
    "abstract": "Segment Anything Model (SAM) has demonstrated impressive zero-shot segmentation capabilities across natural image domains, but it struggles to generalize to the unique challenges of remote sensing data, such as complex terrain, multi-scale objects, and temporal dynamics. In this paper, we introduce TASAM, a terrain and temporally-aware extension of SAM designed specifically for high-resolution remote sensing image segmentation. TASAM integrates three lightweight yet effective modules: a terrain-aware adapter that injects elevation priors, a temporal prompt generator that captures land-cover changes over time, and a multi-scale fusion strategy that enhances fine-grained object delineation. Without retraining the SAM backbone, our approach achieves substantial performance gains across three remote sensing benchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and task-specific models with minimal computational overhead. Our results highlight the value of domain-adaptive augmentation for foundation models and offer a scalable path toward more robust geospatial segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts",
    "url": "http://arxiv.org/abs/2509.14104v1",
    "authors": [
      "Leonard Hackel",
      "Tom Burgert",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-09-17",
    "abstract": "Self-supervised learning through masked autoencoders has attracted great attention for remote sensing (RS) foundation model (FM) development, enabling improved representation learning across diverse sensors and downstream tasks. However, existing RS FMs often either suffer from substantial computational complexity during both training and inference or exhibit limited representational capacity. These issues restrict their practical applicability in RS. To address this limitation, we propose an adaptation for enhancing the efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism into the FM. The integration of Soft MoEs into the FM allows modality-specific expert specialization alongside shared cross-sensor representation learning. To demonstrate the effectiveness of our adaptation, we apply it on the Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic descriptor-driven sampling strategy for the construction of a representative and diverse training set to train our CSMoE model. Extensive experiments on scene classification, semantic segmentation, and content-based image retrieval demonstrate that our adaptation yields a reduction in computational requirements while maintaining or improving representational performance. Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off between representational capacity, accuracy, and computational efficiency. On average, CSMoE achieves more than twice the computational efficiency of existing RS FMs, while maintaining competitive performance across all experiments. These results show the effectiveness of the proposed adaptation for creating computationally efficient RS FMs. The code for the model, the training set creation, and the model weights will be available at https://git.tu-berlin.de/rsim/csmoe.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2509.09572v1",
    "authors": [
      "Sijun Dong",
      "Yuxuan Hu",
      "LiBo Wang",
      "Geng Chen",
      "Xiaoliang Meng"
    ],
    "published": "2025-09-11",
    "abstract": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia",
    "url": "http://arxiv.org/abs/2509.08303v1",
    "authors": [
      "M. Warizmi Wafiq",
      "Peter Cutter",
      "Ate Poortinga",
      "Daniel Marc G. dela Torre",
      "Karis Tenneson",
      "Vanna Teck",
      "Enikoe Bihari",
      "Chanarun Saisaward",
      "Weraphong Suaruang",
      "Andrea McMahon",
      "Andi Vika Faradiba Muin",
      "Karno B. Batiran",
      "Chairil A",
      "Nurul Qomar",
      "Arya Arismaya Metananda",
      "David Ganz",
      "David Saah"
    ],
    "published": "2025-09-10",
    "abstract": "Oil palm cultivation remains one of the leading causes of deforestation in Indonesia. To better track and address this challenge, detailed and reliable mapping is needed to support sustainability efforts and emerging regulatory frameworks. We present an open-access geospatial dataset of oil palm plantations and related land cover types in Indonesia, produced through expert labeling of high-resolution satellite imagery from 2020 to 2024. The dataset provides polygon-based, wall-to-wall annotations across a range of agro-ecological zones and includes a hierarchical typology that distinguishes oil palm planting stages as well as similar perennial crops. Quality was ensured through multi-interpreter consensus and field validation. The dataset was created using wall-to-wall digitization over large grids, making it suitable for training and benchmarking both conventional convolutional neural networks and newer geospatial foundation models. Released under a CC-BY license, it fills a key gap in training data for remote sensing and aims to improve the accuracy of land cover types mapping. By supporting transparent monitoring of oil palm expansion, the resource contributes to global deforestation reduction goals and follows FAIR data principles.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement",
    "url": "http://arxiv.org/abs/2508.20954v1",
    "authors": [
      "Amir Jmal",
      "Chaima Chtourou",
      "Mahdi Louati",
      "Abdelaziz Kallel",
      "Houda Khmila"
    ],
    "published": "2025-08-28",
    "abstract": "In the context of proven climate change, maintaining olive biodiversity through early anomaly detection and treatment using remote sensing technology is crucial, offering effective management solutions. This paper presents an innovative approach to olive tree segmentation from satellite images. By leveraging foundational models and advanced segmentation techniques, the study integrates the Segment Anything Model (SAM) to accurately identify and segment olive trees in agricultural plots. The methodology includes SAM segmentation and corrections based on trees alignement in the field and a learanble constraint about the shape and the size. Our approach achieved a 98\\% accuracy rate, significantly surpassing the initial SAM performance of 82\\%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory",
    "url": "http://arxiv.org/abs/2508.18829v1",
    "authors": [
      "Takayuki Ishikawa",
      "Carmelo Bonannella",
      "Bas J. W. Lerink",
      "Marc Ru\u00dfwurm"
    ],
    "published": "2025-08-26",
    "abstract": "National Forest Inventory (NFI)s serve as the primary source of forest information, providing crucial tree species distribution data. However, maintaining these inventories requires labor-intensive on-site campaigns. Remote sensing approaches, particularly when combined with machine learning, offer opportunities to update NFIs more frequently and at larger scales. While the use of Satellite Image Time Series has proven effective for distinguishing tree species through seasonal canopy reflectance patterns, current approaches rely primarily on Random Forest classifiers with hand-designed features and phenology-based metrics. Using deep features from an available pre-trained remote sensing foundation models offers a complementary strategy. These pre-trained models leverage unannotated global data and are meant to used for general-purpose applications and can then be efficiently fine-tuned with smaller labeled datasets for specific classification tasks. This work systematically investigates how deep features improve tree species classification accuracy in the Netherlands with few annotated data. Data-wise, we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites data and SRTM data using Google Earth Engine. Our results demonstrate that fine-tuning a publicly available remote sensing time series foundation model outperforms the current state-of-the-art in NFI classification in the Netherlands by a large margin of up to 10% across all datasets. This demonstrates that classic hand-defined harmonic features are too simple for this task and highlights the potential of using deep AI features for data-limited application like NFI classification. By leveraging openly available satellite data and pre-trained models, this approach significantly improves classification accuracy compared to traditional methods and can effectively complement existing forest inventory processes.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images",
    "url": "http://arxiv.org/abs/2508.18067v1",
    "authors": [
      "Kaiyu Li",
      "Xiangyong Cao",
      "Ruixun Liu",
      "Shihong Wang",
      "Zixuan Jiang",
      "Zhi Wang",
      "Deyu Meng"
    ],
    "published": "2025-08-25",
    "abstract": "Semantic segmentation of remote sensing (RS) images is pivotal for comprehensive Earth observation, but the demand for interpreting new object categories, coupled with the high expense of manual annotation, poses significant challenges. Although open-vocabulary semantic segmentation (OVSS) offers a promising solution, existing frameworks designed for natural images are insufficient for the unique complexities of RS data. They struggle with vast scale variations and fine-grained details, and their adaptation often relies on extensive, costly annotations. To address this critical gap, this paper introduces SegEarth-OV, the first framework for annotation-free open-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp, a universal upsampler that robustly restores high-resolution spatial details from coarse features, correcting distorted target shapes without any task-specific post-training. We also present a simple yet effective Global Bias Alleviation operation to subtract the inherent global context from patch features, significantly enhancing local semantic fidelity. These components empower SegEarth-OV to effectively harness the rich semantics of pre-trained VLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the framework's universality to other challenging RS modalities like SAR images, where large-scale VLMs are unavailable and expensive to create, we introduce AlignEarth, which is a distillation-based strategy and can efficiently transfer semantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the need to build SAR foundation models from scratch and enabling universal OVSS across diverse sensor types. Extensive experiments on both optical and SAR datasets validate that SegEarth-OV can achieve dramatic improvements over the SOTA methods, establishing a robust foundation for annotation-free and open-world Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing",
    "url": "http://arxiv.org/abs/2508.12409v3",
    "authors": [
      "Liang Lv",
      "Di Wang",
      "Jing Zhang",
      "Lefei Zhang"
    ],
    "published": "2025-08-17",
    "abstract": "Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS) analysis by leveraging unlabeled data through pseudo-labeling and consistency learning. However, existing S4 studies often rely on small-scale datasets and models, limiting their practical applicability. To address this, we propose S5, the first scalable framework for semi-supervised semantic segmentation in RS, which unlocks the potential of vast unlabeled Earth observation data typically underutilized due to costly pixel-level annotations. Built upon existing large-scale RS datasets, S5 introduces a data selection strategy that integrates entropy-based filtering and diversity expansion, resulting in the RS4P-1M dataset. Using this dataset, we systematically scale up S4 into a new pretraining paradigm, S4 pre-training (S4P), to pretrain RS foundation models (RSFMs) of varying sizes on this extensive corpus, significantly boosting their performance on land cover segmentation and object detection tasks. Furthermore, during fine-tuning, we incorporate a Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which enables efficient adaptation to multiple RS benchmarks with fewer parameters. This approach improves the generalization and versatility of RSFMs across diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance across all benchmarks, underscoring the viability of scaling semi-supervised learning for RS applications. All datasets, code, and models will be released at https://github.com/MiliLab/S5",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection",
    "url": "http://arxiv.org/abs/2508.11167v2",
    "authors": [
      "Jianhong Han",
      "Yupei Wang",
      "Liang Chen"
    ],
    "published": "2025-08-15",
    "abstract": "Unsupervised domain adaptation methods have been widely explored to bridge domain gaps. However, in real-world remote-sensing scenarios, privacy and transmission constraints often preclude access to source domain data, which limits their practical applicability. Recently, Source-Free Object Detection (SFOD) has emerged as a promising alternative, aiming at cross-domain adaptation without relying on source data, primarily through a self-training paradigm. Despite its potential, SFOD frequently suffers from training collapse caused by noisy pseudo-labels, especially in remote sensing imagery with dense objects and complex backgrounds. Considering that limited target domain annotations are often feasible in practice, we propose a Vision foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised framework for SFOD in remote sensing images. VG-DETR integrates a Vision Foundation Model (VFM) into the training pipeline in a \"free lunch\" manner, leveraging a small amount of labeled target data to mitigate pseudo-label noise while improving the detector's feature-extraction capability. Specifically, we introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's semantic priors to further assess the reliability of the generated pseudo-labels. By recovering potentially correct predictions from low-confidence outputs, our strategy improves pseudo-label quality and quantity. In addition, a dual-level VFM-guided alignment method is proposed, which aligns detector features with VFM embeddings at both the instance and image levels. Through contrastive learning among fine-grained prototypes and similarity matching between feature maps, this dual-level alignment further enhances the robustness of feature representations against domain gaps. Extensive experiments demonstrate that VG-DETR achieves superior performance in source-free remote sensing detection tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.10568v1",
    "authors": [
      "Humza Naveed",
      "Xina Zeng",
      "Mitch Bryson",
      "Nagita Mehrseresht"
    ],
    "published": "2025-08-14",
    "abstract": "Foundational models have achieved significant success in diverse domains of computer vision. They learn general representations that are easily transferable to tasks not seen during training. One such foundational model is Segment anything model (SAM), which can accurately segment objects in images. We propose adapting the SAM encoder via fine-tuning for remote sensing change detection (RSCD) along with spatial-temporal feature enhancement (STFE) and multi-scale decoder fusion (MSDF) to detect changes robustly at multiple scales. Additionally, we propose a novel cross-entropy masking (CEM) loss to handle high class imbalance in change detection datasets. Our method outperforms state-of-the-art (SOTA) methods on four change detection datasets, Levir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on a large complex S2Looking dataset. The code is available at: https://github.com/humza909/SAM-CEM-CD",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss",
    "url": "http://arxiv.org/abs/2508.09453v1",
    "authors": [
      "Abdul Matin",
      "Tanjim Bin Faruk",
      "Shrideep Pallickara",
      "Sangmi Lee Pallickara"
    ],
    "published": "2025-08-13",
    "abstract": "The proliferation of foundation models, pretrained on large-scale unlabeled datasets, has emerged as an effective approach in creating adaptable and reusable architectures that can be leveraged for various downstream tasks using satellite observations. However, their direct application to hyperspectral remote sensing remains challenging due to inherent spectral disparities and the scarcity of available observations. In this work, we present HyperKD, a novel knowledge distillation framework that enables transferring learned representations from a teacher model into a student model for effective development of a foundation model on hyperspectral images. Unlike typical knowledge distillation frameworks, which use a complex teacher to guide a simpler student, HyperKD enables an inverse form of knowledge transfer across different types of spectral data, guided by a simpler teacher model. Building upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi foundational model into a student tailored for EnMAP hyperspectral imagery. HyperKD addresses the inverse domain adaptation problem with spectral gaps by introducing a feature-based strategy that includes spectral range-based channel alignment, spatial feature-guided masking, and an enhanced loss function tailored for hyperspectral images. HyperKD bridges the substantial spectral domain gap, enabling the effective use of pretrained foundation models for geospatial applications. Extensive experiments show that HyperKD significantly improves representation learning in MAEs, leading to enhanced reconstruction fidelity and more robust performance on downstream tasks such as land cover classification, crop type identification, and soil organic carbon prediction, underpinning the potential of knowledge distillation frameworks in remote sensing analytics with hyperspectral imagery.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks",
    "url": "http://arxiv.org/abs/2508.03566v1",
    "authors": [
      "Xinyu Xiong",
      "Zihuang Wu",
      "Lei Zhang",
      "Lei Lu",
      "Ming Li",
      "Guanbin Li"
    ],
    "published": "2025-08-05",
    "abstract": "Recent studies have highlighted the potential of adapting the Segment Anything Model (SAM) for various downstream tasks. However, constructing a more powerful and generalizable encoder to further enhance performance remains an open challenge. In this work, we propose SAM2-UNeXT, an advanced framework that builds upon the core principles of SAM2-UNet while extending the representational capacity of SAM2 through the integration of an auxiliary DINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue layer, our approach enables more accurate segmentation with a simple architecture, relaxing the need for complex decoder designs. Extensive experiments conducted on four benchmarks, including dichotomous image segmentation, camouflaged object detection, marine animal segmentation, and remote sensing saliency detection, demonstrate the superior performance of our proposed method. The code is available at https://github.com/WZH0120/SAM2-UNeXT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models",
    "url": "http://arxiv.org/abs/2508.01731v1",
    "authors": [
      "Yuxiang Zhang",
      "Wei Li",
      "Mengmeng Zhang",
      "Jiawei Han",
      "Ran Tao",
      "Shunlin Liang"
    ],
    "published": "2025-08-03",
    "abstract": "Recent advances in Remote Sensing Foundation Models (RSFMs) have led to significant breakthroughs in the field. While many RSFMs have been pretrained with massive optical imagery, more multispectral/hyperspectral data remain lack of the corresponding foundation models. To leverage the advantages of spectral imagery in earth observation, we explore whether existing RSFMs can be effectively adapted to process diverse spectral modalities without requiring extensive spectral pretraining. In response to this challenge, we proposed SpectralX, an innovative parameter-efficient fine-tuning framework that adapt existing RSFMs as backbone while introducing a two-stage training approach to handle various spectral inputs, thereby significantly improving domain generalization performance. In the first stage, we employ a masked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) to extract attribute tokens from both spatial and spectral dimensions. Simultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA) that dynamically aggregates multi-attribute expert knowledge while performing layer-wise fine-tuning. With semantic segmentation as downstream task in the second stage, we insert an Attribute-refined Adapter (Are-adapter) into the first stage framework. By iteratively querying low-level semantic features with high-level representations, the model learns to focus on task-beneficial attributes, enabling customized adjustment of RSFMs. Following this two-phase adaptation process, SpectralX is capable of interpreting spectral imagery from new regions or seasons. The codes will be available from the website: https://github.com/YuxiangZhang-BIT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources",
    "url": "http://arxiv.org/abs/2508.00627v1",
    "authors": [
      "Paul Tresson",
      "Pierre Le Coz",
      "Hadrien Tulet",
      "Anthony Malkassian",
      "Maxime R\u00e9jou M\u00e9chain"
    ],
    "published": "2025-08-01",
    "abstract": "Remote sensing has entered a new era with the rapid development of artificial intelligence approaches. However, the implementation of deep learning has largely remained restricted to specialists and has been impractical because it often requires (i) large reference datasets for model training and validation; (ii) substantial computing resources; and (iii) strong coding skills. Here, we introduce IAMAP, a user-friendly QGIS plugin that addresses these three challenges in an easy yet flexible way. IAMAP builds on recent advancements in self-supervised learning strategies, which now provide robust feature extractors, often referred to as foundation models. These generalist models can often be reliably used in few-shot or zero-shot scenarios (i.e., with little to no fine-tuning). IAMAP's interface allows users to streamline several key steps in remote sensing image analysis: (i) extracting image features using a wide range of deep learning architectures; (ii) reducing dimensionality with built-in algorithms; (iii) performing clustering on features or their reduced representations; (iv) generating feature similarity maps; and (v) calibrating and validating supervised machine learning models for prediction. By enabling non-AI specialists to leverage the high-quality features provided by recent deep learning approaches without requiring GPU capacity or extensive reference datasets, IAMAP contributes to the democratization of computationally efficient and energy-conscious deep learning methods.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model",
    "url": "http://arxiv.org/abs/2507.22675v2",
    "authors": [
      "Meiqi Hu",
      "Lingzhi Lu",
      "Chengxi Han",
      "Xiaoping Liu"
    ],
    "published": "2025-07-30",
    "abstract": "Recently, large foundation models trained on vast datasets have demonstrated exceptional capabilities in feature extraction and general feature representation. The ongoing advancements in deep learning-driven large models have shown great promise in accelerating unsupervised change detection methods, thereby enhancing the practical applicability of change detection technologies. Building on this progress, this paper introduces MergeSAM, an innovative unsupervised change detection method for high-resolution remote sensing imagery, based on the Segment Anything Model (SAM). Two novel strategies, MaskMatching and MaskSplitting, are designed to address real-world complexities such as object splitting, merging, and other intricate changes. The proposed method fully leverages SAM's object segmentation capabilities to construct multitemporal masks that capture complex changes, embedding the spatial structure of land cover into the change detection process.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning",
    "url": "http://arxiv.org/abs/2507.20776v1",
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Yingchao Feng",
      "Kaiwen Wei",
      "Wenxin Yin",
      "Wenhui Diao",
      "Mengyu Wang",
      "Hanbo Bi",
      "Kaiyue Kang",
      "Tong Ling",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2025-07-28",
    "abstract": "Remote sensing (RS) images from multiple modalities and platforms exhibit diverse details due to differences in sensor characteristics and imaging perspectives. Existing vision-language research in RS largely relies on relatively homogeneous data sources. Moreover, they still remain limited to conventional visual perception tasks such as classification or captioning. As a result, these methods fail to serve as a unified and standalone framework capable of effectively handling RS imagery from diverse sources in real-world applications. To address these issues, we propose RingMo-Agent, a model designed to handle multi-modal and multi-platform data that performs perception and reasoning tasks based on user textual instructions. Compared with existing models, RingMo-Agent 1) is supported by a large-scale vision-language dataset named RS-VL3M, comprising over 3 million image-text pairs, spanning optical, SAR, and infrared (IR) modalities collected from both satellite and UAV platforms, covering perception and challenging reasoning tasks; 2) learns modality adaptive representations by incorporating separated embedding layers to construct isolated features for heterogeneous modalities and reduce cross-modal interference; 3) unifies task modeling by introducing task-specific tokens and employing a token-based high-dimensional hidden state decoding mechanism designed for long-horizon spatial tasks. Extensive experiments on various RS vision-language tasks demonstrate that RingMo-Agent not only proves effective in both visual understanding and sophisticated analytical tasks, but also exhibits strong generalizability across different platforms and sensing modalities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation",
    "url": "http://arxiv.org/abs/2507.16716v1",
    "authors": [
      "Yiguo He",
      "Junjie Zhu",
      "Yiying Li",
      "Xiaoyu Zhang",
      "Chunping Qiu",
      "Jun Wang",
      "Qiangjuan Huang",
      "Ke Yang"
    ],
    "published": "2025-07-22",
    "abstract": "The application of Vision-language foundation models (VLFMs) to remote sensing (RS) imagery has garnered significant attention due to their superior capability in various downstream tasks. A key challenge lies in the scarcity of high-quality, large-scale, image-text paired training data. Recently, several works introduced extensive image-text datasets for RS and trained their VLFMs. However, due to the rudimentary methods used for generating captions, the quality of datasets is suboptimal, requiring larger volumes of training data, while only yielding modest performance improvements. In this paper, we propose a two-stage method named MpGI(Multi-Perspective Generation and Integration) for generating high-quality text captions for RS images. Firstly, we generate distinct and detailed descriptions from different perspectives using Rule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs generation methods. Next, we utilize Large Language Models (LLMs) to integrate these diverse descriptions into comprehensive captions, capturing details from multiple perspectives. Finally, we have created the HQRS-IT-210K dataset, including about 210,000 RS images and 1.3 million captions. We fine-tuned two VLFMs using our dataset: CLIP, a discriminative model, and CoCa, an image-to-text generative model. This process resulted in our proposed HQRS-CLIP and RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed the previous SOTA RS CLIP model in various downstream tasks while using only 4.2\\% of the training data. RS-CoCa outperforms other advanced approaches across benchmark datasets and can generate captions for RS images that rival or even exceed manual annotations. Dataset, pre-trained models, and codes will be released at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "CLIP"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
    "url": "http://arxiv.org/abs/2507.13812v1",
    "authors": [
      "Yingying Zhang",
      "Lixiang Ru",
      "Kang Wu",
      "Lei Yu",
      "Lei Liang",
      "Yansheng Li",
      "Jingdong Chen"
    ],
    "published": "2025-07-18",
    "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly advanced various Earth observation tasks, such as urban planning, environmental monitoring, and natural disaster management. However, most existing approaches generally require the training of separate backbone networks for each data modality, leading to redundancy and inefficient parameter utilization. Moreover, prevalent pre-training methods typically apply self-supervised learning (SSL) techniques from natural images without adequately accommodating the characteristics of remote sensing (RS) images, such as the complicated semantic distribution within a single RS image. In this work, we present SkySense V2, a unified MM-RSFM that employs a single transformer backbone to handle multiple modalities. This backbone is pre-trained with a novel SSL strategy tailored to the distinct traits of RS data. In particular, SkySense V2 incorporates an innovative adaptive patch merging module and learnable modality prompt tokens to address challenges related to varying resolutions and limited feature diversity across modalities. In additional, we incorporate the mixture of experts (MoE) module to further enhance the performance of the foundation model. SkySense V2 demonstrates impressive generalization abilities through an extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense by an average of 1.8 points.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Deploying Geospatial Foundation Models in the Real World: Lessons from WorldCereal",
    "url": "http://arxiv.org/abs/2508.00858v1",
    "authors": [
      "Christina Butsko",
      "Kristof Van Tricht",
      "Gabriel Tseng",
      "Giorgia Milli",
      "David Rolnick",
      "Ruben Cartuyvels",
      "Inbal Becker Reshef",
      "Zoltan Szantoi",
      "Hannah Kerner"
    ],
    "published": "2025-07-16",
    "abstract": "The increasing availability of geospatial foundation models has the potential to transform remote sensing applications such as land cover classification, environmental monitoring, and change detection. Despite promising benchmark results, the deployment of these models in operational settings is challenging and rare. Standardized evaluation tasks often fail to capture real-world complexities relevant for end-user adoption such as data heterogeneity, resource constraints, and application-specific requirements. This paper presents a structured approach to integrate geospatial foundation models into operational mapping systems. Our protocol has three key steps: defining application requirements, adapting the model to domain-specific data and conducting rigorous empirical testing. Using the Presto model in a case study for crop mapping, we demonstrate that fine-tuning a pre-trained model significantly improves performance over conventional supervised methods. Our results highlight the model's strong spatial and temporal generalization capabilities. Our protocol provides a replicable blueprint for practitioners and lays the groundwork for future research to operationalize foundation models in diverse remote sensing applications. Application of the protocol to the WorldCereal global crop-mapping system showcases the framework's scalability.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Continental-scale habitat distribution modelling with multimodal earth observation foundation models",
    "url": "http://arxiv.org/abs/2507.09732v2",
    "authors": [
      "Sara Si-Moussi",
      "Stephan Hennekens",
      "Sander Mucher",
      "Stan Los",
      "Yoann Cartier",
      "Borja Jim\u00e9nez-Alfaro",
      "Fabio Attorre",
      "Jens-Christian Svenning",
      "Wilfried Thuiller"
    ],
    "published": "2025-07-13",
    "abstract": "Habitats integrate the abiotic conditions, vegetation composition and structure that support biodiversity and sustain nature's contributions to people. Most habitats face mounting pressures from human activities, which requires accurate, high-resolution habitat mapping for effective conservation and restoration. Yet, current habitat maps often fall short in thematic or spatial resolution because they must (1) model several mutually exclusive habitat types that co-occur across landscapes and (2) cope with severe class imbalance that complicates exhaustive multi-class training. Here, we evaluated how high-resolution remote sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat mapping across large geographical extents at fine spatial and thematic resolution. Using vegetation plots from the European Vegetation Archive, we modelled the distribution of Level 3 EUNIS habitat types across Europe and assessed multiple modelling strategies against independent validation datasets. Strategies that exploited the hierarchical nature of habitat classifications resolved classification ambiguities, especially in fragmented habitats. Integrating satellite-borne multispectral and radar imagery, particularly through Earth Observation (EO) Foundation models (EO-FMs), enhanced within-formation discrimination and overall performance. Finally, ensemble machine learning that corrects class imbalance boosted predictive accuracy even further. Our methodological framework is transferable beyond Europe and adaptable to other classification systems. Future research should advance temporal modelling of habitat dynamics, extend to habitat segmentation and quality assessment, and exploit next-generation EO data paired with higher-quality in situ observations.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges",
    "url": "http://arxiv.org/abs/2507.09562v1",
    "authors": [
      "Yidong Jiang"
    ],
    "published": "2025-07-13",
    "abstract": "The Segment Anything Model (SAM) has revolutionized image segmentation through its innovative prompt-based approach, yet the critical role of prompt engineering in its success remains underexplored. This paper presents the first comprehensive survey focusing specifically on prompt engineering techniques for SAM and its variants. We systematically organize and analyze the rapidly growing body of work in this emerging field, covering fundamental methodologies, practical applications, and key challenges. Our review reveals how prompt engineering has evolved from simple geometric inputs to sophisticated multimodal approaches, enabling SAM's adaptation across diverse domains including medical imaging and remote sensing. We identify unique challenges in prompt optimization and discuss promising research directions. This survey fills an important gap in the literature by providing a structured framework for understanding and advancing prompt engineering in foundation models for segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion",
    "url": "http://arxiv.org/abs/2507.09081v1",
    "authors": [
      "Zhenyu Yu",
      "Mohd Yamani Idna Idris",
      "Hua Wang",
      "Pei Wang",
      "Junyi Chen",
      "Kun Wang"
    ],
    "published": "2025-07-11",
    "abstract": "Quantitative remote sensing inversion aims to estimate continuous surface variables-such as biomass, vegetation indices, and evapotranspiration-from satellite observations, supporting applications in ecosystem monitoring, carbon accounting, and land management. With the evolution of remote sensing systems and artificial intelligence, traditional physics-based paradigms are giving way to data-driven and foundation model (FM)-based approaches. This paper systematically reviews the methodological evolution of inversion techniques, from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods (e.g., deep learning, multimodal fusion), and further to foundation models (e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application scenarios, and limitations of each paradigm, with emphasis on recent FM advances in self-supervised pretraining, multi-modal integration, and cross-task adaptation. We also highlight persistent challenges in physical interpretability, domain generalization, limited supervision, and uncertainty quantification. Finally, we envision the development of next-generation foundation models for remote sensing inversion, emphasizing unified modeling capacity, cross-domain generalization, and physical interpretability.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "BioAnalyst: A Foundation Model for Biodiversity",
    "url": "http://arxiv.org/abs/2507.09080v2",
    "authors": [
      "Athanasios Trantas",
      "Martino Mensio",
      "Stylianos Stasinos",
      "Sebastian Gribincea",
      "Taimur Khan",
      "Damian Podareanu",
      "Aliene van der Veen"
    ],
    "published": "2025-07-11",
    "abstract": "Multimodal Foundation Models (FMs) offer a path to learn general-purpose representations from heterogeneous ecological data, easily transferable to downstream tasks. However, practical biodiversity modelling remains fragmented; separate pipelines and models are built for each dataset and objective, which limits reuse across regions and taxa. In response, we present BioAnalyst, to our knowledge the first multimodal Foundation Model tailored to biodiversity analysis and conservation planning in Europe at $0.25^{\\circ}$ spatial resolution targeting regional to national-scale applications. BioAnalyst employs a transformer-based architecture, pre-trained on extensive multimodal datasets that align species occurrence records with remote sensing indicators, climate and environmental variables. Post pre-training, the model is adapted via lightweight roll-out fine-tuning to a range of downstream tasks, including joint species distribution modelling, biodiversity dynamics and population trend forecasting. The model is evaluated on two representative downstream use cases: (i) joint species distribution modelling and with 500 vascular plant species (ii) monthly climate linear probing with temperature and precipitation data. Our findings show that BioAnalyst can provide a strong baseline both for biotic and abiotic tasks, acting as a macroecological simulator with a yearly forecasting horizon and monthly resolution, offering the first application of this type of modelling in the biodiversity domain. We have open-sourced the model weights, training and fine-tuning pipelines to advance AI-driven ecological research.",
    "categories": [
      "foundation_model",
      "fish_plankton"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MAPEX: Modality-Aware Pruning of Experts for Remote Sensing Foundation Models",
    "url": "http://arxiv.org/abs/2507.07527v1",
    "authors": [
      "Joelle Hanna",
      "Linus Scheibenreif",
      "Damian Borth"
    ],
    "published": "2025-07-10",
    "abstract": "Remote sensing data is commonly used for tasks such as flood mapping, wildfire detection, or land-use studies. For each task, scientists carefully choose appropriate modalities or leverage data from purpose-built instruments. Recent work on remote sensing foundation models pre-trains computer vision models on large amounts of remote sensing data. These large-scale models tend to focus on specific modalities, often optical RGB or multispectral data. For many important applications, this introduces a mismatch between the application modalities and the pre-training data. Moreover, the large size of foundation models makes them expensive and difficult to fine-tune on typically small datasets for each task. We address this mismatch with MAPEX, a remote sensing foundation model based on mixture-of-modality experts. MAPEX is pre-trained on multi-modal remote sensing data with a novel modality-conditioned token routing mechanism that elicits modality-specific experts. To apply the model on a specific task, we propose a modality aware pruning technique, which only retains experts specialized for the task modalities. This yields efficient modality-specific models while simplifying fine-tuning and deployment for the modalities of interest. We experimentally validate MAPEX on diverse remote sensing datasets and show strong performance compared to fully supervised training and state-of-the-art remote sensing foundation models. Code is available at https://github.com/HSG-AIML/MAPEX.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models",
    "url": "http://arxiv.org/abs/2507.06231v1",
    "authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Bowen Chen",
      "Jiafan Zhang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-07-08",
    "abstract": "Referring Remote Sensing Image Segmentation provides a flexible and fine-grained framework for remote sensing scene analysis via vision-language collaborative interpretation. Current approaches predominantly utilize a three-stage pipeline encompassing dual-modal encoding, cross-modal interaction, and pixel decoding. These methods demonstrate significant limitations in managing complex semantic relationships and achieving precise cross-modal alignment, largely due to their coupled processing mechanism that conflates target localization with boundary delineation. This architectural coupling amplifies error propagation under semantic ambiguity while restricting model generalizability and interpretability. To address these issues, we propose RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow into a collaborative dual-stage framework: coarse localization followed by fine segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with SAM's segmentation generalizability through strategic foundation model collaboration. Specifically, CLIP is employed as the dual-modal encoder to activate target features within its pre-aligned semantic space and generate localization prompts. To mitigate CLIP's misactivation challenges in multi-entity scenarios described by referring texts, a cascaded second-order prompter is devised, which enhances precision through implicit reasoning via decomposition of text embeddings into complementary semantic subspaces. These optimized semantic prompts subsequently direct the SAM to generate pixel-level refined masks, thereby completing the semantic transmission pipeline. Extensive experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2 surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex semantic interpretation. Code is available at: https://github.com/KyanChen/RSRefSeg2.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "From General to Specialized: The Need for Foundational Models in Agriculture",
    "url": "http://arxiv.org/abs/2507.05390v2",
    "authors": [
      "Vishal Nedungadi",
      "Xingguo Xiong",
      "Aike Potze",
      "Ron Van Bree",
      "Tao Lin",
      "Marc Ru\u00dfwurm",
      "Ioannis N. Athanasiadis"
    ],
    "published": "2025-07-07",
    "abstract": "Food security remains a global concern as population grows and climate change intensifies, demanding innovative solutions for sustainable agricultural productivity. Recent advances in foundation models have demonstrated remarkable performance in remote sensing and climate sciences, and therefore offer new opportunities for agricultural monitoring. However, their application in challenges related to agriculture-such as crop type mapping, crop phenology estimation, and crop yield estimation-remains under-explored. In this work, we quantitatively evaluate existing foundational models to assess their effectivity for a representative set of agricultural tasks. From an agricultural domain perspective, we describe a requirements framework for an ideal agricultural foundation model (CropFM). We then survey and compare existing general-purpose foundational models in this framework and empirically evaluate two exemplary of them in three representative agriculture specific tasks. Finally, we highlight the need for a dedicated foundational model tailored specifically to agriculture.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Time2Agri: Temporal Pretext Tasks for Agricultural Monitoring",
    "url": "http://arxiv.org/abs/2507.04366v1",
    "authors": [
      "Moti Rattan Gupta",
      "Anupam Sobti"
    ],
    "published": "2025-07-06",
    "abstract": "Self Supervised Learning(SSL) has emerged as a prominent paradigm for label-efficient learning, and has been widely utilized by remote sensing foundation models(RSFMs). Recent RSFMs including SatMAE, DoFA, primarily rely on masked autoencoding(MAE), contrastive learning or some combination of them. However, these pretext tasks often overlook the unique temporal characteristics of agricultural landscape, namely nature's cycle. Motivated by this gap, we propose three novel agriculture-specific pretext tasks, namely Time-Difference Prediction(TD), Temporal Frequency Prediction(FP), and Future-Frame Prediction(FF). Comprehensive evaluation on SICKLE dataset shows FF achieves 69.6% IoU on crop mapping and FP reduces yield prediction error to 30.7% MAPE, outperforming all baselines, and TD remains competitive on most tasks. Further, we also scale FF to the national scale of India, achieving 54.2% IoU outperforming all baselines on field boundary delineation on FTW India dataset.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation",
    "url": "http://arxiv.org/abs/2507.00356v1",
    "authors": [
      "Zhiwei Yi",
      "Xin Cheng",
      "Jingyu Ma",
      "Ruifei Zhu",
      "Junwei Tian",
      "Yuanxiu Zhou",
      "Xinge Zhao",
      "Hongzhe Li"
    ],
    "published": "2025-07-01",
    "abstract": "Deep learning methods have significantly advanced the development of intelligent rinterpretation in remote sensing (RS), with foundational model research based on large-scale pre-training paradigms rapidly reshaping various domains of Earth Observation (EO). However, compared to the open accessibility and high spatiotemporal coverage of medium-resolution data, the limited acquisition channels for ultra-high-resolution optical RS imagery have constrained the progress of high-resolution remote sensing vision foundation models (RSVFM). As the world's largest sub-meter-level commercial RS satellite constellation, the Jilin-1 constellation possesses abundant sub-meter-level image resources. This study proposes CGEarthEye, a RSVFM framework specifically designed for Jilin-1 satellite characteristics, comprising five backbones with different parameter scales with totaling 2.1 billion parameters. To enhance the representational capacity of the foundation model, we developed JLSSD, the first 15-million-scale multi-temporal self-supervised learning (SSL) dataset featuring global coverage with quarterly temporal sampling within a single year, constructed through multi-level representation clustering and sampling strategies. The framework integrates seasonal contrast, augmentation-based contrast, and masked patch token contrastive strategies for pre-training. Comprehensive evaluations across 10 benchmark datasets covering four typical RS tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art (SOTA) performance. Further analysis reveals CGEarthEye's superior characteristics in feature visualization, model convergence, parameter efficiency, and practical mapping applications. This study anticipates that the exceptional representation capabilities of CGEarthEye will facilitate broader and more efficient applications of Jilin-1 data in traditional EO application.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition",
    "url": "http://arxiv.org/abs/2506.20174v2",
    "authors": [
      "Man Duc Chuc"
    ],
    "published": "2025-06-25",
    "abstract": "Foundation models are rapidly transforming Earth Observation data mining by enabling generalizable and scalable solutions for key tasks such as scene classification and semantic segmentation. While most efforts in the geospatial domain have focused on developing large models trained from scratch using massive Earth Observation datasets, an alternative strategy that remains underexplored is the reuse and combination of existing pretrained models. In this study, we investigate whether foundation models pretrained on remote sensing and general vision datasets can be effectively combined to improve performance across a diverse set of key Earth Observation tasks. Using the GEO-Bench benchmark, we evaluate several prominent models, including Prithvi, Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions, sensor modalities, and task types. The results show that feature-level ensembling of smaller pretrained models can match or exceed the performance of much larger models, while requiring less training time and computational resources. Moreover, the study highlights the potential of applying knowledge distillation to transfer the strengths of ensembles into more compact models, offering a practical path for deploying foundation models in real-world Earth Observation applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2506.19585v1",
    "authors": [
      "Gencer Sumbul",
      "Chang Xu",
      "Emanuele Dalsasso",
      "Devis Tuia"
    ],
    "published": "2025-06-24",
    "abstract": "From optical sensors to microwave radars, leveraging the complementary strengths of remote sensing (RS) sensors is crucial for achieving dense spatio-temporal monitoring of our planet. In contrast, recent deep learning models, whether task-specific or foundational, are often specific to single sensors or to fixed combinations: adapting such models to different sensory inputs requires both architectural changes and re-training, limiting scalability and generalization across multiple RS sensors. On the contrary, a single model able to modulate its feature representations to accept diverse sensors as input would pave the way to agile and flexible multi-sensor RS data processing. To address this, we introduce SMARTIES, a generic and versatile foundation model lifting sensor-specific/dependent efforts and enabling scalability and generalization to diverse RS sensors: SMARTIES projects data from heterogeneous sensors into a shared spectrum-aware space, enabling the use of arbitrary combinations of bands both for training and inference. To obtain sensor-agnostic representations, we train a single, unified transformer model reconstructing masked multi-sensor data with cross-sensor token mixup. On both single- and multi-modal tasks across diverse sensors, SMARTIES outperforms previous models that rely on sensor-specific pretraining. Our code and pretrained models are available at https://gsumbul.github.io/SMARTIES.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high Spatial Resolution Land Cover Classification",
    "url": "http://arxiv.org/abs/2506.15565v2",
    "authors": [
      "Junhao Wu",
      "Aboagye-Ntow Stephen",
      "Chuyuan Wang",
      "Gang Chen",
      "Xin Huang"
    ],
    "published": "2025-06-18",
    "abstract": "Ultra-high Spatial Resolution (UHSR) Land Cover Classification is increasingly important for urban analysis, enabling fine-scale planning, ecological monitoring, and infrastructure management. It identifies land cover types on sub-meter remote sensing imagery, capturing details such as building outlines, road networks, and distinct boundaries. However, most existing methods focus on 1 m imagery and rely heavily on large-scale annotations, while UHSR data remain scarce and difficult to annotate, limiting practical applicability. To address these challenges, we introduce Baltimore Atlas, a UHSR land cover classification framework that reduces reliance on large-scale training data and delivers high-accuracy results. Baltimore Atlas builds on three key ideas: (1) Baltimore Atlas Dataset, a 0.3 m resolution dataset based on aerial imagery of Baltimore City; (2) FreqWeaver Adapter, a parameter-efficient adapter that transfers SAM2 to this domain, leveraging foundation model knowledge to reduce training data needs while enabling fine-grained detail and structural modeling; (3) Uncertainty-Aware Teacher Student Framework, a semi-supervised framework that exploits unlabeled data to further reduce training dependence and improve generalization across diverse scenes. Using only 5.96% of total model parameters, our approach achieves a 1.78% IoU improvement over existing parameter-efficient tuning strategies and a 3.44% IoU gain compared to state-of-the-art high-resolution remote sensing segmentation methods on the Baltimore Atlas Dataset.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models",
    "url": "http://arxiv.org/abs/2506.08780v1",
    "authors": [
      "Isaac Corley",
      "Lakshay Sharma",
      "Ruth Crasto"
    ],
    "published": "2025-06-10",
    "abstract": "The Landsat program offers over 50 years of globally consistent Earth imagery. However, the lack of benchmarks for this data constrains progress towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and LC100-L. We establish baseline and standardized evaluation methods across both common architectures and Landsat foundation models pretrained on the SSL4EO-L dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract better representations for downstream tasks in comparison to ImageNet, including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and BigEarthNet-L.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation",
    "url": "http://arxiv.org/abs/2506.08772v2",
    "authors": [
      "Jiayi Song",
      "Kaiyu Li",
      "Xiangyong Cao",
      "Deyu Meng"
    ],
    "published": "2025-06-10",
    "abstract": "Semantic segmentation in remote sensing images is crucial for various applications, yet its performance is heavily reliant on large-scale, high-quality pixel-wise annotations, which are notoriously expensive and time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a promising alternative to mitigate this data dependency. However, existing SSS methods often struggle with the inherent distribution mismatch between limited labeled data and abundant unlabeled data, leading to suboptimal generalization. To alleviate this issue, we attempt to introduce the Vision Foundation Models (VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs possess robust generalization capabilities that can effectively bridge this distribution gap and provide strong semantic priors for SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework that leverages the powerful semantic knowledge embedded in VFMs to guide semi-supervised learning in remote sensing. Specifically, RS-MTDF employs multiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing feature-level distillation to align student features with their robust representations. To further enhance discriminative power, the distilled knowledge is seamlessly fused into the student decoder. Extensive experiments on three challenging remote sensing datasets demonstrate that RS-MTDF consistently achieves state-of-the-art performance. Notably, our method outperforms existing approaches across various label ratios on LoveDA and secures the highest IoU in the majority of semantic categories. These results underscore the efficacy of multi-teacher VFM guidance in significantly enhancing both generalization and semantic understanding for remote sensing segmentation. Ablation studies further validate the contribution of each proposed module.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
    "url": "http://arxiv.org/abs/2505.21375v2",
    "authors": [
      "Fengxiang Wang",
      "Mingshuo Chen",
      "Yueying Li",
      "Di Wang",
      "Haotian Wang",
      "Zonghao Guo",
      "Zefan Wang",
      "Boqi Shan",
      "Long Lan",
      "Yulin Wang",
      "Hongzhen Wang",
      "Wenjing Yang",
      "Bo Du",
      "Jing Zhang"
    ],
    "published": "2025-05-27",
    "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA (avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: Background Token Pruning and Anchored Token Selection, to reduce the memory footprint while preserving key semantics.Integrating these techniques, we introduce GeoLLaVA-8K, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping",
    "url": "http://arxiv.org/abs/2505.21357v2",
    "authors": [
      "Wenyuan Li",
      "Shunlin Liang",
      "Keyan Chen",
      "Yongzhe Chen",
      "Han Ma",
      "Jianglei Xu",
      "Yichuan Ma",
      "Shikang Guan",
      "Husheng Fang",
      "Zhenwei Shi"
    ],
    "published": "2025-05-27",
    "abstract": "Accurate crop mapping fundamentally relies on modeling multi-scale spatiotemporal patterns, where spatial scales range from individual field textures to landscape-level context, and temporal scales capture both short-term phenological transitions and full growing-season dynamics. Transformer-based remote sensing foundation models (RSFMs) offer promising potential for crop mapping due to their innate ability for unified spatiotemporal processing. However, current RSFMs remain suboptimal for crop mapping: they either employ fixed spatiotemporal windows that ignore the multi-scale nature of crop systems or completely disregard temporal information by focusing solely on spatial patterns. To bridge these gaps, we present AgriFM, a multi-source remote sensing foundation model specifically designed for agricultural crop mapping. Our approach begins by establishing the necessity of simultaneous hierarchical spatiotemporal feature extraction, leading to the development of a modified Video Swin Transformer architecture where temporal down-sampling is synchronized with spatial scaling operations. This modified backbone enables efficient unified processing of long time-series satellite inputs. AgriFM leverages temporally rich data streams from three satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is pre-trained on a global representative dataset comprising over 25 million image samples supervised by land cover products. The resulting framework incorporates a versatile decoder architecture that dynamically fuses these learned spatiotemporal representations, supporting diverse downstream tasks. Comprehensive evaluations demonstrate AgriFM's superior performance over conventional deep learning approaches and state-of-the-art general-purpose RSFMs across all downstream tasks. Codes will be available at https://github.com/flyakon/AgriFM.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2505.19447v2",
    "authors": [
      "Hengtong Shen",
      "Haiyan Gu",
      "Haitao Li",
      "Yi Yang",
      "Agen Qiu"
    ],
    "published": "2025-05-26",
    "abstract": "Self-Supervised Learning (SSL) enables us to pre-train foundation models without costly labeled data. Among SSL methods, Contrastive Learning (CL) methods are better at obtaining accurate semantic representations in noise interference. However, due to the significant domain gap, while CL methods have achieved great success in many computer vision tasks, they still require specific adaptation for Remote Sensing (RS) images. To this end, we present a novel self-supervised method called PerA, which produces all-purpose RS features through semantically Perfectly Aligned sample pairs. Specifically, PerA obtains features from sampled views by applying spatially disjoint masks to augmented images rather than random cropping. Our framework provides high-quality features by ensuring consistency between teacher and student and predicting learnable mask tokens. Compared to previous contrastive methods, our method demonstrates higher memory efficiency and can be trained with larger batches due to its sparse inputs. Additionally, the proposed method demonstrates remarkable adaptability to uncurated RS data and reduce the impact of the potential semantic inconsistency. We also collect an unlabeled pre-training dataset, which contains about 5 million RS images. We conducted experiments on multiple downstream task datasets and achieved performance comparable to previous state-of-the-art methods with a limited model scale, demonstrating the effectiveness of our approach. We hope this work will contribute to practical remote sensing interpretation works.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models",
    "url": "http://arxiv.org/abs/2505.16793v2",
    "authors": [
      "Xiang Li",
      "Yong Tao",
      "Siyuan Zhang",
      "Siwei Liu",
      "Zhitong Xiong",
      "Chunbo Luo",
      "Lu Liu",
      "Mykola Pechenizkiy",
      "Xiao Xiang Zhu",
      "Tianjin Huang"
    ],
    "published": "2025-05-22",
    "abstract": "Earth observation foundation models have shown strong generalization across multiple Earth observation tasks, but their robustness under real-world perturbations remains underexplored. To bridge this gap, we introduce REOBench, the first comprehensive benchmark for evaluating the robustness of Earth observation foundation models across six tasks and twelve types of image corruptions, including both appearance-based and geometric perturbations. To ensure realistic and fine-grained evaluation, our benchmark focuses on high-resolution optical remote sensing images, which are widely used in critical applications such as urban planning and disaster response. We conduct a systematic evaluation of a broad range of models trained using masked image modeling, contrastive learning, and vision-language pre-training paradigms. Our results reveal that (1) existing Earth observation foundation models experience significant performance degradation when exposed to input corruptions. (2) The severity of degradation varies across tasks, model architectures, backbone sizes, and types of corruption, with performance drop varying from less than 1% to over 20%. (3) Vision-language models show enhanced robustness, particularly in multimodal tasks. REOBench underscores the vulnerability of current Earth observation foundation models to real-world corruptions and provides actionable insights for developing more robust and reliable models. Code and data are publicly available at https://github.com/lx709/REOBench.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation",
    "url": "http://arxiv.org/abs/2505.16540v1",
    "authors": [
      "Inbal Cohen",
      "Boaz Meivar",
      "Peihan Tu",
      "Shai Avidan",
      "Gal Oren"
    ],
    "published": "2025-05-22",
    "abstract": "Segment Anything Models (SAM) have achieved remarkable success in object segmentation tasks across diverse datasets. However, these models are predominantly trained on large-scale semantic segmentation datasets, which introduce a bias toward object shape rather than texture cues in the image. This limitation is critical in domains such as medical imaging, material classification, and remote sensing, where texture changes define object boundaries. In this study, we investigate SAM's bias toward semantics over textures and introduce a new texture-aware foundation model, TextureSAM, which performs superior segmentation in texture-dominant scenarios. To achieve this, we employ a novel fine-tuning approach that incorporates texture augmentation techniques, incrementally modifying training images to emphasize texture features. By leveraging a novel texture-alternation of the ADE20K dataset, we guide TextureSAM to prioritize texture-defined regions, thereby mitigating the inherent shape bias present in the original SAM model. Our extensive experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation datasets. The code and texture-augmented dataset will be publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2505.15334v1",
    "authors": [
      "Bernardin Ligan",
      "Khalide Jbilou",
      "Fahd Kalloubi",
      "Ahmed Ratnani"
    ],
    "published": "2025-05-21",
    "abstract": "Foundation models have achieved great success across diverse domains, including remote sensing (RS), thanks to their versatility and strong generalization abilities. However, most RS foundation models are designed for multispectral data, while hyperspectral imagery (HSI) - with its hundreds of spectral bands - remains less explored. Fine-tuning such models for downstream tasks is also challenging, often demanding considerable memory and storage. In this paper, we propose an efficient framework to fine-tune SpectralGPT, a multispectral foundation model, for hyperspectral image classification (HSIC). We explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including Low-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank Kronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for low-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce KronA+, which applies a similar mechanism to the Kronecker matrices. We evaluate our approach on five datasets from different sensors, showing competitive performance with state-of-the-art HSI models. Our full fine-tuning (FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral foundation model on some datasets while requiring only a quarter of the training epochs. Under the same number of epochs, KronA+ reaches similar performance with far fewer trainable parameters - just 0.056 percent - and adds only approximately 0.2 megabytes of storage, making it the most effective PEFT method tested.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation",
    "url": "http://arxiv.org/abs/2505.15147v1",
    "authors": [
      "Quanwei Liu",
      "Tao Huang",
      "Yanni Dong",
      "Jiaqi Yang",
      "Wei Xiang"
    ],
    "published": "2025-05-21",
    "abstract": "Remote sensing images (RSIs) capture both natural and human-induced changes on the Earth's surface, serving as essential data for environmental monitoring, urban planning, and resource management. Semantic segmentation (SS) of RSIs enables the fine-grained interpretation of surface features, making it a critical task in remote sensing analysis. With the increasing diversity and volume of RSIs collected by sensors on various platforms, traditional processing methods struggle to maintain efficiency and accuracy. In response, deep learning (DL) has emerged as a transformative approach, enabling substantial advances in remote sensing image semantic segmentation (RSISS) by automating feature extraction and improving segmentation accuracy across diverse modalities. This paper revisits the evolution of DL-based RSISS by categorizing existing approaches into four stages: the early pixel-based methods, the prevailing patch-based and tile-based techniques, and the emerging image-based strategies enabled by foundation models. We analyze these developments from the perspective of feature extraction and learning strategies, revealing the field's progression from pixel-level to tile-level and from unimodal to multimodal segmentation. Furthermore, we conduct a comprehensive evaluation of nearly 40 advanced techniques on a unified dataset to quantitatively characterize their performance and applicability. This review offers a holistic view of DL-based SS for RS, highlighting key advancements, comparative insights, and open challenges to guide future research.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives",
    "url": "http://arxiv.org/abs/2505.14361v1",
    "authors": [
      "Xingxing Weng",
      "Chao Pang",
      "Gui-Song Xia"
    ],
    "published": "2025-05-20",
    "abstract": "Vision-language modeling (VLM) aims to bridge the information gap between images and natural language. Under the new paradigm of first pre-training on massive image-text pairs and then fine-tuning on task-specific data, VLM in the remote sensing domain has made significant progress. The resulting models benefit from the absorption of extensive general knowledge and demonstrate strong performance across a variety of remote sensing data analysis tasks. Moreover, they are capable of interacting with users in a conversational manner. In this paper, we aim to provide the remote sensing community with a timely and comprehensive review of the developments in VLM using the two-stage paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing: contrastive learning, visual instruction tuning, and text-conditioned image generation. For each category, we detail the commonly used network architecture and pre-training objectives. Second, we conduct a thorough review of existing works, examining foundation models and task-specific adaptation methods in contrastive-based VLM, architectural upgrades, training strategies and model capabilities in instruction-based VLM, as well as generative foundation models with their representative downstream applications. Third, we summarize datasets used for VLM pre-training, fine-tuning, and evaluation, with an analysis of their construction methodologies (including image sources and caption generation) and key properties, such as scale and task adaptability. Finally, we conclude this survey with insights and discussions on future research directions: cross-modal representation alignment, vague requirement comprehension, explanation-driven model reliability, continually scalable model capabilities, and large-scale datasets featuring richer modalities and greater challenges.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Image Generation"
    ],
    "is_recent": false
  },
  {
    "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts",
    "url": "http://arxiv.org/abs/2505.14088v1",
    "authors": [
      "Xi Chen",
      "Shen Yan",
      "Juelin Zhu",
      "Chen Chen",
      "Yu Liu",
      "Maojun Zhang"
    ],
    "published": "2025-05-20",
    "abstract": "We introduce Land-MoE, a novel approach for multispectral land cover classification (MLCC). Spectral shift, which emerges from disparities in sensors and geospatial conditions, poses a significant challenge in this domain. Existing methods predominantly rely on domain adaptation and generalization strategies, often utilizing small-scale models that exhibit limited performance. In contrast, Land-MoE addresses these issues by hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts, to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner. Specifically, Land-MoE comprises two key modules: the mixture of low-rank token experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages rank-differentiated tokens to generate diverse feature adjustments for individual instances within multispectral images. By dynamically combining learnable low-rank token experts of varying ranks, it enhances the robustness against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on the refined features. This process enables the model to effectively capture frequency band information that is strongly correlated with semantic essence, while simultaneously suppressing frequency noise irrelevant to the task. Comprehensive experiments on MLCC tasks involving cross-sensor and cross-geospatial setups demonstrate that Land-MoE outperforms existing methods by a large margin. Additionally, the proposed approach has also achieved state-of-the-art performance in domain generalization semantic segmentation tasks of RGB remote sensing images.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models",
    "url": "http://arxiv.org/abs/2505.12108v2",
    "authors": [
      "Jiancheng Pan",
      "Shiye Lei",
      "Yuqian Fu",
      "Jiahao Li",
      "Yanxing Liu",
      "Yuze Sun",
      "Xiao He",
      "Long Peng",
      "Xiaomeng Huang",
      "Bo Zhao"
    ],
    "published": "2025-05-17",
    "abstract": "Remote sensing image (RSI) interpretation typically faces challenges due to the scarcity of labeled data, which limits the performance of RSI interpretation tasks. To tackle this challenge, we propose EarthSynth, a diffusion-based generative foundation model that enables synthesizing multi-category, cross-satellite labeled Earth observation for downstream RSI interpretation tasks. To the best of our knowledge, EarthSynth is the first to explore multi-task generation for remote sensing, tackling the challenge of limited generalization in task-oriented synthesis for RSI interpretation. EarthSynth, trained on the EarthSynth-180K dataset, employs the Counterfactual Composition training strategy with a three-dimensional batch-sample selection mechanism to improve training data diversity and enhance category control. Furthermore, a rule-based method of R-Filter is proposed to filter more informative synthetic data for downstream tasks. We evaluate our EarthSynth on scene classification, object detection, and semantic segmentation in open-world scenarios. There are significant improvements in open-vocabulary understanding tasks, offering a practical solution for advancing RSI interpretation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing",
    "url": "http://arxiv.org/abs/2505.11121v1",
    "authors": [
      "Mathis J\u00fcrgen Adler",
      "Leonard Hackel",
      "Gencer Sumbul",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-05-16",
    "abstract": "The development of foundation models through pretraining of vision-language models (VLMs) has recently attracted great attention in remote sensing (RS). VLM pretraining aims to learn image and language alignments from a large number of image-text pairs. Each pretraining image is often associated with multiple captions containing redundant information due to repeated or semantically similar phrases, resulting in increased pretraining and inference time. To overcome this, we introduce a weighted feature aggregation (WFA) strategy for VLM pretraining in RS. Our strategy aims to extract and exploit complementary information from multiple captions per image while reducing redundancies through feature aggregation with importance weighting. To calculate adaptive importance weights for different captions of each image, we propose two techniques: (i) non-parametric uniqueness and (ii) learning-based attention. In the first technique, importance weights are calculated based on the bilingual evaluation understudy (BLEU) scores of the captions to emphasize unique sentences and reduce the influence of repetitive ones. In the second technique, importance weights are learned through an attention mechanism instead of relying on hand-crafted features. The effectiveness of the proposed WFA strategy with the two techniques is analyzed in terms of downstream performance on text-to-image retrieval in RS. Experimental results show that the proposed strategy enables efficient and effective pretraining of VLMs in RS. Based on the experimental analysis, we derive guidelines for selecting appropriate techniques depending on downstream task requirements and resource constraints. The code of this work is publicly available at https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach",
    "url": "http://arxiv.org/abs/2505.03299v1",
    "authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "St\u00e9phane May",
      "S\u00e9bastien Lef\u00e8vre"
    ],
    "published": "2025-05-06",
    "abstract": "Foundation models constitute a significant advancement in computer vision: after a single, albeit costly, training phase, they can address a wide array of tasks. In the field of Earth observation, over 75 remote sensing vision foundation models have been developed in the past four years. However, none has consistently outperformed the others across all available downstream tasks. To facilitate their comparison, we propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning on each one. This method is based on what we call \"capabilities encoding.\" The utility of this novel approach is twofold: we demonstrate its potential to simplify the selection of a foundation model for a given new task, and we employ it to offer a fresh perspective on the existing literature, suggesting avenues for future research. Codes are available at https://github.com/pierreadorni/capabilities-encoding.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery",
    "url": "http://arxiv.org/abs/2505.02829v1",
    "authors": [
      "Jerome Quenum",
      "Wen-Han Hsieh",
      "Tsung-Han Wu",
      "Ritwik Gupta",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published": "2025-05-05",
    "abstract": "Segmentation models can recognize a pre-defined set of objects in images. However, models that can reason over complex user queries that implicitly refer to multiple objects of interest are still in their infancy. Recent advances in reasoning segmentation--generating segmentation masks from complex, implicit query text--demonstrate that vision-language models can operate across an open domain and produce reasonable outputs. However, our experiments show that such models struggle with complex remote-sensing imagery. In this work, we introduce LISAt, a vision-language model designed to describe complex remote-sensing scenes, answer questions about them, and segment objects of interest. We trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES, with 27,615 annotations over 9,205 images, and a multimodal pretraining dataset, PreGRES, containing over 1 million question-answer pairs. LISAt outperforms existing geospatial foundation models such as RS-GPT4V by over 10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 % (gIoU). Our model, datasets, and code are available at https://lisat-bair.github.io/LISAt/",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "From Spaceborne to Airborne: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation",
    "url": "http://arxiv.org/abs/2505.03844v2",
    "authors": [
      "Solene Debuysere",
      "Nicolas Trouve",
      "Nathan Letheule",
      "Olivier Leveque",
      "Elise Colin"
    ],
    "published": "2025-05-05",
    "abstract": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERA's extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model \\cite{Baqu2019SethiR}. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERA's physics-based simulator EMPRISE \\cite{empriseem_ai_images}. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ],
    "is_recent": false
  },
  {
    "title": "A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning",
    "url": "http://arxiv.org/abs/2505.01558v1",
    "authors": [
      "Anan Yaghmour",
      "Melba M. Crawford",
      "Saurabh Prasad"
    ],
    "published": "2025-05-02",
    "abstract": "Remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. Advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. Domain adaptation offers a promising solution to improve model generalization. This paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. We further provide new mathematical insights into MAE-based generative learning for domain-invariant feature learning. Experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes",
    "url": "http://arxiv.org/abs/2504.20303v2",
    "authors": [
      "Junlin Guo",
      "James R. Zimmer-Dauphinee",
      "Jordan M. Nieusma",
      "Siqi Lu",
      "Quan Liu",
      "Ruining Deng",
      "Can Cui",
      "Jialin Yue",
      "Yizhe Lin",
      "Tianyuan Yao",
      "Juming Xiong",
      "Junchao Zhu",
      "Chongyu Qu",
      "Yuechen Yang",
      "Mitchell Wilkes",
      "Xiao Wang",
      "Parker VanValkenburgh",
      "Steven A. Wernke",
      "Yuankai Huo"
    ],
    "published": "2025-04-28",
    "abstract": "By mapping sites at large scales using remotely sensed data, archaeologists can generate unique insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change. Remote sensing surveys complement field-based approaches, and their reach can be especially great when combined with deep learning and computer vision techniques. However, conventional supervised deep learning methods face challenges in annotating fine-grained archaeological features at scale. While recent vision foundation models have shown remarkable success in learning large-scale remote sensing data with minimal annotations, most off-the-shelf solutions are designed for RGB images rather than multi-spectral satellite imagery, such as the 8-band data used in our study. In this paper, we introduce DeepAndes, a transformer-based vision foundation model trained on three million multi-spectral satellite images, specifically tailored for Andean archaeology. DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery, marking the first foundation model designed explicitly for the Andes region. We evaluate its image understanding performance through imbalanced image classification, image instance retrieval, and pixel-level semantic segmentation tasks. Our experiments show that DeepAndes achieves superior F1 scores, mean average precision, and Dice scores in few-shot learning scenarios, significantly outperforming models trained from scratch or pre-trained on smaller datasets. This underscores the effectiveness of large-scale self-supervised pre-training in archaeological remote sensing. Codes will be available on https://github.com/geopacha/DeepAndes.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis",
    "url": "http://arxiv.org/abs/2504.19223v3",
    "authors": [
      "Alexander Baumann",
      "Leonardo Ayala",
      "Silvia Seidlitz",
      "Jan Sellner",
      "Alexander Studier-Fischer",
      "Berkin \u00d6zdemir",
      "Lena Maier-Hein",
      "Slobodan Ilic"
    ],
    "published": "2025-04-27",
    "abstract": "Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing. However, variability in channel dimensionality and captured wavelengths among spectral cameras impede the development of AI-driven methodologies, leading to camera-specific models with limited generalizability and inadequate cross-camera applicability. To address this bottleneck, we introduce CARL, a model for Camera-Agnostic Representation Learning across RGB, multispectral, and hyperspectral imaging modalities. To enable the conversion of a spectral image with any channel dimensionality to a camera-agnostic representation, we introduce a novel spectral encoder, featuring a self-attention-cross-attention mechanism, to distill salient spectral information into learned spectral representations. Spatio-spectral pre-training is achieved with a novel feature-based self-supervision strategy tailored to CARL. Large-scale experiments across the domains of medical imaging, autonomous driving, and satellite imaging demonstrate our model's unique robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. The scalability and versatility of the proposed approach position our model as a backbone for future spectral foundation models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data",
    "url": "http://arxiv.org/abs/2504.18770v1",
    "authors": [
      "Manuel Weber",
      "Carly Beneke"
    ],
    "published": "2025-04-26",
    "abstract": "We propose PyViT-FUSE, a foundation model for earth observation data explicitly designed to handle multi-modal imagery by learning to fuse an arbitrary number of mixed-resolution input bands into a single representation through an attention mechanism. The learned patch tokens are further processed by a stack of vision transformers with a novel pyramidal structure. We train the model on a globally sampled dataset in a self-supervised manner, leveraging core concepts of the SwAV algorithm. We show the interpretability of the fusion mechanism by visualization of the attention scores and the models applicability to downstream tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology",
    "url": "http://arxiv.org/abs/2504.18256v3",
    "authors": [
      "Elena Plekhanova",
      "Damien Robert",
      "Johannes Dollinger",
      "Emilia Arens",
      "Philipp Brun",
      "Jan Dirk Wegner",
      "Niklaus Zimmermann"
    ],
    "published": "2025-04-25",
    "abstract": "With the exacerbation of the biodiversity and climate crises, macroecological pursuits such as global biodiversity mapping become more urgent. Remote sensing offers a wealth of Earth observation data for ecological studies, but the scarcity of labeled datasets remains a major challenge. Recently, self-supervised learning has enabled learning representations from unlabeled data, triggering the development of pretrained geospatial models with generalizable features. However, these models are often trained on datasets biased toward areas of high human activity, leaving entire ecological regions underrepresented. Additionally, while some datasets attempt to address seasonality through multi-date imagery, they typically follow calendar seasons rather than local phenological cycles. To better capture vegetation seasonality at a global scale, we propose a simple phenology-informed sampling strategy and introduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we train an existing model with a season-contrastive objective. We compare representations learned from SSL4Eco against other datasets on diverse ecological downstream tasks and demonstrate that our straightforward sampling method consistently improves representation quality, highlighting the importance of dataset construction. The model pretrained on SSL4Eco reaches state of the art performance on 7 out of 8 downstream tasks spanning (multi-label) classification and regression. We release our code, data, and model weights to support macroecological and computer vision research at https://github.com/PlekhanovaElena/ssl4eco.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2504.17397v2",
    "authors": [
      "Francesc Marti-Escofet",
      "Benedikt Blumenstiel",
      "Linus Scheibenreif",
      "Paolo Fraccaro",
      "Konrad Schindler"
    ],
    "published": "2025-04-24",
    "abstract": "Earth observation (EO) is crucial for monitoring environmental changes, responding to disasters, and managing natural resources. In this context, foundation models facilitate remote sensing image analysis to retrieve relevant geoinformation accurately and efficiently. However, as these models grow in size, fine-tuning becomes increasingly challenging due to the associated computational resources and costs, limiting their accessibility and scalability. Furthermore, full fine-tuning can lead to forgetting pre-trained features and even degrade model generalization. To address this, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution. In this paper, we conduct extensive experiments with various foundation model architectures and PEFT techniques to evaluate their effectiveness on five different EO datasets. Our results provide a comprehensive comparison, offering insights into when and how PEFT methods support the adaptation of pre-trained geospatial models. We demonstrate that PEFT techniques match or even exceed full fine-tuning performance and enhance model generalisation to unseen geographic regions, while reducing training time and memory requirements. Additional experiments investigate the effect of architecture choices such as the decoder type or the use of metadata, suggesting UNet decoders and fine-tuning without metadata as the recommended configuration. We have integrated all evaluated foundation models and techniques into the open-source package TerraTorch to support quick, scalable, and cost-effective model adaptation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Genealogy of Foundation Models in Remote Sensing",
    "url": "http://arxiv.org/abs/2504.17177v2",
    "authors": [
      "Kevin Lane",
      "Morteza Karimzadeh"
    ],
    "published": "2025-04-24",
    "abstract": "Foundation models have garnered increasing attention for representation learning in remote sensing. Many such foundation models adopt approaches that have demonstrated success in computer vision with minimal domain-specific modification. However, the development and application of foundation models in this field are still burgeoning, as there are a variety of competing approaches for how to most effectively leverage remotely sensed data. This paper examines these approaches, along with their roots in the computer vision field. This is done to characterize potential advantages and pitfalls, while outlining future directions to further improve remote sensing-specific foundation models. We discuss the quality of the learned representations and methods to alleviate the need for massive compute resources. We first examine single-sensor remote foundation models to introduce concepts and provide context, and then place emphasis on incorporating the multi-sensor aspect of Earth observations into foundation models. In particular, we explore the extent to which existing approaches leverage multiple sensors in training foundation models in relation to multi-modal foundation models. Finally, we identify opportunities for further harnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote sensing observations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SatelliteCalculator: A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion",
    "url": "http://arxiv.org/abs/2504.13442v1",
    "authors": [
      "Zhenyu Yu",
      "Mohd. Yamani Idna Idris",
      "Pei Wang"
    ],
    "published": "2025-04-18",
    "abstract": "Quantitative remote sensing inversion plays a critical role in environmental monitoring, enabling the estimation of key ecological variables such as vegetation indices, canopy structure, and carbon stock. Although vision foundation models have achieved remarkable progress in classification and segmentation tasks, their application to physically interpretable regression remains largely unexplored. Furthermore, the multi-spectral nature and geospatial heterogeneity of remote sensing data pose significant challenges for generalization and transferability. To address these issues, we introduce SatelliteCalculator, the first vision foundation model tailored for quantitative remote sensing inversion. By leveraging physically defined index formulas, we automatically construct a large-scale dataset of over one million paired samples across eight core ecological indicators. The model integrates a frozen Swin Transformer backbone with a prompt-guided architecture, featuring cross-attentive adapters and lightweight task-specific MLP decoders. Experiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator achieves competitive accuracy across all tasks while significantly reducing inference cost. Our results validate the feasibility of applying foundation models to quantitative inversion, and provide a scalable framework for task-adaptive remote sensing estimation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "SAM-Based Building Change Detection with Distribution-Aware Fourier Adaptation and Edge-Constrained Warping",
    "url": "http://arxiv.org/abs/2504.12619v1",
    "authors": [
      "Yun-Cheng Li",
      "Sen Lei",
      "Yi-Tao Zhao",
      "Heng-Chao Li",
      "Jun Li",
      "Antonio Plaza"
    ],
    "published": "2025-04-17",
    "abstract": "Building change detection remains challenging for urban development, disaster assessment, and military reconnaissance. While foundation models like Segment Anything Model (SAM) show strong segmentation capabilities, SAM is limited in the task of building change detection due to domain gap issues. Existing adapter-based fine-tuning approaches face challenges with imbalanced building distribution, resulting in poor detection of subtle changes and inaccurate edge extraction. Additionally, bi-temporal misalignment in change detection, typically addressed by optical flow, remains vulnerable to background noises. This affects the detection of building changes and compromises both detection accuracy and edge recognition. To tackle these challenges, we propose a new SAM-Based Network with Distribution-Aware Fourier Adaptation and Edge-Constrained Warping (FAEWNet) for building change detection. FAEWNet utilizes the SAM encoder to extract rich visual features from remote sensing images. To guide SAM in focusing on specific ground objects in remote sensing scenes, we propose a Distribution-Aware Fourier Aggregated Adapter to aggregate task-oriented changed information. This adapter not only effectively addresses the domain gap issue, but also pays attention to the distribution of changed buildings. Furthermore, to mitigate noise interference and misalignment in height offset estimation, we design a novel flow module that refines building edge extraction and enhances the perception of changed buildings. Our state-of-the-art results on the LEVIR-CD, S2Looking and WHU-CD datasets highlight the effectiveness of FAEWNet. The code is available at https://github.com/SUPERMAN123000/FAEWNet.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning",
    "url": "http://arxiv.org/abs/2504.11999v1",
    "authors": [
      "Mengyu Wang",
      "Hanbo Bi",
      "Yingchao Feng",
      "Linlin Xin",
      "Shuo Gong",
      "Tianqi Wang",
      "Zhiyuan Yan",
      "Peijin Wang",
      "Wenhui Diao",
      "Xian Sun"
    ],
    "published": "2025-04-16",
    "abstract": "Vision foundation models in remote sensing have been extensively studied due to their superior generalization on various downstream tasks. Synthetic Aperture Radar (SAR) offers all-day, all-weather imaging capabilities, providing significant advantages for Earth observation. However, establishing a foundation model for SAR image interpretation inevitably encounters the challenges of insufficient information utilization and poor interpretability. In this paper, we propose a remote sensing foundation model based on complex-valued SAR data, which simulates the polarimetric decomposition process for pre-training, i.e., characterizing pixel scattering intensity as a weighted combination of scattering bases and scattering coefficients, thereby endowing the foundation model with physical interpretability. Specifically, we construct a series of scattering queries, each representing an independent and meaningful scattering basis, which interact with SAR features in the scattering query decoder and output the corresponding scattering coefficient. To guide the pre-training process, polarimetric decomposition loss and power self-supervision loss are constructed. The former aligns the predicted coefficients with Yamaguchi coefficients, while the latter reconstructs power from the predicted coefficients and compares it to the input image's power. The performance of our foundation model is validated on six typical downstream tasks, achieving state-of-the-art results. Notably, the foundation model can extract stable feature representations and exhibits strong generalization, even in data-scarce conditions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization",
    "url": "http://arxiv.org/abs/2504.10727v1",
    "authors": [
      "Darryl Hannan",
      "John Cooper",
      "Dylan White",
      "Timothy Doster",
      "Henry Kvinge",
      "Yijing Watkins"
    ],
    "published": "2025-04-14",
    "abstract": "Multimodal large language models (MLLMs) have altered the landscape of computer vision, obtaining impressive results across a wide range of tasks, especially in zero-shot settings. Unfortunately, their strong performance does not always transfer to out-of-distribution domains, such as earth observation (EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks, such as image captioning and scene understanding, while failing at tasks that require more fine-grained spatial reasoning, such as object localization. However, MLLMs are advancing rapidly and insights quickly become out-dated. In this work, we analyze more recent MLLMs that have been explicitly trained to include fine-grained spatial reasoning capabilities, benchmarking them on EO object localization tasks. We demonstrate that these models are performant in certain settings, making them well suited for zero-shot scenarios. Additionally, we provide a detailed discussion focused on prompt selection, ground sample distance (GSD) optimization, and analyzing failure cases. We hope that this work will prove valuable as others evaluate whether an MLLM is well suited for a given EO localization task and how to optimize it.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks",
    "url": "http://arxiv.org/abs/2504.10403v3",
    "authors": [
      "Yan Zhu",
      "Jingyang Zhu",
      "Ting Wang",
      "Yuanming Shi",
      "Chunxiao Jiang",
      "Khaled Ben Letaief"
    ],
    "published": "2025-04-14",
    "abstract": "Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation",
    "url": "http://arxiv.org/abs/2504.06962v2",
    "authors": [
      "Thomas Kerdreux",
      "Alexandre Tuel",
      "Quentin Febvre",
      "Alexis Mouche",
      "Bertrand Chapron"
    ],
    "published": "2025-04-09",
    "abstract": "Self-supervised learning (SSL) has enabled the development of vision foundation models for Earth Observation (EO), demonstrating strong transferability across diverse remote sensing tasks. While prior work has focused on network architectures and training strategies, the role of dataset curation, especially in balancing and diversifying pre-training datasets, remains underexplored. In EO, this challenge is amplified by the redundancy and heavy-tailed distributions common in satellite imagery, which can lead to biased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to improve SSL pre-training by maximizing dataset diversity and balance. Our method iteratively refines the training set without requiring a pre-existing feature extractor, making it well-suited for domains where curated datasets are limited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode (WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by ocean observations. We train models from scratch on the entire Sentinel-1 WV archive spanning 10 years. Across three downstream tasks, our results show that dynamic pruning improves both computational efficiency and representation quality, leading to stronger transferability.\n  We also release the weights of OceanSAR-1, the first model in the OceanSAR family, a series of foundation models for ocean observation and analysis using SAR imagery, at github.com/galeio-research/OceanSAR-models/.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation",
    "url": "http://arxiv.org/abs/2504.06220v4",
    "authors": [
      "Xiaoxing Hu",
      "Ziyang Gong",
      "Yupei Wang",
      "Yuru Jia",
      "Fei Lin",
      "Dexiang Gao",
      "Ke An",
      "Jianhong Han",
      "Zhuoran Sun",
      "Gen Luo",
      "Gen Luo",
      "Xue Yang"
    ],
    "published": "2025-04-08",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt powerful Foundation Models (FMs) to diverse downstream tasks while preserving and unleashing their inherent capabilities. However, we have observed that existing PEFT methods, which are often designed with natural imagery in mind, struggle when applied to Remote Sensing (RS) scenarios. This is primarily due to their inability to handle artifact influences, a problem particularly severe in RS image features. To tackle this challenge, we introduce Earth-Adapter, the first PEFT method specifically designed for RS artifacts conquering. Earth-Adapter introduces a novel Mixture of Frequency Adaptation process that combines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT). By utilizing DFT, Earth-Adapter can decompose features into different frequency components, precisely separating artifacts from original features. The MoA then dynamically assigns weights to each adapter expert, allowing for the combination of features across various frequency domains. These simple-yet-effective approaches enable Earth-Adapter to more efficiently overcome the disturbances caused by artifacts than previous PEFT methods, significantly enhancing the FMs' performance on RS scenarios. Experiments on Domain Adaptation (DA), and Domain Generalization (DG) semantic segmentation benchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline Rein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG benchmarks. Our code will be released at https://github.com/VisionXLab/Earth-Adapter.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via Eliminate Before Align and Keyword Explicit Reasoning",
    "url": "http://arxiv.org/abs/2504.05644v1",
    "authors": [
      "Yan Zhang",
      "Zhong Ji",
      "Changxu Meng",
      "Yanwei Pang",
      "Jungong Han"
    ],
    "published": "2025-04-08",
    "abstract": "Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR), which aims at searching for the corresponding targets based on the given query. Among these efforts, the application of Foundation Models (FMs), such as CLIP, to the domain of remote sensing has yielded encouraging outcomes. However, existing FM based methodologies neglect the negative impact of weakly correlated sample pairs and fail to account for the key distinctions among remote sensing texts, leading to biased and superficial exploration of sample pairs. To address these challenges, we propose an approach named iEBAKER (an Improved Eliminate Before Align strategy with Keyword Explicit Reasoning framework) for RSITR. Specifically, we propose an innovative Eliminate Before Align (EBA) strategy to filter out the weakly correlated sample pairs, thereby mitigating their deviations from optimal embedding space during alignment.Further, two specific schemes are introduced from the perspective of whether local similarity and global similarity affect each other. On this basis, we introduce an alternative Sort After Reversed Retrieval (SAR) strategy, aims at optimizing the similarity matrix via reverse retrieval. Additionally, we incorporate a Keyword Explicit Reasoning (KER) module to facilitate the beneficial impact of subtle key concept distinctions. Without bells and whistles, our approach enables a direct transition from FM to RSITR task, eliminating the need for additional pretraining on remote sensing data. Extensive experiments conducted on three popular benchmark datasets demonstrate that our proposed iEBAKER method surpasses the state-of-the-art models while requiring less training data. Our source code will be released at https://github.com/zhangy0822/iEBAKER.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation",
    "url": "http://arxiv.org/abs/2504.03166v2",
    "authors": [
      "Hanbo Bi",
      "Yingchao Feng",
      "Boyuan Tong",
      "Mengyu Wang",
      "Haichen Yu",
      "Yongqiang Mao",
      "Hao Chang",
      "Wenhui Diao",
      "Peijin Wang",
      "Yue Yu",
      "Hanyang Peng",
      "Yehong Zhang",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2025-04-04",
    "abstract": "The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "A Decade of Deep Learning for Remote Sensing Spatiotemporal Fusion: Advances, Challenges, and Opportunities",
    "url": "http://arxiv.org/abs/2504.00901v2",
    "authors": [
      "Enzhe Sun",
      "Yongchuan Cui",
      "Peng Liu",
      "Jining Yan"
    ],
    "published": "2025-04-01",
    "abstract": "Remote sensing spatiotemporal fusion (STF) addresses the fundamental trade-off between temporal and spatial resolution by combining high temporal-low spatial and high spatial-low temporal imagery. This paper presents the first comprehensive survey of deep learning advances in remote sensing STF over the past decade. We establish a systematic taxonomy of deep learning architectures including Convolutional Neural Networks (CNNs), Transformers, Generative Adversarial Networks (GANs), diffusion models, and sequence models, revealing significant growth in deep learning adoption for STF tasks. Our analysis reveals that CNN-based methods dominate spatial feature extraction, while Transformer architectures show superior performance in capturing long-range temporal dependencies. GAN and diffusion models demonstrate exceptional capability in detail reconstruction, substantially outperforming traditional methods in structural similarity and spectral fidelity. Through comprehensive experiments on seven benchmark datasets comparing ten representative methods, we validate these findings and quantify the performance trade-offs between different approaches. We identify five critical challenges: time-space conflicts, limited generalization across datasets, computational efficiency for large-scale processing, multi-source heterogeneous fusion, and insufficient benchmark diversity. The survey highlights promising opportunities in foundation models, hybrid architectures, and self-supervised learning approaches that could address current limitations and enable multimodal applications. The specific models, datasets, and other information mentioned in this article have been collected in: https://github.com/yc-cui/Deep-Learning-Spatiotemporal-Fusion-Survey.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer",
      "GAN",
      "Diffusion Models"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "FlexiMo: A Flexible Remote Sensing Foundation Model",
    "url": "http://arxiv.org/abs/2503.23844v1",
    "authors": [
      "Xuyang Li",
      "Chenyu Li",
      "Pedram Ghamisi",
      "Danfeng Hong"
    ],
    "published": "2025-03-31",
    "abstract": "The rapid expansion of multi-source satellite imagery drives innovation in Earth observation, opening unprecedented opportunities for Remote Sensing Foundation Models to harness diverse data. However, many existing models remain constrained by fixed spatial resolutions and patch sizes, limiting their ability to fully exploit the heterogeneous spatial characteristics inherent in satellite imagery. To address these challenges, we propose FlexiMo, a flexible remote sensing foundation model that endows the pre-trained model with the flexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a spatial resolution-aware module that employs a parameter-free alignment embedding mechanism to dynamically recalibrate patch embeddings based on the input image's resolution and dimensions. This design not only preserves critical token characteristics and ensures multi-scale feature fidelity but also enables efficient feature extraction without requiring modifications to the underlying network architecture. In addition, FlexiMo incorporates a lightweight channel adaptation module that leverages prior spectral information from sensors. This mechanism allows the model to process images with varying numbers of channels while maintaining the data's intrinsic physical properties. Extensive experiments on diverse multimodal, multi-resolution, and multi-scale datasets demonstrate that FlexiMo significantly enhances model generalization and robustness. In particular, our method achieves outstanding performance across a range of downstream tasks, including scene classification, land cover classification, urban building segmentation, and cloud detection. By enabling parameter-efficient and physically consistent adaptation, FlexiMo paves the way for more adaptable and effective foundation models in real-world remote sensing applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Efficient Adaptation For Remote Sensing Visual Grounding",
    "url": "http://arxiv.org/abs/2503.23083v3",
    "authors": [
      "Hasan Moughnieh",
      "Mohamad Chalhoub",
      "Hasan Nasrallah",
      "Cristiano Nattero",
      "Paolo Campanella",
      "Giovanni Nico",
      "Ali J. Ghandour"
    ],
    "published": "2025-03-29",
    "abstract": "Adapting pre-trained models has become an effective strategy in artificial intelligence, offering a scalable and efficient alternative to training models from scratch. In the context of remote sensing (RS), where visual grounding(VG) remains underexplored, this approach enables the deployment of powerful vision-language models to achieve robust cross-modal understanding while significantly reducing computational overhead. To address this, we applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for RS-specific VG tasks. Specifically, we evaluated LoRA placement across different modules in Grounding DINO and used BitFit and adapters to fine-tune the OFA foundation model pre-trained on general-purpose VG datasets. This approach achieved performance comparable to or surpassing current State Of The Art (SOTA) models while significantly reducing computational costs. This study highlights the potential of PEFT techniques to advance efficient and precise multi-modal analysis in RS, offering a practical and cost-effective alternative to full model training.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery",
    "url": "http://arxiv.org/abs/2503.22516v1",
    "authors": [
      "Samira Alkaee Taleghan",
      "Morteza Karimzadeh",
      "Andrew P. Barrett",
      "Walter N. Meier",
      "Farnoush Banaei-Kashani"
    ],
    "published": "2025-03-28",
    "abstract": "Accurate segmentation of sea ice types is essential for mapping and operational forecasting of sea ice conditions for safe navigation and resource extraction in ice-covered waters, as well as for understanding polar climate processes. While deep learning methods have shown promise in automating sea ice segmentation, they often rely on extensive labeled datasets which require expert knowledge and are time-consuming to create. Recently, foundation models (FMs) have shown excellent results for segmenting remote sensing images by utilizing pre-training on large datasets using self-supervised techniques. However, their effectiveness for sea ice segmentation remains unexplored, especially given sea ice's complex structures, seasonal changes, and unique spectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagery characteristics including banding and scalloping noise, and varying ice backscatter characteristics, which are often missing in standard remote sensing pre-training datasets. In particular, SAR images over polar regions are acquired using different modes than used to capture the images at lower latitudes by the same sensors that form training datasets for FMs. This study evaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1 SAR imagery, focusing on their seasonal and spatial generalization. Among the selected models, Prithvi-600M outperforms the baseline models, while CROMA achieves a very similar performance in F1-score. Our contributions include offering a systematic methodology for selecting FMs for sea ice data analysis, a comprehensive benchmarking study on performances of FMs for sea ice segmentation with tailored performance metrics, and insights into existing gaps and future directions for improving domain-specific models in polar applications using SAR data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "A Survey on Remote Sensing Foundation Models: From Vision to Multimodality",
    "url": "http://arxiv.org/abs/2503.22081v1",
    "authors": [
      "Ziyue Huang",
      "Hongxi Yan",
      "Qiqi Zhan",
      "Shuai Yang",
      "Mingming Zhang",
      "Chenkai Zhang",
      "YiMing Lei",
      "Zeming Liu",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "published": "2025-03-28",
    "abstract": "The rapid advancement of remote sensing foundation models, particularly vision and multimodal models, has significantly enhanced the capabilities of intelligent geospatial data interpretation. These models combine various data modalities, such as optical, radar, and LiDAR imagery, with textual and geographic information, enabling more comprehensive analysis and understanding of remote sensing data. The integration of multiple modalities allows for improved performance in tasks like object detection, land cover classification, and change detection, which are often challenged by the complex and heterogeneous nature of remote sensing data. However, despite these advancements, several challenges remain. The diversity in data types, the need for large-scale annotated datasets, and the complexity of multimodal fusion techniques pose significant obstacles to the effective deployment of these models. Moreover, the computational demands of training and fine-tuning multimodal models require significant resources, further complicating their practical application in remote sensing image interpretation tasks. This paper provides a comprehensive review of the state-of-the-art in vision and multimodal foundation models for remote sensing, focusing on their architecture, training methods, datasets and application scenarios. We discuss the key challenges these models face, such as data alignment, cross-modal transfer learning, and scalability, while also identifying emerging research directions aimed at overcoming these limitations. Our goal is to provide a clear understanding of the current landscape of remote sensing foundation models and inspire future research that can push the boundaries of what these models can achieve in real-world applications. The list of resources collected by the paper can be found in the https://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2503.21841v1",
    "authors": [
      "Jingtao Li",
      "Yingyi Liu",
      "Xinyu Wang",
      "Yunning Peng",
      "Chen Sun",
      "Shaoyu Wang",
      "Zhendong Sun",
      "Tian Ke",
      "Xiao Jiang",
      "Tangwei Lu",
      "Anran Zhao",
      "Yanfei Zhong"
    ],
    "published": "2025-03-27",
    "abstract": "Advanced interpretation of hyperspectral remote sensing images benefits many precise Earth observation tasks. Recently, visual foundation models have promoted the remote sensing interpretation but concentrating on RGB and multispectral images. Due to the varied hyperspectral channels,existing foundation models would face image-by-image tuning situation, imposing great pressure on hardware and time resources. In this paper, we propose a tuning-free hyperspectral foundation model called HyperFree, by adapting the existing visual prompt engineering. To process varied channel numbers, we design a learned weight dictionary covering full-spectrum from $0.4 \\sim 2.5 \\, \u03bc\\text{m}$, supporting to build the embedding layer dynamically. To make the prompt design more tractable, HyperFree can generate multiple semantic-aware masks for one prompt by treating feature distance as semantic-similarity. After pre-training HyperFree on constructed large-scale high-resolution hyperspectral images, HyperFree (1 prompt) has shown comparable results with specialized models (5 shots) on 5 tasks and 11 datasets.Code and dataset are accessible at https://rsidea.whu.edu.cn/hyperfree.htm.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "DGTRSD & DGTRS-CLIP: A Dual-Granularity Remote Sensing Image-Text Dataset and Vision Language Foundation Model for Alignment",
    "url": "http://arxiv.org/abs/2503.19311v2",
    "authors": [
      "Weizhi Chen",
      "Yupeng Deng",
      "Jin Wei",
      "Jingbo Chen",
      "Jiansheng Chen",
      "Yuman Feng",
      "Zhihao Xi",
      "Diyou Liu",
      "Kai Li",
      "Yu Meng"
    ],
    "published": "2025-03-25",
    "abstract": "Vision Language Foundation Models based on CLIP architecture for remote sensing primarily rely on short text captions, which often result in incomplete semantic representations. Although longer captions convey richer information, existing models struggle to process them effectively because of limited text-encoding capacity, and there remains a shortage of resources that align remote sensing images with both short text and long text captions. To address this gap, we introduce DGTRSD, a dual-granularity remote sensing image-text dataset, where each image is paired with both a short text caption and a long text description, providing a solid foundation for dual-granularity semantic modeling. Based on this, we further propose DGTRS-CLIP, a dual-granularity curriculum learning framework that combines short text and long text supervision to achieve dual-granularity semantic alignment. Extensive experiments on four typical zero-shot tasks: long text cross-modal retrieval, short text cross-modal retrieval, image classification, and semantic localization demonstrate that DGTRS-CLIP consistently outperforms existing methods across all tasks. The code has been open-sourced and is available at https://github.com/MitsuiChen14/DGTRS.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2503.18540v1",
    "authors": [
      "Guneet Mutreja",
      "Philipp Schuegraf",
      "Ksenia Bittner"
    ],
    "published": "2025-03-24",
    "abstract": "Recent advances in self-supervised learning have led to the development of foundation models that have significantly advanced performance in various computer vision tasks. However, despite their potential, these models often overlook the crucial role of high-resolution digital surface models (DSMs) in understanding urban environments, particularly for building-level analysis, which is essential for applications like digital twins. To address this gap, we introduce HiRes-FusedMIM, a novel pre-trained model specifically designed to leverage the rich information contained within high-resolution RGB and DSM data. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling (SimMIM) architecture with a multi-objective loss function that combines reconstruction and contrastive objectives, enabling it to learn powerful, joint representations from both modalities. We conducted a comprehensive evaluation of HiRes-FusedMIM on a diverse set of downstream tasks, including classification, semantic segmentation, and instance segmentation. Our results demonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art geospatial methods on several building-related datasets, including WHU Aerial and LoveDA, demonstrating its effectiveness in capturing and leveraging fine-grained building information; 2) Incorporating DSMs during pre-training consistently improves performance compared to using RGB data alone, highlighting the value of elevation information for building-level analysis; 3) The dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB and DSM data, significantly outperforms a single-encoder model on the Vaihingen segmentation task, indicating the benefits of learning specialized representations for each modality. To facilitate further research and applications in this direction, we will publicly release the trained model weights.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and Perspectives",
    "url": "http://arxiv.org/abs/2503.18384v2",
    "authors": [
      "Yuan Gao",
      "Shaobo Xia",
      "Pu Wang",
      "Xiaohuan Xi",
      "Sheng Nie",
      "Cheng Wang"
    ],
    "published": "2025-03-24",
    "abstract": "Light detection and ranging (LiDAR) remote sensing encompasses two major directions: data interpretation and parameter inversion. However, both directions rely heavily on costly and labor-intensive labeled data and field measurements, which constrains their scalability and spatiotemporal adaptability. Weakly Supervised Learning (WSL) provides a unified framework to address these limitations. This paper departs from the traditional view that treats interpretation and inversion as separate tasks and offers a systematic review of recent advances in LiDAR remote sensing from a unified WSL perspective. We cover typical WSL settings including incomplete supervision(e.g., sparse point labels), inexact supervision (e.g., scene-level tags), inaccurate supervision (e.g., noisy labels), and cross-domain supervision (e.g., domain adaptation/generalization) and corresponding techniques such as pseudo-labeling, consistency regularization, self-training, and label refinement, which collectively enable robust learning from limited and weak annotations.We further analyze LiDAR-specific challenges (e.g., irregular geometry, data sparsity, domain heterogeneity) that require tailored weak supervision, and examine how sparse LiDAR observations can guide joint learning with other remote-sensing data for continuous surface-parameter retrieval. Finally, we highlight future directions where WSL acts as a bridge between LiDAR and foundation models to leverage large-scale multimodal datasets and reduce labeling costs, while also enabling broader WSL-driven advances in generalization, open-world adaptation, and scalable LiDAR remote sensing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations",
    "url": "http://arxiv.org/abs/2503.16683v1",
    "authors": [
      "Zeping Liu",
      "Fan Zhang",
      "Junfeng Jiao",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published": "2025-03-20",
    "abstract": "Advancements in vision and language foundation models have inspired the development of geo-foundation models (GeoFMs), enhancing performance across diverse geospatial tasks. However, many existing GeoFMs primarily focus on overhead remote sensing (RS) data while neglecting other data modalities such as ground-level imagery. A key challenge in multimodal GeoFM development is to explicitly model geospatial relationships across modalities, which enables generalizability across tasks, spatial scales, and temporal contexts. To address these limitations, we propose GAIR, a novel multimodal GeoFM architecture integrating overhead RS data, street view (SV) imagery, and their geolocation metadata. We utilize three factorized neural encoders to project an SV image, its geolocation, and an RS image into the embedding space. The SV image needs to be located within the RS image's spatial footprint but does not need to be at its geographic center. In order to geographically align the SV image and RS image, we propose a novel implicit neural representations (INR) module that learns a continuous RS image representation and looks up the RS embedding at the SV image's geolocation. Next, these geographically aligned SV embedding, RS embedding, and location embedding are trained with contrastive learning objectives from unlabeled data. We evaluate GAIR across 10 geospatial tasks spanning RS image-based, SV image-based, and location embedding-based benchmarks. Experimental results demonstrate that GAIR outperforms state-of-the-art GeoFMs and other strong baselines, highlighting its effectiveness in learning generalizable and transferable geospatial representations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding",
    "url": "http://arxiv.org/abs/2503.16426v1",
    "authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Bowen Chen",
      "Wenyuan Li",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-03-20",
    "abstract": "The advancement of remote sensing technology has improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretations. However, existing methods exhibit limited generalization capabilities across varied applications. While some contemporary foundation models demonstrate potential, they are hindered by insufficient cross-task adaptability and primarily process low-resolution imagery of restricted sizes, thus failing to fully exploit high-resolution data or leverage comprehensive large-scene semantics. Crucially, remote sensing imagery differs fundamentally from natural images, as key foreground targets (eg., maritime objects, artificial structures) often occupy minimal spatial proportions (~1%) and exhibit sparse distributions. Efficiently modeling cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a significant challenge yet remains critical for remote sensing image understanding. Motivated by the selective attention mechanisms inherent to the human visual system, we propose DynamicVis, a dynamic visual perception foundation model for remote sensing imagery. The framework integrates a novel dynamic region perception backbone based on the selective state space model, which strategically balances localized detail extraction with global contextual integration, enabling computationally efficient encoding of large-scale data while maintaining architectural scalability. To enhance cross-task knowledge transferring, we introduce a multi-instance learning paradigm utilizing meta-embedding representations, trained on million-scale region-level annotations. Evaluations across nine downstream tasks demonstrate the model's versatility. DynamicVis achieves multi-level feature modeling with exceptional efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and 833 MB GPU memory (3% of ViT's).",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Falcon: A Remote Sensing Vision-Language Foundation Model (Technical Report)",
    "url": "http://arxiv.org/abs/2503.11070v2",
    "authors": [
      "Kelu Yao",
      "Nuo Xu",
      "Rong Yang",
      "Yingying Xu",
      "Zhuoyan Gao",
      "Titinunt Kitrungrotsakul",
      "Yi Ren",
      "Pu Zhang",
      "Jin Wang",
      "Ning Wei",
      "Chao Li"
    ],
    "published": "2025-03-14",
    "abstract": "This paper introduces a holistic vision-language foundation model tailored for remote sensing, named Falcon. Falcon offers a unified, prompt-based paradigm that effectively executes comprehensive and complex remote sensing tasks. Falcon demonstrates powerful understanding and reasoning abilities at the image, region, and pixel levels. Specifically, given simple natural language instructions and remote sensing images, Falcon can produce impressive results in text form across 14 distinct tasks, i.e., image classification, object detection, segmentation, image captioning, and etc. To facilitate Falcon's training and empower its representation capacity to encode rich spatial and semantic information, we developed Falcon_SFT, a large-scale, multi-task, instruction-tuning dataset in the field of remote sensing. The Falcon_SFT dataset consists of approximately 78 million high-quality data samples, covering 5.6 million multi-spatial resolution and multi-view remote sensing images with diverse instructions. It features hierarchical annotations and undergoes manual sampling verification to ensure high data quality and reliability. Extensive comparative experiments are conducted, which verify that Falcon achieves remarkable performance over 67 datasets and 14 tasks, despite having only 0.7B parameters. We release the complete dataset, code, and model weights at https://github.com/TianHuiLab/Falcon, hoping to help further develop the open-source community.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Towards Privacy-preserved Pre-training of Remote Sensing Foundation Models with Federated Mutual-guidance Learning",
    "url": "http://arxiv.org/abs/2503.11051v1",
    "authors": [
      "Jieyi Tan",
      "Chengwei Zhang",
      "Bo Dang",
      "Yansheng Li"
    ],
    "published": "2025-03-14",
    "abstract": "Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with a data-centralized paradigm, through self-supervision on large-scale curated remote sensing data. For each institution, however, pre-training RSFMs with limited data in a standalone manner may lead to suboptimal performance, while aggregating remote sensing data from multiple institutions for centralized pre-training raises privacy concerns. Seeking for collaboration is a promising solution to resolve this dilemma, where multiple institutions can collaboratively train RSFMs without sharing private data. In this paper, we propose a novel privacy-preserved pre-training framework (FedSense), which enables multiple institutions to collaboratively train RSFMs without sharing private data. However, it is a non-trivial task hindered by a vicious cycle, which results from model drift by remote sensing data heterogeneity and high communication overhead. To break this vicious cycle, we introduce Federated Mutual-guidance Learning. Specifically, we propose a Server-to-Clients Guidance (SCG) mechanism to guide clients updates towards global-flatness optimal solutions. Additionally, we propose a Clients-to-Server Guidance (CSG) mechanism to inject local knowledge into the server by low-bit communication. Extensive experiments on four downstream tasks demonstrate the effectiveness of our FedSense in both full-precision and communication-reduced scenarios, showcasing remarkable communication efficiency and performance gains.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing",
    "url": "http://arxiv.org/abs/2503.10392v2",
    "authors": [
      "Fengxiang Wang",
      "Yulin Wang",
      "Mingshuo Chen",
      "Haiyan Zhao",
      "Yangang Sun",
      "Shuo Wang",
      "Hongzhen Wang",
      "Di Wang",
      "Long Lan",
      "Wenjing Yang",
      "Jing Zhang"
    ],
    "published": "2025-03-13",
    "abstract": "Recent advances in self-supervised learning for Vision Transformers (ViTs) have fueled breakthroughs in remote sensing (RS) foundation models. However, the quadratic complexity of self-attention poses a significant barrier to scalability, particularly for large models and high-resolution images. While the linear-complexity Mamba architecture offers a promising alternative, existing RS applications of Mamba remain limited to supervised tasks on small, domain-specific datasets. To address these challenges, we propose RoMA, a framework that enables scalable self-supervised pretraining of Mamba-based RS foundation models using large-scale, diverse, unlabeled data. RoMA enhances scalability for high-resolution images through a tailored auto-regressive learning strategy, incorporating two key innovations: 1) a rotation-aware pretraining mechanism combining adaptive cropping with angular embeddings to handle sparsely distributed objects with arbitrary orientations, and 2) multi-scale token prediction objectives that address the extreme variations in object scales inherent to RS imagery. Systematic empirical studies validate that Mamba adheres to RS data and parameter scaling laws, with performance scaling reliably as model and data size increase. Furthermore, experiments across scene classification, object detection, and semantic segmentation tasks demonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based counterparts in both accuracy and computational efficiency. The source code and pretrained models will be released at https://github.com/MiliLab/RoMA.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Light-weighted foundation model for seismic data processing based on representative and non-redundant pre-training dataset",
    "url": "http://arxiv.org/abs/2503.10092v1",
    "authors": [
      "Xintong Dong",
      "Wenshuo Yu",
      "Jun Lin",
      "Zhenbo Guo",
      "Hongzhou Wang",
      "Jianhao Yang"
    ],
    "published": "2025-03-13",
    "abstract": "In the fields of computer vision (CV) and remote sensing (RS), foundational models typically follow the \"big data + large model parameters\" paradigm. However, the application of this strategy in seismic data processing faces several challenges: seismic data is difficult to obtain and the scarcity of publicly available datasets make it difficult to construct large-scale datasets. Additionally, the high computational cost associated with a large number of model parameters restricts widespread research in this domain. Therefore, we propose a lightweight seismic processing foundational model paradigm (SPFM), which aims to overcome the limitations of traditional methods by data engineering and network architecture innovation. Specifically, we propose an innovative dataset construction strategy that generates more seismic data by data augmentation techniques, including collecting publicly available field data and using generative diffusion models (GDM) for data enhancement. Furthermore, we optimize the data distribution by employing dimensionality reduction, cluster analysis, and stratified sampling methods, reducing redundant information while preserving important seismic features, thus constructing a comprehensive dataset. In terms of network architecture design, we introduce the selective structured state-space model (Mamba) structure, which effectively captures global features of seismic data and alleviates the quadratic growth of computational complexity inherent in Transformer-based models, thereby improving computational efficiency. This model, pre-trained with only four A800 GPUs, outperforms traditional methods across multiple tasks, including denoising, interpolation, frequency-band extrapolation, and resolution enhancement. The lightweight paradigm provides an solution for seismic data processing, advancing the generalization and accessibility of seismic data processing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Diffusion Models"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Isolated Channel Vision Transformers: From Single-Channel Pretraining to Multi-Channel Finetuning",
    "url": "http://arxiv.org/abs/2503.09826v2",
    "authors": [
      "Wenyi Lian",
      "Patrick Micke",
      "Joakim Lindblad",
      "Nata\u0161a Sladoje"
    ],
    "published": "2025-03-12",
    "abstract": "Vision Transformers (ViTs) have achieved remarkable success in standard RGB image processing tasks. However, applying ViTs to multi-channel imaging (MCI) data, e.g., for medical and remote sensing applications, remains a challenge. In particular, MCI data often consist of layers acquired from different modalities. Directly training ViTs on such data can obscure complementary information and impair the performance. In this paper, we introduce a simple yet effective pretraining framework for large-scale MCI datasets. Our method, named Isolated Channel ViT (IC-ViT), patchifies image channels individually and thereby enables pretraining for multimodal multi-channel tasks. We show that this channel-wise patchifying is a key technique for MCI processing. More importantly, one can pretrain the IC-ViT on single channels and finetune it on downstream multi-channel datasets. This pretraining framework captures dependencies between patches as well as channels and produces robust feature representation. Experiments on various tasks and benchmarks, including JUMP-CP and CHAMMI for cell microscopy imaging, and So2Sat-LCZ42 for satellite imaging, show that the proposed IC-ViT delivers 4-14 percentage points of performance improvement over existing channel-adaptive approaches. Further, its efficient training makes it a suitable candidate for large-scale pretraining of foundation models on heterogeneous data. Our code is available at https://github.com/shermanlian/IC-ViT.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Visual and Text Prompt Segmentation: A Novel Multi-Model Framework for Remote Sensing",
    "url": "http://arxiv.org/abs/2503.07911v1",
    "authors": [
      "Xing Zi",
      "Kairui Jin",
      "Xian Tao",
      "Jun Li",
      "Ali Braytee",
      "Rajiv Ratn Shah",
      "Mukesh Prasad"
    ],
    "published": "2025-03-10",
    "abstract": "Pixel-level segmentation is essential in remote sensing, where foundational vision models like CLIP and Segment Anything Model(SAM) have demonstrated significant capabilities in zero-shot segmentation tasks. Despite their advances, challenges specific to remote sensing remain substantial. Firstly, The SAM without clear prompt constraints, often generates redundant masks, and making post-processing more complex. Secondly, the CLIP model, mainly designed for global feature alignment in foundational models, often overlooks local objects crucial to remote sensing. This oversight leads to inaccurate recognition or misplaced focus in multi-target remote sensing imagery. Thirdly, both models have not been pre-trained on multi-scale aerial views, increasing the likelihood of detection failures. To tackle these challenges, we introduce the innovative VTPSeg pipeline, utilizing the strengths of Grounding DINO, CLIP, and SAM for enhanced open-vocabulary image segmentation. The Grounding DINO+(GD+) module generates initial candidate bounding boxes, while the CLIP Filter++(CLIP++) module uses a combination of visual and textual prompts to refine and filter out irrelevant object bounding boxes, ensuring that only pertinent objects are considered. Subsequently, these refined bounding boxes serve as specific prompts for the FastSAM model, which executes precise segmentation. Our VTPSeg is validated by experimental and ablation study results on five popular remote sensing image segmentation datasets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?",
    "url": "http://arxiv.org/abs/2503.07890v2",
    "authors": [
      "Yuru Jia",
      "Valerio Marsocci",
      "Ziyang Gong",
      "Xue Yang",
      "Maarten Vergauwen",
      "Andrea Nascetti"
    ],
    "published": "2025-03-10",
    "abstract": "Self-supervised learning (SSL) has revolutionized representation learning in Remote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage vast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs primarily employ objectives like contrastive learning or masked image modeling, owing to their proven success in learning transferable representations. However, generative diffusion models, which demonstrate the potential to capture multi-grained semantics essential for RS tasks during image generation, remain underexplored for discriminative applications. This prompts the question: can generative diffusion models also excel and serve as GFMs with sufficient discriminative power? In this work, we answer this question with SatDiFuser, a framework that transforms a diffusion-based generative geospatial foundation model into a powerful pretraining tool for discriminative RS. By systematically analyzing multi-stage, noise-dependent diffusion features, we develop three fusion strategies to effectively leverage these diverse representations. Extensive experiments on remote sensing benchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving gains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in classification, demonstrating the capacity of diffusion-based generative foundation models to rival or exceed discriminative GFMs. The source code is available at: https://github.com/yurujaja/SatDiFuser.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Image Generation"
    ],
    "is_recent": false
  },
  {
    "title": "A Recipe for Improving Remote Sensing VLM Zero Shot Generalization",
    "url": "http://arxiv.org/abs/2503.08722v2",
    "authors": [
      "Aviad Barzilai",
      "Yotam Gigi",
      "Amr Helmy",
      "Vered Silverman",
      "Yehonathan Refael",
      "Bolous Jaber",
      "Tomer Shekel",
      "George Leifman",
      "Genady Beryozkin"
    ],
    "published": "2025-03-10",
    "abstract": "Foundation models have had a significant impact across various AI applications, enabling use cases that were previously impossible. Contrastive Visual Language Models (VLMs), in particular, have outperformed other techniques in many tasks. However, their prevalence in remote sensing (RS) is still limited, due to the scarcity of diverse remote-sensing visual-language datasets. In this work we introduce two novel image-caption datasets for training of remote sensing foundation models. The first dataset pairs aerial and satellite imagery with captions generated by Gemini using landmarks extracted from Google Maps. The second dataset utilizes public web images and their corresponding alt-text, filtered for the remote sensing domain, resulting in a diverse dataset with greater breadth in image styles and subject matter. These datasets are used to pre-train the MaMMUT~\\citep{kuo2023mammutsimplearchitecturejoint} VLM architecture, resulting in state-of-the-art generalization performance in zero-shot cross-modal retrieval on well-known public benchmarks. Finally, we present our ongoing research to distill image-level knowledge gained in the VLM contrastive training procedure to enhance the model's localization ability. Specifically, we iteratively generate pseudo-labels for image regions based on the model's attention maps and use these labels for further training. To mitigate noisy attention maps and create robust segmentation masks, we introduce a novel attention-pooling mechanism called the Smooth-Attention-Operation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "A General Purpose Spectral Foundational Model for Both Proximal and Remote Sensing Spectral Imaging",
    "url": "http://arxiv.org/abs/2503.01628v1",
    "authors": [
      "William Michael Laprade",
      "Jesper Cairo Westergaard",
      "Svend Christensen",
      "Mads Nielsen",
      "Anders Bjorholm Dahl"
    ],
    "published": "2025-03-03",
    "abstract": "Spectral imaging data acquired via multispectral and hyperspectral cameras can have hundreds of channels, where each channel records the reflectance at a specific wavelength and bandwidth. Time and resource constraints limit our ability to collect large spectral datasets, making it difficult to build and train predictive models from scratch. In the RGB domain, we can often alleviate some of the limitations of smaller datasets by using pretrained foundational models as a starting point. However, most existing foundation models are pretrained on large datasets of 3-channel RGB images, severely limiting their effectiveness when used with spectral imaging data. The few spectral foundation models that do exist usually have one of two limitations: (1) they are built and trained only on remote sensing data limiting their application in proximal spectral imaging, (2) they utilize the more widely available multispectral imaging datasets with less than 15 channels restricting their use with hundred-channel hyperspectral images. To alleviate these issues, we propose a large-scale foundational model and dataset built upon the masked autoencoder architecture that takes advantage of spectral channel encoding, spatial-spectral masking and ImageNet pretraining for an adaptable and robust model for downstream spectral imaging tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Lossy Neural Compression for Geospatial Analytics: A Review",
    "url": "http://arxiv.org/abs/2503.01505v2",
    "authors": [
      "Carlos Gomes",
      "Isabelle Wittmann",
      "Damien Robert",
      "Johannes Jakubik",
      "Tim Reichelt",
      "Michele Martone",
      "Stefano Maurogiovanni",
      "Rikard Vinge",
      "Jonas Hurst",
      "Erik Scheurer",
      "Rocco Sedona",
      "Thomas Brunschwiler",
      "Stefan Kesselheim",
      "Matej Batic",
      "Philip Stier",
      "Jan Dirk Wegner",
      "Gabriele Cavallaro",
      "Edzer Pebesma",
      "Michael Marszalek",
      "Miguel A Belenguer-Plomer",
      "Kennedy Adriko",
      "Paolo Fraccaro",
      "Romeo Kienzler",
      "Rania Briq",
      "Sabrina Benassou",
      "Michele Lazzarini",
      "Conrad M Albrecht"
    ],
    "published": "2025-03-03",
    "abstract": "Over the past decades, there has been an explosion in the amount of available Earth Observation (EO) data. The unprecedented coverage of the Earth's surface and atmosphere by satellite imagery has resulted in large volumes of data that must be transmitted to ground stations, stored in data centers, and distributed to end users. Modern Earth System Models (ESMs) face similar challenges, operating at high spatial and temporal resolutions, producing petabytes of data per simulated day. Data compression has gained relevance over the past decade, with neural compression (NC) emerging from deep learning and information theory, making EO data and ESM outputs ideal candidates due to their abundance of unlabeled data. In this review, we outline recent developments in NC applied to geospatial data. We introduce the fundamental concepts of NC including seminal works in its traditional applications to image and video compression domains with focus on lossy compression. We discuss the unique characteristics of EO and ESM data, contrasting them with \"natural images\", and explain the additional challenges and opportunities they present. Moreover, we review current applications of NC across various EO modalities and explore the limited efforts in ESM compression to date. The advent of self-supervised learning (SSL) and foundation models (FM) has advanced methods to efficiently distill representations from vast unlabeled data. We connect these developments to NC for EO, highlighting the similarities between the two fields and elaborate on the potential of transferring compressed feature representations for machine--to--machine communication. Based on insights drawn from this review, we devise future directions relevant to applications in EO and ESM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking",
    "url": "http://arxiv.org/abs/2502.18748v1",
    "authors": [
      "Shaheer Mohamed",
      "Tharindu Fernando",
      "Sridha Sridharan",
      "Peyman Moghadam",
      "Clinton Fookes"
    ],
    "published": "2025-02-26",
    "abstract": "Hyperspectral object tracking using snapshot mosaic cameras is emerging as it provides enhanced spectral information alongside spatial data, contributing to a more comprehensive understanding of material properties. Using transformers, which have consistently outperformed convolutional neural networks (CNNs) in learning better feature representations, would be expected to be effective for Hyperspectral object tracking. However, training large transformers necessitates extensive datasets and prolonged training periods. This is particularly critical for complex tasks like object tracking, and the scarcity of large datasets in the hyperspectral domain acts as a bottleneck in achieving the full potential of powerful transformer models. This paper proposes an effective methodology that adapts large pretrained transformer-based foundation models for hyperspectral object tracking. We propose an adaptive, learnable spatial-spectral token fusion module that can be extended to any transformer-based backbone for learning inherent spatial-spectral features in hyperspectral data. Furthermore, our model incorporates a cross-modality training pipeline that facilitates effective learning across hyperspectral datasets collected with different sensor modalities. This enables the extraction of complementary knowledge from additional modalities, whether or not they are present during testing. Our proposed model also achieves superior performance with minimal training iterations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching",
    "url": "http://arxiv.org/abs/2502.18104v1",
    "authors": [
      "Han Nie",
      "Bin Luo",
      "Jun Liu",
      "Zhitao Fu",
      "Huan Zhou",
      "Shuo Zhang",
      "Weixing Liu"
    ],
    "published": "2025-02-25",
    "abstract": "The ideal goal of image matching is to achieve stable and efficient performance in unseen domains. However, many existing learning-based optical-SAR image matching methods, despite their effectiveness in specific scenarios, exhibit limited generalization and struggle to adapt to practical applications. Repeatedly training or fine-tuning matching models to address domain differences is not only not elegant enough but also introduces additional computational overhead and data production costs. In recent years, general foundation models have shown great potential for enhancing generalization. However, the disparity in visual domains between natural and remote sensing images poses challenges for their direct application. Therefore, effectively leveraging foundation models to improve the generalization of optical-SAR image matching remains challenge. To address the above challenges, we propose PromptMID, a novel approach that constructs modality-invariant descriptors using text prompts based on land use classification as priors information for optical and SAR image matching. PromptMID extracts multi-scale modality-invariant features by leveraging pre-trained diffusion models and visual foundation models (VFMs), while specially designed feature aggregation modules effectively fuse features across different granularities. Extensive experiments on optical-SAR image datasets from four diverse regions demonstrate that PromptMID outperforms state-of-the-art matching methods, achieving superior results in both seen and unseen domains and exhibiting strong cross-domain generalization capabilities. The source code will be made publicly available https://github.com/HanNieWHU/PromptMID.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "CARE: Confidence-Aware Regression Estimation of building density fine-tuning EO Foundation Models",
    "url": "http://arxiv.org/abs/2502.13734v2",
    "authors": [
      "Nikolaos Dionelis",
      "Jente Bosmans",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-02-19",
    "abstract": "Performing accurate confidence quantification and assessment in pixel-wise regression tasks, which are downstream applications of AI Foundation Models for Earth Observation (EO), is important for deep neural networks to predict their failures, improve their performance and enhance their capabilities in real-world applications, for their practical deployment. For pixel-wise regression tasks, specifically utilizing remote sensing data from satellite imagery in EO Foundation Models, confidence quantification is a critical challenge. The focus of this research work is on developing a Foundation Model using EO satellite data that computes and assigns a confidence metric alongside regression outputs to improve the reliability and interpretability of predictions generated by deep neural networks. To this end, we develop, train and evaluate the proposed Confidence-Aware Regression Estimation (CARE) Foundation Model. Our model CARE computes and assigns confidence to regression results as downstream tasks of a Foundation Model for EO data, and performs a confidence-aware self-corrective learning method for the low-confidence regions. We evaluate the model CARE, and experimental results on multi-spectral data from the Copernicus Sentinel-2 satellite constellation to estimate the building density (i.e. monitoring urban growth), show that the proposed method can be successfully applied to important regression problems in EO and remote sensing. We also show that our model CARE outperforms other baseline methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "S2C: Learning Noise-Resistant Differences for Unsupervised Change Detection in Multimodal Remote Sensing Images",
    "url": "http://arxiv.org/abs/2502.12604v1",
    "authors": [
      "Lei Ding",
      "Xibing Zuo",
      "Danfeng Hong",
      "Haitao Guo",
      "Jun Lu",
      "Zhihui Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-02-18",
    "abstract": "Unsupervised Change Detection (UCD) in multimodal Remote Sensing (RS) images remains a difficult challenge due to the inherent spatio-temporal complexity within data, and the heterogeneity arising from different imaging sensors. Inspired by recent advancements in Visual Foundation Models (VFMs) and Contrastive Learning (CL) methodologies, this research aims to develop CL methodologies to translate implicit knowledge in VFM into change representations, thus eliminating the need for explicit supervision. To this end, we introduce a Semantic-to-Change (S2C) learning framework for UCD in both homogeneous and multimodal RS images. Differently from existing CL methodologies that typically focus on learning multi-temporal similarities, we introduce a novel triplet learning strategy that explicitly models temporal differences, which are crucial to the CD task. Furthermore, random spatial and spectral perturbations are introduced during the training to enhance robustness to temporal noise. In addition, a grid sparsity regularization is defined to suppress insignificant changes, and an IoU-matching algorithm is developed to refine the CD results. Experiments on four benchmark CD datasets demonstrate that the proposed S2C learning framework achieves significant improvements in accuracy, surpassing current state-of-the-art by over 31\\%, 9\\%, 23\\%, and 15\\%, respectively. It also demonstrates robustness and sample efficiency, suitable for training and adaptation of various Visual Foundation Models (VFMs) or backbone neural networks. The relevant code will be available at: github.com/DingLei14/S2C.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "A Survey of Sample-Efficient Deep Learning for Change Detection in Remote Sensing: Tasks, Strategies, and Challenges",
    "url": "http://arxiv.org/abs/2502.02835v1",
    "authors": [
      "Lei Ding",
      "Danfeng Hong",
      "Maofan Zhao",
      "Hongruixuan Chen",
      "Chenyu Li",
      "Jie Deng",
      "Naoto Yokoya",
      "Lorenzo Bruzzone",
      "Jocelyn Chanussot"
    ],
    "published": "2025-02-05",
    "abstract": "In the last decade, the rapid development of deep learning (DL) has made it possible to perform automatic, accurate, and robust Change Detection (CD) on large volumes of Remote Sensing Images (RSIs). However, despite advances in CD methods, their practical application in real-world contexts remains limited due to the diverse input data and the applicational context. For example, the collected RSIs can be time-series observations, and more informative results are required to indicate the time of change or the specific change category. Moreover, training a Deep Neural Network (DNN) requires a massive amount of training samples, whereas in many cases these samples are difficult to collect. To address these challenges, various specific CD methods have been developed considering different application scenarios and training resources. Additionally, recent advancements in image generation, self-supervision, and visual foundation models (VFMs) have opened up new approaches to address the 'data-hungry' issue of DL-based CD. The development of these methods in broader application scenarios requires further investigation and discussion. Therefore, this article summarizes the literature methods for different CD tasks and the available strategies and techniques to train and deploy DL-based CD methods in sample-limited scenarios. We expect that this survey can provide new insights and inspiration for researchers in this field to develop more effective CD methods that can be applied in a wider range of contexts.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Detection",
      "Image Generation"
    ],
    "is_recent": false
  },
  {
    "title": "SatMamba: Development of Foundation Models for Remote Sensing Imagery Using State Space Models",
    "url": "http://arxiv.org/abs/2502.00435v1",
    "authors": [
      "Chuc Man Duc",
      "Hiromichi Fukui"
    ],
    "published": "2025-02-01",
    "abstract": "Foundation models refer to deep learning models pretrained on large unlabeled datasets through self-supervised algorithms. In the Earth science and remote sensing communities, there is growing interest in transforming the use of Earth observation data, including satellite and aerial imagery, through foundation models. Various foundation models have been developed for remote sensing, such as those for multispectral, high-resolution, and hyperspectral images, and have demonstrated superior performance on various downstream tasks compared to traditional supervised models. These models are evolving rapidly, with capabilities to handle multispectral, multitemporal, and multisensor data. Most studies use masked autoencoders in combination with Vision Transformers (ViTs) as the backbone for pretraining. While the models showed promising performance, ViTs face challenges, such as quadratic computational scaling with input length, which may limit performance on multiband and multitemporal data with long sequences. This research aims to address these challenges by proposing SatMamba, a new pretraining framework that combines masked autoencoders with State Space Model, offering linear computational scaling. Experiments on high-resolution imagery across various downstream tasks show promising results, paving the way for more efficient foundation models and unlocking the full potential of Earth observation data. The source code is available in https://github.com/mdchuc/HRSFM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Simple Aerial Detection Baseline of Multimodal Language Models",
    "url": "http://arxiv.org/abs/2501.09720v3",
    "authors": [
      "Qingyun Li",
      "Yushi Chen",
      "Xinya Shu",
      "Dong Chen",
      "Xin He",
      "Yi Yu",
      "Xue Yang"
    ],
    "published": "2025-01-16",
    "abstract": "The multimodal language models (MLMs) based on generative pre-trained Transformer are considered powerful candidates for unifying various domains and tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding performance in multiple tasks, such as visual question answering and visual grounding. In addition to visual grounding that detects specific objects corresponded to given instruction, aerial detection, which detects all objects of multiple categories, is also a valuable and challenging task for RS foundation models. However, aerial detection has not been explored by existing RS MLMs because the autoregressive prediction mechanism of MLMs differs significantly from the detection outputs. In this paper, we present a simple baseline for applying MLMs to aerial detection for the first time, named LMMRotate. Specifically, we first introduce a normalization method to transform detection outputs into textual outputs to be compatible with the MLM framework. Then, we propose a evaluation method, which ensures a fair comparison between MLMs and conventional object detection models. We construct the baseline by fine-tuning open-source general-purpose MLMs and achieve impressive detection performance comparable to conventional detector. We hope that this baseline will serve as a reference for future MLM development, enabling more comprehensive capabilities for understanding RS images. Code is available at https://github.com/Li-Qingyun/mllm-mmrotate.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation Models",
    "url": "http://arxiv.org/abs/2501.06809v1",
    "authors": [
      "Keyan Chen",
      "Jiafan Zhang",
      "Chenyang Liu",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-12",
    "abstract": "Referring remote sensing image segmentation is crucial for achieving fine-grained visual understanding through free-format textual input, enabling enhanced scene and object extraction in remote sensing applications. Current research primarily utilizes pre-trained language models to encode textual descriptions and align them with visual modalities, thereby facilitating the expression of relevant visual features. However, these approaches often struggle to establish robust alignments between fine-grained semantic concepts, leading to inconsistent representations across textual and visual information. To address these limitations, we introduce a referring remote sensing image segmentation foundational model, RSRefSeg. RSRefSeg leverages CLIP for visual and textual encoding, employing both global and local textual semantics as filters to generate referring-related visual activation features in the latent space. These activated features then serve as input prompts for SAM, which refines the segmentation masks through its robust visual generalization capabilities. Experimental results on the RRSIS-D dataset demonstrate that RSRefSeg outperforms existing methods, underscoring the effectiveness of foundational models in enhancing multimodal task comprehension. The code is available at \\url{https://github.com/KyanChen/RSRefSeg}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Semantic-CD: Remote Sensing Image Semantic Change Detection towards Open-vocabulary Setting",
    "url": "http://arxiv.org/abs/2501.06808v1",
    "authors": [
      "Yongshuo Zhu",
      "Lu Li",
      "Keyan Chen",
      "Chenyang Liu",
      "Fugen Zhou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-12",
    "abstract": "Remote sensing image semantic change detection is a method used to analyze remote sensing images, aiming to identify areas of change as well as categorize these changes within images of the same location taken at different times. Traditional change detection methods often face challenges in generalizing across semantic categories in practical scenarios. To address this issue, we introduce a novel approach called Semantic-CD, specifically designed for semantic change detection in remote sensing images. This method incorporates the open vocabulary semantics from the vision-language foundation model, CLIP. By utilizing CLIP's extensive vocabulary knowledge, our model enhances its ability to generalize across categories and improves segmentation through fully decoupled multi-task learning, which includes both binary change detection and semantic change detection tasks. Semantic-CD consists of four main components: a bi-temporal CLIP visual encoder for extracting features from bi-temporal images, an open semantic prompter for creating semantic cost volume maps with open vocabulary, a binary change detection decoder for generating binary change detection masks, and a semantic change detection decoder for producing semantic labels. Experimental results on the SECOND dataset demonstrate that Semantic-CD achieves more accurate masks and reduces semantic classification errors, illustrating its effectiveness in applying semantic priors from vision-language foundation models to SCD tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model",
    "url": "http://arxiv.org/abs/2501.00895v2",
    "authors": [
      "Chenyang Liu",
      "Keyan Chen",
      "Rui Zhao",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2025-01-01",
    "abstract": "Generative foundation models have advanced large-scale text-driven natural image generation, becoming a prominent research trend across various vertical domains. However, in the remote sensing field, there is still a lack of research on large-scale text-to-image (text2image) generation technology. Existing remote sensing image-text datasets are small in scale and confined to specific geographic areas and scene types. Besides, existing text2image methods have struggled to achieve global-scale, multi-resolution controllable, and unbounded image generation. To address these challenges, this paper presents two key contributions: the Git-10M dataset and the Text2Earth foundation model. Git-10M is a global-scale image-text dataset comprising 10.5 million image-text pairs, 5 times larger than the previous largest one. The dataset covers a wide range of geographic scenes and contains resolution information, significantly surpassing existing datasets in both size and diversity. Building on Git-10M, we propose Text2Earth, a 1.3 billion parameter generative foundation model based on the diffusion framework to model global-scale remote sensing scenes. Text2Earth integrates a resolution guidance mechanism, enabling users to specify image resolutions. A dynamic condition adaptation strategy is proposed for training and inference to improve image quality. Text2Earth excels in zero-shot text2image generation and demonstrates robust generalization and flexibility across multiple tasks, including unbounded scene construction, image editing, and cross-modal image generation. This robust capability surpasses previous models restricted to the basic fixed size and limited scene types. On the previous benchmark dataset, Text2Earth outperforms previous models with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA metric.Our project page is https://chen-yang-liu.github.io/Text2Earth",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Image Generation"
    ],
    "is_recent": false
  },
  {
    "title": "SeaMo: A Season-Aware Multimodal Foundation Model for Remote Sensing",
    "url": "http://arxiv.org/abs/2412.19237v2",
    "authors": [
      "Xuyang Li",
      "Chenyu Li",
      "Gemine Vivone",
      "Danfeng Hong"
    ],
    "published": "2024-12-26",
    "abstract": "Remote Sensing (RS) data encapsulates rich multi-dimensional information essential for Earth observation. Its vast volume, diverse sources, and temporal continuity make it particularly well-suited for developing large Visual Foundation Models (VFMs). These models serve as powerful feature extractors, leveraging extensive RS data for pretraining and subsequent fine-tuning in various geoscientific applications. However, existing VFMs in the RS domain often concentrate on specific image characteristics, neglecting the full season-aware potential of RS data. To bridge this gap, we introduce SeaMo, a novel VFM that effectively integrates multimodal and multi-seasonal RS information. SeaMo leverages a masked image modeling framework to fully exploit the spatial, spectral, and seasonal dimensions of RS data. Specifically, we employ unaligned spatial region selection to capture spatial heterogeneity, incorporate multi-source inputs for enhanced multimodal integration, and introduce temporal-multimodal fusion blocks to assimilate seasonal variations effectively. By explicitly modeling the complex, season-dependent attributes of RS data, SeaMo enhances generalization, robustness, and adaptability across geoscientific tasks. Extensive experiments and ablation studies demonstrate its superior performance, underscoring its potential as a foundational model for Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SAModified: A Foundation Model-Based Zero-Shot Approach for Refining Noisy Land-Use Land-Cover Maps",
    "url": "http://arxiv.org/abs/2412.12552v1",
    "authors": [
      "Sparsh Pekhale",
      "Rakshith Sathish",
      "Sathisha Basavaraju",
      "Divya Sharma"
    ],
    "published": "2024-12-17",
    "abstract": "Land-use and land cover (LULC) analysis is critical in remote sensing, with wide-ranging applications across diverse fields such as agriculture, utilities, and urban planning. However, automating LULC map generation using machine learning is rendered challenging due to noisy labels. Typically, the ground truths (e.g. ESRI LULC, MapBioMass) have noisy labels that hamper the model's ability to learn to accurately classify the pixels. Further, these erroneous labels can significantly distort the performance metrics of a model, leading to misleading evaluations. Traditionally, the ambiguous labels are rectified using unsupervised algorithms. These algorithms struggle not only with scalability but also with generalization across different geographies. To overcome these challenges, we propose a zero-shot approach using the foundation model, Segment Anything Model (SAM), to automatically delineate different land parcels/regions and leverage them to relabel the unsure pixels by using the local label statistics within each detected region. We achieve a significant reduction in label noise and an improvement in the performance of the downstream segmentation model by $\\approx 5\\%$ when trained with denoised labels.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications",
    "url": "http://arxiv.org/abs/2412.02732v2",
    "authors": [
      "Daniela Szwarcman",
      "Sujit Roy",
      "Paolo Fraccaro",
      "\u00deorsteinn El\u00ed G\u00edslason",
      "Benedikt Blumenstiel",
      "Rinki Ghosal",
      "Pedro Henrique de Oliveira",
      "Joao Lucas de Sousa Almeida",
      "Rocco Sedona",
      "Yanghui Kang",
      "Srija Chakraborty",
      "Sizhe Wang",
      "Carlos Gomes",
      "Ankur Kumar",
      "Myscon Truong",
      "Denys Godwin",
      "Hyunho Lee",
      "Chia-Yu Hsu",
      "Ata Akbari Asanjan",
      "Besart Mujeci",
      "Disha Shidham",
      "Trevor Keenan",
      "Paulo Arevalo",
      "Wenwen Li",
      "Hamed Alemohammad",
      "Pontus Olofsson",
      "Christopher Hain",
      "Robert Kennedy",
      "Bianca Zadrozny",
      "David Bell",
      "Gabriele Cavallaro",
      "Campbell Watson",
      "Manil Maskey",
      "Rahul Ramachandran",
      "Juan Bernabe Moreno"
    ],
    "published": "2024-12-03",
    "abstract": "This technical report presents Prithvi-EO-2.0, a new geospatial foundation model that offers significant improvements over its predecessor, Prithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's Harmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M and 600M parameter models incorporate temporal and location embeddings for enhanced performance across various geospatial tasks. Through extensive benchmarking with GEO-Bench, the 600M version outperforms the previous Prithvi-EO model by 8\\% across a range of tasks. It also outperforms six other geospatial foundation models when benchmarked on remote sensing tasks from different domains and resolutions (i.e. from 0.1m to 15m). The results demonstrate the versatility of the model in both classical earth observation and high-resolution applications. Early involvement of end-users and subject matter experts (SMEs) are among the key factors that contributed to the project's success. In particular, SME involvement allowed for constant feedback on model and dataset design, as well as successful customization for diverse SME-led applications in disaster response, land use and crop mapping, and ecosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and IBM terratorch, with additional resources on GitHub. The project exemplifies the Trusted Open Science approach embraced by all involved organizations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Remote Sensing SpatioTemporal Vision-Language Models: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2412.02573v3",
    "authors": [
      "Chenyang Liu",
      "Jiafan Zhang",
      "Keyan Chen",
      "Man Wang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "published": "2024-12-03",
    "abstract": "The interpretation of multi-temporal remote sensing imagery is critical for monitoring Earth's dynamic processes-yet previous change detection methods, which produce binary or semantic masks, fall short of providing human-readable insights into changes. Recent advances in Vision-Language Models (VLMs) have opened a new frontier by fusing visual and linguistic modalities, enabling spatio-temporal vision-language understanding: models that not only capture spatial and temporal dependencies to recognize changes but also provide a richer interactive semantic analysis of temporal images (e.g., generate descriptive captions and answer natural-language queries). In this survey, we present the first comprehensive review of RS-STVLMs. The survey covers the evolution of models from early task-specific models to recent general foundation models that leverage powerful large language models. We discuss progress in representative tasks, such as change captioning, change question answering, and change grounding. Moreover, we systematically dissect the fundamental components and key technologies underlying these models, and review the datasets and evaluation metrics that have driven the field. By synthesizing task-level insights with a deep dive into shared architectural patterns, we aim to illuminate current achievements and chart promising directions for future research in spatio-temporal vision-language understanding for remote sensing. We will keep tracing related works at https://github.com/Chen-Yang-Liu/Awesome-RS-SpatioTemporal-VLMs",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model",
    "url": "http://arxiv.org/abs/2411.17984v3",
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Hanbo Bi",
      "Boyuan Tong",
      "Zhaozhi Wang",
      "Wenhui Diao",
      "Hao Chang",
      "Yingchao Feng",
      "Ziqi Zhang",
      "Yaowei Wang",
      "Qixiang Ye",
      "Kun Fu",
      "Xian Sun"
    ],
    "published": "2024-11-27",
    "abstract": "Remote sensing foundation models largely break away from the traditional paradigm of designing task-specific models, offering greater scalability across multiple tasks. However, they face challenges such as low computational efficiency and limited interpretability, especially when dealing with large-scale remote sensing images. To overcome these, we draw inspiration from heat conduction, a physical process modeling local heat diffusion. Building on this idea, we are the first to explore the potential of using the parallel computing model of heat conduction to simulate the local region correlations in high-resolution remote sensing images, and introduce RS-vHeat, an efficient multi-modal remote sensing foundation model. Specifically, RS-vHeat 1) applies the Heat Conduction Operator (HCO) with a complexity of $O(N^{1.5})$ and a global receptive field, reducing computational overhead while capturing remote sensing object structure information to guide heat diffusion; 2) learns the frequency distribution representations of various scenes through a self-supervised strategy based on frequency domain hierarchical masking and multi-domain reconstruction; 3) significantly improves efficiency and performance over state-of-the-art techniques across 4 tasks and 10 datasets. Compared to attention-based remote sensing foundation models, we reduce memory usage by 84\\%, FLOPs by 24\\% and improves throughput by 2.7 times. The code will be made publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SatVision-TOA: A Geospatial Foundation Model for Coarse-Resolution All-Sky Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2411.17000v1",
    "authors": [
      "Caleb S. Spradlin",
      "Jordan A. Caraballo-Vega",
      "Jian Li",
      "Mark L. Carroll",
      "Jie Gong",
      "Paul M. Montesano"
    ],
    "published": "2024-11-26",
    "abstract": "Foundation models have the potential to transform the landscape of remote sensing (RS) data analysis by enabling large computer vision models to be pre-trained on vast amounts of remote sensing data. These models can then be fine-tuned with small amounts of labeled training and applied to a variety of applications. Most existing foundation models are designed for high spatial resolution, cloud-free satellite imagery or photos, limiting their applicability in scenarios that require frequent temporal monitoring or broad spectral profiles. As a result, foundation models trained solely on cloud-free images have limited utility for applications that involve atmospheric variables or require atmospheric corrections. We introduce SatVision-TOA, a novel foundation model pre-trained on 14-band MODIS L1B Top-Of-Atmosphere (TOA) radiance imagery, addressing the need for models pre-trained to handle moderate- and coarse-resolution all-sky remote sensing data. The SatVision-TOA model is pre-trained using a Masked-Image-Modeling (MIM) framework and the SwinV2 architecture, and learns detailed contextual representations through self-supervised learning without the need for labels. It is a 3 billion parameter model that is trained on 100 million images. To our knowledge this is the largest foundation model trained solely on satellite RS imagery. Results show that SatVision-TOA achieves superior performance over baseline methods on downstream tasks such as 3D cloud retrieval. Notably, the model achieves a mean intersection over union (mIOU) of 0.46, a substantial improvement over the baseline mIOU of 0.22. Additionally, the rate of false negative results in the fine-tuning task were reduced by over 50% compared to the baseline. Our work advances pre-trained vision modeling for multispectral RS by learning from a variety of atmospheric and aerosol conditions to improve cloud and land surface monitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2411.13127v2",
    "authors": [
      "Xuechao Zou",
      "Shun Zhang",
      "Kai Li",
      "Shiying Wang",
      "Junliang Xing",
      "Lei Jin",
      "Congyan Lang",
      "Pin Tao"
    ],
    "published": "2024-11-20",
    "abstract": "Cloud segmentation is a critical challenge in remote sensing image interpretation, as its accuracy directly impacts the effectiveness of subsequent data processing and analysis. Recently, vision foundation models (VFM) have demonstrated powerful generalization capabilities across various visual tasks. In this paper, we present a parameter-efficient adaptive approach, termed Cloud-Adapter, designed to enhance the accuracy and robustness of cloud segmentation. Our method leverages a VFM pretrained on general domain data, which remains frozen, eliminating the need for additional training. Cloud-Adapter incorporates a lightweight spatial perception module that initially utilizes a convolutional neural network (ConvNet) to extract dense spatial representations. These multi-scale features are then aggregated and serve as contextual inputs to an adapting module, which modulates the frozen transformer layers within the VFM. Experimental results demonstrate that the Cloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the frozen backbone, achieves substantial performance gains. Cloud-Adapter consistently achieves state-of-the-art performance across various cloud segmentation datasets from multiple satellite sources, sensor series, data processing levels, land cover scenarios, and annotation granularities. We have released the code and model checkpoints at https://xavierjiezou.github.io/Cloud-Adapter/ to support further research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Aquila: A Hierarchically Aligned Visual-Language Model for Enhanced Remote Sensing Image Comprehension",
    "url": "http://arxiv.org/abs/2411.06074v1",
    "authors": [
      "Kaixuan Lu",
      "Ruiqian Zhang",
      "Xiao Huang",
      "Yuxing Xie"
    ],
    "published": "2024-11-09",
    "abstract": "Recently, large vision language models (VLMs) have made significant strides in visual language capabilities through visual instruction tuning, showing great promise in the field of remote sensing image interpretation. However, existing remote sensing vision language models (RSVLMs) often fall short in capturing the complex characteristics of remote sensing scenes, as they typically rely on low resolution, single scale visual features and simplistic methods to map visual features to language features. In this paper, we present Aquila, an advanced visual language foundation model designed to enable richer visual feature representation and more precise visual-language feature alignment for remote sensing images. Our approach introduces a learnable Hierarchical Spatial Feature Integration (SFI) module that supports high resolution image inputs and aggregates multi scale visual features, allowing for the detailed representation of complex visual information. Additionally, the SFI module is repeatedly integrated into the layers of the large language model (LLM) to achieve deep visual language feature alignment, without compromising the model's performance in natural language processing tasks. These innovations, capturing detailed visual effects through higher resolution and multi scale input, and enhancing feature alignment significantly improve the model's ability to learn from image text data. We validate the effectiveness of Aquila through extensive quantitative experiments and qualitative analyses, demonstrating its superior performance.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "STARS: Sensor-agnostic Transformer Architecture for Remote Sensing",
    "url": "http://arxiv.org/abs/2411.05714v1",
    "authors": [
      "Ethan King",
      "Jaime Rodriguez",
      "Diego Llanes",
      "Timothy Doster",
      "Tegan Emerson",
      "James Koch"
    ],
    "published": "2024-11-08",
    "abstract": "We present a sensor-agnostic spectral transformer as the basis for spectral foundation models. To that end, we introduce a Universal Spectral Representation (USR) that leverages sensor meta-data, such as sensing kernel specifications and sensing wavelengths, to encode spectra obtained from any spectral instrument into a common representation, such that a single model can ingest data from any sensor. Furthermore, we develop a methodology for pre-training such models in a self-supervised manner using a novel random sensor-augmentation and reconstruction pipeline to learn spectral features independent of the sensing paradigm. We demonstrate that our architecture can learn sensor independent spectral features that generalize effectively to sensors not seen during training. This work sets the stage for training foundation models that can both leverage and be effective for the growing diversity of spectral data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Joint-Optimized Unsupervised Adversarial Domain Adaptation in Remote Sensing Segmentation with Prompted Foundation Model",
    "url": "http://arxiv.org/abs/2411.05878v2",
    "authors": [
      "Shuchang Lyu",
      "Qi Zhao",
      "Guangliang Cheng",
      "Yiwei He",
      "Zheng Zhou",
      "Guangbiao Wang",
      "Zhenwei Shi"
    ],
    "published": "2024-11-08",
    "abstract": "Unsupervised Domain Adaptation for Remote Sensing Semantic Segmentation (UDA-RSSeg) addresses the challenge of adapting a model trained on source domain data to target domain samples, thereby minimizing the need for annotated data across diverse remote sensing scenes. This task presents two principal challenges: (1) severe inconsistencies in feature representation across different remote sensing domains, and (2) a domain gap that emerges due to the representation bias of source domain patterns when translating features to predictive logits. To tackle these issues, we propose a joint-optimized adversarial network incorporating the \"Segment Anything Model (SAM) (SAM-JOANet)\" for UDA-RSSeg. Our approach integrates SAM to leverage its robust generalized representation capabilities, thereby alleviating feature inconsistencies. We introduce a finetuning decoder designed to convert SAM-Encoder features into predictive logits. Additionally, a feature-level adversarial-based prompted segmentor is employed to generate class-agnostic maps, which guide the finetuning decoder's feature representations. The network is optimized end-to-end, combining the prompted segmentor and the finetuning decoder. Extensive evaluations on benchmark datasets, including ISPRS (Potsdam/Vaihingen) and CITY-OSM (Paris/Chicago), demonstrate the effectiveness of our method. The results, supported by visualization and analysis, confirm the method's interpretability and robustness. The code of this paper is available at https://github.com/CV-ShuchangLyu/SAM-JOANet.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "In the Era of Prompt Learning with Vision-Language Models",
    "url": "http://arxiv.org/abs/2411.04892v1",
    "authors": [
      "Ankit Jha"
    ],
    "published": "2024-11-07",
    "abstract": "Large-scale foundation models like CLIP have shown strong zero-shot generalization but struggle with domain shifts, limiting their adaptability. In our work, we introduce \\textsc{StyLIP}, a novel domain-agnostic prompt learning strategy for Domain Generalization (DG). StyLIP disentangles visual style and content in CLIP`s vision encoder by using style projectors to learn domain-specific prompt tokens and combining them with content features. Trained contrastively, this approach enables seamless adaptation across domains, outperforming state-of-the-art methods on multiple DG benchmarks. Additionally, we propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s frozen vision backbone to learn domain-invariant prompts through image style and content features. By aligning domains in embedding space with entropy minimization, AD-CLIP effectively handles domain shifts, even when only target domain samples are available. Lastly, we outline future work on class discovery using prompt learning for semantic segmentation in remote sensing, focusing on identifying novel or rare classes in unstructured environments. This paves the way for more adaptive and generalizable models in complex, real-world scenarios.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2410.22629v3",
    "authors": [
      "Ziyang Gong",
      "Zhixiang Wei",
      "Di Wang",
      "Xiaoxing Hu",
      "Xianzheng Ma",
      "Hongruixuan Chen",
      "Yuru Jia",
      "Yupeng Deng",
      "Zhenming Ji",
      "Xiangwei Zhu",
      "Xue Yang",
      "Naoto Yokoya",
      "Jing Zhang",
      "Bo Du",
      "Junchi Yan",
      "Liangpei Zhang"
    ],
    "published": "2024-10-30",
    "abstract": "The field of Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. Despite the substantial domain gaps in RS images that are characterized by variabilities such as location, wavelength, and sensor type, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies targeting the RSDG issue, especially for semantic segmentation tasks, where existing models are developed for specific unknown domains, struggling with issues of underfitting on other unknown scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 cross-domain settings across various regions, spectral bands, platforms, and climates, providing a comprehensive framework for testing the generalizability of future RSDG models. Extensive experiments on this benchmark demonstrate the superiority of CrossEarth over existing state-of-the-art methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery",
    "url": "http://arxiv.org/abs/2410.19965v1",
    "authors": [
      "Philipe Dias",
      "Aristeidis Tsaris",
      "Jordan Bowman",
      "Abhishek Potnis",
      "Jacob Arndt",
      "H. Lexie Yang",
      "Dalton Lunga"
    ],
    "published": "2024-10-25",
    "abstract": "While the pretraining of Foundation Models (FMs) for remote sensing (RS) imagery is on the rise, models remain restricted to a few hundred million parameters. Scaling models to billions of parameters has been shown to yield unprecedented benefits including emergent abilities, but requires data scaling and computing resources typically not available outside industry R&D labs. In this work, we pair high-performance computing resources including Frontier supercomputer, America's first exascale system, and high-resolution optical RS data to pretrain billion-scale FMs. Our study assesses performance of different pretrained variants of vision Transformers across image classification, semantic segmentation and object detection benchmarks, which highlight the importance of data scaling for effective model scaling. Moreover, we discuss construction of a novel TIU pretraining dataset, model initialization, with data and pretrained models intended for public release. By discussing technical challenges and details often lacking in the related literature, this work is intended to offer best practices to the geospatial community toward efficient training and benchmarking of larger FMs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Foundation Models for Remote Sensing and Earth Observation: A Survey",
    "url": "http://arxiv.org/abs/2410.16602v3",
    "authors": [
      "Aoran Xiao",
      "Weihao Xuan",
      "Junjue Wang",
      "Jiaxing Huang",
      "Dacheng Tao",
      "Shijian Lu",
      "Naoto Yokoya"
    ],
    "published": "2024-10-22",
    "abstract": "Remote Sensing (RS) is a crucial technology for observing, monitoring, and interpreting our planet, with broad applications across geoscience, economics, humanitarian fields, etc. While artificial intelligence (AI), particularly deep learning, has achieved significant advances in RS, unique challenges persist in developing more intelligent RS systems, including the complexity of Earth's environments, diverse sensor modalities, distinctive feature patterns, varying spatial and spectral resolutions, and temporal dynamics. Meanwhile, recent breakthroughs in large Foundation Models (FMs) have expanded AI's potential across many domains due to their exceptional generalizability and zero-shot transfer capabilities. However, their success has largely been confined to natural data like images and video, with degraded performance and even failures for RS data of various non-optical modalities. This has inspired growing interest in developing Remote Sensing Foundation Models (RSFMs) to address the complex demands of Earth Observation (EO) tasks, spanning the surface, atmosphere, and oceans. This survey systematically reviews the emerging field of RSFMs. It begins with an outline of their motivation and background, followed by an introduction of their foundational concepts. It then categorizes and reviews existing RSFM studies including their datasets and technical contributions across Visual Foundation Models (VFMs), Visual-Language Models (VLMs), Large Language Models (LLMs), and beyond. In addition, we benchmark these models against publicly available datasets, discuss existing challenges, and propose future research directions in this rapidly evolving field. A project associated with this survey has been built at https://github.com/xiaoaoran/awesome-RSFMs .",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "MANet: Fine-Tuning Segment Anything Model for Multimodal Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2410.11160v1",
    "authors": [
      "Xianping Ma",
      "Xiaokang Zhang",
      "Man-On Pun",
      "Bo Huang"
    ],
    "published": "2024-10-15",
    "abstract": "Multimodal remote sensing data, collected from a variety of sensors, provide a comprehensive and integrated perspective of the Earth's surface. By employing multimodal fusion techniques, semantic segmentation offers more detailed insights into geographic scenes compared to single-modality approaches. Building upon recent advancements in vision foundation models, particularly the Segment Anything Model (SAM), this study introduces a novel Multimodal Adapter-based Network (MANet) for multimodal remote sensing semantic segmentation. At the core of this approach is the development of a Multimodal Adapter (MMAdapter), which fine-tunes SAM's image encoder to effectively leverage the model's general knowledge for multimodal data. In addition, a pyramid-based Deep Fusion Module (DFM) is incorporated to further integrate high-level geographic features across multiple scales before decoding. This work not only introduces a novel network for multimodal fusion, but also demonstrates, for the first time, SAM's powerful generalization capabilities with Digital Surface Model (DSM) data. Experimental results on two well-established fine-resolution multimodal remote sensing datasets, ISPRS Vaihingen and ISPRS Potsdam, confirm that the proposed MANet significantly surpasses current models in the task of multimodal semantic segmentation. The source code for this work will be accessible at https://github.com/sstary/SSRS.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Exploring Foundation Models in Remote Sensing Image Change Detection: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2410.07824v1",
    "authors": [
      "Zihan Yu",
      "Tianxiao Li",
      "Yuxin Zhu",
      "Rongze Pan"
    ],
    "published": "2024-10-10",
    "abstract": "Change detection, as an important and widely applied technique in the field of remote sensing, aims to analyze changes in surface areas over time and has broad applications in areas such as environmental monitoring, urban development, and land use analysis.In recent years, deep learning, especially the development of foundation models, has provided more powerful solutions for feature extraction and data fusion, effectively addressing these complexities. This paper systematically reviews the latest advancements in the field of change detection, with a focus on the application of foundation models in remote sensing tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2410.06194v1",
    "authors": [
      "Shiyu Miao",
      "Delong Chen",
      "Fan Liu",
      "Chuanyi Zhang",
      "Yanhui Gu",
      "Shengjie Guo",
      "Jun Zhou"
    ],
    "published": "2024-10-08",
    "abstract": "The Direct Segment Anything Model (DirectSAM) excels in class-agnostic contour extraction. In this paper, we explore its use by applying it to optical remote sensing imagery, where semantic contour extraction-such as identifying buildings, road networks, and coastlines-holds significant practical value. Those applications are currently handled via training specialized small models separately on small datasets in each domain. We introduce a foundation model derived from DirectSAM, termed DirectSAM-RS, which not only inherits the strong segmentation capability acquired from natural images, but also benefits from a large-scale dataset we created for remote sensing semantic contour extraction. This dataset comprises over 34k image-text-contour triplets, making it at least 30 times larger than individual dataset. DirectSAM-RS integrates a prompter module: a text encoder and cross-attention layers attached to the DirectSAM architecture, which allows flexible conditioning on target class labels or referring expressions. We evaluate the DirectSAM-RS in both zero-shot and fine-tuning setting, and demonstrate that it achieves state-of-the-art performance across several downstream benchmarks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "PointSAM: Pointly-Supervised Segment Anything Model for Remote Sensing Images",
    "url": "http://arxiv.org/abs/2409.13401v2",
    "authors": [
      "Nanqing Liu",
      "Xun Xu",
      "Yongyi Su",
      "Haojie Zhang",
      "Heng-Chao Li"
    ],
    "published": "2024-09-20",
    "abstract": "Segment Anything Model (SAM) is an advanced foundational model for image segmentation, which is gradually being applied to remote sensing images (RSIs). Due to the domain gap between RSIs and natural images, traditional methods typically use SAM as a source pre-trained model and fine-tune it with fully supervised masks. Unlike these methods, our work focuses on fine-tuning SAM using more convenient and challenging point annotations. Leveraging SAM's zero-shot capabilities, we adopt a self-training framework that iteratively generates pseudo-labels for training. However, if the pseudo-labels contain noisy labels, there is a risk of error accumulation. To address this issue, we extract target prototypes from the target dataset and use the Hungarian algorithm to match them with prediction prototypes, preventing the model from learning in the wrong direction. Additionally, due to the complex backgrounds and dense distribution of objects in RSI, using point prompts may result in multiple objects being recognized as one. To solve this problem, we propose a negative prompt calibration method based on the non-overlapping nature of instance masks. In brief, we use the prompts of overlapping masks as corresponding negative signals, resulting in refined masks. Combining the above methods, we propose a novel Pointly-supervised Segment Anything Model named PointSAM. We conduct experiments on RSI datasets, including WHU, HRSID, and NWPU VHR-10, and the results show that our method significantly outperforms direct testing with SAM, SAM2, and other comparison methods. Furthermore, we introduce PointSAM as a point-to-box converter and achieve encouraging results, suggesting that this method can be extended to other point-supervised tasks. The code is available at https://github.com/Lans1ng/PointSAM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "RingMo-Aerial: An Aerial Remote Sensing Foundation Model With Affine Transformation Contrastive Learning",
    "url": "http://arxiv.org/abs/2409.13366v4",
    "authors": [
      "Wenhui Diao",
      "Haichen Yu",
      "Kaiyue Kang",
      "Tong Ling",
      "Di Liu",
      "Yingchao Feng",
      "Hanbo Bi",
      "Libo Ren",
      "Xuexue Li",
      "Yongqiang Mao",
      "Xian Sun"
    ],
    "published": "2024-09-20",
    "abstract": "Aerial Remote Sensing (ARS) vision tasks present significant challenges due to the unique viewing angle characteristics. Existing research has primarily focused on algorithms for specific tasks, which have limited applicability in a broad range of ARS vision applications. This paper proposes RingMo-Aerial, aiming to fill the gap in foundation model research in the field of ARS vision. A Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) mechanism is introduced to strengthen the model's capacity for small-object representation. Complementarily, an affine transformation-based contrastive learning method improves its adaptability to the tilted viewing angles inherent in ARS tasks. Furthermore, the ARS-Adapter, an efficient parameter fine-tuning method, is proposed to improve the model's adaptability and performance in various ARS vision tasks. Experimental results demonstrate that RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This indicates the practicality and efficacy of RingMo-Aerial in enhancing the performance of ARS vision tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "On the Generalizability of Foundation Models for Crop Type Mapping",
    "url": "http://arxiv.org/abs/2409.09451v4",
    "authors": [
      "Yi-Chia Chang",
      "Adam J. Stewart",
      "Favyen Bastani",
      "Piper Wolters",
      "Shreya Kannan",
      "George R. Huber",
      "Jingtong Wang",
      "Arindam Banerjee"
    ],
    "published": "2024-09-14",
    "abstract": "Foundation models pre-trained using self-supervised learning have shown powerful transfer learning capabilities on various downstream tasks, including language understanding, text generation, and image recognition. The Earth observation (EO) field has produced several foundation models pre-trained directly on multispectral satellite imagery for applications like precision agriculture, wildfire and drought monitoring, and natural disaster response. However, few studies have investigated the ability of these models to generalize to new geographic locations, and potential concerns of geospatial bias -- models trained on data-rich developed nations not transferring well to data-scarce developing nations -- remain. We evaluate three popular EO foundation models, SSL4EO-S12, SatlasPretrain, and ImageNet, on five crop classification datasets across five continents. Results show that pre-trained weights designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform general pre-trained weights like ImageNet. While only 100 labeled images are sufficient for achieving high overall accuracy, 900 images are required to mitigate class imbalance and improve average accuracy.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Detecting Looted Archaeological Sites from Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2409.09432v1",
    "authors": [
      "Elliot Vincent",
      "Mehra\u00efl Saroufim",
      "Jonathan Chemla",
      "Yves Ubelmann",
      "Philippe Marquis",
      "Jean Ponce",
      "Mathieu Aubry"
    ],
    "published": "2024-09-14",
    "abstract": "Archaeological sites are the physical remains of past human activity and one of the main sources of information about past societies and cultures. However, they are also the target of malevolent human actions, especially in countries having experienced inner turmoil and conflicts. Because monitoring these sites from space is a key step towards their preservation, we introduce the DAFA Looted Sites dataset, \\datasetname, a labeled multi-temporal remote sensing dataset containing 55,480 images acquired monthly over 8 years across 675 Afghan archaeological sites, including 135 sites looted during the acquisition period. \\datasetname~is particularly challenging because of the limited number of training samples, the class imbalance, the weak binary annotations only available at the level of the time series, and the subtlety of relevant changes coupled with important irrelevant ones over a long time period. It is also an interesting playground to assess the performance of satellite image time series (SITS) classification methods on a real and important use case. We evaluate a large set of baselines, outline the substantial benefits of using foundation models and show the additional boost that can be provided by using complete time series instead of using a single image.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Pushing the Limits of Vision-Language Models in Remote Sensing without Human Annotations",
    "url": "http://arxiv.org/abs/2409.07048v1",
    "authors": [
      "Keumgang Cha",
      "Donggeun Yu",
      "Junghoon Seo"
    ],
    "published": "2024-09-11",
    "abstract": "The prominence of generalized foundation models in vision-language integration has witnessed a surge, given their multifarious applications. Within the natural domain, the procurement of vision-language datasets to construct these foundation models is facilitated by their abundant availability and the ease of web crawling. Conversely, in the remote sensing domain, although vision-language datasets exist, their volume is suboptimal for constructing robust foundation models. This study introduces an approach to curate vision-language datasets by employing an image decoding machine learning model, negating the need for human-annotated labels. Utilizing this methodology, we amassed approximately 9.6 million vision-language paired datasets in VHR imagery. The resultant model outperformed counterparts that did not leverage publicly available vision-language datasets, particularly in downstream tasks such as zero-shot classification, semantic localization, and image-text retrieval. Moreover, in tasks exclusively employing vision encoders, such as linear probing and k-NN classification, our model demonstrated superior efficacy compared to those relying on domain-specific vision-language datasets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability",
    "url": "http://arxiv.org/abs/2409.00489v1",
    "authors": [
      "Chia-Yu Hsu",
      "Wenwen Li",
      "Sizhe Wang"
    ],
    "published": "2024-08-31",
    "abstract": "Research on geospatial foundation models (GFMs) has become a trending topic in geospatial artificial intelligence (AI) research due to their potential for achieving high generalizability and domain adaptability, reducing model training costs for individual researchers. Unlike large language models, such as ChatGPT, constructing visual foundation models for image analysis, particularly in remote sensing, encountered significant challenges such as formulating diverse vision tasks into a general problem framework. This paper evaluates the recently released NASA-IBM GFM Prithvi for its predictive performance on high-level image analysis tasks across multiple benchmark datasets. Prithvi was selected because it is one of the first open-source GFMs trained on time-series of high-resolution remote sensing imagery. A series of experiments were designed to assess Prithvi's performance as compared to other pre-trained task-specific AI models in geospatial image analysis. New strategies, including band adaptation, multi-scale feature generation, and fine-tuning techniques, are introduced and integrated into an image analysis pipeline to enhance Prithvi's domain adaptation capability and improve model performance. In-depth analyses reveal Prithvi's strengths and weaknesses, offering insights for both improving Prithvi and developing future visual foundation models for geospatial tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "OSM+: Billion-Level Open Street Map Data Processing System for City-wide Experiments",
    "url": "http://arxiv.org/abs/2512.06743v1",
    "authors": [
      "Guanjie Zheng",
      "Ziyang Su",
      "Yiheng Wang",
      "Yuhang Luo",
      "Hongwei Zhang",
      "Xuanhe Zhou",
      "Linghe Kong",
      "Fan Wu",
      "Wen Ling"
    ],
    "published": "2025-12-07",
    "abstract": "Road network data can provide rich information about cities and thus become the base for various urban research. However, processing large volume world-wide road network data requires intensive computing resources and the processed results might be different to be unified for testing downstream tasks. Therefore, in this paper, we process the OpenStreetMap data via a distributed computing of 5,000 cores on cloud services and release a structured world-wide 1-billion-vertex road network graph dataset with high accessibility (opensource and downloadable to the whole world) and usability (open-box graph structure and easy spatial query interface). To demonstrate how this dataset can be utilized easily, we present three illustrative use cases, including traffic prediction, city boundary detection and traffic policy control, and conduct extensive experiments for these three tasks. (1) For the well-investigated traffic prediction tasks, we release a new benchmark with 31 cities (traffic data processed and combined with our released OSM+ road network dataset), to provide much larger spatial coverage and more comprehensive evaluation of compared algorithms than the previously frequently-used datasets. This new benchmark will push the algorithms on their scalability from hundreds of road network intersections to thousands of intersections. (2) While for the more advanced traffic policy control task which requires interaction with the road network, we release a new 6 city datasets with much larger scale than the previous datasets. This brings new challenge for thousand-scale multi-agent coordination. (3) Along with the OSM+ dataset, the release of data converters facilitates the integration of multimodal spatial-temporal data for geospatial foundation model training, thereby expediting the process of uncovering compelling scientific insights. PVLDB Reference Forma",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "First On-Orbit Demonstration of a Geospatial Foundation Model",
    "url": "http://arxiv.org/abs/2512.01181v1",
    "authors": [
      "Andrew Du",
      "Roberto Del Prete",
      "Alejandro Mousist",
      "Nick Manser",
      "Fabrice Marre",
      "Andrew Barton",
      "Carl Seubert",
      "Gabriele Meoni",
      "Tat-Jun Chin"
    ],
    "published": "2025-12-01",
    "abstract": "Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale",
    "url": "http://arxiv.org/abs/2512.02055v1",
    "authors": [
      "Mirela G. Tulbure",
      "Julio Caineta",
      "Mark Broich",
      "Mollie D. Gaines",
      "Philippe Rufin",
      "Leon-Friedrich Thomas",
      "Hamed Alemohammad",
      "Jan Hemmerling",
      "Patrick Hostert"
    ],
    "published": "2025-11-27",
    "abstract": "Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.\n  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.\n  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
    "url": "http://arxiv.org/abs/2511.15658v1",
    "authors": [
      "Naomi Simumba",
      "Nils Lehmann",
      "Paolo Fraccaro",
      "Hamed Alemohammad",
      "Geeth De Mel",
      "Salman Khan",
      "Manil Maskey",
      "Nicolas Longepe",
      "Xiao Xiang Zhu",
      "Hannah Kerner",
      "Juan Bernabe-Moreno",
      "Alexander Lacoste"
    ],
    "published": "2025-11-19",
    "abstract": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.\n  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation",
    "url": "http://arxiv.org/abs/2511.10370v1",
    "authors": [
      "Kai-Hendrik Cohrs",
      "Zuzanna Osika",
      "Maria Gonzalez-Calabuig",
      "Vishal Nedungadi",
      "Ruben Cartuyvels",
      "Steffen Knoblauch",
      "Joppe Massant",
      "Shruti Nath",
      "Patrick Ebel",
      "Vasileios Sitokonstantinou"
    ],
    "published": "2025-11-13",
    "abstract": "Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands",
    "url": "http://arxiv.org/abs/2511.10177v1",
    "authors": [
      "Tishya Chhabra",
      "Manisha Bajpai",
      "Walter Zesk",
      "Skylar Tibbits"
    ],
    "published": "2025-11-13",
    "abstract": "We present an initial evaluation of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on shoreline delineation of small sandy islands using satellite images. We curated and labeled a dataset of 225 multispectral images of two Maldivian islands, which we publicly release, and fine-tuned both the 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Our experiments show that even with as few as 5 training images, the models achieve high performance (F1 of 0.94, IoU of 0.79). Our results demonstrate the strong transfer learning capability of Prithvi, underscoring the potential of such models to support coastal monitoring in data-poor regions.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2",
    "url": "http://arxiv.org/abs/2511.05461v1",
    "authors": [
      "Olivier Dietrich",
      "Merlin Alfredsson",
      "Emilia Arens",
      "Nando Metzger",
      "Torben Peters",
      "Linus Scheibenreif",
      "Jan Dirk Wegner",
      "Konrad Schindler"
    ],
    "published": "2025-11-07",
    "abstract": "Natural disasters demand rapid damage assessment to guide humanitarian response. Here, we investigate whether medium-resolution Earth observation images from the Copernicus program can support building damage assessment, complementing very-high resolution imagery with often limited availability. We introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the established xBD benchmark. In a series of experiments, we demonstrate that building damage can be detected and mapped rather well in many disaster scenarios, despite the moderate 10$\\,$m ground sampling distance. We also find that, for damage mapping at that resolution, architectural sophistication does not seem to bring much advantage: more complex model architectures tend to struggle with generalization to unseen disasters, and geospatial foundation models bring little practical benefit. Our results suggest that Copernicus images are a viable data source for rapid, wide-area damage assessment and could play an important role alongside VHR imagery. We release the xBD-S12 dataset, code, and trained models to support further research.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation",
    "url": "http://arxiv.org/abs/2511.04766v1",
    "authors": [
      "Dhenenjay Yadav",
      "Rohan Sawai"
    ],
    "published": "2025-11-06",
    "abstract": "Foundation models (FMs) offer powerful representations for geospatial analysis, but adapting them effectively remains challenging. Standard adaptation methods, whether full fine-tuning or efficient frozen-backbone approaches, typically employ decoders with fixed regularization strategies, failing to account for the significant heterogeneity in satellite imagery. We introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder architecture designed to address this limitation. DARN integrates three key innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and (3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide theoretical justifications linking DARN's optimization to stationary point convergence and its mechanism to adaptive information bottlenecks. Empirically, DARN demonstrates exceptional performance across both major adaptation paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering substantial advantages crucial for real-world deployment: superior out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms), enhanced robustness (17% relative reduction in corruption error), and improved performance on minority classes. DARN offers a more intelligent, robust, and efficient approach to leveraging FMs in critical geospatial applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability",
    "url": "http://arxiv.org/abs/2511.04474v1",
    "authors": [
      "Wenwen Li",
      "Sizhe Wang",
      "Hyunho Lee",
      "Chenyan Lu",
      "Sujit Roy",
      "Rahul Ramachandran",
      "Chia-Yu Hsu"
    ],
    "published": "2025-11-06",
    "abstract": "Landslides cause severe damage to lives, infrastructure, and the environment, making accurate and timely mapping essential for disaster preparedness and response. However, conventional deep learning models often struggle when applied across different sensors, regions, or under conditions of limited training data. To address these challenges, we present a three-axis analytical framework of sensor, label, and domain for adapting geospatial foundation models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a series of experiments, we show that it consistently outperforms task-specific CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other GeoFMs (TerraMind, SatMAE). The model, built on global pretraining, self-supervision, and adaptable fine-tuning, proved resilient to spectral variation, maintained accuracy under label scarcity, and generalized more reliably across diverse datasets and geographic settings. Alongside these strengths, we also highlight remaining challenges such as computational cost and the limited availability of reusable AI-ready training data for landslide research. Overall, our study positions GeoFMs as a step toward more robust and scalable approaches for landslide risk reduction and environmental monitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing",
    "url": "http://arxiv.org/abs/2510.26609v1",
    "authors": [
      "Shayan Nejadshamsi",
      "Yuanyuan Zhang",
      "Shadi Zaki",
      "Brock Porth",
      "Lysa Porth",
      "Vahab Khoshdel"
    ],
    "published": "2025-10-30",
    "abstract": "Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing), a deep learning model designed for high-resolution, intra-field canola yield prediction. CYPRESS leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, CYPRESS demonstrates superior performance over existing deep learning-based yield prediction models, highlighting the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, CYPRESS offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi",
    "url": "http://arxiv.org/abs/2510.25954v1",
    "authors": [
      "Lynn Metz",
      "Rachel Haggard",
      "Michael Moszczynski",
      "Samer Asbah",
      "Chris Mwase",
      "Patricia Khomani",
      "Tyler Smith",
      "Hannah Cooper",
      "Annie Mwale",
      "Arbaaz Muslim",
      "Gautam Prasad",
      "Mimi Sun",
      "Tomer Shekel",
      "Joydeep Paul",
      "Anna Carter",
      "Shravya Shetty",
      "Dylan Green"
    ],
    "published": "2025-10-29",
    "abstract": "The reliability of routine health data in low and middle-income countries (LMICs) is often constrained by reporting delays and incomplete coverage, necessitating the exploration of novel data sources and analytics. Geospatial Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse spatial, temporal, and behavioral data into mathematical embeddings that can be efficiently used for downstream prediction tasks. This study evaluated the predictive performance of three GeoFM embedding sources - Google Population Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite imagery), and mobile phone call detail records (CDR) - for modeling 15 routine health programmatic outputs in Malawi, and compared their utility to traditional geospatial interpolation methods. We used XGBoost models on data from 552 health catchment areas (January 2021-May 2023), assessing performance with R2, and using an 80/20 training and test data split with 5-fold cross-validation used in training. While predictive performance was mixed, the embedding-based approaches improved upon baseline geostatistical methods in 13 of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three embedding sources produced the most robust predictions, achieving average 5-fold cross validated R2 values for indicators like population density (0.63), new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64, 0.68, and 0.55, respectively. Prediction was poor for prediction targets with low primary data availability, such as TB and malnutrition cases. These results demonstrate that GeoFM embeddings imbue a modest predictive improvement for select health and demographic outcomes in an LMIC context. We conclude that the integration of multiple GeoFM sources is an efficient and valuable tool for supplementing and strengthening constrained routine health information systems.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures",
    "url": "http://arxiv.org/abs/2511.00073v1",
    "authors": [
      "Harald Kristen",
      "Daniel Kulmer",
      "Manuela Hirschmugl"
    ],
    "published": "2025-10-29",
    "abstract": "Rapid climate change and other disturbances in alpine ecosystems demand frequent habitat monitoring, yet manual mapping remains prohibitively expensive for the required temporal resolution. We employ deep learning for change detection using long-term alpine habitat data from Gesaeuse National Park, Austria, addressing a major gap in applying geospatial foundation models (GFMs) to complex natural environments with fuzzy class boundaries and highly imbalanced classes. We compare two paradigms: post-classification change detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the transformer ChangeViT against U-Net baselines. Using high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus U-Net's 41% for multi-class habitat change, while both reach 67% for binary change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's 23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy. Although overall accuracies are lower than in more homogeneous landscapes, they reflect realistic performance for complex alpine habitats. Future work will integrate object-based post-processing and physical constraints to enhance applicability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Global Chlorophyll-\\textit{a} Retrieval algorithm from Sentinel 2 Using Residual Deep Learning and Novel Machine Learning Water Classification",
    "url": "http://arxiv.org/abs/2510.24124v1",
    "authors": [
      "Yotam Sherf",
      "Bar Efrati",
      "Gabriel Rozman",
      "Moshe Harel"
    ],
    "published": "2025-10-28",
    "abstract": "We present the Global Water Classifier (GWC), a supervised, geospatially extensive Machine Learning (ML) classifier trained on Sen2Cor corrected Sentinel-2 surface reflectance data. Using nearly 100 globally distributed inland water bodies, GWC distinguishes water across Chlorophyll-a (Chla) levels from non-water spectra (clouds, sun glint, snow, ice, aquatic vegetation, land and sediments) and shows geographically stable performance.\n  Building on this foundation model, we perform Chla retrieval based on a matchup Sentinel-2 reflectance data with the United States Geological Survey (USGS) AquaMatch in-situ dataset, covering diverse geographical and hydrological conditions.\n  We train an XGBoost regressor on 13626 matchup points. The positive labeled scenes by the GWC consistently outperform the negatives and produce more accurate Chla retrieval values, which confirms the classifiers advantage in reducing various interferences.\n  Next, residual analysis of the regression predictions revealed structured errors, motivating a residual CNN (RCNN) correction stage. We add a CNN residual stage trained on normalized residuals, which yield substantial improvement. Our algorithm was tested on 867 water bodies with over 2,000 predictions and Chla values up to 1000~mg$/m^{3}$, achieving $R^2$ = 0.79, MAE = 13.52~mg$/m^{3}$, and slope = 0.91, demonstrating robust, scalable, and globally transferable performance without additional tuning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping",
    "url": "http://arxiv.org/abs/2510.23364v1",
    "authors": [
      "Hyeongkyun Kim",
      "Orestis Oikonomou"
    ],
    "published": "2025-10-27",
    "abstract": "Flood susceptibility mapping (FSM) is vital for disaster prevention but remains challenging in data-scarce regions where hydrodynamic models require dense geophysical inputs. This work introduces ZeroFlood, a geospatial foundation model framework for data-efficient FSM. The approach fine-tunes Geospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning, enabling flood prediction from basic Earth observation data such as Sentinel-1 or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich regions, ZeroFlood bridges data availability gaps through cross-modal representation learning. Experiments with TerraMind and Prithvi GFMs show that TiM enhances model robustness, with the TerraMind-Large configuration achieving an F1 score of 67.21. The results demonstrate the feasibility of foundation-model-based FSM as a scalable and data-efficient solution for flood risk management.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction",
    "url": "http://arxiv.org/abs/2510.18773v1",
    "authors": [
      "Jannis Fleckenstein",
      "David Kreismann",
      "Tamara Rosemary Govindasamy",
      "Thomas Brunschwiler",
      "Etienne Vos",
      "Mattia Rigotti"
    ],
    "published": "2025-10-21",
    "abstract": "As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data, yet conventional machine learning models with limited data often produce inaccurate predictions, particularly in underserved areas. Geospatial foundation models trained on global unstructured data offer a promising alternative by demonstrating strong generalization and requiring only minimal fine-tuning. In this study, an empirical ground truth of urban heat patterns is established by quantifying cooling effects from green spaces and benchmarking them against model predictions to evaluate the model's accuracy. The foundation model is subsequently fine-tuned to predict land surface temperatures under future climate scenarios, and its practical value is demonstrated through a simulated inpainting that highlights its role for mitigation support. The results indicate that foundation models offer a powerful way for evaluating urban heat island mitigation strategies in data-scarce regions to support more climate-resilient cities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning",
    "url": "http://arxiv.org/abs/2510.18318v3",
    "authors": [
      "Aaron Bell",
      "Amit Aides",
      "Amr Helmy",
      "Arbaaz Muslim",
      "Aviad Barzilai",
      "Aviv Slobodkin",
      "Bolous Jaber",
      "David Schottlander",
      "George Leifman",
      "Joydeep Paul",
      "Mimi Sun",
      "Nadav Sherman",
      "Natalie Williams",
      "Per Bjornsson",
      "Roy Lee",
      "Ruth Alcantara",
      "Thomas Turnbull",
      "Tomer Shekel",
      "Vered Silverman",
      "Yotam Gigi",
      "Adam Boulanger",
      "Alex Ottenwess",
      "Ali Ahmadalipour",
      "Anna Carter",
      "Behzad Vahedi",
      "Charles Elliott",
      "David Andre",
      "Elad Aharoni",
      "Gia Jung",
      "Hassler Thurston",
      "Jacob Bien",
      "Jamie McPike",
      "Jessica Sapick",
      "Juliet Rothenberg",
      "Kartik Hegde",
      "Kel Markert",
      "Kim Philipp Jablonski",
      "Luc Houriez",
      "Monica Bharel",
      "Phing VanLee",
      "Reuven Sayag",
      "Sebastian Pilarski",
      "Shelley Cazares",
      "Shlomi Pasternak",
      "Siduo Jiang",
      "Thomas Colthurst",
      "Yang Chen",
      "Yehonathan Refael",
      "Yochai Blau",
      "Yuval Carny",
      "Yael Maguire",
      "Avinatan Hassidim",
      "James Manyika",
      "Tim Thelin",
      "Genady Beryozkin",
      "Gautam Prasad",
      "Luke Barrington",
      "Yossi Matias",
      "Niv Efron",
      "Shravya Shetty"
    ],
    "published": "2025-10-21",
    "abstract": "Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of geospatial AI models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet. This approach is built upon foundation models across three key domains--Planet-scale Imagery, Population, and Environment--and an intelligent Gemini-powered reasoning engine. We present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together, they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities. To handle complex, multi-step queries, we developed a Gemini-powered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools. On a new benchmark of real-world crisis scenarios, our agent demonstrates the ability to deliver critical and timely insights, effectively bridging the gap between raw geospatial data and actionable understanding.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence",
    "url": "http://arxiv.org/abs/2510.16555v1",
    "authors": [
      "Qiongyan Wang",
      "Xingchen Zou",
      "Yutian Jiang",
      "Haomin Wen",
      "Jiaheng Wei",
      "Qingsong Wen",
      "Yuxuan Liang"
    ],
    "published": "2025-10-18",
    "abstract": "Rapid urbanization intensifies the demand for Urban General Intelligence (UGI), referring to AI systems that can understand and reason about complex urban environments. Recent studies have built urban foundation models using supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit persistent geospatial bias, producing regionally skewed predictions and limited generalization. To this end, we propose Urban-R1, a reinforcement learning-based post-training framework that aligns MLLMs with the objectives of UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize reasoning across geographic groups and employs urban region profiling as a proxy task to provide measurable rewards from multimodal urban data. Extensive experiments across diverse regions and tasks show that Urban-R1 effectively mitigates geo-bias and improves cross-region generalization, outperforming both SFT-trained and closed-source models. Our results highlight reinforcement learning alignment as a promising pathway toward equitable and trustworthy urban intelligence.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast",
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning",
    "url": "http://arxiv.org/abs/2510.09894v1",
    "authors": [
      "Junyuan Liu",
      "Quan Qin",
      "Guangsheng Dong",
      "Xinglei Wang",
      "Jiazhuang Feng",
      "Zichao Zeng",
      "Tao Cheng"
    ],
    "published": "2025-10-10",
    "abstract": "General-purpose spatial representations are essential for building transferable geospatial foundation models (GFMs). Among them, the AlphaEarth Foundation (AE) represents a major step toward a global, unified representation of the Earth's surface, learning 10-meter embeddings from multi-source Earth Observation (EO) data that capture rich physical and environmental patterns across diverse landscapes. However, such EO-driven representations remain limited in capturing the functional and socioeconomic dimensions of cities, as they primarily encode physical and spectral patterns rather than human activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched Representation Learning), a lightweight framework that adapts AlphaEarth to human-centered urban analysis through multimodal alignment guided by Points of Interest (POIs). AETHER aligns AE embeddings with textual representations of POIs, enriching physically grounded EO features with semantic cues about urban functions and socioeconomic contexts. In Greater London, AETHER achieves consistent gains over the AE baseline, with a 7.2% relative improvement in land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler divergence for socioeconomic mapping. Built upon pretrained AE, AETHER leverages a lightweight multimodal alignment to enrich it with human-centered semantics while remaining computationally efficient and scalable for urban applications. By coupling EO with human-centered semantics, it advances geospatial foundation models toward general-purpose urban representations that integrate both physical form and functional meaning.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Foundation Models for Astrobiology: Paper I -- Workshop and Overview",
    "url": "http://arxiv.org/abs/2510.08636v1",
    "authors": [
      "Ryan Felton",
      "Caleb Scharf",
      "Stuart Bartlett",
      "Nathalie A. Cabrol",
      "Victoria Da Poian",
      "Diana Gentry",
      "Jian Gong",
      "Adrienne Hoarfrost",
      "Manil Maskey",
      "Floyd Nichols",
      "Conor A. Nixon",
      "Tejas Panambur",
      "Joseph Pasterski",
      "Anton S. Petrov",
      "Anirudh Prabhu",
      "Brenda Thomson",
      "Hamed Valizadegan",
      "Kimberley Warren-Rhodes",
      "David Wettergreen",
      "Michael L. Wong",
      "Anastasia Yanchilina"
    ],
    "published": "2025-10-08",
    "abstract": "Advances in machine learning over the past decade have resulted in a proliferation of algorithmic applications for encoding, characterizing, and acting on complex data that may contain many high dimensional features. Recently, the emergence of deep-learning models trained across very large datasets has created a new paradigm for machine learning in the form of Foundation Models. Foundation Models are programs trained on very large and broad datasets with an extensive number of parameters. Once built, these powerful, and flexible, models can be utilized in less resource-intensive ways to build many different, downstream applications that can integrate previously disparate, multimodal data. The development of these applications can be done rapidly and with a much lower demand for machine learning expertise. And the necessary infrastructure and models themselves are already being established within agencies such as NASA and ESA. At NASA this work is across several divisions of the Science Mission Directorate including the NASA Goddard and INDUS Large Language Models and the Prithvi Geospatial Foundation Model. And ESA initiatives to bring Foundation Models to Earth observations has led to the development of TerraMind. A workshop was held by the NASA Ames Research Center and the SETI Institute, in February 2025, to investigate the potential of Foundation Models for astrobiological research and to determine what steps would be needed to build and utilize such a model or models. This paper shares the findings and recommendations of that workshop, and describes clear near-term, and future opportunities in the development of a Foundation Model (or Models) for astrobiology applications. These applications would include a biosignature, or life characterization, task, a mission development and operations task, and a natural language task for integrating and supporting astrobiology research needs.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment",
    "url": "http://arxiv.org/abs/2510.05617v1",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Iffanice Houndayi",
      "Rym Oualha",
      "Mohamed Aziz Cherif",
      "Kobby Panford-Quainoo",
      "Arnu Pretorius"
    ],
    "published": "2025-10-07",
    "abstract": "Open-access multispectral imagery from missions like Landsat 8-9 and Sentinel-2 has fueled the development of geospatial foundation models (GFMs) for humanitarian and environmental applications. Yet, their deployment remains limited by (i) the absence of automated geospatial data pipelines and (ii) the large size of fine-tuned models. Existing GFMs lack workflows for processing raw satellite imagery, and downstream adaptations often retain the full complexity of the original encoder.\n  We present InstaGeo, an open-source, end-to-end framework that addresses these challenges by integrating: (1) automated data curation to transform raw imagery into model-ready datasets; (2) task-specific model distillation to derive compact, compute-efficient models; and (3) seamless deployment as interactive web-map applications. Using InstaGeo, we reproduced datasets from three published studies and trained models with marginal mIoU differences of -0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for desert locust prediction. The distilled models are up to 8x smaller than standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal accuracy loss.\n  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp improvement over prior baselines. Moreover, InstaGeo enables users to progress from raw data to model deployment within a single working day.\n  By unifying data preparation, model compression, and deployment, InstaGeo transforms research-grade GFMs into practical, low-carbon tools for real-time, large-scale Earth observation. This approach shifts geospatial AI toward data quality and application-driven innovation. Source code, datasets, and model checkpoints are available at: https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial Machine Learning Libraries",
    "url": "http://arxiv.org/abs/2510.02572v2",
    "authors": [
      "Adam J. Stewart",
      "Caleb Robinson",
      "Arindam Banerjee"
    ],
    "published": "2025-10-02",
    "abstract": "Recent advances in machine learning have been supported by the emergence of domain-specific software libraries, enabling streamlined workflows and increased reproducibility. For geospatial machine learning (GeoML), the availability of Earth observation data has outpaced the development of domain libraries to handle its unique challenges, such as varying spatial resolutions, spectral properties, temporal cadence, data coverage, coordinate systems, and file formats. This chapter presents a comprehensive overview of GeoML libraries, analyzing their evolution, core functionalities, and the current ecosystem. It also introduces popular GeoML libraries such as TorchGeo, eo-learn, and Raster Vision, detailing their architecture, supported data types, and integration with ML frameworks. Additionally, it discusses common methodologies for data preprocessing, spatial--temporal joins, benchmarking, and the use of pretrained models. Through a case study in crop type mapping, it demonstrates practical applications of these tools. Best practices in software design, licensing, and testing are highlighted, along with open challenges and future directions, particularly the rise of foundation models and the need for governance in open-source geospatial software. Our aim is to guide practitioners, developers, and researchers in navigating and contributing to the rapidly evolving GeoML landscape.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model",
    "url": "http://arxiv.org/abs/2509.16617v1",
    "authors": [
      "David Kreismann"
    ],
    "published": "2025-09-20",
    "abstract": "As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data. However, predictive analytics methods based on conventional machine learning models and limited data infrastructure often provide inaccurate predictions, especially in underserved areas. In this context, geospatial foundation models trained on unstructured global data demonstrate strong generalization and require minimal fine-tuning, offering an alternative for predictions where traditional approaches are limited. This study fine-tunes a geospatial foundation model to predict urban land surface temperatures under future climate scenarios and explores its response to land cover changes using simulated vegetation strategies. The fine-tuned model achieved pixel-wise downscaling errors below 1.74 \u00b0C and aligned with ground truth patterns, demonstrating an extrapolation capacity up to 3.62 \u00b0C.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines",
    "url": "http://arxiv.org/abs/2509.18182v1",
    "authors": [
      "Isabelle Tingzon",
      "Yoji Toriumi",
      "Caroline Gevaert"
    ],
    "published": "2025-09-18",
    "abstract": "Detailed structural building information is used to estimate potential damage from hazard events like cyclones, floods, and landslides, making them critical for urban resilience planning and disaster risk reduction. However, such information is often unavailable in many small island developing states (SIDS) in climate-vulnerable regions like the Caribbean. To address this data gap, we present an AI-driven workflow to automatically infer rooftop attributes from high-resolution satellite imagery, with Saint Vincent and the Grenadines as our case study. Here, we compare the utility of geospatial foundation models combined with shallow classifiers against fine-tuned deep learning models for rooftop classification. Furthermore, we assess the impact of incorporating additional training data from neighboring SIDS to improve model performance. Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof material classification, respectively. Combined with local capacity building, our work aims to provide SIDS with novel capabilities to harness AI and Earth Observation (EO) data to enable more efficient, evidence-based urban governance.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Scalable Geospatial Data Generation Using AlphaEarth Foundations Model",
    "url": "http://arxiv.org/abs/2508.11739v1",
    "authors": [
      "Luc Houriez",
      "Sebastian Pilarski",
      "Behzad Vahedi",
      "Ali Ahmadalipour",
      "Teo Honda Scully",
      "Nicholas Aflitto",
      "David Andre",
      "Caroline Jaffe",
      "Martha Wedner",
      "Rich Mazzola",
      "Josh Jeffery",
      "Ben Messinger",
      "Sage McGinley-Smith",
      "Sarah Russell"
    ],
    "published": "2025-08-15",
    "abstract": "High-quality labeled geospatial datasets are essential for extracting insights and understanding our planet. Unfortunately, these datasets often do not span the entire globe and are limited to certain geographic regions where data was collected. Google DeepMind's recently released AlphaEarth Foundations (AEF) provides an information-dense global geospatial representation designed to serve as a useful input across a wide gamut of tasks. In this article we propose and evaluate a methodology which leverages AEF to extend geospatial labeled datasets beyond their initial geographic regions. We show that even basic models like random forests or logistic regression can be used to accomplish this task. We investigate a case study of extending LANDFIRE's Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for EvtPhys, model predictions align with ground truth. Trained models achieve 81% and 73% classification accuracy on EvtPhys validation sets in the USA and Canada, despite discussed limitations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "PlaceFM: A Training-free Geospatial Foundation Model of Places using Large-Scale Point of Interest Data",
    "url": "http://arxiv.org/abs/2507.02921v2",
    "authors": [
      "Mohammad Hashemi",
      "Hossein Amiri",
      "Andreas Zufle"
    ],
    "published": "2025-06-25",
    "abstract": "With the rapid growth and continual updates of geospatial data from diverse sources, geospatial foundation model pre-training for urban representation learning has emerged as a key research direction for advancing data-driven urban planning. Spatial structure is fundamental to effective geospatial intelligence systems; however, existing foundation models often lack the flexibility to reason about places, context-rich regions spanning multiple spatial granularities that may consist of many spatially and semantically related points of interest. To address this gap, we propose PlaceFM, a geospatial foundation model that captures place representations through a training-free, clustering-based approach. PlaceFM summarizes the entire point of interest graph constructed from U.S. Foursquare data, producing general-purpose region embeddings while automatically identifying places of interest. These embeddings can be directly integrated into geolocation data pipelines to support a variety of urban downstream tasks. Without the need for costly pre-training, PlaceFM provides a scalable and efficient solution for multi-granular geospatial analysis. Extensive experiments on two real-world prediction tasks, ZIP code-level population density and housing prices, demonstrate that PlaceFM not only outperforms most state-of-the-art graph-based geospatial foundation models but also achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation is available at https://github.com/mohammadhashemii/PlaceFM.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning",
    "url": "http://arxiv.org/abs/2506.17302v1",
    "authors": [
      "Yijun Lin",
      "Theresa Chen",
      "Colby Brungard",
      "Grunwald Sabine",
      "Sue Ives",
      "Matt Macander",
      "Timm Nawrocki",
      "Yao-Yi Chiang",
      "Nic Jelinski"
    ],
    "published": "2025-06-17",
    "abstract": "Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and localized simulations, remains a critical yet underdeveloped task, despite the region's ecological importance and extensive permafrost coverage. As permafrost thaw accelerates due to climate change, it threatens infrastructure stability and key ecosystem services, such as soil carbon storage. High-resolution soil maps are essential for characterizing permafrost distribution, identifying vulnerable areas, and informing adaptation strategies. We present MISO, a vision-based machine learning (ML) model to produce statewide fine-scale soil maps for near-surface permafrost and soil taxonomy. The model integrates a geospatial foundation model for visual feature extraction, implicit neural representations for continuous spatial prediction, and contrastive learning for multimodal alignment and geo-location awareness. We compare MISO with Random Forest (RF), a traditional ML model that has been widely used in soil mapping applications. Spatial cross-validation and regional analysis across Permafrost Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better to remote, unseen locations and achieves higher recall than RF, which is critical for monitoring permafrost thaw and related environmental processes. These findings demonstrate the potential of advanced ML approaches for fine-scale soil mapping and provide practical guidance for future soil sampling and infrastructure planning in permafrost-affected landscapes. The project will be released at https://github.com/knowledge-computing/Peatland-permafrost.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places",
    "url": "http://arxiv.org/abs/2506.14570v1",
    "authors": [
      "Mohammad Hashemi",
      "Andreas Zufle"
    ],
    "published": "2025-06-17",
    "abstract": "Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation",
    "url": "http://arxiv.org/abs/2506.13599v1",
    "authors": [
      "Yuwei Du",
      "Jie Feng",
      "Jian Yuan",
      "Yong Li"
    ],
    "published": "2025-06-16",
    "abstract": "Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered \\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation (\\textbf{CAMS}), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. \\textbf{CAMS} comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that \\textbf{CAMS} achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, \\textbf{CAMS} generates more realistic and plausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "HyBiomass: Global Hyperspectral Imagery Benchmark Dataset for Evaluating Geospatial Foundation Models in Forest Aboveground Biomass Estimation",
    "url": "http://arxiv.org/abs/2506.11314v1",
    "authors": [
      "Aaron Banze",
      "Timoth\u00e9e Stassin",
      "Nassim Ait Ali Braham",
      "R\u0131dvan Salih Kuzu",
      "Simon Besnard",
      "Michael Schmitt"
    ],
    "published": "2025-06-12",
    "abstract": "Comprehensive evaluation of geospatial foundation models (Geo-FMs) requires benchmarking across diverse tasks, sensors, and geographic regions. However, most existing benchmark datasets are limited to segmentation or classification tasks, and focus on specific geographic areas. To address this gap, we introduce a globally distributed dataset for forest aboveground biomass (AGB) estimation, a pixel-wise regression task. This benchmark dataset combines co-located hyperspectral imagery (HSI) from the Environmental Mapping and Analysis Program (EnMAP) satellite and predictions of AGB density estimates derived from the Global Ecosystem Dynamics Investigation lidars, covering seven continental regions. Our experimental results on this dataset demonstrate that the evaluated Geo-FMs can match or, in some cases, surpass the performance of a baseline U-Net, especially when fine-tuning the encoder. We also find that the performance difference between the U-Net and Geo-FMs depends on the dataset size for each region and highlight the importance of the token patch size in the Vision Transformer backbone for accurate predictions in pixel-wise regression tasks. By releasing this globally distributed hyperspectral benchmark dataset, we aim to facilitate the development and evaluation of Geo-FMs for HSI applications. Leveraging this dataset additionally enables research into geographic bias and generalization capacity of Geo-FMs. The dataset and source code will be made publicly available.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial Foundation Models to Enable Progress on Sustainable Development Goals",
    "url": "http://arxiv.org/abs/2505.24528v2",
    "authors": [
      "Pedram Ghamisi",
      "Weikang Yu",
      "Xiaokang Zhang",
      "Aldino Rizaldy",
      "Jian Wang",
      "Chufeng Zhou",
      "Richard Gloaguen",
      "Gustau Camps-Valls"
    ],
    "published": "2025-05-30",
    "abstract": "Foundation Models (FMs) are large-scale, pre-trained artificial intelligence (AI) systems that have revolutionized natural language processing and computer vision, and are now advancing geospatial analysis and Earth Observation (EO). They promise improved generalization across tasks, scalability, and efficient adaptation with minimal labeled data. However, despite the rapid proliferation of geospatial FMs, their real-world utility and alignment with global sustainability goals remain underexplored. We introduce SustainFM, a comprehensive benchmarking framework grounded in the 17 Sustainable Development Goals with extremely diverse tasks ranging from asset wealth prediction to environmental hazard detection. This study provides a rigorous, interdisciplinary assessment of geospatial FMs and offers critical insights into their role in attaining sustainability goals. Our findings show: (1) While not universally superior, FMs often outperform traditional approaches across diverse tasks and datasets. (2) Evaluating FMs should go beyond accuracy to include transferability, generalization, and energy efficiency as key criteria for their responsible use. (3) FMs enable scalable, SDG-grounded solutions, offering broad utility for tackling complex sustainability challenges. Critically, we advocate for a paradigm shift from model-centric development to impact-driven deployment, and emphasize metrics such as energy efficiency, robustness to domain shifts, and ethical considerations.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations",
    "url": "http://arxiv.org/abs/2505.17136v1",
    "authors": [
      "Yuhan Ji",
      "Song Gao",
      "Ying Nie",
      "Ivan Maji\u0107",
      "Krzysztof Janowicz"
    ],
    "published": "2025-05-22",
    "abstract": "Applying AI foundation models directly to geospatial datasets remains challenging due to their limited ability to represent and reason with geographical entities, specifically vector-based geometries and natural language descriptions of complex spatial relations. To address these issues, we investigate the extent to which a well-known-text (WKT) representation of geometries and their spatial relations (e.g., topological predicates) are preserved during spatial reasoning when the geospatial vector data are passed to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt engineering-based, and everyday language-based evaluation. Our experiment results demonstrate that both the embedding-based and prompt engineering-based approaches to geospatial question-answering tasks with GPT models can achieve an accuracy of over 0.6 on average for the identification of topological spatial relations between two geometries. Among the evaluated models, GPT-4 with few-shot prompting achieved the highest performance with over 0.66 accuracy on topological spatial relation inference. Additionally, GPT-based reasoner is capable of properly comprehending inverse topological spatial relations and including an LLM-generated geometry can enhance the effectiveness for geographic entity retrieval. GPT-4 also exhibits the ability to translate certain vernacular descriptions about places into formal topological relations, and adding the geometry-type or place-type context in prompts may improve inference accuracy, but it varies by instance. The performance of these spatial reasoning tasks offers valuable insights for the refinement of LLMs with geographical knowledge towards the development of geo-foundation models capable of geospatial reasoning.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GAIA: A Foundation Model for Operational Atmospheric Dynamics",
    "url": "http://arxiv.org/abs/2505.18179v2",
    "authors": [
      "Ata Akbari Asanjan",
      "Olivia Alexander",
      "Tom Berg",
      "Stephen Peng",
      "Jad Makki",
      "Clara Zhang",
      "Matt Yang",
      "Disha Shidham",
      "Srija Chakraborty",
      "William Bender",
      "Cara Crawford",
      "Arun Ravindran",
      "Olivier Raiman",
      "David Potere",
      "David Bell"
    ],
    "published": "2025-05-15",
    "abstract": "We introduce GAIA (Geospatial Artificial Intelligence for Atmospheres), a hybrid self-supervised geospatial foundation model that fuses Masked Autoencoders (MAE) with self-distillation with no labels (DINO) to generate semantically rich representations from global geostationary satellite imagery. Pre-trained on 15 years of globally-merged infrared observations (2001-2015), GAIA learns disentangled representations that capture atmospheric dynamics rather than trivial diurnal patterns, as evidenced by distributed principal component structure and temporal coherence analysis. We demonstrate robust reconstruction capabilities across varying data availability (30-95% masking), achieving superior gap-filling performance on real missing data patterns. When transferred to downstream tasks, GAIA consistently outperforms an MAE-only baseline: improving atmospheric river segmentation (F1: 0.58 vs 0.52), enhancing tropical cyclone detection (storm-level recall: 81% vs 75%, early detection: 29% vs 17%), and maintaining competitive precipitation estimation performance. Analysis reveals that GAIA's hybrid objectives encourage learning of spatially coherent, object-centric features distributed across multiple principal components rather than concentrated representations focused on reconstruction. This work demonstrates that combining complementary self-supervised objectives yields more transferable representations for diverse atmospheric modeling tasks. Model weights and code are available at: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series",
    "url": "http://arxiv.org/abs/2505.08723v1",
    "authors": [
      "Xiaolei Qin",
      "Di Wang",
      "Jing Zhang",
      "Fengxiang Wang",
      "Xin Su",
      "Bo Du",
      "Liangpei Zhang"
    ],
    "published": "2025-05-13",
    "abstract": "Satellite image time series (SITS) provide continuous observations of the Earth's surface, making them essential for applications such as environmental management and disaster assessment. However, existing spatiotemporal foundation models rely on plain vision transformers, which encode entire temporal sequences without explicitly capturing multiscale spatiotemporal relationships between land objects. This limitation hinders their effectiveness in downstream tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision transformer foundation model tailored for SITS analysis. At its core, we introduce a spatiotemporal gyroscope attention mechanism that dynamically captures evolving multiscale patterns across both time and space. For pre-training, we curate MillionST, a large-scale dataset of one million images from 100,000 geographic locations, each captured across 10 temporal phases over five years, encompassing diverse geospatial changes and seasonal variations. Leveraging this dataset, we adapt masked image modeling to pre-train TiMo, enabling it to effectively learn and encode generalizable spatiotemporal representations.Extensive experiments across multiple spatiotemporal tasks-including deforestation monitoring, land cover segmentation, crop type classification, and flood detection-demonstrate TiMo's superiority over state-of-the-art methods. Code, model, and dataset will be released at https://github.com/MiliLab/TiMo.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "url": "http://arxiv.org/abs/2505.03368v2",
    "authors": [
      "Stef De Sabbata",
      "Stefano Mizzaro",
      "Kevin Roitero"
    ],
    "published": "2025-05-06",
    "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and \"reasoning\" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call \"how LLMs think about geographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "Autoencoder",
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data",
    "url": "http://arxiv.org/abs/2504.11172v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Paolo Fraccaro",
      "Valerio Marsocci",
      "Johannes Jakubik",
      "Stefano Maurogiovanni",
      "Mikolaj Czerkawski",
      "Rocco Sedona",
      "Gabriele Cavallaro",
      "Thomas Brunschwiler",
      "Juan Bernabe-Moreno",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-04-15",
    "abstract": "Large-scale foundation models in Earth Observation can learn versatile, label-efficient representations by leveraging massive amounts of unlabeled data. However, existing public datasets are often limited in scale, geographic coverage, or sensor variety. We introduce TerraMesh, a new globally diverse, multimodal dataset combining optical, synthetic aperture radar, elevation, and land-cover modalities in an Analysis-Ready Data format. TerraMesh includes over 9~million samples with eight spatiotemporal aligned modalities, enabling large-scale pre-training. We provide detailed data processing steps, comprehensive statistics, and empirical evidence demonstrating improved model performance when pre-trained on TerraMesh. The dataset is hosted at https://huggingface.co/datasets/ibm-esa-geospatial/TerraMesh.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
    "url": "http://arxiv.org/abs/2504.11171v4",
    "authors": [
      "Johannes Jakubik",
      "Felix Yang",
      "Benedikt Blumenstiel",
      "Erik Scheurer",
      "Rocco Sedona",
      "Stefano Maurogiovanni",
      "Jente Bosmans",
      "Nikolaos Dionelis",
      "Valerio Marsocci",
      "Niklas Kopp",
      "Rahul Ramachandran",
      "Paolo Fraccaro",
      "Thomas Brunschwiler",
      "Gabriele Cavallaro",
      "Juan Bernabe-Moreno",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-04-15",
    "abstract": "We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces \"Thinking-in-Modalities\" (TiM) -- the capability of generating additional artificial data during finetuning and inference to improve the model output -- and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TerraTorch: The Geospatial Foundation Models Toolkit",
    "url": "http://arxiv.org/abs/2503.20563v1",
    "authors": [
      "Carlos Gomes",
      "Benedikt Blumenstiel",
      "Joao Lucas de Sousa Almeida",
      "Pedro Henrique de Oliveira",
      "Paolo Fraccaro",
      "Francesc Marti Escofet",
      "Daniela Szwarcman",
      "Naomi Simumba",
      "Romeo Kienzler",
      "Bianca Zadrozny"
    ],
    "published": "2025-03-26",
    "abstract": "TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial Foundation Models built on PyTorch Lightning and tailored for satellite, weather, and climate data. It integrates domain-specific data modules, pre-defined tasks, and a modular model factory that pairs any backbone with diverse decoder heads. These components allow researchers and practitioners to fine-tune supported models in a no-code fashion by simply editing a training configuration. By consolidating best practices for model development and incorporating the automated hyperparameter optimization extension Iterate, TerraTorch reduces the expertise and time required to fine-tune or benchmark models on new Earth Observation use cases. Furthermore, TerraTorch directly integrates with GEO-Bench, allowing for systematic and reproducible benchmarking of Geospatial Foundation Models. TerraTorch is open sourced under Apache 2.0, available at https://github.com/IBM/terratorch, and can be installed via pip install terratorch.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data",
    "url": "http://arxiv.org/abs/2503.12843v3",
    "authors": [
      "Haozhe Si",
      "Yuxuan Wan",
      "Minh Do",
      "Deepak Vasisht",
      "Han Zhao",
      "Hendrik F. Hamann"
    ],
    "published": "2025-03-17",
    "abstract": "Geospatial raster data, such as that collected by satellite-based imaging systems at different times and spectral bands, hold immense potential for enabling a wide range of high-impact applications. This potential stems from the rich information that is spatially and temporally contextualized across multiple channels and sensing modalities. Recent work has adapted existing self-supervised learning approaches for such geospatial data. However, they fall short of scalable model architectures, leading to inflexibility and computational inefficiencies when faced with an increasing number of channels and modalities. To address these limitations, we introduce Low-rank Efficient Spatial-Spectral Vision Transformer with three key innovations: i) the LESS Attention Block that approximates high-dimensional spatial-spectral attention through Kronecker's product of the low-dimensional spatial and spectral attention components; ii) the Continuous Positional-Channel Embedding Layer that preserves both the continuity and physical characteristics of each spatial-spectral patch; and iii) the Perception Field Mask that exploits local spatial dependencies by constraining attention to neighboring patches. To evaluate the proposed innovations, we construct GFM-Bench, which serves as a comprehensive benchmark for such geospatial raster data. We pretrain LESS ViT using a Hyperspectral Masked Autoencoder framework with integrated positional and channel masking strategies. Experimental results demonstrate that our proposed method achieves competitive performance against state-of-the-art multi-modal geospatial foundation models while outperforming them on cross-satellite generalization tasks with higher computational efficiency. The flexibility and extensibility of our framework make it a promising direction for future geospatial data analysis tasks that involve a wide range of modalities and channels.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection",
    "url": "http://arxiv.org/abs/2503.09493v2",
    "authors": [
      "Romain Thoreau",
      "Valerio Marsocci",
      "Dawa Derksen"
    ],
    "published": "2025-03-12",
    "abstract": "As large-scale heterogeneous data sets become increasingly available, adapting foundation models at low cost has become a key issue. Seminal works in natural language processing, e.g. Low-Rank Adaptation (LoRA), leverage the low \"intrinsic rank\" of parameter updates during adaptation. In this paper, we argue that incorporating stronger inductive biases in both data and models can enhance the adaptation of Geospatial Foundation Models (GFMs), pretrained on RGB satellite images, to other types of optical satellite data. Specifically, the pretrained parameters of GFMs serve as a strong prior for the spatial structure of multispectral images. For this reason, we introduce DEFLECT (Deflecting Embeddings for Finetuning Latent representations for Earth and Climate Tasks), a novel strategy for adapting GFMs to multispectral satellite imagery with very few additional parameters. DEFLECT improves the representation capabilities of the extracted features, particularly enhancing spectral information, which is essential for geoscience and environmental-related tasks. We demonstrate the effectiveness of our method across three different GFMs and five diverse datasets, ranging from forest monitoring to marine environment segmentation. Compared to competing methods, DEFLECT achieves on-par or higher accuracy with 5-10$\\times$ fewer parameters for classification and segmentation tasks. The code will be made publicly available.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "SSL4EO-S12 v1.1: A Multimodal, Multiseasonal Dataset for Pretraining, Updated",
    "url": "http://arxiv.org/abs/2503.00168v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Nassim Ait Ali Braham",
      "Conrad M Albrecht",
      "Stefano Maurogiovanni",
      "Paolo Fraccaro"
    ],
    "published": "2025-02-28",
    "abstract": "This technical report presents SSL4EO-S12 v1.1, a multimodal, multitemporal Earth Observation dataset designed for pretraining large-scale foundation models. Building on the success of SSL4EO-S12 v1.0, the new version addresses the previous challenges of data misalignment and a limited data structure for low-barrier, analysis-ready EO processing. SSL4EO-S12 v1.1 covers the world's 10,000 largest cities and its surroundings within a 50 km radius across four seasons, resulting in a diverse collection of nearly one million patches. SSL4EO-S12 v1.1 packages the data in Zarr file format for cloud-efficient loading and representation of meta-information such as including cloud masks and geolocation. Released under the CC-BY-4.0 license, SSL4EO-S12 v1.1 facilitates open research and provides a robust foundation for future advancements in self-supervised learning and geospatial analysis. The dataset is available online through https://datapub.fz-juelich.de/ssl4eo-s12, and we provided additional resources at https://github.com/DLR-MF-DAS/SSL4EO-S12-v1.1.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "How Does the Spatial Distribution of Pre-training Data Affect Geospatial Foundation Models?",
    "url": "http://arxiv.org/abs/2501.12535v1",
    "authors": [
      "Mirali Purohit",
      "Gedeon Muhawenayo",
      "Esther Rolf",
      "Hannah Kerner"
    ],
    "published": "2025-01-21",
    "abstract": "Foundation models have made rapid advances in many domains including Earth observation, where Geospatial Foundation Models (GFMs) can help address global challenges such as climate change, agriculture, and disaster response. Previous work on GFMs focused on tailoring model architecture and pre-text tasks, and did not investigate the impact of pre-training data selection on model performance. However, recent works from other domains show that the pre-training data distribution is an important factor influencing the performance of the foundation models. With this motivation, our research explores how the geographic distribution of pre-training data affects the performance of GFMs. We evaluated several pre-training data distributions by sampling different compositions from a global data pool. Our experiments with two GFMs on downstream tasks indicate that balanced and globally representative data compositions often outperform region-specific sampling, highlighting the importance of diversity and global coverage in pre-training data. Our results suggest that the most appropriate data sampling technique may depend on the specific GFM architecture. These findings will support the development of robust GFMs by incorporating quality pre-training data distributions, ultimately improving machine learning solutions for Earth observation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models",
    "url": "http://arxiv.org/abs/2501.00316v2",
    "authors": [
      "Mahir Labib Dihan",
      "Md Tanvir Hassan",
      "Md Tanvir Parvez",
      "Md Hasebul Hasan",
      "Md Almash Alam",
      "Muhammad Aamir Cheema",
      "Mohammed Eunus Ali",
      "Md Rizwan Parvez"
    ],
    "published": "2024-12-31",
    "abstract": "Recent advancements in foundation models have improved autonomous tool usage and reasoning, but their capabilities in map-based reasoning remain underexplored. To address this, we introduce MapEval, a benchmark designed to assess foundation models across three distinct tasks - textual, API-based, and visual reasoning - through 700 multiple-choice questions spanning 180 cities and 54 countries, covering spatial relationships, navigation, travel planning, and real-world map interactions. Unlike prior benchmarks that focus on simple location queries, MapEval requires models to handle long-context reasoning, API interactions, and visual map analysis, making it the most comprehensive evaluation framework for geospatial AI. On evaluation of 30 foundation models, including Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro, none surpass 67% accuracy, with open-source models performing significantly worse and all models lagging over 20% behind human performance. These results expose critical gaps in spatial inference, as models struggle with distances, directions, route planning, and place-specific reasoning, highlighting the need for better geospatial AI to bridge the gap between foundation models and real-world navigation. All the resources are available at: https://mapeval.github.io/.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2412.04204v2",
    "authors": [
      "Valerio Marsocci",
      "Yuru Jia",
      "Georges Le Bellier",
      "David Kerekes",
      "Liang Zeng",
      "Sebastian Hafner",
      "Sebastian Gerard",
      "Eric Brune",
      "Ritu Yadav",
      "Ali Shibli",
      "Heng Fang",
      "Yifang Ban",
      "Maarten Vergauwen",
      "Nicolas Audebert",
      "Andrea Nascetti"
    ],
    "published": "2024-12-05",
    "abstract": "Geospatial Foundation Models (GFMs) have emerged as powerful tools for extracting representations from Earth observation data, but their evaluation remains inconsistent and narrow. Existing works often evaluate on suboptimal downstream datasets and tasks, that are often too easy or too narrow, limiting the usefulness of the evaluations to assess the real-world applicability of GFMs. Additionally, there is a distinct lack of diversity in current evaluation protocols, which fail to account for the multiplicity of image resolutions, sensor types, and temporalities, which further complicates the assessment of GFM performance. In particular, most existing benchmarks are geographically biased towards North America and Europe, questioning the global applicability of GFMs. To overcome these challenges, we introduce PANGAEA, a standardized evaluation protocol that covers a diverse set of datasets, tasks, resolutions, sensor modalities, and temporalities. It establishes a robust and widely applicable benchmark for GFMs. We evaluate the most popular GFMs openly available on this benchmark and analyze their performance across several domains. In particular, we compare these models to supervised baselines (e.g. UNet and vanilla ViT), and assess their effectiveness when faced with limited labeled data. Our findings highlight the limitations of GFMs, under different scenarios, showing that they do not consistently outperform supervised models. PANGAEA is designed to be highly extensible, allowing for the seamless inclusion of new datasets, models, and tasks in future research. By releasing the evaluation code and benchmark, we aim to enable other researchers to replicate our experiments and build upon our work, fostering a more principled evaluation protocol for large pre-trained geospatial models. The code is available at https://github.com/VMarsocci/pangaea-bench.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "General Geospatial Inference with a Population Dynamics Foundation Model",
    "url": "http://arxiv.org/abs/2411.07207v5",
    "authors": [
      "Mohit Agarwal",
      "Mimi Sun",
      "Chaitanya Kamath",
      "Arbaaz Muslim",
      "Prithul Sarker",
      "Joydeep Paul",
      "Hector Yee",
      "Marcin Sieniek",
      "Kim Jablonski",
      "Swapnil Vispute",
      "Atul Kumar",
      "Yael Mayer",
      "David Fork",
      "Sheila de Guia",
      "Jamie McPike",
      "Adam Boulanger",
      "Tomer Shekel",
      "David Schottlander",
      "Yao Xiao",
      "Manjit Chakravarthy Manukonda",
      "Yun Liu",
      "Neslihan Bulut",
      "Sami Abu-el-haija",
      "Bryan Perozzi",
      "Monica Bharel",
      "Von Nguyen",
      "Luke Barrington",
      "Niv Efron",
      "Yossi Matias",
      "Greg Corrado",
      "Krish Eswaran",
      "Shruthi Prabhakara",
      "Shravya Shetty",
      "Gautam Prasad"
    ],
    "published": "2024-11-11",
    "abstract": "Supporting the health and well-being of dynamic populations around the world requires governmental agencies, organizations and researchers to understand and reason over complex relationships between human behavior and local contexts in order to identify high-risk groups and strategically allocate limited resources. Traditional approaches to these classes of problems often entail developing manually curated, task-specific features and models to represent human behavior and the natural and built environment, which can be challenging to adapt to new, or even, related tasks. To address this, we introduce a Population Dynamics Foundation Model (PDFM) that aims to capture the relationships between diverse data modalities and is applicable to a broad range of geospatial tasks. We first construct a geo-indexed dataset for postal codes and counties across the United States, capturing rich aggregated information on human behavior from maps, busyness, and aggregated search trends, and environmental factors such as weather and air quality. We then model this data and the complex relationships between locations using a graph neural network, producing embeddings that can be adapted to a wide range of downstream tasks using relatively simple models. We evaluate the effectiveness of our approach by benchmarking it on 27 downstream tasks spanning three distinct domains: health indicators, socioeconomic factors, and environmental measurements. The approach achieves state-of-the-art performance on all 27 geospatial interpolation tasks, and on 25 out of the 27 extrapolation and super-resolution tasks. We combined the PDFM with a state-of-the-art forecasting foundation model, TimesFM, to predict unemployment and poverty, achieving performance that surpasses fully supervised forecasting. The full set of embeddings and sample code are publicly available for researchers.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MapSAM: Adapting Segment Anything Model for Automated Feature Detection in Historical Maps",
    "url": "http://arxiv.org/abs/2411.06971v1",
    "authors": [
      "Xue Xia",
      "Daiwei Zhang",
      "Wenxuan Song",
      "Wei Huang",
      "Lorenz Hurni"
    ],
    "published": "2024-11-11",
    "abstract": "Automated feature detection in historical maps can significantly accelerate the reconstruction of the geospatial past. However, this process is often constrained by the time-consuming task of manually digitizing sufficient high-quality training data. The emergence of visual foundation models, such as the Segment Anything Model (SAM), offers a promising solution due to their remarkable generalization capabilities and rapid adaptation to new data distributions. Despite this, directly applying SAM in a zero-shot manner to historical map segmentation poses significant challenges, including poor recognition of certain geospatial features and a reliance on input prompts, which limits its ability to be fully automated. To address these challenges, we introduce MapSAM, a parameter-efficient fine-tuning strategy that adapts SAM into a prompt-free and versatile solution for various downstream historical map segmentation tasks. Specifically, we employ Weight-Decomposed Low-Rank Adaptation (DoRA) to integrate domain-specific knowledge into the image encoder. Additionally, we develop an automatic prompt generation process, eliminating the need for manual input. We further enhance the positional prompt in SAM, transforming it into a higher-level positional-semantic prompt, and modify the cross-attention mechanism in the mask decoder with masked attention for more effective feature aggregation. The proposed MapSAM framework demonstrates promising performance across two distinct historical map segmentation tasks: one focused on linear features and the other on areal features. Experimental results show that it adapts well to various features, even when fine-tuned with extremely limited data (e.g. 10 shots).",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Multimodal Contrastive Learning of Urban Space Representations from POI Data",
    "url": "http://arxiv.org/abs/2411.06229v1",
    "authors": [
      "Xinglei Wang",
      "Tao Cheng",
      "Stephen Law",
      "Zichao Zeng",
      "Lu Yin",
      "Junyuan Liu"
    ],
    "published": "2024-11-09",
    "abstract": "Existing methods for learning urban space representations from Point-of-Interest (POI) data face several limitations, including issues with geographical delineation, inadequate spatial information modelling, underutilisation of POI semantic attributes, and computational inefficiencies. To address these issues, we propose CaLLiPer (Contrastive Language-Location Pre-training), a novel representation learning model that directly embeds continuous urban spaces into vector representations that can capture the spatial and semantic distribution of urban environment. This model leverages a multimodal contrastive learning objective, aligning location embeddings with textual POI descriptions, thereby bypassing the need for complex training corpus construction and negative sampling. We validate CaLLiPer's effectiveness by applying it to learning urban space representations in London, UK, where it demonstrates 5-15% improvement in predictive performance for land use classification and socioeconomic mapping tasks compared to state-of-the-art methods. Visualisations of the learned representations further illustrate our model's advantages in capturing spatial variations in urban semantics with high accuracy and fine resolution. Additionally, CaLLiPer achieves reduced training time, showcasing its efficiency and scalability. This work provides a promising pathway for scalable, semantically rich urban space representation learning that can support the development of geospatial foundation models. The implementation code is available at https://github.com/xlwang233/CaLLiPer.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Evaluating the Effectiveness of Large Language Models in Representing and Understanding Movement Trajectories",
    "url": "http://arxiv.org/abs/2409.00335v1",
    "authors": [
      "Yuhan Ji",
      "Song Gao"
    ],
    "published": "2024-08-31",
    "abstract": "This research focuses on assessing the ability of AI foundation models in representing the trajectories of movements. We utilize one of the large language models (LLMs) (i.e., GPT-J) to encode the string format of trajectories and then evaluate the effectiveness of the LLM-based representation for trajectory data analysis. The experiments demonstrate that while the LLM-based embeddings can preserve certain trajectory distance metrics (i.e., the correlation coefficients exceed 0.74 between the Cosine distance derived from GPT-J embeddings and the Hausdorff and Dynamic Time Warping distances on raw trajectories), challenges remain in restoring numeric values and retrieving spatial neighbors in movement trajectory analytics. In addition, the LLMs can understand the spatiotemporal dependency contained in trajectories and have good accuracy in location prediction tasks. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using LLMs.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Self-Supervised Representation Learning for Geospatial Objects: A Survey",
    "url": "http://arxiv.org/abs/2408.12133v2",
    "authors": [
      "Yile Chen",
      "Weiming Huang",
      "Kaiqi Zhao",
      "Yue Jiang",
      "Gao Cong"
    ],
    "published": "2024-08-22",
    "abstract": "The proliferation of various data sources in urban and territorial environments has significantly facilitated the development of geospatial artificial intelligence (GeoAI) across a wide range of geospatial applications. However, geospatial data, which is inherently linked to geospatial objects, often exhibits data heterogeneity that necessitates specialized fusion and representation strategies while simultaneously being inherently sparse in labels for downstream tasks. Consequently, there is a growing demand for techniques that can effectively leverage geospatial data without heavy reliance on task-specific labels and model designs. This need aligns with the principles of self-supervised learning (SSL), which has garnered increasing attention for its ability to learn effective and generalizable representations directly from data without extensive labeled supervision. This paper presents a comprehensive and up-to-date survey of SSL techniques specifically applied to or developed for geospatial objects in three primary vector geometric types: Point, Polyline, and Polygon. We systematically categorize various SSL techniques into predictive and contrastive methods, and analyze their adaptation to different data types for representation learning across various downstream tasks. Furthermore, we examine the emerging trends in SSL for geospatial objects, particularly the gradual advancements towards geospatial foundation models. Finally, we discuss key challenges in current research and outline promising directions for future investigation. By offering a structured analysis of existing studies, this paper aims to inspire continued progress in integrating SSL with geospatial objects, and the development of geospatial foundation models in a longer term.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Causally Informed Pretraining Approach for Multimodal Foundation Models: Applications in Remote Sensing",
    "url": "http://arxiv.org/abs/2407.19660v3",
    "authors": [
      "Praveen Ravirathinam",
      "Ankush Khandelwal",
      "Rahul Ghosh",
      "Vipin Kumar"
    ],
    "published": "2024-07-29",
    "abstract": "Self-supervised learning has emerged as a powerful paradigm for pretraining foundation models using large-scale data. Existing pretraining approaches predominantly rely on masked reconstruction or next-token prediction strategies, demonstrating strong performance across various downstream tasks, including geoscience applications. However, these approaches do not fully capture the causal interplay between different geospatial and environmental variables. To address this limitation, we propose Causally Informed Variable-Step Forecasting (CI-VSF), a novel pretraining task that models forecasting as a conditional generation task, where driver variables (e.g., weather) inform the prediction of response variables (e.g., satellite imagery). We demonstrate that pretraining in such a fashion leads to enhanced performance when finetuned on both prediction (e.g., crop mapping, missing image prediction, soil moisture estimation) and forecasting (e.g., future image forecasting, soil moisture forecasting) downstream tasks when compared to other pretraining approaches. While we use remote sensing as our main application to demonstrate the efficacy of our proposed pretraining strategy over existing paradigms, it is applicable to any domain that involves known causal relationships amongst a set of variables.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Fine-tuning of Geospatial Foundation Models for Aboveground Biomass Estimation",
    "url": "http://arxiv.org/abs/2406.19888v1",
    "authors": [
      "Michal Muszynski",
      "Levente Klein",
      "Ademir Ferreira da Silva",
      "Anjani Prasad Atluri",
      "Carlos Gomes",
      "Daniela Szwarcman",
      "Gurkanwar Singh",
      "Kewen Gu",
      "Maciel Zortea",
      "Naomi Simumba",
      "Paolo Fraccaro",
      "Shraddha Singh",
      "Steve Meliksetian",
      "Campbell Watson",
      "Daiki Kimura",
      "Harini Srinivasan"
    ],
    "published": "2024-06-28",
    "abstract": "Global vegetation structure mapping is critical for understanding the global carbon cycle and maximizing the efficacy of nature-based carbon sequestration initiatives. Moreover, vegetation structure mapping can help reduce the impacts of climate change by, for example, guiding actions to improve water security, increase biodiversity and reduce flood risk. Global satellite measurements provide an important set of observations for monitoring and managing deforestation and degradation of existing forests, natural forest regeneration, reforestation, biodiversity restoration, and the implementation of sustainable agricultural practices. In this paper, we explore the effectiveness of fine-tuning of a geospatial foundation model to estimate above-ground biomass (AGB) using space-borne data collected across different eco-regions in Brazil. The fine-tuned model architecture consisted of a Swin-B transformer as the encoder (i.e., backbone) and a single convolutional layer for the decoder head. All results were compared to a U-Net which was trained as the baseline model Experimental results of this sparse-label prediction task demonstrate that the fine-tuned geospatial foundation model with a frozen encoder has comparable performance to a U-Net trained from scratch. This is despite the fine-tuned model having 13 times less parameters requiring optimization, which saves both time and compute resources. Further, we explore the transfer-learning capabilities of the geospatial foundation models by fine-tuning on satellite imagery with sparse labels from different eco-regions in Brazil.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Geode: A Zero-shot Geospatial Question-Answering Agent with Explicit Reasoning and Precise Spatio-Temporal Retrieval",
    "url": "http://arxiv.org/abs/2407.11014v1",
    "authors": [
      "Devashish Vikas Gupta",
      "Azeez Syed Ali Ishaqui",
      "Divya Kiran Kadiyala"
    ],
    "published": "2024-06-26",
    "abstract": "Large language models (LLMs) have shown promising results in learning and contextualizing information from different forms of data. Recent advancements in foundational models, particularly those employing self-attention mechanisms, have significantly enhanced our ability to comprehend the semantics of diverse data types. One such area that could highly benefit from multi-modality is in understanding geospatial data, which inherently has multiple modalities. However, current Natural Language Processing (NLP) mechanisms struggle to effectively address geospatial queries. Existing pre-trained LLMs are inadequately equipped to meet the unique demands of geospatial data, lacking the ability to retrieve precise spatio-temporal data in real-time, thus leading to significantly reduced accuracy in answering complex geospatial queries. To address these limitations, we introduce Geode--a pioneering system designed to tackle zero-shot geospatial question-answering tasks with high precision using spatio-temporal data retrieval. Our approach represents a significant improvement in addressing the limitations of current LLM models, demonstrating remarkable improvement in geospatial question-answering abilities compared to existing state-of-the-art pre-trained models.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Evaluating and Benchmarking Foundation Models for Earth Observation and Geospatial AI",
    "url": "http://arxiv.org/abs/2406.18295v1",
    "authors": [
      "Nikolaos Dionelis",
      "Casper Fibaek",
      "Luke Camilleri",
      "Andreas Luyts",
      "Jente Bosmans",
      "Bertrand Le Saux"
    ],
    "published": "2024-06-26",
    "abstract": "When we are primarily interested in solving several problems jointly with a given prescribed high performance accuracy for each target application, then Foundation Models should for most cases be used rather than problem-specific models. We focus on the specific Computer Vision application of Foundation Models for Earth Observation (EO) and geospatial AI. These models can solve important problems we are tackling, including for example land cover classification, crop type mapping, flood segmentation, building density estimation, and road regression segmentation. In this paper, we show that for a limited number of labelled data, Foundation Models achieve improved performance compared to problem-specific models. In this work, we also present our proposed evaluation benchmark for Foundation Models for EO. Benchmarking the generalization performance of Foundation Models is important as it has become difficult to standardize a fair comparison across the many different models that have been proposed recently. We present the results using our evaluation benchmark for EO Foundation Models and show that Foundation Models are label efficient in the downstream tasks and help us solve problems we are tackling in EO and remote sensing.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "GFM4MPM: Towards Geospatial Foundation Models for Mineral Prospectivity Mapping",
    "url": "http://arxiv.org/abs/2406.12756v1",
    "authors": [
      "Angel Daruna",
      "Vasily Zadorozhnyy",
      "Georgina Lukoczki",
      "Han-Pang Chiu"
    ],
    "published": "2024-06-18",
    "abstract": "Machine Learning (ML) for Mineral Prospectivity Mapping (MPM) remains a challenging problem as it requires the analysis of associations between large-scale multi-modal geospatial data and few historical mineral commodity observations (positive labels). Recent MPM works have explored Deep Learning (DL) as a modeling tool with more representation capacity. However, these overparameterized methods may be more prone to overfitting due to their reliance on scarce labeled data. While a large quantity of unlabeled geospatial data exists, no prior MPM works have considered using such information in a self-supervised manner. Our MPM approach uses a masked image modeling framework to pretrain a backbone neural network in a self-supervised manner using unlabeled geospatial data alone. After pretraining, the backbone network provides feature extraction for downstream MPM tasks. We evaluated our approach alongside existing methods to assess mineral prospectivity of Mississippi Valley Type (MVT) and Clastic-Dominated (CD) Lead-Zinc deposits in North America and Australia. Our results demonstrate that self-supervision promotes robustness in learned features, improving prospectivity predictions. Additionally, we leverage explainable artificial intelligence techniques to demonstrate that individual predictions can be interpreted from a geological perspective.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Towards Vision-Language Geo-Foundation Model: A Survey",
    "url": "http://arxiv.org/abs/2406.09385v1",
    "authors": [
      "Yue Zhou",
      "Litong Feng",
      "Yiping Ke",
      "Xue Jiang",
      "Junchi Yan",
      "Xue Yang",
      "Wayne Zhang"
    ],
    "published": "2024-06-13",
    "abstract": "Vision-Language Foundation Models (VLFMs) have made remarkable progress on various multimodal tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding. However, most methods rely on training with general image datasets, and the lack of geospatial data leads to poor performance on earth observation. Numerous geospatial image-text pair datasets and VLFMs fine-tuned on them have been proposed recently. These new approaches aim to leverage large-scale, multimodal geospatial data to build versatile intelligent models with diverse geo-perceptive capabilities, which we refer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper thoroughly reviews VLGFMs, summarizing and analyzing recent developments in the field. In particular, we introduce the background and motivation behind the rise of VLGFMs, highlighting their unique research significance. Then, we systematically summarize the core technologies employed in VLGFMs, including data construction, model architectures, and applications of various multimodal geospatial tasks. Finally, we conclude with insights, issues, and discussions regarding future research directions. To the best of our knowledge, this is the first comprehensive literature review of VLGFMs. We keep tracing related works at https://github.com/zytx121/Awesome-VLGFM.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SeeFar: Satellite Agnostic Multi-Resolution Dataset for Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2406.06776v1",
    "authors": [
      "James Lowman",
      "Kelly Liu Zheng",
      "Roydon Fraser",
      "Jesse Van Griensven The",
      "Mojtaba Valipour"
    ],
    "published": "2024-06-10",
    "abstract": "SeeFar is an evolving collection of multi-resolution satellite images from public and commercial satellites. We specifically curated this dataset for training geospatial foundation models, unconstrained by satellite type. In recent years, advances in technology have made satellite imagery more accessible than ever. More earth-observing satellites have been launched in the last five years than in the previous fifty. Modern commercial satellites now offer up to 100 times the spatial resolution of public access satellites. However, the high cost and limited historical availability of commercial satellite imagery is a barrier to the training of foundational models, impacting what images can be used during inference. The SeeFar dataset represents a step towards training models that are satellite-agnostic by combining multi-resolution commercial and public access pre-processed images. This will enable users to utilize historical data alongside higher-resolution, more expensive satellite imagery, offering greater flexibility during inference. To achieve this, we describe a process for standardizing data from diverse satellite sources, normalizing different data formats, and aligning spectral bands to enhance interoperability. The SeeFar dataset includes images at a resolution of 384x384 pixels, spanning four spectral bands (Blue, Green, Red, and Near-Infrared) and expanding spatial resolutions (starting with 30, 10, 1.5, and 1.0 meters), all in cloud-optimized GeoTIFF format. It also provides consistent and comprehensive metadata to enhance data transparency and reliability. By aggregating data from multiple sources, SeeFar makes processed and consistent satellite data accessible to a wider range of users - from researchers to policymakers - fostering competition and innovation in satellite imagery analysis. The dataset is available at \\url{coastalcarbon.ai/seefar}.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery",
    "url": "http://arxiv.org/abs/2405.02512v2",
    "authors": [
      "Yohei Nakayama",
      "Jiawei Su",
      "Luis M. Pazos-Out\u00f3n"
    ],
    "published": "2024-05-03",
    "abstract": "Recent advancements in foundation models have significantly impacted various fields, including natural language processing, computer vision, and multi-modal tasks. One area that stands to benefit greatly is Earth observation, where these models can efficiently process large-scale, unlabeled geospatial data. In this work we extend the SwinMAE model to integrate temporal information for satellite time-series data. The architecture employs a hierarchical 3D Masked Autoencoder (MAE) with Video Swin Transformer blocks to effectively capture multi-scale spatio-temporal dependencies in satellite imagery. To enhance transfer learning, we incorporate both encoder and decoder pretrained weights, along with skip connections to preserve scale-specific information. This forms an architecture similar to SwinUNet with an additional temporal component. Our approach shows significant performance improvements over existing state-of-the-art foundation models for all the evaluated downstream tasks: land cover segmentation, building density prediction, flood mapping, wildfire scar mapping and multi-temporal crop segmentation. Particularly, in the land cover segmentation task of the PhilEO Bench dataset, it outperforms other geospatial foundation models with a 10.4% higher accuracy.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "NLP-enabled Trajectory Map-matching in Urban Road Networks using a Transformer-based Encoder-decoder",
    "url": "http://arxiv.org/abs/2404.12460v4",
    "authors": [
      "Sevin Mohammadi",
      "Andrew W. Smyth"
    ],
    "published": "2024-04-18",
    "abstract": "Vehicular trajectory data from geolocation telematics is vital for analyzing urban mobility patterns. Map-matching aligns noisy, sparsely sampled GPS trajectories with digital road maps to reconstruct accurate vehicle paths. Traditional methods rely on geometric proximity, topology, and shortest-path heuristics, but they overlook two key factors: (1) drivers may prefer routes based on local road characteristics rather than shortest paths, revealing learnable shared preferences, and (2) GPS noise varies spatially due to multipath effects. These factors can reduce the effectiveness of conventional methods in complex scenarios and increase the effort required for heuristic-based implementations. This study introduces a data-driven, deep learning-based map-matching framework, formulating the task as machine translation, inspired by NLP. Specifically, a transformer-based encoder-decoder model learns contextual representations of noisy GPS points to infer trajectory behavior and road structures in an end-to-end manner. Trained on large-scale trajectory data, the method improves path estimation accuracy. Experiments on synthetic trajectories show that this approach outperforms conventional methods by integrating contextual awareness. Evaluation on real-world GPS traces from Manhattan, New York, achieves 75% accuracy in reconstructing navigated routes. These results highlight the effectiveness of transformers in capturing drivers' trajectory behaviors, spatial dependencies, and noise patterns, offering a scalable, robust solution for map-matching. This work contributes to advancing trajectory-driven foundation models for geospatial modeling and urban mobility applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Pretraining Billion-scale Geospatial Foundational Models on Frontier",
    "url": "http://arxiv.org/abs/2404.11706v1",
    "authors": [
      "Aristeidis Tsaris",
      "Philipe Ambrozio Dias",
      "Abhishek Potnis",
      "Junqi Yin",
      "Feiyi Wang",
      "Dalton Lunga"
    ],
    "published": "2024-04-17",
    "abstract": "As AI workloads increase in scope, generalization capability becomes challenging for small task-specific models and their demand for large amounts of labeled training samples increases. On the contrary, Foundation Models (FMs) are trained with internet-scale unlabeled data via self-supervised learning and have been shown to adapt to various tasks with minimal fine-tuning. Although large FMs have demonstrated significant impact in natural language processing and computer vision, efforts toward FMs for geospatial applications have been restricted to smaller size models, as pretraining larger models requires very large computing resources equipped with state-of-the-art hardware accelerators. Current satellite constellations collect 100+TBs of data a day, resulting in images that are billions of pixels and multimodal in nature. Such geospatial data poses unique challenges opening up new opportunities to develop FMs. We investigate billion scale FMs and HPC training profiles for geospatial applications by pretraining on publicly available data. We studied from end-to-end the performance and impact in the solution by scaling the model size. Our larger 3B parameter size model achieves up to 30% improvement in top1 scene classification accuracy when comparing a 100M parameter model. Moreover, we detail performance experiments on the Frontier supercomputer, America's first exascale system, where we study different model and data parallel approaches using PyTorch's Fully Sharded Data Parallel library. Specifically, we study variants of the Vision Transformer architecture (ViT), conducting performance analysis for ViT models with size up to 15B parameters. By discussing throughput and performance bottlenecks under different parallelism configurations, we offer insights on how to leverage such leadership-class HPC resources when developing large models for geospatial imagery applications.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2404.01260v1",
    "authors": [
      "Boran Han",
      "Shuai Zhang",
      "Xingjian Shi",
      "Markus Reichstein"
    ],
    "published": "2024-04-01",
    "abstract": "In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa",
    "url": "http://arxiv.org/abs/2403.06860v2",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Mukhtar Opeyemi Yusuf",
      "Kobby Panford-Quainoo",
      "Arnu Pretorius"
    ],
    "published": "2024-03-11",
    "abstract": "Desert locust swarms present a major threat to agriculture and food security. Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures. We curated a dataset from the United Nations Food and Agriculture Organization's (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images. Our approach employed custom deep learning models (three-dimensional and LSTM-based recurrent convolutional networks), along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023. These models notably outperformed existing baselines, with the Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding from our research is that multi-spectral earth observation images alone are sufficient for effective locust breeding ground prediction without the need to explicitly incorporate climatic or environmental features.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models",
    "url": "http://arxiv.org/abs/2403.02059v2",
    "authors": [
      "Benedikt Blumenstiel",
      "Viktoria Moor",
      "Romeo Kienzler",
      "Thomas Brunschwiler"
    ],
    "published": "2024-03-04",
    "abstract": "Image retrieval enables an efficient search through vast amounts of satellite imagery and returns similar images to a query. Deep learning models can identify images across various semantic concepts without the need for annotations. This work proposes to use Geospatial Foundation Models, like Prithvi, for remote sensing image retrieval with multiple benefits: i) the models encode multi-spectral satellite data and ii) generalize without further fine-tuning. We introduce two datasets to the retrieval task and observe a strong performance: Prithvi processes six bands and achieves a mean Average Precision of 97.62% on BigEarthNet-43 and 44.51% on ForestNet-12, outperforming other RGB-based models. Further, we evaluate three compression methods with binarized embeddings balancing retrieval speed and accuracy. They match the retrieval speed of much shorter hash codes while maintaining the same accuracy as floating-point embeddings but with a 32-fold compression. The code is available at https://github.com/IBM/remote-sensing-image-retrieval.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Large Language Models are Geographically Biased",
    "url": "http://arxiv.org/abs/2402.02680v2",
    "authors": [
      "Rohin Manvi",
      "Samar Khanna",
      "Marshall Burke",
      "David Lobell",
      "Stefano Ermon"
    ],
    "published": "2024-02-05",
    "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\u03c1$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\u03c1$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping",
    "url": "http://arxiv.org/abs/2401.08787v1",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Sizhe Wang",
      "Yezhou Yang",
      "Hyunho Lee",
      "Anna Liljedahl",
      "Chandi Witharana",
      "Yili Yang",
      "Brendan M. Rogers",
      "Samantha T. Arundel",
      "Matthew B. Jones",
      "Kenton McHenry",
      "Patricia Solis"
    ],
    "published": "2024-01-16",
    "abstract": "This paper assesses trending AI foundation models, especially emerging computer vision foundation models and their performance in natural landscape feature segmentation. While the term foundation model has quickly garnered interest from the geospatial domain, its definition remains vague. Hence, this paper will first introduce AI foundation models and their defining characteristics. Built upon the tremendous success achieved by Large Language Models (LLMs) as the foundation models for language tasks, this paper discusses the challenges of building foundation models for geospatial artificial intelligence (GeoAI) vision tasks. To evaluate the performance of large AI vision models, especially Meta's Segment Anything Model (SAM), we implemented different instance segmentation pipelines that minimize the changes to SAM to leverage its power as a foundation model. A series of prompt strategies was developed to test SAM's performance regarding its theoretical upper bound of predictive accuracy, zero-shot performance, and domain adaptability through fine-tuning. The analysis used two permafrost feature datasets, ice-wedge polygons and retrogressive thaw slumps because (1) these landform features are more challenging to segment than manmade features due to their complicated formation mechanisms, diverse forms, and vague boundaries; (2) their presence and changes are important indicators for Arctic warming and climate change. The results show that although promising, SAM still has room for improvement to support AI-augmented terrain mapping. The spatial and domain generalizability of this finding is further validated using a more general dataset EuroCrop for agricultural field mapping. Finally, we discuss future research directions that strengthen SAM's applicability in challenging geospatial domains.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting",
    "url": "http://arxiv.org/abs/2510.25563v1",
    "authors": [
      "V\u00edctor Medina",
      "Giovanny A. Cuervo-Londo\u00f1o",
      "Javier S\u00e1nchez"
    ],
    "published": "2025-10-29",
    "abstract": "The accurate prediction of oceanographic variables is crucial for understanding climate change, managing marine resources, and optimizing maritime activities. Traditional ocean forecasting relies on numerical models; however, these approaches face limitations in terms of computational cost and scalability. In this study, we adapt Aurora, a foundational deep learning model originally designed for atmospheric forecasting, to predict sea surface temperature (SST) in the Canary Upwelling System. By fine-tuning this model with high-resolution oceanographic reanalysis data, we demonstrate its ability to capture complex spatiotemporal patterns while reducing computational demands. Our methodology involves a staged fine-tuning process, incorporating latitude-weighted error metrics and optimizing hyperparameters for efficient learning. The experimental results show that the model achieves a low RMSE of 0.119K, maintaining high anomaly correlation coefficients (ACC $\\approx 0.997$). The model successfully reproduces large-scale SST structures but faces challenges in capturing finer details in coastal regions. This work contributes to the field of data-driven ocean forecasting by demonstrating the feasibility of using deep learning models pre-trained in different domains for oceanic applications. Future improvements include integrating additional oceanographic variables, increasing spatial resolution, and exploring physics-informed neural networks to enhance interpretability and understanding. These advancements can improve climate modeling and ocean prediction accuracy, supporting decision-making in environmental and economic sectors.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis",
    "url": "http://arxiv.org/abs/2510.03555v1",
    "authors": [
      "Peiran Quan",
      "Zifan Gu",
      "Zhuo Zhao",
      "Qin Zhou",
      "Donghan M. Yang",
      "Ruichen Rong",
      "Yang Xie",
      "Guanghua Xiao"
    ],
    "published": "2025-10-03",
    "abstract": "Foundation models (FMs) have transformed computational pathology by providing powerful, general-purpose feature extractors. However, adapting and benchmarking individual FMs for specific diagnostic tasks is often time-consuming and resource-intensive, especially given their scale and diversity. To address this challenge, we introduce Group-Aggregative Selection Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that seamlessly integrates features from multiple FMs, preserving their complementary strengths without requiring manual feature selection or extensive task-specific fine-tuning. Across classification tasks in three cancer datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL consistently achieves superior or on-par performance relative to individual FMs and established MIL methods, demonstrating its robustness and generalizability. By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines model deployment for pathology and provides a scalable foundation for future multimodal and precision oncology applications.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Finetuning AI Foundation Models to Develop Subgrid-Scale Parameterizations: A Case Study on Atmospheric Gravity Waves",
    "url": "http://arxiv.org/abs/2509.03816v1",
    "authors": [
      "Aman Gupta",
      "Aditi Sheshadri",
      "Sujit Roy",
      "Johannes Schmude",
      "Vishal Gaur",
      "Wei Ji Leong",
      "Manil Maskey",
      "Rahul Ramachandran"
    ],
    "published": "2025-09-04",
    "abstract": "Global climate models parameterize a range of atmospheric-oceanic processes like gravity waves, clouds, moist convection, and turbulence that cannot be sufficiently resolved. These subgrid-scale closures for unresolved processes are a leading source of model uncertainty. Here, we present a new approach to developing machine learning parameterizations of small-scale climate processes by fine-tuning a pre-trained AI foundation model (FM). FMs are largely unexplored in climate research. A pre-trained encoder-decoder from a 2.3 billion parameter FM (NASA and IBM Research's Prithvi WxC) -- which contains a latent probabilistic representation of atmospheric evolution -- is fine-tuned (or reused) to create a deep learning parameterization for atmospheric gravity waves (GWs). The parameterization captures GW effects for a coarse-resolution climate model by learning the fluxes from an atmospheric reanalysis with 10 times finer resolution. A comparison of monthly averages and instantaneous evolution with a machine learning model baseline (an Attention U-Net) reveals superior predictive performance of the FM parameterization throughout the atmosphere, even in regions excluded from pre-training. This performance boost is quantified using the Hellinger distance, which is 0.11 for the baseline and 0.06 for the fine-tuned model. Our findings emphasize the versatility and reusability of FMs, which could be used to accomplish a range of atmosphere- and climate-related applications, leading the way for the creation of observations-driven and physically accurate parameterizations for more earth-system processes.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets",
    "url": "http://arxiv.org/abs/2506.14765v4",
    "authors": [
      "Nikolaos Dionelis",
      "Riccardo Musto",
      "Jente Bosmans",
      "Simone Sarti",
      "Giancarlo Paoletti",
      "S\u00e9bastien Lef\u00e8vre",
      "Bertrand Le Saux",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "published": "2025-06-17",
    "abstract": "Today, Earth Observation (EO) satellites generate massive volumes of data. To fully exploit this, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for downstream tasks with minimal labeled data. In this paper, we study scaling-up FMs: we train our models on the pretraining dataset MajorTOM 23TB which includes all regions, and the performance on average is competitive versus models pretrained on more specialized datasets which are substantially smaller and include only land. The additional data of oceans and ice do not decrease the performance on land-focused downstream tasks. These results indicate that large FMs trained on global datasets for a wider variety of downstream tasks can be useful for downstream applications that only require a subset of the information included in their training. The second contribution is the exploration of U-Net Convolutional Neural Network (CNN), Vision Transformers (ViT), and Mamba State-Space Models (SSM) as FMs. U-Net captures local correlations amongst pixels, while ViT and Mamba capture local and distant correlations. We develop various models using different architectures, including U-Net, ViT, and Mamba, and different number of parameters. We evaluate the FLoating-point OPerations (FLOPs) needed by the models. We fine-tune on the PhilEO Bench for different downstream tasks: roads, buildings, and land cover. For most n-shots for roads and buildings, U-Net 200M-2T outperforms the other models. Using Mamba, we achieve comparable results on the downstream tasks, with less computational expenses. We also compare with the recent FM TerraMind which we evaluate on PhilEO Bench.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial",
    "url": "http://arxiv.org/abs/2506.10386v1",
    "authors": [
      "Jerry Yan",
      "Chinmay Talegaonkar",
      "Nicholas Antipa",
      "Eric Terrill",
      "Sophia Merrifield"
    ],
    "published": "2025-06-12",
    "abstract": "The burial state of anthropogenic objects on the seafloor provides insight into localized sedimentation dynamics and is also critical for assessing ecological risks, potential pollutant transport, and the viability of recovery or mitigation strategies for hazardous materials such as munitions. Accurate burial depth estimation from remote imagery remains difficult due to partial occlusion, poor visibility, and object degradation. This work introduces a computer vision pipeline, called PoseIDON, which combines deep foundation model features with multiview photogrammetry to estimate six degrees of freedom object pose and the orientation of the surrounding seafloor from ROV video. Burial depth is inferred by aligning CAD models of the objects with observed imagery and fitting a local planar approximation of the seafloor. The method is validated using footage of 54 objects, including barrels and munitions, recorded at a historic ocean dumpsite in the San Pedro Basin. The model achieves a mean burial depth error of approximately 10 centimeters and resolves spatial burial patterns that reflect underlying sediment transport processes. This approach enables scalable, non-invasive mapping of seafloor burial and supports environmental assessment at contaminated sites.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "MAX: Masked Autoencoder for X-ray Fluorescence in Geological Investigation",
    "url": "http://arxiv.org/abs/2410.12330v1",
    "authors": [
      "An-Sheng Lee",
      "Yu-Wen Pao",
      "Hsuan-Tien Lin",
      "Sofia Ya Hsuan Liou"
    ],
    "published": "2024-10-16",
    "abstract": "Pre-training foundation models has become the de-facto procedure for deep learning approaches, yet its application remains limited in the geological studies, where in needs of the model transferability to break the shackle of data scarcity. Here we target on the X-ray fluorescence (XRF) scanning data, a standard high-resolution measurement in extensive scientific drilling projects. We propose a scalable self-supervised learner, masked autoencoders on XRF spectra (MAX), to pre-train a foundation model covering geological records from multiple regions of the Pacific and Southern Ocean. In pre-training, we find that masking a high proportion of the input spectrum (50\\%) yields a nontrivial and meaningful self-supervisory task. For downstream tasks, we select the quantification of XRF spectra into two costly geochemical measurements, CaCO$_3$ and total organic carbon, due to their importance in understanding the paleo-oceanic carbon system. Our results show that MAX, requiring only one-third of the data, outperforms models without pre-training in terms of quantification accuracy. Additionally, the model's generalizability improves by more than 60\\% in zero-shot tests on new materials, with explainability further ensuring its robustness. Thus, our approach offers a promising pathway to overcome data scarcity in geological discovery by leveraging the self-supervised foundation model and fast-acquired XRF scanning data.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Sea ice floe segmentation in close-range optical imagery using active contour and foundation models",
    "url": "http://arxiv.org/abs/2409.06641v5",
    "authors": [
      "Giulio Passerotti",
      "Alberto Alberello",
      "Marcello Vichi",
      "Luke G. Bennetts",
      "James Bailey",
      "Alessandro Toffoli"
    ],
    "published": "2024-09-10",
    "abstract": "The size of sea ice floes in the marginal ice zone (MIZ) is a key factor influencing ice coverage, albedo, wave propagation, and ocean--atmosphere energy exchanges. Floe size can be observed by processing visual-range imagery from ships, aircraft, or satellites. However, autonomously capturing floe boundaries remains challenging, particularly due to sea ice heterogeneity, which impairs boundary definition and reduces image clarity. This study evaluates the accuracy of sea ice floe segmentation using the gradient vector flow (GVF) active contour method, the deep learning-based Segment Anything Model (SAM), and a hybrid approach combining GVF and SAM. Methods are evaluated on a representative subset of a large dataset of close-range, high-resolution imagery collected from cameras aboard an icebreaker during an Antarctic winter expedition. Spanning a wide range of ice conditions and image clarity in the MIZ, the subset provides a rigorous segmentation test bed. Performance is assessed in terms of floe detection accuracy, size distribution, and ice concentration, with results compared against a manually segmented benchmark. Results indicate SAM, in prompt-driven mode, offers the best balance between accuracy and computational efficiency. Its strong performance in estimating sea ice concentration and detecting floes, while maintaining close agreement with benchmark floe size distributions, makes it suitable for real-time applications and scalable analyses of large imagery datasets. Compared with SAM, the combined SAM-GVF method provides more accurate floe boundary delineation, although at much higher computational cost, and is therefore better suited for analyses requiring precise floe shapes.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Recent Advances on Machine Learning for Computational Fluid Dynamics: A Survey",
    "url": "http://arxiv.org/abs/2408.12171v1",
    "authors": [
      "Haixin Wang",
      "Yadi Cao",
      "Zijie Huang",
      "Yuxuan Liu",
      "Peiyan Hu",
      "Xiao Luo",
      "Zezheng Song",
      "Wanjia Zhao",
      "Jilin Liu",
      "Jinan Sun",
      "Shikun Zhang",
      "Long Wei",
      "Yue Wang",
      "Tailin Wu",
      "Zhi-Ming Ma",
      "Yizhou Sun"
    ],
    "published": "2024-08-22",
    "abstract": "This paper explores the recent advancements in enhancing Computational Fluid Dynamics (CFD) tasks through Machine Learning (ML) techniques. We begin by introducing fundamental concepts, traditional methods, and benchmark datasets, then examine the various roles ML plays in improving CFD. The literature systematically reviews papers in recent five years and introduces a novel classification for forward modeling: Data-driven Surrogates, Physics-Informed Surrogates, and ML-assisted Numerical Solutions. Furthermore, we also review the latest ML methods in inverse design and control, offering a novel classification and providing an in-depth discussion. Then we highlight real-world applications of ML for CFD in critical scientific and engineering disciplines, including aerodynamics, combustion, atmosphere & ocean science, biology fluid, plasma, symbolic regression, and reduced order modeling. Besides, we identify key challenges and advocate for future research directions to address these challenges, such as multi-scale representation, physical knowledge encoding, scientific foundation model and automatic scientific discovery. This review serves as a guide for the rapidly expanding ML for CFD community, aiming to inspire insights for future advancements. We draw the conclusion that ML is poised to significantly transform CFD research by enhancing simulation accuracy, reducing computational time, and enabling more complex analyses of fluid dynamics. The paper resources can be viewed at https://github.com/WillDreamer/Awesome-AI4CFD.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "WV-Net: A foundation model for SAR WV-mode satellite imagery trained using contrastive self-supervised learning on 10 million images",
    "url": "http://arxiv.org/abs/2406.18765v1",
    "authors": [
      "Yannik Glaser",
      "Justin E. Stopa",
      "Linnea M. Wolniewicz",
      "Ralph Foster",
      "Doug Vandemark",
      "Alexis Mouche",
      "Bertrand Chapron",
      "Peter Sadowski"
    ],
    "published": "2024-06-26",
    "abstract": "The European Space Agency's Copernicus Sentinel-1 (S-1) mission is a constellation of C-band synthetic aperture radar (SAR) satellites that provide unprecedented monitoring of the world's oceans. S-1's wave mode (WV) captures 20x20 km image patches at 5 m pixel resolution and is unaffected by cloud cover or time-of-day. The mission's open data policy has made SAR data easily accessible for a range of applications, but the need for manual image annotations is a bottleneck that hinders the use of machine learning methods. This study uses nearly 10 million WV-mode images and contrastive self-supervised learning to train a semantic embedding model called WV-Net. In multiple downstream tasks, WV-Net outperforms a comparable model that was pre-trained on natural images (ImageNet) with supervised learning. Experiments show improvements for estimating wave height (0.50 vs 0.60 RMSE using linear probing), estimating near-surface air temperature (0.90 vs 0.97 RMSE), and performing multilabel-classification of geophysical and atmospheric phenomena (0.96 vs 0.95 micro-averaged AUROC). WV-Net embeddings are also superior in an unsupervised image-retrieval task and scale better in data-sparse settings. Together, these results demonstrate that WV-Net embeddings can support geophysical research by providing a convenient foundation model for a variety of data analysis and exploration tasks.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Foundation Model for the Earth System",
    "url": "http://arxiv.org/abs/2405.13063v3",
    "authors": [
      "Cristian Bodnar",
      "Wessel P. Bruinsma",
      "Ana Lucic",
      "Megan Stanley",
      "Anna Vaughan",
      "Johannes Brandstetter",
      "Patrick Garvan",
      "Maik Riechert",
      "Jonathan A. Weyn",
      "Haiyu Dong",
      "Jayesh K. Gupta",
      "Kit Thambiratnam",
      "Alexander T. Archibald",
      "Chun-Chieh Wu",
      "Elizabeth Heider",
      "Max Welling",
      "Richard E. Turner",
      "Paris Perdikaris"
    ],
    "published": "2024-05-20",
    "abstract": "Reliable forecasts of the Earth system are crucial for human progress and safety from natural disasters. Artificial intelligence offers substantial potential to improve prediction accuracy and computational efficiency in this field, however this remains underexplored in many domains. Here we introduce Aurora, a large-scale foundation model for the Earth system trained on over a million hours of diverse data. Aurora outperforms operational forecasts for air quality, ocean waves, tropical cyclone tracks, and high-resolution weather forecasting at orders of magnitude smaller computational expense than dedicated existing systems. With the ability to fine-tune Aurora to diverse application domains at only modest computational cost, Aurora represents significant progress in making actionable Earth system predictions accessible to anyone.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "A Comprehensive Evaluation of Histopathology Foundation Models for Ovarian Cancer Subtype Classification",
    "url": "http://arxiv.org/abs/2405.09990v2",
    "authors": [
      "Jack Breen",
      "Katie Allen",
      "Kieran Zucker",
      "Lucy Godson",
      "Nicolas M. Orsi",
      "Nishant Ravikumar"
    ],
    "published": "2024-05-16",
    "abstract": "Large pretrained transformers are increasingly being developed as generalised foundation models which can underpin powerful task-specific artificial intelligence models. Histopathology foundation models show great promise across many tasks, but analyses have typically been limited by arbitrary hyperparameters that were not tuned to the specific task. We report the most rigorous single-task validation of histopathology foundation models to date, specifically in ovarian cancer morphological subtyping. Attention-based multiple instance learning classifiers were compared using three ImageNet-pretrained feature extractors and fourteen histopathology foundation models. The training set consisted of 1864 whole slide images from 434 ovarian carcinoma cases at Leeds Teaching Hospitals NHS Trust. Five-class classification performance was evaluated through five-fold cross-validation, and these cross-validation models were ensembled for hold-out testing and external validation on the Transcanadian Study and OCEAN Challenge datasets. The best-performing model used the H-optimus-0 foundation model, with five-class balanced accuracies of 89%, 97%, and 74% in the test sets. Normalisations and augmentations aided the performance of the ImageNet-pretrained ResNets, but these were still outperformed by 13 of the 14 foundation models. Hyperparameter tuning the downstream classifiers improved performance by a median 1.9% balanced accuracy, with many improvements being statistically significant. Histopathology foundation models offer a clear benefit to ovarian cancer subtyping, improving classification performance to a degree where clinical utility is tangible, albeit with an increased computational burden. Such models could provide a second opinion to histopathologists diagnosing challenging cases and may improve the accuracy, objectivity, and efficiency of pathological diagnoses overall.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "ResNet",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers",
    "url": "http://arxiv.org/abs/2401.17870v2",
    "authors": [
      "Shan Zhao",
      "Zhitong Xiong",
      "Xiao Xiang Zhu"
    ],
    "published": "2024-01-31",
    "abstract": "Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enhances predictability on four surface and five upper-level atmospheric variables at a two-week lead time. Furthermore, the teleconnection-filtered features improve the spatial granularity of outputs significantly, indicating their potential physical consistency. Our research underscores the importance of atmospheric and oceanic teleconnections in driving future weather conditions. Besides, it presents a resource-efficient pathway for researchers to leverage existing foundation models on versatile downstream tasks.",
    "categories": [
      "ocean",
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing",
    "url": "http://arxiv.org/abs/2512.11490v1",
    "authors": [
      "Emanuel S\u00e1nchez Aimar",
      "Gulnaz Zhambulova",
      "Fahad Shahbaz Khan",
      "Yonghao Xu",
      "Michael Felsberg"
    ],
    "published": "2025-12-12",
    "abstract": "Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion",
    "url": "http://arxiv.org/abs/2512.06504v1",
    "authors": [
      "Andrii Lysyi",
      "Anatoliy Sachenko",
      "Pavlo Radiuk",
      "Mykola Lysyi",
      "Oleksandr Melnychenko",
      "Diana Zahorodnia"
    ],
    "published": "2025-12-06",
    "abstract": "The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A 3D virtual geographic environment for flood representation towards risk communication",
    "url": "http://arxiv.org/abs/2512.03839v1",
    "authors": [
      "Weilian Li",
      "Jun Zhu",
      "Saied Pirasteh",
      "Qing Zhu",
      "Yukun Guo",
      "Lan Luo",
      "Youness Dehbi"
    ],
    "published": "2025-12-03",
    "abstract": "Risk communication seeks to develop a shared understanding of disaster among stakeholders, thereby amplifying public awareness and empowering them to respond more effectively to emergencies. However, existing studies have overemphasized specialized numerical modelling, making the professional output challenging to understand and use by non-research stakeholders. In this context, this article proposes a 3D virtual geographic environment for flood representation towards risk communication, which integrates flood modelling, parallel computation, and 3D representation in a pipeline. Finally, a section of the Rhine River in Bonn, Germany, is selected for experiment analysis. The experimental results show that the proposed approach is capable of flood modelling and 3D representation within a few hours, the parallel speedup ratio reached 6.45. The intuitive flood scene with 3D city models is beneficial for promoting flood risk communication and is particularly helpful for participants without direct experience of floods to understand its spatiotemporal process. It also can be embedded in the Geospatial Infrastructure Management Ecosystem (GeoIME) cloud application for intelligent flood systems.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding",
    "url": "http://arxiv.org/abs/2512.03558v1",
    "authors": [
      "Huy Quang Ung",
      "Guillaume Habault",
      "Yasutaka Nishimura",
      "Hao Niu",
      "Roberto Legaspi",
      "Tomoki Oya",
      "Ryoichi Kojima",
      "Masato Taya",
      "Chihiro Ono",
      "Atsunori Minamikawa",
      "Yan Liu"
    ],
    "published": "2025-12-03",
    "abstract": "The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "SX-GeoTree: Self-eXplaining Geospatial Regression Tree Incorporating the Spatial Similarity of Feature Attributions",
    "url": "http://arxiv.org/abs/2511.19845v1",
    "authors": [
      "Chaogui Kang",
      "Lijian Luo",
      "Qingfeng Guan",
      "Yu Liu"
    ],
    "published": "2025-11-25",
    "abstract": "Decision trees remain central for tabular prediction but struggle with (i) capturing spatial dependence and (ii) producing locally stable (robust) explanations. We present SX-GeoTree, a self-explaining geospatial regression tree that integrates three coupled objectives during recursive splitting: impurity reduction (MSE), spatial residual control (global Moran's I), and explanation robustness via modularity maximization on a consensus similarity network formed from (a) geographically weighted regression (GWR) coefficient distances (stimulus-response similarity) and (b) SHAP attribution distances (explanatory similarity). We recast local Lipschitz continuity of feature attributions as a network community preservation problem, enabling scalable enforcement of spatially coherent explanations without per-sample neighborhood searches. Experiments on two exemplar tasks (county-level GDP in Fujian, n=83; point-wise housing prices in Seattle, n=21,613) show SX-GeoTree maintains competitive predictive accuracy (within 0.01 $R^{2}$ of decision trees) while improving residual spatial evenness and doubling attribution consensus (modularity: Fujian 0.19 vs 0.09; Seattle 0.10 vs 0.05). Ablation confirms Moran's I and modularity terms are complementary; removing either degrades both spatial residual structure and explanation stability. The framework demonstrates how spatial similarity - extended beyond geometric proximity through GWR-derived local relationships - can be embedded in interpretable models, advancing trustworthy geospatial machine learning and offering a transferable template for domain-aware explainability.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Beta Distribution Learning for Reliable Roadway Crash Risk Assessment",
    "url": "http://arxiv.org/abs/2511.04886v1",
    "authors": [
      "Ahmad Elallaf",
      "Nathan Jacobs",
      "Xinyue Ye",
      "Mei Chen",
      "Gongbo Liang"
    ],
    "published": "2025-11-07",
    "abstract": "Roadway traffic accidents represent a global health crisis, responsible for over a million deaths annually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions inherent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in critical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that contribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions--a critical feature for trustworthy AI in safety-critical applications. Our model outperforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assessments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Cropland Mapping using Geospatial Embeddings",
    "url": "http://arxiv.org/abs/2511.02923v1",
    "authors": [
      "Ivan Zvonkov",
      "Gabriel Tseng",
      "Inbal Becker-Reshef",
      "Hannah Kerner"
    ],
    "published": "2025-11-04",
    "abstract": "Accurate and up-to-date land cover maps are essential for understanding land use change, a key driver of climate change. Geospatial embeddings offer a more efficient and accessible way to map landscape features, yet their use in real-world mapping applications remains underexplored. In this work, we evaluated the utility of geospatial embeddings for cropland mapping in Togo. We produced cropland maps using embeddings from Presto and AlphaEarth. Our findings show that geospatial embeddings can simplify workflows, achieve high-accuracy cropland classification and ultimately support better assessments of land use change and its climate impacts.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Predicting Household Water Consumption Using Satellite and Street View Images in Two Indian Cities",
    "url": "http://arxiv.org/abs/2510.26957v1",
    "authors": [
      "Qiao Wang",
      "Joseph George"
    ],
    "published": "2025-10-30",
    "abstract": "Monitoring household water use in rapidly urbanizing regions is hampered by costly, time-intensive enumeration methods and surveys. We investigate whether publicly available imagery-satellite tiles, Google Street View (GSV) segmentation-and simple geospatial covariates (nightlight intensity, population density) can be utilized to predict household water consumption in Hubballi-Dharwad, India. We compare four approaches: survey features (benchmark), CNN embeddings (satellite, GSV, combined), and GSV semantic maps with auxiliary data. Under an ordinal classification framework, GSV segmentation plus remote-sensing covariates achieves 0.55 accuracy for water use, approaching survey-based models (0.59 accuracy). Error analysis shows high precision at extremes of the household water consumption distribution, but confusion among middle classes is due to overlapping visual proxies. We also compare and contrast our estimates for household water consumption to that of household subjective income. Our findings demonstrate that open-access imagery, coupled with minimal geospatial data, offers a promising alternative to obtaining reliable household water consumption estimates using surveys in urban analytics.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL",
    "url": "http://arxiv.org/abs/2510.21045v2",
    "authors": [
      "Ali Khosravi Kazazi",
      "Zhenlong Li",
      "M. Naser Lessani",
      "Guido Cervone"
    ],
    "published": "2025-10-23",
    "abstract": "The complexity of Structured Query Language (SQL) and the specialized nature of geospatial functions in tools like PostGIS present significant barriers to non-experts seeking to analyze spatial data. While Large Language Models (LLMs) offer promise for translating natural language into SQL (Text-to-SQL), single-agent approaches often struggle with the semantic and syntactic complexities of spatial queries. To address this, we propose a multi-agent framework designed to accurately translate natural language questions into spatial SQL queries. The framework integrates several innovative components, including a knowledge base with programmatic schema profiling and semantic enrichment, embeddings for context retrieval, and a collaborative multi-agent pipeline as its core. This pipeline comprises specialized agents for entity extraction, metadata retrieval, query logic formulation, SQL generation, and a review agent that performs programmatic and semantic validation of the generated SQL to ensure correctness (self-verification). We evaluate our system using both the non-spatial KaggleDBQA benchmark and a new, comprehensive SpatialQueryQA benchmark that includes diverse geometry types, predicates, and three levels of query complexity. On KaggleDBQA, the system achieved an overall accuracy of 81.2% (221 out of 272 questions) after the review agent's review and corrections. For spatial queries, the system achieved an overall accuracy of 87.7% (79 out of 90 questions), compared with 76.7% without the review agent. Beyond accuracy, results also show that in some instances the system generates queries that are more semantically aligned with user intent than those in the benchmarks. This work makes spatial analysis more accessible, and provides a robust, generalizable foundation for spatial Text-to-SQL systems, advancing the development of autonomous GIS.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Neighbor-aware informal settlement mapping with graph convolutional networks",
    "url": "http://arxiv.org/abs/2509.26171v1",
    "authors": [
      "Thomas Hallopeau",
      "Joris Gu\u00e9rin",
      "Laurent Demagistri",
      "Christovam Barcellos",
      "Nadine Dessay"
    ],
    "published": "2025-09-30",
    "abstract": "Mapping informal settlements is crucial for addressing challenges related to urban planning, public health, and infrastructure in rapidly growing cities. Geospatial machine learning has emerged as a key tool for detecting and mapping these areas from remote sensing data. However, existing approaches often treat spatial units independently, neglecting the relational structure of the urban fabric. We propose a graph-based framework that explicitly incorporates local geographical context into the classification process. Each spatial unit (cell) is embedded in a graph structure along with its adjacent neighbors, and a lightweight Graph Convolutional Network (GCN) is trained to classify whether the central cell belongs to an informal settlement. Experiments are conducted on a case study in Rio de Janeiro using spatial cross-validation across five distinct zones, ensuring robustness and generalizability across heterogeneous urban landscapes. Our method outperforms standard baselines, improving Kappa coefficient by 17 points over individual cell classification. We also show that graph-based modeling surpasses simple feature concatenation of neighboring cells, demonstrating the benefit of encoding spatial structure for urban scene understanding.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision Embed2Scale Challenge (CVPR 2025)",
    "url": "http://arxiv.org/abs/2509.06993v1",
    "authors": [
      "Zirui Xu",
      "Raphael Tang",
      "Mike Bianco",
      "Qi Zhang",
      "Rishi Madhok",
      "Nikolaos Karianakis",
      "Fuxun Yu"
    ],
    "published": "2025-09-03",
    "abstract": "EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational geospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into embedding vectors that faciliatetes various downstream tasks, e.g., classification, regression, etc. In this technical report, we introduce our proposed method for the Top-1 winning solution on the Embed2Scale Challenge.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework",
    "url": "http://arxiv.org/abs/2509.01910v2",
    "authors": [
      "Furong Jia",
      "Lanxin Liu",
      "Ce Hou",
      "Fan Zhang",
      "Xinyan Liu",
      "Yu Liu"
    ],
    "published": "2025-09-02",
    "abstract": "Worldwide geo-localization involves determining the exact geographic location of images captured globally, typically guided by geographic cues such as climate, landmarks, and architectural styles. Despite advancements in geo-localization models like GeoCLIP, which leverages images and location alignment via contrastive learning for accurate predictions, the interpretability of these models remains insufficiently explored. Current concept-based interpretability methods fail to align effectively with Geo-alignment image-location embedding objectives, resulting in suboptimal interpretability and performance. To address this gap, we propose a novel framework integrating global geo-localization with concept bottlenecks. Our method inserts a Concept-Aware Alignment Module that jointly projects image and location embeddings onto a shared bank of geographic concepts (e.g., tropical climate, mountain, cathedral) and minimizes a concept-level loss, enhancing alignment in a concept-specific subspace and enabling robust interpretability. To our knowledge, this is the first work to introduce interpretability into geo-localization. Extensive experiments demonstrate that our approach surpasses GeoCLIP in geo-localization accuracy and boosts performance across diverse geospatial prediction tasks, revealing richer semantic insights into geographic decision-making processes.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "The Living Library of Trees: Mapping Knowledge Ecology in the Arnold Arboretum",
    "url": "http://arxiv.org/abs/2509.00114v1",
    "authors": [
      "Johan Malmstedt",
      "Giacomo Nanni",
      "Dario Rodighiero"
    ],
    "published": "2025-08-28",
    "abstract": "As biodiversity loss and climate change accelerate, botanical gardens serve as vital infrastructures for research, education, and conservation. This project focuses on the Arnold Arboretum of Harvard University, a 281-acre living museum founded in 1872 in Boston. Drawing on more than a century of curatorial data, the research combines historical analysis with computational methods to visualize the biographies of plants and people. The resulting platform reveals patterns of care and scientific observations, along with the collective dimensions embedded in botanical data. Using techniques from artificial intelligence, geospatial mapping, and information design, the project frames the arboretum as a system of shared agency--an active archive of more-than-human affinities that records the layered memory of curatorial labor, the situated nature of knowledge production, and the potential of design to bridge archival record and future care.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities",
    "url": "http://arxiv.org/abs/2508.19305v1",
    "authors": [
      "Chen Chu",
      "Cyrus Shahabi"
    ],
    "published": "2025-08-26",
    "abstract": "Spatial representation learning is essential for GeoAI applications such as urban analytics, enabling the encoding of shapes, locations, and spatial relationships (topological and distance-based) of geo-entities like points, polylines, and polygons. Existing methods either target a single geo-entity type or, like Poly2Vec, decompose entities into simpler components to enable Fourier transformation, introducing high computational cost. Moreover, since the transformed space lacks geometric alignment, these methods rely on uniform, non-adaptive sampling, which blurs fine-grained features like edges and boundaries. To address these limitations, we introduce Geo2Vec, a novel method inspired by signed distance fields (SDF) that operates directly in the original space. Geo2Vec adaptively samples points and encodes their signed distances (positive outside, negative inside), capturing geometry without decomposition. A neural network trained to approximate the SDF produces compact, geometry-aware, and unified representations for all geo-entity types. Additionally, we propose a rotation-invariant positional encoding to model high-frequency spatial variations and construct a structured and robust embedding space for downstream GeoAI models. Empirical results show that Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieving greater efficiency in real-world GeoAI applications. Code and Data can be found at: https://github.com/chuchen2017/GeoNeuralRepresentation.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders",
    "url": "http://arxiv.org/abs/2508.07020v1",
    "authors": [
      "Tanjim Bin Faruk",
      "Abdul Matin",
      "Shrideep Pallickara",
      "Sangmi Lee Pallickara"
    ],
    "published": "2025-08-09",
    "abstract": "Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of contiguous spectral bands, enabling fine-grained mapping of soils, crops, and land cover. While self-supervised Masked Autoencoders excel on RGB and low-band multispectral data, they struggle to exploit the intricate spatial-spectral correlations in 200+ band hyperspectral images. We introduce TerraMAE, a novel HSI encoding framework specifically designed to learn highly representative spatial-spectral embeddings for diverse geospatial analyses. TerraMAE features an adaptive channel grouping strategy, based on statistical reflectance properties to capture spectral similarities, and an enhanced reconstruction loss function that incorporates spatial and spectral quality metrics. We demonstrate TerraMAE's effectiveness through superior spatial-spectral information preservation in high-fidelity image reconstruction. Furthermore, we validate its practical utility and the quality of its learned representations through strong performance on three key downstream geospatial tasks: crop identification, land cover classification, and soil texture prediction.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution",
    "url": "http://arxiv.org/abs/2508.06584v1",
    "authors": [
      "Kalana Wijegunarathna",
      "Kristin Stock",
      "Christopher B. Jones"
    ],
    "published": "2025-08-08",
    "abstract": "The development, integration, and maintenance of geospatial databases rely heavily on efficient and accurate matching procedures of Geospatial Entity Resolution (ER). While resolution of points-of-interest (POIs) has been widely addressed, resolution of entities with diverse geometries has been largely overlooked. This is partly due to the lack of a uniform technique for embedding heterogeneous geometries seamlessly into a neural network framework. Existing neural approaches simplify complex geometries to a single point, resulting in significant loss of spatial information. To address this limitation, we propose Omni, a geospatial ER model featuring an omni-geometry encoder. This encoder is capable of embedding point, line, polyline, polygon, and multi-polygon geometries, enabling the model to capture the complex geospatial intricacies of the places being compared. Furthermore, Omni leverages transformer-based pre-trained language models over individual textual attributes of place records in an Attribute Affinity mechanism. The model is rigorously tested on existing point-only datasets and a new diverse-geometry geospatial ER dataset. Omni produces up to 12% (F1) improvement over existing methods.\n  Furthermore, we test the potential of Large Language Models (LLMs) to conduct geospatial ER, experimenting with prompting strategies and learning scenarios, comparing the results of pre-trained language model-based methods with LLMs. Results indicate that LLMs show competitive results.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data",
    "url": "http://arxiv.org/abs/2507.22291v2",
    "authors": [
      "Christopher F. Brown",
      "Michal R. Kazmierski",
      "Valerie J. Pasquarella",
      "William J. Rucklidge",
      "Masha Samsikova",
      "Chenhui Zhang",
      "Evan Shelhamer",
      "Estefania Lahera",
      "Olivia Wiles",
      "Simon Ilyushchenko",
      "Noel Gorelick",
      "Lihui Lydia Zhang",
      "Sophia Alj",
      "Emily Schechter",
      "Sean Askay",
      "Oliver Guinan",
      "Rebecca Moore",
      "Alexis Boukouvalas",
      "Pushmeet Kohli"
    ],
    "published": "2025-07-29",
    "abstract": "Unprecedented volumes of Earth observation data are continually collected around the world, but high-quality labels remain scarce given the effort required to make physical measurements and observations. This has led to considerable investment in bespoke modeling efforts translating sparse labels into maps. Here we introduce AlphaEarth Foundations, an embedding field model yielding a highly general, geospatial representation that assimilates spatial, temporal, and measurement contexts across multiple sources, enabling accurate and efficient production of maps and monitoring systems from local to global scales. The embeddings generated by AlphaEarth Foundations are the only to consistently outperform a suite of other well-known/widely accepted featurization approaches tested on a diverse set of mapping evaluations without re-training. We have released a dataset of global, annual, analysis-ready embedding field layers from 2017 through 2024.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation",
    "url": "http://arxiv.org/abs/2507.12857v2",
    "authors": [
      "Shiqi Huang",
      "Shuting He",
      "Huaiyuan Qin",
      "Bihan Wen"
    ],
    "published": "2025-07-17",
    "abstract": "Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$ ($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary $\\textbf{RE}$mote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at https://github.com/HuangShiqi128/SCORE.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "IMAIA: Interactive Maps AI Assistant for Travel Planning and Geo-Spatial Intelligence",
    "url": "http://arxiv.org/abs/2507.06993v3",
    "authors": [
      "Jieren Deng",
      "Zhizhang Hu",
      "Ziyan He",
      "Aleksandar Cvetkovic",
      "Pak Kiu Chung",
      "Dragomir Yankov",
      "Chiqun Zhang"
    ],
    "published": "2025-07-09",
    "abstract": "Map applications are still largely point-and-click, making it difficult to ask map-centric questions or connect what a camera sees to the surrounding geospatial context with view-conditioned inputs. We introduce IMAIA, an interactive Maps AI Assistant that enables natural-language interaction with both vector (street) maps and satellite imagery, and augments camera inputs with geospatial intelligence to help users understand the world. IMAIA comprises two complementary components. Maps Plus treats the map as first-class context by parsing tiled vector/satellite views into a grid-aligned representation that a language model can query to resolve deictic references (e.g., ``the flower-shaped building next to the park in the top-right''). Places AI Smart Assistant (PAISA) performs camera-aware place understanding by fusing image--place embeddings with geospatial signals (location, heading, proximity) to ground a scene, surface salient attributes, and generate concise explanations. A lightweight multi-agent design keeps latency low and exposes interpretable intermediate decisions. Across map-centric QA and camera-to-place grounding tasks, IMAIA improves accuracy and responsiveness over strong baselines while remaining practical for user-facing deployments. By unifying language, maps, and geospatial cues, IMAIA moves beyond scripted tools toward conversational mapping that is both spatially grounded and broadly usable.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TrajSceneLLM: A Multimodal Perspective on Semantic GPS Trajectory Analysis",
    "url": "http://arxiv.org/abs/2506.16401v1",
    "authors": [
      "Chunhou Ji",
      "Qiumeng Li"
    ],
    "published": "2025-06-19",
    "abstract": "GPS trajectory data reveals valuable patterns of human mobility and urban dynamics, supporting a variety of spatial applications. However, traditional methods often struggle to extract deep semantic representations and incorporate contextual map information. We propose TrajSceneLLM, a multimodal perspective for enhancing semantic understanding of GPS trajectories. The framework integrates visualized map images (encoding spatial context) and textual descriptions generated through LLM reasoning (capturing temporal sequences and movement dynamics). Separate embeddings are generated for each modality and then concatenated to produce trajectory scene embeddings with rich semantic content which are further paired with a simple MLP classifier. We validate the proposed framework on Travel Mode Identification (TMI), a critical task for analyzing travel choices and understanding mobility behavior. Our experiments show that these embeddings achieve significant performance improvement, highlighting the advantage of our LLM-driven method in capturing deep spatio-temporal dependencies and reducing reliance on handcrafted features. This semantic enhancement promises significant potential for diverse downstream applications and future research in geospatial artificial intelligence. The source code and dataset are publicly available at: https://github.com/februarysea/TrajSceneLLM.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity",
    "url": "http://arxiv.org/abs/2506.08051v1",
    "authors": [
      "Mahmuda Sultana Mimi",
      "Md Monzurul Islam",
      "Anannya Ghosh Tusti",
      "Shriyank Somvanshi",
      "Subasish Das"
    ],
    "published": "2025-06-09",
    "abstract": "Understanding the spatial and temporal dynamics of automated vehicle (AV) crash severity is critical for advancing urban mobility safety and infrastructure planning. In this work, we introduce ST-GraphNet, a spatio-temporal graph neural network framework designed to model and predict AV crash severity by using both fine-grained and region-aggregated spatial graphs. Using a balanced dataset of 2,352 real-world AV-related crash reports from Texas (2024), including geospatial coordinates, crash timestamps, SAE automation levels, and narrative descriptions, we construct two complementary graph representations: (1) a fine-grained graph with individual crash events as nodes, where edges are defined via spatio-temporal proximity; and (2) a coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical Spatial Indexing (H3)-based spatial cells, connected through hexagonal adjacency. Each node in the graph is enriched with multimodal data, including semantic, spatial, and temporal attributes, including textual embeddings from crash narratives using a pretrained Sentence-BERT model. We evaluate various graph neural network (GNN) architectures, such as Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN (DSTGCN), to classify crash severity and predict high-risk regions. Our proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3 graph, achieves a test accuracy of 97.74\\%, substantially outperforming the best fine-grained model (64.7\\% test accuracy). These findings highlight the effectiveness of spatial aggregation, dynamic message passing, and multi-modal feature integration in capturing the complex spatio-temporal patterns underlying AV crash severity.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery",
    "url": "http://arxiv.org/abs/2506.03388v1",
    "authors": [
      "Pengyu Chen",
      "Xiao Huang",
      "Teng Fei",
      "Sicheng Wang"
    ],
    "published": "2025-06-03",
    "abstract": "Environmental soundscapes convey substantial ecological and social information regarding urban environments; however, their potential remains largely untapped in large-scale geographic analysis. In this study, we investigate the extent to which urban sounds correspond with visual scenes by comparing various visual representation strategies in capturing acoustic semantics. We employ a multimodal approach that integrates geo-referenced sound recordings with both street-level and remote sensing imagery across three major global cities: London, New York, and Tokyo. Utilizing the AST model for audio, along with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV for semantic segmentation, we extract embeddings and class-level features to evaluate cross-modal similarity. The results indicate that street view embeddings demonstrate stronger alignment with environmental sounds compared to segmentation outputs, whereas remote sensing segmentation is more effective in interpreting ecological categories through a Biophony--Geophony--Anthrophony (BGA) framework. These findings imply that embedding-based models offer superior semantic alignment, while segmentation-based methods provide interpretable links between visual structure and acoustic ecology. This work advances the burgeoning field of multimodal urban sensing by offering novel perspectives for incorporating sound into geospatial analysis.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings",
    "url": "http://arxiv.org/abs/2506.02868v1",
    "authors": [
      "Amal S. Perera",
      "David Fernandez",
      "Chandi Witharana",
      "Elias Manos",
      "Michael Pimenta",
      "Anna K. Liljedahl",
      "Ingmar Nitze",
      "Yili Yang",
      "Todd Nicholson",
      "Chia-Yu Hsu",
      "Wenwen Li",
      "Guido Grosse"
    ],
    "published": "2025-06-03",
    "abstract": "Accurate mapping of permafrost landforms, thaw disturbances, and human-built infrastructure at pan-Arctic scale using sub-meter satellite imagery is increasingly critical. Handling petabyte-scale image data requires high-performance computing and robust feature detection models. While convolutional neural network (CNN)-based deep learning approaches are widely used for remote sensing (RS),similar to the success in transformer based large language models, Vision Transformers (ViTs) offer advantages in capturing long-range dependencies and global context via attention mechanisms. ViTs support pretraining via self-supervised learning-addressing the common limitation of labeled data in Arctic feature detection and outperform CNNs on benchmark datasets. Arctic also poses challenges for model generalization, especially when features with the same semantic class exhibit diverse spectral characteristics. To address these issues for Arctic feature detection, we integrate geospatial location embeddings into ViTs to improve adaptation across regions. This work investigates: (1) the suitability of pre-trained ViTs as feature extractors for high-resolution Arctic remote sensing tasks, and (2) the benefit of combining image and location embeddings. Using previously published datasets for Arctic feature detection, we evaluate our models on three tasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and human-built infrastructure. We empirically explore multiple configurations to fuse image embeddings and location embeddings. Results show that ViTs with location embeddings outperform prior CNN-based models on two of the three tasks including F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating the potential of transformer-based models with spatial awareness for Arctic RS applications.",
    "categories": [
      "foundation_model",
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale",
    "url": "http://arxiv.org/abs/2506.01297v3",
    "authors": [
      "Ya Wen",
      "Jixuan Cai",
      "Qiyao Ma",
      "Linyan Li",
      "Xinhua Chen",
      "Chris Webster",
      "Yulun Zhou"
    ],
    "published": "2025-06-02",
    "abstract": "Representation learning of geospatial locations remains a core challenge in achieving general geospatial intelligence. Current embedding methods often lack versatility, limiting their utility across diverse tasks in both human and natural domains. We present MobCLIP, the first nationwide general-purpose location encoder, integrating an unprecedented diversity of data modalities through effective and scalable multimodal fusion. Adopting a novel CLIP-based architecture, our framework aligns 100M+ POIs, nationwide remote sensing imagery, and structured demographic statistics with a billion-edge mobility graph. By tokenizing spatial locations into grid cells inspired by Vision Transformers, we establish a unified representation space bridging mobility patterns and multimodal features. To rigorously evaluate the general-purpose effectiveness of MobCLIP, we construct a benchmark dataset composed of 11 downstream prediction tasks across social, economic, and natural domains. Experiments show that MobCLIP, with four input modalities and a compact 128-dimensional representation space, achieves significantly superior general-purpose predictive performances than state-of-the-art models by an average of 35%. Thanks to the effective integration of human-centric modalities, the performance gain is particularly profound in human-centric tasks, such as energy consumption (+260%), offline retail consumption amount (+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we further demonstrate the scaling behavior in geospatial representation learning. We open-source code and pretrained models at: https://github.com/ylzhouchris/MobCLIP.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "Transformer",
      "LLM",
      "CLIP"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series",
    "url": "http://arxiv.org/abs/2506.14786v1",
    "authors": [
      "Haobo Li",
      "Eunseo Jung",
      "Zixin Chen",
      "Zhaowei Wang",
      "Yueya Wang",
      "Huamin Qu",
      "Alexis Kai Hon Lau"
    ],
    "published": "2025-05-27",
    "abstract": "Multimodal time series forecasting is foundational in various fields, such as utilizing satellite imagery and numerical data for predicting typhoons in climate science. However, existing multimodal approaches primarily focus on utilizing text data to help time series forecasting, leaving the visual data in existing time series datasets untouched. Furthermore, it is challenging for models to effectively capture the physical information embedded in visual data, such as satellite imagery's temporal and geospatial context, which extends beyond images themselves. To address this gap, we propose physics-informed positional encoding (PIPE), a lightweight method that embeds physical information into vision language models (VLMs). PIPE introduces two key innovations: (1) a physics-informed positional indexing scheme for mapping physics to positional IDs, and (2) a variant-frequency positional encoding mechanism for encoding frequency information of physical variables and sequential order of tokens within the embedding space. By preserving both the physical information and sequential order information, PIPE significantly improves multimodal alignment and forecasting accuracy. Through the experiments on the most representative and the largest open-sourced satellite image dataset, PIPE achieves state-of-the-art performance in both deep learning forecasting and climate domain methods, demonstrating superiority across benchmarks, including a 12% improvement in typhoon intensity forecasting over prior works. Our code is provided in the supplementary material.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LVM",
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Less is More: Multimodal Region Representation via Pairwise Inter-view Learning",
    "url": "http://arxiv.org/abs/2505.18178v1",
    "authors": [
      "Min Namgung",
      "Yijun Lin",
      "JangHyeon Lee",
      "Yao-Yi Chiang"
    ],
    "published": "2025-05-15",
    "abstract": "With the increasing availability of geospatial datasets, researchers have explored region representation learning (RRL) to analyze complex region characteristics. Recent RRL methods use contrastive learning (CL) to capture shared information between two modalities but often overlook task-relevant unique information specific to each modality. Such modality-specific details can explain region characteristics that shared information alone cannot capture. Bringing information factorization to RRL can address this by factorizing multimodal data into shared and unique information. However, existing factorization approaches focus on two modalities, whereas RRL can benefit from various geospatial data. Extending factorization beyond two modalities is non-trivial because modeling high-order relationships introduces a combinatorial number of learning objectives, increasing model complexity. We introduce Cross modal Knowledge Injected Embedding, an information factorization approach for RRL that captures both shared and unique representations. CooKIE uses a pairwise inter-view learning approach that captures high-order information without modeling high-order dependency, avoiding exhaustive combinations. We evaluate CooKIE on three regression tasks and a land use classification task in New York City and Delhi, India. Results show that CooKIE outperforms existing RRL methods and a factorized RRL model, capturing multimodal information with fewer training parameters and floating-point operations per second (FLOPs). We release the code: https://github.com/MinNamgung/CooKIE.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model",
    "url": "http://arxiv.org/abs/2504.09644v1",
    "authors": [
      "Kaiyu Li",
      "Zepeng Xin",
      "Li Pang",
      "Chao Pang",
      "Yupeng Deng",
      "Jing Yao",
      "Guisong Xia",
      "Deyu Meng",
      "Zhi Wang",
      "Xiangyong Cao"
    ],
    "published": "2025-04-13",
    "abstract": "Remote sensing has become critical for understanding environmental dynamics, urban planning, and disaster management. However, traditional remote sensing workflows often rely on explicit segmentation or detection methods, which struggle to handle complex, implicit queries that require reasoning over spatial context, domain knowledge, and implicit user intent. Motivated by this, we introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit querying and reasoning and generates the mask of the target region. To advance this task, we construct and release the first large-scale benchmark dataset called EarthReason, which comprises 5,434 manually annotated image masks with over 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a simple yet effective language-guided segmentation baseline that integrates a hierarchical visual encoder, a large language model (LLM) for instruction parsing, and a tailored mask generator for spatial correlation. The design of SegEarth-R1 incorporates domain-specific adaptations, including aggressive visual token compression to handle ultra-high-resolution remote sensing images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. Extensive experiments demonstrate that SegEarth-R1 achieves state-of-the-art performance on both reasoning and referring segmentation tasks, significantly outperforming traditional and LLM-based segmentation methods. Our data and code will be released at https://github.com/earth-insights/SegEarth-R1.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "S2Vec: Self-Supervised Geospatial Embeddings",
    "url": "http://arxiv.org/abs/2504.16942v1",
    "authors": [
      "Shushman Choudhury",
      "Elad Aharoni",
      "Chandrakumari Suvarna",
      "Iveel Tsogsuren",
      "Abdul Rahman Kreidieh",
      "Chun-Ta Lu",
      "Neha Arora"
    ],
    "published": "2025-04-10",
    "abstract": "Scalable general-purpose representations of the built environment are crucial for geospatial artificial intelligence applications. This paper introduces S2Vec, a novel self-supervised framework for learning such geospatial embeddings. S2Vec uses the S2 Geometry library to partition large areas into discrete S2 cells, rasterizes built environment feature vectors within cells as images, and applies masked autoencoding on these rasterized images to encode the feature vectors. This approach yields task-agnostic embeddings that capture local feature characteristics and broader spatial relationships. We evaluate S2Vec on three large-scale socioeconomic prediction tasks, showing its competitive performance against state-of-the-art image-based embeddings. We also explore the benefits of combining S2Vec embeddings with image-based embeddings downstream, showing that such multimodal fusion can often improve performance. Our results highlight how S2Vec can learn effective general-purpose geospatial representations and how it can complement other data modalities in geospatial artificial intelligence.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MapQA: Open-domain Geospatial Question Answering on Map Data",
    "url": "http://arxiv.org/abs/2503.07871v1",
    "authors": [
      "Zekun Li",
      "Malcolm Grossman",
      "Eric",
      "Qasemi",
      "Mihir Kulkarni",
      "Muhao Chen",
      "Yao-Yi Chiang"
    ],
    "published": "2025-03-10",
    "abstract": "Geospatial question answering (QA) is a fundamental task in navigation and point of interest (POI) searches. While existing geospatial QA datasets exist, they are limited in both scale and diversity, often relying solely on textual descriptions of geo-entities without considering their geometries. A major challenge in scaling geospatial QA datasets for reasoning lies in the complexity of geospatial relationships, which require integrating spatial structures, topological dependencies, and multi-hop reasoning capabilities that most text-based QA datasets lack. To address these limitations, we introduce MapQA, a novel dataset that not only provides question-answer pairs but also includes the geometries of geo-entities referenced in the questions. MapQA is constructed using SQL query templates to extract question-answer pairs from OpenStreetMap (OSM) for two study regions: Southern California and Illinois. It consists of 3,154 QA pairs spanning nine question types that require geospatial reasoning, such as neighborhood inference and geo-entity type identification. Compared to existing datasets, MapQA expands both the number and diversity of geospatial question types. We explore two approaches to tackle this challenge: (1) a retrieval-based language model that ranks candidate geo-entities by embedding similarity, and (2) a large language model (LLM) that generates SQL queries from natural language questions and geo-entity attributes, which are then executed against an OSM database. Our findings indicate that retrieval-based methods effectively capture concepts like closeness and direction but struggle with questions that require explicit computations (e.g., distance calculations). LLMs (e.g., GPT and Gemini) excel at generating SQL queries for one-hop reasoning but face challenges with multi-hop reasoning, highlighting a key bottleneck in advancing geospatial QA systems.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Divide and Conquer Self-Supervised Learning for High-Content Imaging",
    "url": "http://arxiv.org/abs/2503.07444v1",
    "authors": [
      "Lucas Farndale",
      "Paul Henderson",
      "Edward W Roberts",
      "Ke Yuan"
    ],
    "published": "2025-03-10",
    "abstract": "Self-supervised representation learning methods often fail to learn subtle or complex features, which can be dominated by simpler patterns which are much easier to learn. This limitation is particularly problematic in applications to science and engineering, as complex features can be critical for discovery and analysis. To address this, we introduce Split Component Embedding Registration (SpliCER), a novel architecture which splits the image into sections and distils information from each section to guide the model to learn more subtle and complex features without compromising on simpler features. SpliCER is compatible with any self-supervised loss function and can be integrated into existing methods without modification. The primary contributions of this work are as follows: i) we demonstrate that existing self-supervised methods can learn shortcut solutions when simple and complex features are both present; ii) we introduce a novel self-supervised training method, SpliCER, to overcome the limitations of existing methods, and achieve significant downstream performance improvements; iii) we demonstrate the effectiveness of SpliCER in cutting-edge medical and geospatial imaging settings. SpliCER offers a powerful new tool for representation learning, enabling models to uncover complex features which could be overlooked by other methods.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings",
    "url": "http://arxiv.org/abs/2502.19781v2",
    "authors": [
      "Aayush Dhakal",
      "Srikumar Sastry",
      "Subash Khanal",
      "Adeel Ahmad",
      "Eric Xing",
      "Nathan Jacobs"
    ],
    "published": "2025-02-27",
    "abstract": "The choice of representation for geographic location significantly impacts the accuracy of models for a broad range of geospatial tasks, including fine-grained species classification, population density estimation, and biome classification. Recent works like SatCLIP and GeoCLIP learn such representations by contrastively aligning geolocation with co-located images. While these methods work exceptionally well, in this paper, we posit that the current training strategies fail to fully capture the important visual features. We provide an information-theoretic perspective on why the resulting embeddings from these methods discard crucial visual information that is important for many downstream tasks. To solve this problem, we propose a novel retrieval-augmented strategy called RANGE. We build our method on the intuition that the visual features of a location can be estimated by combining the visual features from multiple similar-looking locations. We evaluate our method across a wide variety of tasks. Our results show that RANGE outperforms the existing state-of-the-art models with significant margins in most tasks. We show gains of up to 13.1% on classification tasks and 0.145 $R^2$ on regression tasks. All our code and models will be made available at: https://github.com/mvrl/RANGE.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in Multimodal Geospatial Learning",
    "url": "http://arxiv.org/abs/2503.05774v1",
    "authors": [
      "Theodor Lundqvist",
      "Ludvig Delvret"
    ],
    "published": "2025-02-25",
    "abstract": "Existing methods for self-supervised representation learning of geospatial regions and map entities rely extensively on the design of pretext tasks, often involving augmentations or heuristic sampling of positive and negative pairs based on spatial proximity. This reliance introduces biases and limits the representations' expressiveness and generalisability. Consequently, the literature has expressed a pressing need to explore different methods for modelling geospatial data. To address the key difficulties of such methods, namely multimodality, heterogeneity, and the choice of pretext tasks, we present GeoJEPA, a versatile multimodal fusion model for geospatial data built on the self-supervised Joint-Embedding Predictive Architecture. With GeoJEPA, we aim to eliminate the widely accepted augmentation- and sampling biases found in self-supervised geospatial representation learning. GeoJEPA uses self-supervised pretraining on a large dataset of OpenStreetMap attributes, geometries and aerial images. The results are multimodal semantic representations of urban regions and map entities that we evaluate both quantitatively and qualitatively. Through this work, we uncover several key insights into JEPA's ability to handle multimodal data.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "JEPA"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Novel Scene Coupling Semantic Mask Network for Remote Sensing Image Segmentation",
    "url": "http://arxiv.org/abs/2501.13130v1",
    "authors": [
      "Xiaowen Ma",
      "Rongrong Lian",
      "Zhenkai Wu",
      "Renxiang Guan",
      "Tingfeng Hong",
      "Mengjiao Zhao",
      "Mengting Ma",
      "Jiangtao Nie",
      "Zhenhong Du",
      "Siyang Song",
      "Wei Zhang"
    ],
    "published": "2025-01-22",
    "abstract": "As a common method in the field of computer vision, spatial attention mechanism has been widely used in semantic segmentation of remote sensing images due to its outstanding long-range dependency modeling capability. However, remote sensing images are usually characterized by complex backgrounds and large intra-class variance that would degrade their analysis performance. While vanilla spatial attention mechanisms are based on dense affine operations, they tend to introduce a large amount of background contextual information and lack of consideration for intrinsic spatial correlation. To deal with such limitations, this paper proposes a novel scene-Coupling semantic mask network, which reconstructs the vanilla attention with scene coupling and local global semantic masks strategies. Specifically, scene coupling module decomposes scene information into global representations and object distributions, which are then embedded in the attention affinity processes. This Strategy effectively utilizes the intrinsic spatial correlation between features so that improve the process of attention modeling. Meanwhile, local global semantic masks module indirectly correlate pixels with the global semantic masks by using the local semantic mask as an intermediate sensory element, which reduces the background contextual interference and mitigates the effect of intra-class variance. By combining the above two strategies, we propose the model SCSM, which not only can efficiently segment various geospatial objects in complex scenarios, but also possesses inter-clean and elegant mathematical representations. Experimental results on four benchmark datasets demonstrate the the effectiveness of the above two strategies for improving the attention modeling of remote sensing images. The dataset and code are available at https://github.com/xwmaxwma/rssegmentation",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities",
    "url": "http://arxiv.org/abs/2412.14123v3",
    "authors": [
      "Guillaume Astruc",
      "Nicolas Gonthier",
      "Clement Mallet",
      "Loic Landrieu"
    ],
    "published": "2024-12-18",
    "abstract": "Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and scale-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of 5 multimodal datasets with varying characteristics and $11$ distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned or probed, we reach state-of-the-art results on the test sets of GeoPlex and for 6 external datasets across various environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, climate type classification, and segmentation of flood, burn scar, and deforestation. The code and models are available at https://github.com/gastruc/AnySat.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "JEPA"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Global and Dense Embeddings of Earth: Major TOM Floating in the Latent Space",
    "url": "http://arxiv.org/abs/2412.05600v1",
    "authors": [
      "Mikolaj Czerkawski",
      "Marcin Kluczek",
      "J\u0119drzej S. Bojanowski"
    ],
    "published": "2024-12-07",
    "abstract": "With the ever-increasing volumes of the Earth observation data present in the archives of large programmes such as Copernicus, there is a growing need for efficient vector representations of the underlying raw data. The approach of extracting feature representations from pretrained deep neural networks is a powerful approach that can provide semantic abstractions of the input data. However, the way this is done for imagery archives containing geospatial data has not yet been defined. In this work, an extension is proposed to an existing community project, Major TOM, focused on the provision and standardization of open and free AI-ready datasets for Earth observation. Furthermore, four global and dense embedding datasets are released openly and for free along with the publication of this manuscript, resulting in the most comprehensive global open dataset of geospatial visual embeddings in terms of covered Earth's surface.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Advancing Large Language Models for Spatiotemporal and Semantic Association Mining of Similar Environmental Events",
    "url": "http://arxiv.org/abs/2411.12880v1",
    "authors": [
      "Yuanyuan Tian",
      "Wenwen Li",
      "Lei Hu",
      "Xiao Chen",
      "Michael Brook",
      "Michael Brubaker",
      "Fan Zhang",
      "Anna K. Liljedahl"
    ],
    "published": "2024-11-19",
    "abstract": "Retrieval and recommendation are two essential tasks in modern search tools. This paper introduces a novel retrieval-reranking framework leveraging Large Language Models (LLMs) to enhance the spatiotemporal and semantic associated mining and recommendation of relevant unusual climate and environmental events described in news articles and web posts. This framework uses advanced natural language processing techniques to address the limitations of traditional manual curation methods in terms of high labor cost and lack of scalability. Specifically, we explore an optimized solution to employ cutting-edge embedding models for semantically analyzing spatiotemporal events (news) and propose a Geo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria including spatial proximity, temporal association, semantic similarity, and category-instructed similarity to rank and identify similar spatiotemporal events. We apply the proposed framework to a dataset of four thousand Local Environmental Observer (LEO) Network events, achieving top performance in recommending similar events among multiple cutting-edge dense retrieval models. The search and recommendation pipeline can be applied to a wide range of similar data search tasks dealing with geospatial and temporal data. We hope that by linking relevant events, we can better aid the general public to gain an enhanced understanding of climate change and its impact on different communities.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Pattern Integration and Enhancement Vision Transformer for Self-Supervised Learning in Remote Sensing",
    "url": "http://arxiv.org/abs/2411.06091v1",
    "authors": [
      "Kaixuan Lu",
      "Ruiqian Zhang",
      "Xiao Huang",
      "Yuxing Xie",
      "Xiaogang Ning",
      "Hanchao Zhang",
      "Mengke Yuan",
      "Pan Zhang",
      "Tao Wang",
      "Tongkui Liao"
    ],
    "published": "2024-11-09",
    "abstract": "Recent self-supervised learning (SSL) methods have demonstrated impressive results in learning visual representations from unlabeled remote sensing images. However, most remote sensing images predominantly consist of scenographic scenes containing multiple ground objects without explicit foreground targets, which limits the performance of existing SSL methods that focus on foreground targets. This raises the question: Is there a method that can automatically aggregate similar objects within scenographic remote sensing images, thereby enabling models to differentiate knowledge embedded in various geospatial patterns for improved feature representation? In this work, we present the Pattern Integration and Enhancement Vision Transformer (PIEViT), a novel self-supervised learning framework designed specifically for remote sensing imagery. PIEViT utilizes a teacher-student architecture to address both image-level and patch-level tasks. It employs the Geospatial Pattern Cohesion (GPC) module to explore the natural clustering of patches, enhancing the differentiation of individual features. The Feature Integration Projection (FIP) module further refines masked token reconstruction using geospatially clustered patches. We validated PIEViT across multiple downstream tasks, including object detection, semantic segmentation, and change detection. Experiments demonstrated that PIEViT enhances the representation of internal patch features, providing significant improvements over existing self-supervised baselines. It achieves excellent results in object detection, land cover classification, and change detection, underscoring its robustness, generalization, and transferability for remote sensing image interpretation tasks.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Geometric Feature Enhanced Knowledge Graph Embedding and Spatial Reasoning",
    "url": "http://arxiv.org/abs/2410.18345v1",
    "authors": [
      "Lei Hu",
      "Wenwen Li",
      "Yunqiang Zhu"
    ],
    "published": "2024-10-24",
    "abstract": "Geospatial Knowledge Graphs (GeoKGs) model geoentities (e.g., places and natural features) and spatial relationships in an interconnected manner, providing strong knowledge support for geographic applications, including data retrieval, question-answering, and spatial reasoning. However, existing methods for mining and reasoning from GeoKGs, such as popular knowledge graph embedding (KGE) techniques, lack geographic awareness. This study aims to enhance general-purpose KGE by developing new strategies and integrating geometric features of spatial relations, including topology, direction, and distance, to infuse the embedding process with geographic intuition. The new model is tested on downstream link prediction tasks, and the results show that the inclusion of geometric features, particularly topology and direction, improves prediction accuracy for both geoentities and spatial relations. Our research offers new perspectives for integrating spatial concepts and principles into the GeoKG mining process, providing customized GeoAI solutions for geospatial challenges.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Identification of Anomalous Geospatial Trajectories via Persistent Homology",
    "url": "http://arxiv.org/abs/2410.03889v1",
    "authors": [
      "Kyle Evans-Lee",
      "Kevin Lamb"
    ],
    "published": "2024-10-04",
    "abstract": "We present a novel method for analyzing geospatial trajectory data using topological data analysis (TDA) to identify a specific class of anomalies, commonly referred to as crop circles, in AIS data. This approach is the first of its kind to be applied to spatiotemporal data. By embedding $2+1$-dimensional spatiotemporal data into $\\mathbb{R}^3$, we utilize persistent homology to detect loops within the trajectories in $\\mathbb{R}^2$. Our research reveals that, under normal conditions, trajectory data embedded in $\\mathbb{R}^3$ over time do not form loops. Consequently, we can effectively identify anomalies characterized by the presence of loops within the trajectories. This method is robust and capable of detecting loops that are invariant to small perturbations, variations in geometric shape, and local coordinate projections. Additionally, our approach provides a novel perspective on anomaly detection, offering enhanced sensitivity and specificity in identifying atypical patterns in geospatial data. This approach has significant implications for various applications, including maritime navigation, environmental monitoring, and surveillance.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large Language Model Empowered Graph-based Learning Approach",
    "url": "http://arxiv.org/abs/2408.17258v3",
    "authors": [
      "Tong Nie",
      "Junlin He",
      "Yuewen Mei",
      "Guoyang Qin",
      "Guilong Li",
      "Jian Sun",
      "Wei Ma"
    ],
    "published": "2024-08-30",
    "abstract": "The proliferation of e-commerce and urbanization has significantly intensified delivery operations in urban areas, boosting the volume and complexity of delivery demand. Data-driven predictive methods, especially those utilizing machine learning techniques, have emerged to handle these complexities in urban delivery demand management problems. One particularly pressing issue that has yet to be sufficiently addressed is the joint estimation and prediction of city-wide delivery demand, as well as the generalization of the model to new cities. To this end, we formulate this problem as a transferable graph-based spatiotemporal learning task. First, an individual-collective message-passing neural network model is formalized to capture the interaction between demand patterns of associated regions. Second, by exploiting recent advances in large language models (LLMs), we extract general geospatial knowledge encodings from the unstructured locational data using the embedding generated by LLMs. Last, to encourage the cross-city generalization of the model, we integrate the encoding into the demand predictor in a transferable way. Comprehensive empirical evaluation results on two real-world delivery datasets, including eight cities in China and the US, demonstrate that our model significantly outperforms state-of-the-art baselines in accuracy, efficiency, and transferability.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Urban context and delivery performance: Modelling service time for cargo bikes and vans across diverse urban environments",
    "url": "http://arxiv.org/abs/2409.06730v1",
    "authors": [
      "Maxwell Schrader",
      "Navish Kumar",
      "Esben S\u00f8rig",
      "Soonmyeong Yoon",
      "Akash Srivastava",
      "Kai Xu",
      "Maria Astefanoaei",
      "Nicolas Collignon"
    ],
    "published": "2024-08-27",
    "abstract": "Light goods vehicles (LGV) used extensively in the last mile of delivery are one of the leading polluters in cities. Cargo-bike logistics and Light Electric Vehicles (LEVs) have been put forward as a high impact candidate for replacing LGVs. Studies have estimated over half of urban van deliveries being replaceable by cargo-bikes, due to their faster speeds, shorter parking times and more efficient routes across cities. However, the logistics sector suffers from a lack of publicly available data, particularly pertaining to cargo-bike deliveries, thus limiting the understanding of their potential benefits. Specifically, service time (which includes cruising for parking, and walking to destination) is a major, but often overlooked component of delivery time modelling. The aim of this study is to establish a framework for measuring the performance of delivery vehicles, with an initial focus on modelling service times of vans and cargo-bikes across diverse urban environments. We introduce two datasets that allow for in-depth analysis and modelling of service times of cargo bikes and use existing datasets to reason about differences in delivery performance across vehicle types. We introduce a modelling framework to predict the service times of deliveries based on urban context. We employ Uber's H3 index to divide cities into hexagonal cells and aggregate OpenStreetMap tags for each cell, providing a detailed assessment of urban context. Leveraging this spatial grid, we use GeoVex to represent micro-regions as points in a continuous vector space, which then serve as input for predicting vehicle service times. We show that geospatial embeddings can effectively capture urban contexts and facilitate generalizations to new contexts and cities. Our methodology addresses the challenge of limited comparative data available for different vehicle types within the same urban settings.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoReasoner: Reasoning On Geospatially Grounded Context For Natural Language Understanding",
    "url": "http://arxiv.org/abs/2408.11366v1",
    "authors": [
      "Yibo Yan",
      "Joey Lee"
    ],
    "published": "2024-08-21",
    "abstract": "In human reading and communication, individuals tend to engage in geospatial reasoning, which involves recognizing geographic entities and making informed inferences about their interrelationships. To mimic such cognitive process, current methods either utilize conventional natural language understanding toolkits, or directly apply models pretrained on geo-related natural language corpora. However, these methods face two significant challenges: i) they do not generalize well to unseen geospatial scenarios, and ii) they overlook the importance of integrating geospatial context from geographical databases with linguistic information from the Internet. To handle these challenges, we propose GeoReasoner, a language model capable of reasoning on geospatially grounded natural language. Specifically, it first leverages Large Language Models (LLMs) to generate a comprehensive location description based on linguistic and geospatial information. It also encodes direction and distance information into spatial embedding via treating them as pseudo-sentences. Consequently, the model is trained on both anchor-level and neighbor-level inputs to learn geo-entity representation. Extensive experimental results demonstrate GeoReasoner's superiority in three tasks: toponym recognition, toponym linking, and geo-entity typing, compared to the state-of-the-art baselines.",
    "categories": [
      "foundation_model",
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "A Unified Framework for Next-Gen Urban Forecasting via LLM-driven Dependency Retrieval and GeoTransformer",
    "url": "http://arxiv.org/abs/2408.08852v4",
    "authors": [
      "Yuhao Jia",
      "Zile Wu",
      "Shengao Yi",
      "Yifei Sun",
      "Xiao Huang"
    ],
    "published": "2024-08-16",
    "abstract": "Urban forecasting has increasingly benefited from high-dimensional spatial data through two primary approaches: graph-based methods that rely on predefined spatial structures, and region-based methods that focus on learning expressive urban representations. Although these methods have laid a strong foundation, they either rely heavily on structured spatial data, struggle to adapt to task-specific dependencies, or fail to integrate holistic urban context. Moreover, no existing framework systematically integrates these two paradigms and overcomes their respective limitations. To address this gap, we propose a novel, unified framework for high-dimensional urban forecasting, composed of three key components: (1) the Urban Region Representation Module that organizes latent embeddings and semantic descriptions for each region, (2) the Task-aware Dependency Retrieval module that selects relevant context regions based on natural language prompts, and (3) the Prediction Module, exemplified by our proposed GeoTransformer architecture, which adopts a novel geospatial attention mechanism to incorporate spatial proximity and information entropy as priors. Our framework is modular, supports diverse representation methods and forecasting models, and can operate even with minimal input. Quantitative experiments and qualitative analysis across six urban forecasting tasks demonstrate strong task generalization and validate the framework's effectiveness.",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems",
    "url": "http://arxiv.org/abs/2401.10279v1",
    "authors": [
      "Sean Tucker"
    ],
    "published": "2024-01-12",
    "abstract": "Geospatial Location Embedding (GLE) helps a Large Language Model (LLM) assimilate and analyze spatial data. GLE emergence in Geospatial Artificial Intelligence (GeoAI) is precipitated by the need for deeper geospatial awareness in our complex contemporary spaces and the success of LLMs in extracting deep meaning in Generative AI. We searched Google Scholar, Science Direct, and arXiv for papers on geospatial location embedding and LLM and reviewed articles focused on gaining deeper spatial \"knowing\" through LLMs. We screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE themes - Entity Location Embedding (ELE), Document Location Embedding (DLE), Sequence Location Embedding (SLE), and Token Location Embedding (TLE). Synthesis is tabular and narrative, including a dialogic conversation between \"Space\" and \"LLM.\" Though GLEs aid spatial understanding by superimposing spatial data, they emphasize the need to advance in the intricacies of spatial modalities and generalized reasoning. GLEs signal the need for a Spatial Foundation/Language Model (SLM) that embeds spatial knowing within the model architecture. The SLM framework advances Spatial Artificial Intelligence Systems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to physical space. The resulting spatially imbued Language Model is unique. It simultaneously represents actual space and an AI-capable space, paving the way for AI native geo storage, analysis, and multi-modality as the basis for Spatial Artificial Intelligence Systems (SPAIS).",
    "categories": [
      "geo_reasoning",
      "foundation_model"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening",
    "url": "http://arxiv.org/abs/2512.08331v1",
    "authors": [
      "Xianghong Xiao",
      "Zeyu Xia",
      "Zhou Fei",
      "Jinliang Xiao",
      "Haorui Chen",
      "Liangjian Deng"
    ],
    "published": "2025-12-09",
    "abstract": "Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Near-real time fires detection using satellite imagery in Sudan conflict",
    "url": "http://arxiv.org/abs/2512.07925v1",
    "authors": [
      "Kuldip Singh Atwal",
      "Dieter Pfoser",
      "Daniel Rothbart"
    ],
    "published": "2025-12-08",
    "abstract": "The challenges of ongoing war in Sudan highlight the need for rapid monitoring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitoring. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our results indicate that using 8-band imagery or time series of such imagery only result in marginal gains.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2512.03257v1",
    "authors": [
      "Mark Moussa",
      "Andre Williams",
      "Seth Roffe",
      "Douglas Morton"
    ],
    "published": "2025-12-02",
    "abstract": "Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.\n  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.\n  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Fusion or Confusion? Assessing the impact of visible-thermal image fusion for automated wildlife detection",
    "url": "http://arxiv.org/abs/2511.22768v2",
    "authors": [
      "Camille Dionne-Pierre",
      "Samuel Foucher",
      "J\u00e9r\u00f4me Th\u00e9au",
      "J\u00e9r\u00f4me Lema\u00eetre",
      "Patrick Charbonneau",
      "Maxime Brousseau",
      "Mathieu Varin"
    ],
    "published": "2025-11-27",
    "abstract": "Efficient wildlife monitoring methods are necessary for biodiversity conservation and management. The combination of remote sensing, aerial imagery and deep learning offer promising opportunities to renew or improve existing survey methods. The complementary use of visible (VIS) and thermal infrared (TIR) imagery can add information compared to a single-source image and improve results in an automated detection context. However, the alignment and fusion process can be challenging, especially since visible and thermal images usually have different fields of view (FOV) and spatial resolutions. This research presents a case study on the great blue heron (Ardea herodias) to evaluate the performances of synchronous aerial VIS and TIR imagery to automatically detect individuals and nests using a YOLO11n model. Two VIS-TIR fusion methods were tested and compared: an early fusion approach and a late fusion approach, to determine if the addition of the TIR image gives any added value compared to a VIS-only model. VIS and TIR images were automatically aligned using a deep learning model. A principal component analysis fusion method was applied to VIS-TIR image pairs to form the early fusion dataset. A classification and regression tree was used to process the late fusion dataset, based on the detection from the VIS-only and TIR-only trained models. Across all classes, both late and early fusion improved the F1 score compared to the VIS-only model. For the main class, occupied nest, the late fusion improved the F1 score from 90.2 (VIS-only) to 93.0%. This model was also able to identify false positives from both sources with 90% recall. Although fusion methods seem to give better results, this approach comes with a limiting TIR FOV and alignment constraints that eliminate data. Using an aircraft-mounted very high-resolution visible sensor could be an interesting option for operationalizing surveys.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Hierarchical Semi-Supervised Active Learning for Remote Sensing",
    "url": "http://arxiv.org/abs/2511.18058v2",
    "authors": [
      "Wei Huang",
      "Zhitong Xiong",
      "Chenying Liu",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-22",
    "abstract": "The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be publicly available.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2511.16322v1",
    "authors": [
      "Ching-Heng Cheng",
      "Chih-Chung Hsu"
    ],
    "published": "2025-11-20",
    "abstract": "Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection",
    "url": "http://arxiv.org/abs/2511.16143v1",
    "authors": [
      "Quanqing Ma",
      "Jiaen Chen",
      "Peng Wang",
      "Yao Zheng",
      "Qingzhan Zhao",
      "Yuchen Zheng"
    ],
    "published": "2025-11-20",
    "abstract": "Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "SpectralTrain: A Universal Framework for Hyperspectral Image Classification",
    "url": "http://arxiv.org/abs/2511.16084v1",
    "authors": [
      "Meihua Zhou",
      "Liping Yu",
      "Jiawei Cai",
      "Wai Kin Fung",
      "Ruiguo Hu",
      "Jiarui Zhao",
      "Wenzhuo Liu",
      "Nan Wan"
    ],
    "published": "2025-11-20",
    "abstract": "Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images",
    "url": "http://arxiv.org/abs/2511.13552v1",
    "authors": [
      "Sining Chen",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-17",
    "abstract": "Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Mapping the Vanishing and Transformation of Urban Villages in China",
    "url": "http://arxiv.org/abs/2511.13507v1",
    "authors": [
      "Wenyu Zhang",
      "Yao Tong",
      "Yiqiu Liu",
      "Rui Cao"
    ],
    "published": "2025-11-17",
    "abstract": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)",
    "url": "http://arxiv.org/abs/2511.11882v2",
    "authors": [
      "Simon Durand",
      "Samuel Foucher",
      "Alexandre Delplanque",
      "Jo\u00eblle Taillon",
      "J\u00e9r\u00f4me Th\u00e9au"
    ],
    "published": "2025-11-14",
    "abstract": "Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production",
    "url": "http://arxiv.org/abs/2511.11880v1",
    "authors": [
      "David Montero",
      "Miguel D. Mahecha",
      "Francesco Martinuzzi",
      "C\u00e9sar Aybar",
      "Anne Klosterhalfen",
      "Alexander Knohl",
      "Jes\u00fas Anaya",
      "Clemens Mosig",
      "Sebastian Wieneke"
    ],
    "published": "2025-11-14",
    "abstract": "Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "PGDM: Physically guided diffusion model for land surface temperature downscaling",
    "url": "http://arxiv.org/abs/2511.05964v1",
    "authors": [
      "Huanyu Zhang",
      "Bo-Hui Tang",
      "Tian Hu",
      "Yun Jiang",
      "Zhao-Liang Li"
    ],
    "published": "2025-11-08",
    "abstract": "Land surface temperature (LST) is a fundamental parameter in thermal infrared remote sensing, while current LST products are often constrained by the trade-off between spatial and temporal resolutions. To mitigate this limitation, numerous studies have been conducted to enhance the resolutions of LST data, with a particular emphasis on the spatial dimension (commonly known as LST downscaling). Nevertheless, a comprehensive benchmark dataset tailored for this task remains scarce. In addition, existing downscaling models face challenges related to accuracy, practical usability, and the capability to self-evaluate their uncertainties. To overcome these challenges, this study first compiled three representative datasets, including one dataset over mainland China containing 22,909 image patches for model training and evaluation, as well as two datasets covering 40 heterogeneous regions worldwide for external evaluation. Subsequently, grounded in the surface energy balance (SEB)-based geophysical reasoning, we proposed the physically guided diffusion model (PGDM) for LST downscaling. In this framework, the downscaling task was formulated as an inference problem, aiming to sample from the posterior distribution of high-spatial-resolution (HR) LST conditioned on low-spatial-resolution (LR) LST observations and a suite of HR geophysical priors. Comprehensive evaluations demonstrate the effectiveness of PGDM, which generates high-quality downscaling results and outperforms existing representative interpolation, kernel-driven, hybrid, and deep learning approaches. Finally, by exploiting the inherent stochasticity of PGDM, the scene-level standard deviation of multiple generations was computed, revealing a strong positive linear correlation with the actual downscaling error...",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data",
    "url": "http://arxiv.org/abs/2511.04304v1",
    "authors": [
      "Robin Spanier",
      "Thorsten Hoeser",
      "Claudia Kuenzer"
    ],
    "published": "2025-11-06",
    "abstract": "The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning",
    "url": "http://arxiv.org/abs/2511.03004v1",
    "authors": [
      "Dakota Hester",
      "Vitor S. Martins",
      "Lucas B. Ferreira",
      "Thainara M. A. Lima"
    ],
    "published": "2025-11-04",
    "abstract": "Deep learning semantic segmentation methods have shown promising performance for very high 1-m resolution land cover classification, but the challenge of collecting large volumes of representative training data creates a significant barrier to widespread adoption of such models for meter-scale land cover mapping over large areas. In this study, we present a novel label-efficient approach for statewide 1-m land cover classification using only 1,000 annotated reference image patches with self-supervised deep learning. We use the \"Bootstrap Your Own Latent\" pre-training strategy with a large amount of unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to pre-train a ResNet-101 convolutional encoder. The learned encoder weights were subsequently transferred into multiple deep semantic segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then fine-tuned using very small training dataset sizes with cross-validation (250, 500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall accuracy and 75.58% macro F1 score using an ensemble of the best performing U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more than 123 billion pixels over the state of Mississippi, USA. Detailed qualitative and quantitative analysis revealed accurate mapping of open water and forested areas, while highlighting challenges in accurate delineation between cropland, herbaceous, and barren land cover types. These results show that self-supervised learning is an effective strategy for reducing the need for large volumes of manually annotated data, directly addressing a major limitation to high spatial resolution land cover mapping at scale.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "ResNet"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Automatic Extraction of Road Networks by using Teacher-Student Adaptive Structural Deep Belief Network and Its Application to Landslide Disaster",
    "url": "http://arxiv.org/abs/2511.05567v1",
    "authors": [
      "Shin Kamada",
      "Takumi Ichimura"
    ],
    "published": "2025-11-04",
    "abstract": "An adaptive structural learning method of Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) has been developed as one of prominent deep learning models. The neuron generation-annihilation algorithm in RBM and layer generation algorithm in DBN make an optimal network structure for given input during the learning. In this paper, our model is applied to an automatic recognition method of road network system, called RoadTracer. RoadTracer can generate a road map on the ground surface from aerial photograph data. A novel method of RoadTracer using the Teacher-Student based ensemble learning model of Adaptive DBN is proposed, since the road maps contain many complicated features so that a model with high representation power to detect should be required. The experimental results showed the detection accuracy of the proposed model was improved from 40.0\\% to 89.0\\% on average in the seven major cities among the test dataset. In addition, we challenged to apply our method to the detection of available roads when landslide by natural disaster is occurred, in order to rapidly obtain a way of transportation. For fast inference, a small size of the trained model was implemented on a small embedded edge device as lightweight deep learning. We reported the detection results for the satellite image before and after the rainfall disaster in Japan.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery",
    "url": "http://arxiv.org/abs/2510.27224v1",
    "authors": [
      "Mahmoud El Hussieni",
      "Bahad\u0131r K. G\u00fcnt\u00fcrk",
      "Hasan F. Ate\u015f",
      "O\u011fuz Hano\u011flu"
    ],
    "published": "2025-10-31",
    "abstract": "Accurate building instance segmentation and height classification are critical for urban planning, 3D city modeling, and infrastructure monitoring. This paper presents a detailed analysis of YOLOv11, the recent advancement in the YOLO series of deep learning models, focusing on its application to joint building extraction and discrete height classification from satellite imagery. YOLOv11 builds on the strengths of earlier YOLO models by introducing a more efficient architecture that better combines features at different scales, improves object localization accuracy, and enhances performance in complex urban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000 annotated buildings across 12 cities -- we evaluate YOLOv11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLOv11 achieves strong instance segmentation performance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust classification accuracy across five predefined height tiers. The model excels in handling occlusions, complex building shapes, and class imbalance, particularly for rare high-rise structures. Comparative analysis confirms that YOLOv11 outperforms earlier multitask frameworks in both detection accuracy and inference speed, making it well-suited for real-time, large-scale urban mapping. This research highlights YOLOv11's potential to advance semantic urban reconstruction through streamlined categorical height modeling, offering actionable insights for future developments in remote sensing and geospatial intelligence.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2",
    "url": "http://arxiv.org/abs/2510.26653v1",
    "authors": [
      "Daniela Martin",
      "Joseph Gallego"
    ],
    "published": "2025-10-30",
    "abstract": "Accurate estimation of sea ice drift is critical for Arctic navigation, climate research, and operational forecasting. While optical flow, a computer vision technique for estimating pixel wise motion between consecutive images, has advanced rapidly in computer vision, its applicability to geophysical problems and to satellite SAR imagery remains underexplored. Classical optical flow methods rely on mathematical models and strong assumptions about motion, which limit their accuracy in complex scenarios. Recent deep learning based approaches have substantially improved performance and are now the standard in computer vision, motivating their application to sea ice drift estimation. We present the first large scale benchmark of 48 deep learning optical flow models on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the Arctic. Our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods, which have substantially improved motion estimation accuracy compared to classical methods, can be effectively transferred to polar remote sensing. Optical flow produces spatially continuous drift fields, providing motion estimates for every image pixel rather than at sparse buoy locations, offering new opportunities for navigation and climate modeling.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning",
    "url": "http://arxiv.org/abs/2510.24321v1",
    "authors": [
      "Ivica Dimitrovski",
      "Vlatko Spasev",
      "Ivan Kitanovski"
    ],
    "published": "2025-10-28",
    "abstract": "Remote sensing applications increasingly rely on deep learning for scene classification. However, their performance is often constrained by the scarcity of labeled data and the high cost of annotation across diverse geographic and sensor domains. While recent vision-language models like CLIP have shown promise by learning transferable representations at scale by aligning visual and textual modalities, their direct application to remote sensing remains suboptimal due to significant domain gaps and the need for task-specific semantic adaptation. To address this critical challenge, we systematically explore prompt learning as a lightweight and efficient adaptation strategy for few-shot remote sensing image scene classification. We evaluate several representative methods, including Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints. These approaches reflect complementary design philosophies: from static context optimization to conditional prompts for enhanced generalization, multi-modal prompts for joint vision-language adaptation, and semantically regularized prompts for stable learning without forgetting. We benchmark these prompt-learning methods against two standard baselines: zero-shot CLIP with hand-crafted prompts and a linear probe trained on frozen CLIP features. Through extensive experiments on multiple benchmark remote sensing datasets, including cross-dataset generalization tests, we demonstrate that prompt learning consistently outperforms both baselines in few-shot scenarios. Notably, Prompting with Self-Regulating Constraints achieves the most robust cross-domain performance. Our findings underscore prompt learning as a scalable and efficient solution for bridging the domain gap in satellite and aerial imagery, providing a strong foundation for future research in this field.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM",
      "CLIP"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Review of End-to-End Precipitation Prediction Using Remote Sensing Data: from Divination to Machine Learning",
    "url": "http://arxiv.org/abs/2510.22855v1",
    "authors": [
      "Yugong Zeng",
      "Jonathan Wu"
    ],
    "published": "2025-10-26",
    "abstract": "Precipitation prediction has undergone a profound transformation -- from early symbolic and empirical methods rooted in divination and observation, to modern technologies based on atmospheric physics and artificial intelligence. This review traces the historical and technological evolution of precipitation forecasting, presenting a survey about end-to-end precipitation prediction technologies that spans ancient practices, the foundations of meteorological science, the rise of numerical weather prediction (NWP), and the emergence of machine learning (ML) and deep learning (DL) models. We first explore traditional and indigenous forecasting methods, then describe the development of physical modeling and statistical frameworks that underpin contemporary operational forecasting. Particular emphasis is placed on recent advances in neural network-based approaches, including automated deep learning, interpretability-driven design, and hybrid physical-data models. By compositing research across multiple eras and paradigms, this review not only depicts the history of end-to-end precipitation prediction but also outlines future directions in next generation forecasting systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Enpowering Your Pansharpening Models with Generalizability: Unified Distribution is All You Need",
    "url": "http://arxiv.org/abs/2510.22217v1",
    "authors": [
      "Yongchuan Cui",
      "Peng Liu",
      "Hui Zhang"
    ],
    "published": "2025-10-25",
    "abstract": "Existing deep learning-based models for remote sensing pansharpening exhibit exceptional performance on training datasets. However, due to sensor-specific characteristics and varying imaging conditions, these models suffer from substantial performance degradation when applied to unseen satellite data, lacking generalizability and thus limiting their applicability. We argue that the performance drops stem primarily from distributional discrepancies from different sources and the key to addressing this challenge lies in bridging the gap between training and testing distributions. To validate the idea and further achieve a \"train once, deploy forever\" capability, this paper introduces a novel and intuitive approach to enpower any pansharpening models with generalizability by employing a unified distribution strategy (UniPAN). Specifically, we construct a distribution transformation function that normalizes the pixels sampled from different sources to conform to an identical distribution. The deep models are trained on the transformed domain, and during testing on new datasets, the new data are also transformed to match the training distribution. UniPAN aims to train and test the model on a unified and consistent distribution, thereby enhancing its generalizability. Extensive experiments validate the efficacy of UniPAN, demonstrating its potential to significantly enhance the performance of deep pansharpening models across diverse satellite sensors. Codes: https://github.com/yc-cui/UniPAN.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters",
    "url": "http://arxiv.org/abs/2510.19329v1",
    "authors": [
      "Panagiotis Agrafiotis",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-10-22",
    "abstract": "Accurate, detailed, and regularly updated bathymetry, coupled with complex semantic content, is essential for under-mapped shallow-water environments facing increasing climatological and anthropogenic pressures. However, existing approaches that derive either depth or seabed classes from remote sensing imagery treat these tasks in isolation, forfeiting the mutual benefits of their interaction and hindering the broader adoption of deep learning methods. To address these limitations, we introduce Seabed-Net, a unified multi-task framework that simultaneously predicts bathymetry and pixel-based seabed classification from remote sensing imagery of various resolutions. Seabed-Net employs dual-branch encoders for bathymetry estimation and pixel-based seabed classification, integrates cross-task features via an Attention Feature Fusion module and a windowed Swin-Transformer fusion block, and balances objectives through dynamic task uncertainty weighting. In extensive evaluations at two heterogeneous coastal sites, it consistently outperforms traditional empirical models and traditional machine learning regression methods, achieving up to 75\\% lower RMSE. It also reduces bathymetric RMSE by 10-30\\% compared to state-of-the-art single-task and multi-task baselines and improves seabed classification accuracy up to 8\\%. Qualitative analyses further demonstrate enhanced spatial consistency, sharper habitat boundaries, and corrected depth biases in low-contrast regions. These results confirm that jointly modeling depth with both substrate and seabed habitats yields synergistic gains, offering a robust, open solution for integrated shallow-water mapping. Code and pretrained weights are available at https://github.com/pagraf/Seabed-Net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance",
    "url": "http://arxiv.org/abs/2510.16445v1",
    "authors": [
      "Chien Thai",
      "Mai Xuan Trang",
      "Huong Ninh",
      "Hoang Hiep Ly",
      "Anh Son Le"
    ],
    "published": "2025-10-18",
    "abstract": "Detecting rotated objects accurately and efficiently is a significant challenge in computer vision, particularly in applications such as aerial imagery, remote sensing, and autonomous driving. Although traditional object detection frameworks are effective for axis-aligned objects, they often underperform in scenarios involving rotated objects due to their limitations in capturing orientation variations. This paper introduces an improved loss function aimed at enhancing detection accuracy and robustness by leveraging the Gaussian bounding box representation and Bhattacharyya distance. In addition, we advocate for the use of an anisotropic Gaussian representation to address the issues associated with isotropic variance in square-like objects. Our proposed method addresses these challenges by incorporating a rotation-invariant loss function that effectively captures the geometric properties of rotated objects. We integrate this proposed loss function into state-of-the-art deep learning-based rotated object detection detectors, and extensive experiments demonstrated significant improvements in mean Average Precision metrics compared to existing methods. The results highlight the potential of our approach to establish new benchmark in rotated object detection, with implications for a wide range of applications requiring precise and reliable object localization irrespective of orientation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Deep Learning Based Domain Adaptation Methods in Remote Sensing: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2510.15615v1",
    "authors": [
      "Shuchang Lyu",
      "Qi Zhao",
      "Zheng Zhou",
      "Meng Li",
      "You Zhou",
      "Dingding Yao",
      "Guangliang Cheng",
      "Huiyu Zhou",
      "Zhenwei Shi"
    ],
    "published": "2025-10-17",
    "abstract": "Domain adaptation is a crucial and increasingly important task in remote sensing, aiming to transfer knowledge from a source domain a differently distributed target domain. It has broad applications across various real-world applications, including remote sensing element interpretation, ecological environment monitoring, and urban/rural planning. However, domain adaptation in remote sensing poses significant challenges due to differences in data, such as variations in ground sampling distance, imaging modes from various sensors, geographical landscapes, and environmental conditions. In recent years, deep learning has emerged as a powerful tool for feature representation and cross-domain knowledge transfer, leading to widespread adoption in remote sensing tasks. In this paper, we present a comprehensive survey of significant advancements in deep learning based domain adaptation for remote sensing. We first introduce the preliminary knowledge to clarify key concepts, mathematical notations, and the taxonomy of methodologies. We then organize existing algorithms from multiple perspectives, including task categorization, input mode, supervision paradigm, and algorithmic granularity, providing readers with a structured understanding of the field. Next, we review widely used datasets and summarize the performance of state-of-the-art methods to provide an overview of current progress. We also identify open challenges and potential directions to guide future research in domain adaptation for remote sensing. Compared to previous surveys, this work addresses a broader range of domain adaptation tasks in remote sensing, rather than concentrating on a few subfields. It also presents a systematic taxonomy, providing a more comprehensive and organized understanding of the field. As a whole, this survey can inspire the research community, foster understanding, and guide future work in the field.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Rethinking deep learning: linear regression remains a key benchmark in predicting terrestrial water storage",
    "url": "http://arxiv.org/abs/2510.10799v1",
    "authors": [
      "Wanshu Nie",
      "Sujay V. Kumar",
      "Junyu Chen",
      "Long Zhao",
      "Olya Skulovich",
      "Jinwoong Yoo",
      "Justin Pflug",
      "Shahryar Khalique Ahmad",
      "Goutam Konapala"
    ],
    "published": "2025-10-12",
    "abstract": "Recent advances in machine learning such as Long Short-Term Memory (LSTM) models and Transformers have been widely adopted in hydrological applications, demonstrating impressive performance amongst deep learning models and outperforming physical models in various tasks. However, their superiority in predicting land surface states such as terrestrial water storage (TWS) that are dominated by many factors such as natural variability and human driven modifications remains unclear. Here, using the open-access, globally representative HydroGlobe dataset - comprising a baseline version derived solely from a land surface model simulation and an advanced version incorporating multi-source remote sensing data assimilation - we show that linear regression is a robust benchmark, outperforming the more complex LSTM and Temporal Fusion Transformer for TWS prediction. Our findings highlight the importance of including traditional statistical models as benchmarks when developing and evaluating deep learning models. Additionally, we emphasize the critical need to establish globally representative benchmark datasets that capture the combined impact of natural variability and human interventions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data",
    "url": "http://arxiv.org/abs/2510.09845v1",
    "authors": [
      "Nicholas LaHaye",
      "Thilanka Munashinge",
      "Hugo Lee",
      "Xiaohua Pan",
      "Gonzalo Gonzalez Abad",
      "Hazem Mahmoud",
      "Jennifer Wei"
    ],
    "published": "2025-10-10",
    "abstract": "This work demonstrates the possibilities for improving wildfire and air quality management in the western United States by leveraging the unprecedented hourly data from NASA's TEMPO satellite mission and advances in self-supervised deep learning. Here we demonstrate the efficacy of deep learning for mapping the near real-time hourly spread of wildfire fronts and smoke plumes using an innovative self-supervised deep learning-system: successfully distinguishing smoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across the smoke and fire masks generated from different sensing modalities as well as significant improvement over operational products for the same cases.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Hyperspectral data augmentation with transformer-based diffusion models",
    "url": "http://arxiv.org/abs/2510.08363v1",
    "authors": [
      "Mattia Ferrari",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-09",
    "abstract": "The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models",
    "url": "http://arxiv.org/abs/2510.07008v1",
    "authors": [
      "Gianmarco Perantoni",
      "Giulio Weikmann",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-08",
    "abstract": "The temporal consistency of yearly land-cover maps is of great importance to model the evolution and change of the land cover over the years. In this paper, we focus the attention on a novel approach to classification of yearly satellite image time series (SITS) that combines deep learning with Bayesian modelling, using Hidden Markov Models (HMMs) integrated with Transformer Encoder (TE) based DNNs. The proposed approach aims to capture both i) intricate temporal correlations in yearly SITS and ii) specific patterns in multiyear crop type sequences. It leverages the cascade classification of an HMM layer built on top of the TE, discerning consistent yearly crop-type sequences. Validation on a multiyear crop type classification dataset spanning 47 crop types and six years of Sentinel-2 acquisitions demonstrates the importance of modelling temporal consistency in the predicted labels. HMMs enhance the overall performance and F1 scores, emphasising the effectiveness of the proposed approach.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Label-frugal satellite image change detection with generative virtual exemplar learning",
    "url": "http://arxiv.org/abs/2510.06926v1",
    "authors": [
      "Hichem Sahbi"
    ],
    "published": "2025-10-08",
    "abstract": "Change detection is a major task in remote sensing which consists in finding all the occurrences of changes in multi-temporal satellite or aerial images. The success of existing methods, and particularly deep learning ones, is tributary to the availability of hand-labeled training data that capture the acquisition conditions and the subjectivity of the user (oracle). In this paper, we devise a novel change detection algorithm, based on active learning. The main contribution of our work resides in a new model that measures how important is each unlabeled sample, and provides an oracle with only the most critical samples (also referred to as virtual exemplars) for further labeling. These exemplars are generated, using an invertible graph convnet, as the optimum of an adversarial loss that (i) measures representativity, diversity and ambiguity of the data, and thereby (ii) challenges (the most) the current change detection criteria, leading to a better re-estimate of these criteria in the subsequent iterations of active learning. Extensive experiments show the positive impact of our label-efficient learning model against comparative methods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Explaining raw data complexity to improve satellite onboard processing",
    "url": "http://arxiv.org/abs/2510.06858v2",
    "authors": [
      "Adrien Dorise",
      "Marjorie Bellizzi",
      "Adrien Girard",
      "Benjamin Francesconi",
      "St\u00e9phane May"
    ],
    "published": "2025-10-08",
    "abstract": "With increasing processing power, deploying AI models for remote sensing directly onboard satellites is becoming feasible. However, new constraints arise, mainly when using raw, unprocessed sensor data instead of preprocessed ground-based products. While current solutions primarily rely on preprocessed sensor images, few approaches directly leverage raw data. This study investigates the effects of utilising raw data on deep learning models for object detection and classification tasks. We introduce a simulation workflow to generate raw-like products from high-resolution L1 imagery, enabling systemic evaluation. Two object detection models (YOLOv11n and YOLOX-S) are trained on both raw and L1 datasets, and their performance is compared using standard detection metrics and explainability tools. Results indicate that while both models perform similarly at low to medium confidence thresholds, the model trained on raw data struggles with object boundary identification at high confidence levels. It suggests that adapting AI architectures with improved contouring methods can enhance object detection on raw images, improving onboard AI for remote sensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data",
    "url": "http://arxiv.org/abs/2510.05760v1",
    "authors": [
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-07",
    "abstract": "Deep learning has gained broad interest in remote sensing image scene classification thanks to the effectiveness of deep neural networks in extracting the semantics from complex data. However, deep networks require large amounts of training samples to obtain good generalization capabilities and are sensitive to errors in the training labels. This is a problem in remote sensing since highly reliable labels can be obtained at high costs and in limited amount. However, many sources of less reliable labeled data are available, e.g., obsolete digital maps. In order to train deep networks with larger datasets, we propose both the combination of single or multiple weak sources of labeled data with a small but reliable dataset to generate multisource labeled datasets and a novel training strategy where the reliability of each source is taken in consideration. This is done by exploiting the transition matrices describing the statistics of the errors of each source. The transition matrices are embedded into the labels and used during the training process to weigh each label according to the related source. The proposed method acts as a weighting scheme at gradient level, where each instance contributes with different weights to the optimization of different classes. The effectiveness of the proposed method is validated by experiments on different datasets. The results proved the robustness and capability of leveraging on unreliable source of labels of the proposed method.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images",
    "url": "http://arxiv.org/abs/2510.04916v1",
    "authors": [
      "Giulio Weikmann",
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-06",
    "abstract": "Deep learning has become increasingly important in remote sensing image classification due to its ability to extract semantic information from complex data. Classification tasks often include predefined label hierarchies that represent the semantic relationships among classes. However, these hierarchies are frequently overlooked, and most approaches focus only on fine-grained classification schemes. In this paper, we present a novel Semantics-Aware Hierarchical Consensus (SAHC) method for learning hierarchical features and relationships by integrating hierarchy-specific classification heads within a deep network architecture, each specialized in different degrees of class granularity. The proposed approach employs trainable hierarchy matrices, which guide the network through the learning of the hierarchical structure in a self-supervised manner. Furthermore, we introduce a hierarchical consensus mechanism to ensure consistent probability distributions across different hierarchical levels. This mechanism acts as a weighted ensemble being able to effectively leverage the inherent structure of the hierarchical classification task. The proposed SAHC method is evaluated on three benchmark datasets with different degrees of hierarchical complexity on different tasks, using distinct backbone architectures to effectively emphasize its adaptability. Experimental results show both the effectiveness of the proposed approach in guiding network learning and the robustness of the hierarchical consensus for remote sensing image classification tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification",
    "url": "http://arxiv.org/abs/2510.04628v1",
    "authors": [
      "Hao Liu",
      "Yunhao Gao",
      "Wei Li",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-06",
    "abstract": "Deep learning-based methods have achieved significant success in remote sensing Earth observation data analysis. Numerous feature fusion techniques address multimodal remote sensing image classification by integrating global and local features. However, these techniques often struggle to extract structural and detail features from heterogeneous and redundant multimodal images. With the goal of introducing frequency domain learning to model key and sparse detail features, this paper introduces the spatial-spectral-frequency interaction network (S$^2$Fin), which integrates pairwise fusion modules across the spatial, spectral, and frequency domains. Specifically, we propose a high-frequency sparse enhancement transformer that employs sparse spatial-spectral attention to optimize the parameters of the high-frequency filter. Subsequently, a two-level spatial-frequency fusion strategy is introduced, comprising an adaptive frequency channel module that fuses low-frequency structures with enhanced high-frequency details, and a high-frequency resonance mask that emphasizes sharp edges via phase similarity. In addition, a spatial-spectral attention fusion module further enhances feature extraction at intermediate layers of the network. Experiments on four benchmark multimodal datasets with limited labeled data demonstrate that S$^2$Fin performs superior classification, outperforming state-of-the-art methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks",
    "url": "http://arxiv.org/abs/2510.03725v1",
    "authors": [
      "Thomas Hallopeau",
      "Joris Gu\u00e9rin",
      "Laurent Demagistri",
      "Youssef Fouzai",
      "Renata Gracie",
      "Vanderlei Pascoal De Matos",
      "Helen Gurgel",
      "Nadine Dessay"
    ],
    "published": "2025-10-04",
    "abstract": "While deep learning methods for detecting informal settlements have already been developed, they have not yet fully utilized the potential offered by recent pretrained neural networks. We compare two types of pretrained neural networks for detecting the favelas of Rio de Janeiro: 1. Generic networks pretrained on large diverse datasets of unspecific images, 2. A specialized network pretrained on satellite imagery. While the latter is more specific to the target task, the former has been pretrained on significantly more images. Hence, this research investigates whether task specificity or data volume yields superior performance in urban informal settlement detection.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Overview of GeoLifeCLEF 2023: Species Composition Prediction with High Spatial Resolution at Continental Scale Using Remote Sensing",
    "url": "http://arxiv.org/abs/2509.25816v1",
    "authors": [
      "Christophe Botella",
      "Benjamin Deneu",
      "Diego Marcos",
      "Maximilien Servajean",
      "Theo Larcher",
      "Cesar Leblanc",
      "Joaquim Estopinan",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2025-09-30",
    "abstract": "Understanding the spatio-temporal distribution of species is a cornerstone of ecology and conservation. By pairing species observations with geographic and environmental predictors, researchers can model the relationship between an environment and the species which may be found there. To advance the state-of-the-art in this area with deep learning models and remote sensing data, we organized an open machine learning challenge called GeoLifeCLEF 2023. The training dataset comprised 5 million plant species observations (single positive label per sample) distributed across Europe and covering most of its flora, high-resolution rasters: remote sensing imagery, land cover, elevation, in addition to coarse-resolution data: climate, soil and human footprint variables. In this multi-label classification task, we evaluated models ability to predict the species composition in 22 thousand small plots based on standardized surveys. This paper presents an overview of the competition, synthesizes the approaches used by the participating teams, and analyzes the main results. In particular, we highlight the biases faced by the methods fitted to single positive labels when it comes to the multi-label evaluation, and the new and effective learning strategy combining single and multi-label data in training.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Spatial-Spectral Binarized Neural Network for Panchromatic and Multi-spectral Images Fusion",
    "url": "http://arxiv.org/abs/2509.23321v2",
    "authors": [
      "Yizhen Jiang",
      "Mengting Ma",
      "Anqi Zhu",
      "Xiaowen Ma",
      "Jiaxin Li",
      "Wei Zhang"
    ],
    "published": "2025-09-27",
    "abstract": "Remote sensing pansharpening aims to reconstruct spatial-spectral properties during the fusion of panchromatic (PAN) images and low-resolution multi-spectral (LR-MS) images, finally generating the high-resolution multi-spectral (HR-MS) images. Although deep learning-based models have achieved excellent performance, they often come with high computational complexity, which hinder their applications on resource-limited devices. In this paper, we explore the feasibility of applying the binary neural network (BNN) to pan-sharpening. Nevertheless, there are two main issues with binarizing pan-sharpening models: (i) the binarization will cause serious spectral distortion due to the inconsistent spectral distribution of the PAN/LR-MS images; (ii) the common binary convolution kernel is difficult to adapt to the multi-scale and anisotropic spatial features of remote sensing objects, resulting in serious degradation of contours. To address the above issues, we design the customized spatial-spectral binarized convolution (S2B-Conv), which is composed of the Spectral-Redistribution Mechanism (SRM) and Gabor Spatial Feature Amplifier (GSFA). Specifically, SRM employs an affine transformation, generating its scaling and bias parameters through a dynamic learning process. GSFA, which randomly selects different frequencies and angles within a preset range, enables to better handle multi-scale and-directional spatial features. A series of S2B-Conv form a brand-new binary network for pan-sharpening, dubbed as S2BNet. Extensive quantitative and qualitative experiments have shown our high-efficiency binarized pan-sharpening method can attain a promising performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification",
    "url": "http://arxiv.org/abs/2509.23310v1",
    "authors": [
      "Hao Liu",
      "Yongjie Zheng",
      "Yuhan Kang",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-09-27",
    "abstract": "Deep learning-based techniques for the analysis of multimodal remote sensing data have become popular due to their ability to effectively integrate complementary spatial, spectral, and structural information from different sensors. Recently, denoising diffusion probabilistic models (DDPMs) have attracted attention in the remote sensing community due to their powerful ability to capture robust and complex spatial-spectral distributions. However, pre-training multimodal DDPMs may result in modality imbalance, and effectively leveraging diffusion features to guide complementary diversity feature extraction remains an open question. To address these issues, this paper proposes a balanced diffusion-guided fusion (BDGF) framework that leverages multimodal diffusion features to guide a multi-branch network for land-cover classification. Specifically, we propose an adaptive modality masking strategy to encourage the DDPMs to obtain a modality-balanced rather than spectral image-dominated data distribution. Subsequently, these diffusion features hierarchically guide feature extraction among CNN, Mamba, and transformer networks by integrating feature fusion, group channel attention, and cross-attention mechanisms. Finally, a mutual learning strategy is developed to enhance inter-branch collaboration by aligning the probability entropy and feature similarity of individual subnetworks. Extensive experiments on four multimodal remote sensing datasets demonstrate that the proposed method achieves superior classification performance. The code is available at https://github.com/HaoLiu-XDU/BDGF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution",
    "url": "http://arxiv.org/abs/2510.00033v1",
    "authors": [
      "Usman Muhammad",
      "Jorma Laaksonen"
    ],
    "published": "2025-09-26",
    "abstract": "Hyperspectral single image super-resolution (SISR) is a challenging task due to the difficulty of restoring fine spatial details while preserving spectral fidelity across a wide range of wavelengths, which limits the performance of conventional deep learning models. To address this challenge, we introduce Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly integrated into standard 2D convolutional architectures to enhance both spatial resolution and spectral integrity. The SSUF combines spectral unmixing with spectral--spatial feature extraction and guides a ResNet-based convolutional neural network for improved reconstruction. In addition, we propose a custom Spatial-Spectral Gradient Loss function that integrates mean squared error with spatial and spectral gradient components, encouraging accurate reconstruction of both spatial and spectral features. Experiments on three public remote sensing hyperspectral datasets demonstrate that the proposed hybrid deep learning model achieves competitive performance while reducing model complexity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "ResNet"
    ],
    "applications": [
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images",
    "url": "http://arxiv.org/abs/2509.20918v1",
    "authors": [
      "Qinfeng Zhu",
      "Han Li",
      "Liang He",
      "Lei Fan"
    ],
    "published": "2025-09-25",
    "abstract": "Semantic segmentation of remote sensing imagery is a fundamental task in computer vision, supporting a wide range of applications such as land use classification, urban planning, and environmental monitoring. However, this task is often challenged by the high spatial resolution, complex scene structures, and diverse object scales present in remote sensing data. To address these challenges, various deep learning architectures have been proposed, including convolutional neural networks, Vision Transformers, and the recently introduced Vision Mamba. Vision Mamba features a global receptive field and low computational complexity, demonstrating both efficiency and effectiveness in image segmentation. However, its reliance on global scanning tends to overlook critical local features, such as textures and edges, which are essential for achieving accurate segmentation in remote sensing contexts. To tackle this limitation, we propose SwinMamba, a novel framework inspired by the Swin Transformer. SwinMamba integrates localized Mamba-style scanning within shifted windows with a global receptive field, to enhance the model's perception of both local and global features. Specifically, the first two stages of SwinMamba perform local scanning to capture fine-grained details, while its subsequent two stages leverage global scanning to fuse broader contextual information. In our model, the use of overlapping shifted windows enhances inter-region information exchange, facilitating more robust feature integration across the entire image. Extensive experiments on the LoveDA and ISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art methods, underscoring its effectiveness and potential as a superior solution for semantic segmentation of remotely sensed imagery.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
    "url": "http://arxiv.org/abs/2509.20234v3",
    "authors": [
      "Tom Burgert",
      "Oliver Stoll",
      "Paolo Rota",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-09-24",
    "abstract": "The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Optimal Transport Based Hyperspectral Unmixing for Highly Mixed Observations",
    "url": "http://arxiv.org/abs/2509.20417v1",
    "authors": [
      "D. Doutsas",
      "B. Figliuzzi"
    ],
    "published": "2025-09-24",
    "abstract": "We propose a novel approach based on optimal transport (OT) for tackling the problem of highly mixed data in blind hyperspectral unmixing. Our method constrains the distribution of the estimated abundance matrix to resemble a targeted Dirichlet distribution more closely. The novelty lies in using OT to measure the discrepancy between the targeted and true abundance distributions, which we incorporate as a regularization term in our optimization problem. We demonstrate the efficiency of our method through a case study involving an unsupervised deep learning approach. Our experiments show that the proposed approach allows for a better estimation of the endmembers in the presence of highly mixed data, while displaying robustness to the choice of target abundance distribution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy",
    "url": "http://arxiv.org/abs/2509.19665v2",
    "authors": [
      "Manuel Perez-Carrasco",
      "Maya Nasr",
      "Sebastien Roche",
      "Chris Chan Miller",
      "Zhan Zhang",
      "Core Francisco Park",
      "Eleanor Walker",
      "Cecilia Garraffo",
      "Douglas Finkbeiner",
      "Ritesh Gautam",
      "Steven Wofsy"
    ],
    "published": "2025-09-24",
    "abstract": "Effective cloud and cloud shadow detection is a critical prerequisite for accurate retrieval of concentrations of atmospheric methane or other trace gases in hyperspectral remote sensing. This challenge is especially pertinent for MethaneSAT and for its airborne companion mission, MethaneAIR. In this study, we use machine learning methods to address the cloud and cloud shadow detection problem for sensors with these high spatial resolutions instruments. Cloud and cloud shadows in remote sensing data need to be effectively screened out as they bias methane retrievals in remote sensing imagery and impact the quantification of emissions. We deploy and evaluate conventional techniques including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP), with advanced deep learning architectures, namely UNet and a Spectral Channel Attention Network (SCAN) method. Our results show that conventional methods struggle with spatial coherence and boundary definition, affecting the detection of clouds and cloud shadows. Deep learning models substantially improve detection quality: UNet performs best in preserving spatial structure, while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses UNet on MethaneSAT data, underscoring the benefits of incorporating spectral attention for satellite specific features. This in depth assessment of various disparate machine learning techniques demonstrates the strengths and effectiveness of advanced deep learning architectures in providing robust, scalable solutions for clouds and cloud shadow screening towards enhancing methane emission quantification capacity of existing and next generation hyperspectral missions. Our data and code is publicly available at https://doi.org/10.7910/DVN/IKLZOJ",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Communications to Circulations: Real-Time 3D Wind Field Prediction Using 5G GNSS Signals and Deep Learning",
    "url": "http://arxiv.org/abs/2509.16068v3",
    "authors": [
      "Yuchen Ye",
      "Chaoxia Yuan",
      "Mingyu Li",
      "Aoqi Zhou",
      "Hong Liang",
      "Chunqing Shang",
      "Kezuan Wang",
      "Yifeng Zheng",
      "Cong Chen"
    ],
    "published": "2025-09-19",
    "abstract": "Accurate atmospheric wind field information is crucial for various applications, including weather forecasting, aviation safety, and disaster risk reduction. However, obtaining high spatiotemporal resolution wind data remains challenging due to limitations in traditional in-situ observations and remote sensing techniques, as well as the computational expense and biases of numerical weather prediction (NWP) models. This paper introduces G-WindCast, a novel deep learning framework that leverages signal strength variations from 5G Global Navigation Satellite System (GNSS) signals to forecast three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward Neural Networks (FNN) and Transformer networks to capture complex, nonlinear, and spatiotemporal relationships between GNSS-derived features and wind dynamics. Our preliminary results demonstrate promising accuracy in real-time wind forecasts (up to 30 minutes lead time). The model exhibits robustness across forecast horizons and different pressure levels, and its predictions for wind fields show superior agreement with ground-based radar wind profiler compared to concurrent European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5). Furthermore, we show that the system can maintain excellent performance for localized forecasting even with a significantly reduced number of GNSS stations (e.g., around 100), highlighting its cost-effectiveness and scalability. This interdisciplinary approach underscores the transformative potential of exploiting non-traditional data sources and deep learning for advanced environmental monitoring and real-time atmospheric applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland",
    "url": "http://arxiv.org/abs/2509.18176v1",
    "authors": [
      "Wendong Yao",
      "Saeed Azadnejad",
      "Binhua Huang",
      "Shane Donohue",
      "Soumyabrata Dev"
    ],
    "published": "2025-09-17",
    "abstract": "Monitoring ground displacement is crucial for urban infrastructure stability and mitigating geological hazards. However, forecasting future deformation from sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data remains a significant challenge. This paper introduces a novel deep learning framework that transforms these sparse point measurements into a dense spatio-temporal tensor. This methodological shift allows, for the first time, the direct application of advanced computer vision architectures to this forecasting problem. We design and implement a hybrid Convolutional Neural Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to simultaneously learn spatial patterns and temporal dependencies from the generated data tensor. The model's performance is benchmarked against powerful machine learning baselines, Light Gradient Boosting Machine and LASSO regression, using Sentinel-1 data from eastern Ireland. Results demonstrate that the proposed architecture provides significantly more accurate and spatially coherent forecasts, establishing a new performance benchmark for this task. Furthermore, an interpretability analysis reveals that baseline models often default to simplistic persistence patterns, highlighting the necessity of our integrated spatio-temporal approach to capture the complex dynamics of ground deformation. Our findings confirm the efficacy and potential of spatio-temporal deep learning for high-resolution deformation forecasting.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction",
    "url": "http://arxiv.org/abs/2509.10308v1",
    "authors": [
      "Joshua Dimasaka",
      "Christian Gei\u00df",
      "Robert Muir-Wood",
      "Emily So"
    ],
    "published": "2025-09-12",
    "abstract": "In the aftermath of disasters, many institutions worldwide face challenges in continually monitoring changes in disaster risk, limiting the ability of key decision-makers to assess progress towards the UN Sendai Framework for Disaster Risk Reduction 2015-2030. While numerous efforts have substantially advanced the large-scale modeling of hazard and exposure through Earth observation and data-driven methods, progress remains limited in modeling another equally important yet challenging element of the risk equation: physical vulnerability. To address this gap, we introduce Graph Categorical Structured Variational Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for modeling physical vulnerability by integrating deep learning, graph representation, and categorical probabilistic inference, using time-series satellite-derived datasets and prior expert belief systems. We introduce a weakly supervised first-order transition matrix that reflects the changes in the spatiotemporal distribution of physical vulnerability in two disaster-stricken and socioeconomically disadvantaged areas: (1) the cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the mudslide-affected city of Freetown in Sierra Leone. Our work reveals post-disaster regional dynamics in physical vulnerability, offering valuable insights into localized spatiotemporal auditing and sustainable strategies for post-disaster risk reduction.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery",
    "url": "http://arxiv.org/abs/2509.08949v1",
    "authors": [
      "Yibin Wang",
      "Wondimagegn Beshah",
      "Padmanava Dash",
      "Haifeng Wang"
    ],
    "published": "2025-09-10",
    "abstract": "The use of unmanned aerial systems (UASs) has increased tremendously in the current decade. They have significantly advanced remote sensing with the capability to deploy and image the terrain as per required spatial, spectral, temporal, and radiometric resolutions for various remote sensing applications. One of the major advantages of UAS imagery is that images can be acquired in cloudy conditions by flying the UAS under the clouds. The limitation to the technology is that the imagery is often sullied by cloud shadows. Images taken over water are additionally affected by sun glint. These are two pose serious issues for estimating water quality parameters from the UAS images. This study proposes a novel machine learning approach first to identify and extract regions with cloud shadows and sun glint and separate such regions from non-obstructed clear sky regions and sun-glint unaffected regions. The data was extracted from the images at pixel level to train an U-Net based deep learning model and best settings for model training was identified based on the various evaluation metrics from test cases. Using this evaluation, a high-quality image correction model was determined, which was used to recover the cloud shadow and sun glint areas in the images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2509.03961v1",
    "authors": [
      "Yijun Zhou",
      "Yikui Zhai",
      "Zilu Ying",
      "Tingfeng Xian",
      "Wenlve Zhou",
      "Zhiheng Zhou",
      "Xiaolin Tian",
      "Xudong Jia",
      "Hongsheng Zhang",
      "C. L. Philip Chen"
    ],
    "published": "2025-09-04",
    "abstract": "Although deep learning has advanced remote sensing change detection (RSCD), most methods rely solely on image modality, limiting feature representation, change pattern modeling, and generalization especially under illumination and noise disturbances. To address this, we propose MMChange, a multimodal RSCD method that combines image and text modalities to enhance accuracy and robustness. An Image Feature Refinement (IFR) module is introduced to highlight key regions and suppress environmental noise. To overcome the semantic limitations of image features, we employ a vision language model (VLM) to generate semantic descriptions of bitemporal images. A Textual Difference Enhancement (TDE) module then captures fine grained semantic shifts, guiding the model toward meaningful changes. To bridge the heterogeneity between modalities, we design an Image Text Feature Fusion (ITFF) module that enables deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and SYSUCD demonstrate that MMChange consistently surpasses state of the art methods across multiple metrics, validating its effectiveness for multimodal RSCD. Code is available at: https://github.com/yikuizhai/MMChange.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing",
    "url": "http://arxiv.org/abs/2509.03376v1",
    "authors": [
      "Hui Chen",
      "Liangyu Liu",
      "Xianchao Xiu",
      "Wanquan Liu"
    ],
    "published": "2025-09-03",
    "abstract": "Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote sensing images into a set of endmembers and their corresponding abundances. Despite significant progress in this field using deep learning, most methods fail to simultaneously characterize global dependencies and local consistency, making it difficult to preserve both long-range interactions and boundary details. This letter proposes a novel transformer-guided content-adaptive graph unmixing framework (T-CAGU), which overcomes these challenges by employing a transformer to capture global dependencies and introducing a content-adaptive graph neural network to enhance local relationships. Unlike previous work, T-CAGU integrates multiple propagation orders to dynamically learn the graph structure, ensuring robustness against noise. Furthermore, T-CAGU leverages a graph residual mechanism to preserve global information and stabilize training. Experimental results demonstrate its superiority over the state-of-the-art methods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Information transmission: Inferring change area from change moment in time series remote sensing images",
    "url": "http://arxiv.org/abs/2509.03112v1",
    "authors": [
      "Jialu Li",
      "Chen Wu",
      "Meiqi Hu"
    ],
    "published": "2025-09-03",
    "abstract": "Time series change detection is a critical task for exploring ecosystem dynamics using time series remote sensing images, because it can simultaneously indicate where and when change occur. While deep learning has shown excellent performance in this domain, it continues to approach change area detection and change moment identification as distinct tasks. Given that change area can be inferred from change moment, we propose a time series change detection network, named CAIM-Net (Change Area Inference from Moment Network), to ensure consistency between change area and change moment results. CAIM-Net infers change area from change moment based on the intrinsic relationship between time series analysis and spatial change detection. The CAIM-Net comprises three key steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction, and Fine Change Moment Extraction and Change Area Inference. In the Difference Extraction and Enhancement, a lightweight encoder with batch dimension stacking is designed to rapidly extract difference features. Subsequently, boundary enhancement convolution is applied to amplify these difference features. In the Coarse Change Moment Extraction, the enhanced difference features from the first step are used to spatiotemporal correlation analysis, and then two distinct methods are employed to determine coarse change moments. In the Fine Change Moment Extraction and Change Area Inference, a multiscale temporal Class Activation Mapping (CAM) module first increases the weight of the change-occurring moment from coarse change moments. Then the weighted change moment is used to infer change area based on the fact that pixels with the change moment must have undergone a change.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Time Series Analysis"
    ],
    "is_recent": false
  },
  {
    "title": "HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision",
    "url": "http://arxiv.org/abs/2509.01882v2",
    "authors": [
      "Shubham Laxmikant Deshmukh",
      "Matthew Wilchek",
      "Feras A. Batarseh"
    ],
    "published": "2025-09-02",
    "abstract": "Ongoing advancements in computer vision, particularly in pattern recognition and scene classification, have enabled new applications in environmental monitoring. Deep learning now offers non-contact methods for assessing water quality and detecting contamination, both critical for disaster response and public health protection. This work introduces HydroVision, a deep learning-based scene classification framework that estimates optically active water quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored Dissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and Turbidity from standard Red-Green-Blue (RGB) images of surface water. HydroVision supports early detection of contamination trends and strengthens monitoring by regulatory agencies during external environmental stressors, industrial activities, and force majeure events. The model is trained on more than 500,000 seasonally varied images collected from the United States Geological Survey Hydrologic Imagery Visualization and Information System between 2022 and 2024. This approach leverages widely available RGB imagery as a scalable, cost-effective alternative to traditional multispectral and hyperspectral remote sensing. Four state-of-the-art convolutional neural networks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer are evaluated through transfer learning to identify the best-performing architecture. DenseNet121 achieves the highest validation performance, with an R2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for real-world water quality monitoring across diverse conditions. While the current model is optimized for well-lit imagery, future work will focus on improving robustness under low-light and obstructed scenarios to expand its operational utility.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "VGG",
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Classification",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment",
    "url": "http://arxiv.org/abs/2509.01183v2",
    "authors": [
      "Bingnan Yang",
      "Mi Zhang",
      "Zhili Zhang",
      "Zhan Zhang",
      "Yuanxin Zhao",
      "Xiangyun Hu",
      "Jianya Gong"
    ],
    "published": "2025-09-01",
    "abstract": "High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at https://github.com/Yangbn97/SegAssess.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification",
    "url": "http://arxiv.org/abs/2509.00677v1",
    "authors": [
      "Qingyu Wang",
      "Xue Jiang",
      "Guozheng Xu"
    ],
    "published": "2025-08-31",
    "abstract": "Multimodal fusion has made great progress in the field of remote sensing image classification due to its ability to exploit the complementary spatial-spectral information. Deep learning methods such as CNN and Transformer have been widely used in these domains. State Space Models recently highlighted that prior methods suffer from quadratic computational complexity. As a result, modeling longer-range dependencies of spatial-spectral features imposes an overwhelming burden on the network. Mamba solves this problem by incorporating time-varying parameters into ordinary SSM and performing hardware optimization, but it cannot perform feature fusion directly. In order to make full use of Mamba's low computational burden and explore the potential of internal structure in multimodal feature fusion, we propose Cross State Fusion Mamba (CSFMamba) Network. Specifically, we first design the preprocessing module of remote sensing image information for the needs of Mamba structure, and combine it with CNN to extract multi-layer features. Secondly, a cross-state module based on Mamba operator is creatively designed to fully fuse the feature of the two modalities. The advantages of Mamba and CNN are combined by designing a more powerful backbone. We capture the fusion relationship between HSI and LiDAR modalities with stronger full-image understanding. The experimental results on two datasets of MUUFL and Houston2018 show that the proposed method outperforms the experimental results of Transformer under the premise of reducing the network training burden.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation",
    "url": "http://arxiv.org/abs/2509.00598v2",
    "authors": [
      "Boyi Li",
      "Ce Zhang",
      "Richard M. Timmerman",
      "Wenxuan Bao"
    ],
    "published": "2025-08-30",
    "abstract": "The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global-Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual-Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual-Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "DAOVI: Distortion-Aware Omnidirectional Video Inpainting",
    "url": "http://arxiv.org/abs/2509.00396v1",
    "authors": [
      "Ryosuke Seshimo",
      "Mariko Isogawa"
    ],
    "published": "2025-08-30",
    "abstract": "Omnidirectional videos that capture the entire surroundings are employed in a variety of fields such as VR applications and remote sensing. However, their wide field of view often causes unwanted objects to appear in the videos. This problem can be addressed by video inpainting, which enables the natural removal of such objects while preserving both spatial and temporal consistency. Nevertheless, most existing methods assume processing ordinary videos with a narrow field of view and do not tackle the distortion in equirectangular projection of omnidirectional videos. To address this issue, this paper proposes a novel deep learning model for omnidirectional video inpainting, called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI introduces a module that evaluates temporal motion information in the image space considering geodesic distance, as well as a depth-aware feature propagation module in the feature space that is designed to address the geometric distortion inherent to omnidirectional videos. The experimental results demonstrate that our proposed method outperforms existing methods both quantitatively and qualitatively.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "The point is the mask: scaling coral reef segmentation with weak supervision",
    "url": "http://arxiv.org/abs/2508.18958v1",
    "authors": [
      "Matteo Contini",
      "Victor Illien",
      "Sylvain Poulain",
      "Serge Bernard",
      "Julien Barde",
      "Sylvain Bonhommeau",
      "Alexis Joly"
    ],
    "published": "2025-08-26",
    "abstract": "Monitoring coral reefs at large spatial scales remains an open challenge, essential for assessing ecosystem health and informing conservation efforts. While drone-based aerial imagery offers broad spatial coverage, its limited resolution makes it difficult to reliably distinguish fine-scale classes, such as coral morphotypes. At the same time, obtaining pixel-level annotations over large spatial extents is costly and labor-intensive, limiting the scalability of deep learning-based segmentation methods for aerial imagery. We present a multi-scale weakly supervised semantic segmentation framework that addresses this challenge by transferring fine-scale ecological information from underwater imagery to aerial data. Our method enables large-scale coral reef mapping from drone imagery with minimal manual annotation, combining classification-based supervision, spatial interpolation and self-distillation techniques. We demonstrate the efficacy of the approach, enabling large-area segmentation of coral morphotypes and demonstrating flexibility for integrating new classes. This study presents a scalable, cost-effective methodology for high-resolution reef monitoring, combining low-cost data collection, weakly supervised deep learning and multi-scale remote sensing.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency",
    "url": "http://arxiv.org/abs/2508.18693v1",
    "authors": [
      "Zhitong Cheng",
      "Yiran Jiang",
      "Yulong Ge",
      "Yufeng Li",
      "Zhongheng Qin",
      "Rongzhi Lin",
      "Jianwei Ma"
    ],
    "published": "2025-08-26",
    "abstract": "Domain shift, characterized by degraded model performance during transition from labeled source domains to unlabeled target domains, poses a persistent challenge for deploying deep learning systems. Current unsupervised domain adaptation (UDA) methods predominantly rely on fine-tuning feature extractors - an approach limited by inefficiency, reduced interpretability, and poor scalability to modern architectures.\n  Our analysis reveals that models pretrained on large-scale data exhibit domain-invariant geometric patterns in their feature space, characterized by intra-class clustering and inter-class separation, thereby preserving transferable discriminative structures. These findings indicate that domain shifts primarily manifest as boundary misalignment rather than feature degradation.\n  Unlike fine-tuning entire pre-trained models - which risks introducing unpredictable feature distortions - we propose the Feature-space Planes Searcher (FPS): a novel domain adaptation framework that optimizes decision boundaries by leveraging these geometric patterns while keeping the feature encoder frozen. This streamlined approach enables interpretative analysis of adaptation while substantially reducing memory and computational costs through offline feature extraction, permitting full-dataset optimization in a single computation cycle.\n  Evaluations on public benchmarks demonstrate that FPS achieves competitive or superior performance to state-of-the-art methods. FPS scales efficiently with multimodal large models and shows versatility across diverse domains including protein structure prediction, remote sensing classification, and earthquake detection. We anticipate FPS will provide a simple, effective, and generalizable paradigm for transfer learning, particularly in domain adaptation tasks. .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Robust Small Methane Plume Segmentation in Satellite Imagery",
    "url": "http://arxiv.org/abs/2508.16282v1",
    "authors": [
      "Khai Duc Minh Tran",
      "Hoa Van Nguyen",
      "Aimuni Binti Muhammad Rawi",
      "Hareeshrao Athinarayanarao",
      "Ba-Ngu Vo"
    ],
    "published": "2025-08-22",
    "abstract": "This paper tackles the challenging problem of detecting methane plumes, a potent greenhouse gas, using Sentinel-2 imagery. This contributes to the mitigation of rapid climate change. We propose a novel deep learning solution based on U-Net with a ResNet34 encoder, integrating dual spectral enhancement techniques (Varon ratio and Sanchez regression) to optimise input features for heightened sensitivity. A key achievement is the ability to detect small plumes down to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing traditional methods limited to larger plumes. Experiments show our approach achieves a 78.39% F1-score on the validation set, demonstrating superior performance in sensitivity and precision over existing remote sensing techniques for automated methane monitoring, especially for small plumes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "IRSAMap:Towards Large-Scale, High-Resolution Land Cover Map Vectorization",
    "url": "http://arxiv.org/abs/2508.16272v1",
    "authors": [
      "Yu Meng",
      "Ligao Deng",
      "Zhihao Xi",
      "Jiansheng Chen",
      "Jingbo Chen",
      "Anzhi Yue",
      "Diyou Liu",
      "Kai Li",
      "Chenhao Wang",
      "Kaiyu Li",
      "Yupeng Deng",
      "Xian Sun"
    ],
    "published": "2025-08-22",
    "abstract": "With the enhancement of remote sensing image resolution and the rapid advancement of deep learning, land cover mapping is transitioning from pixel-level segmentation to object-based vector modeling. This shift demands more from deep learning models, requiring precise object boundaries and topological consistency. However, existing datasets face three main challenges: limited class annotations, small data scale, and lack of spatial structural information. To overcome these issues, we introduce IRSAMap, the first global remote sensing dataset for large-scale, high-resolution, multi-feature land cover vector mapping. IRSAMap offers four key advantages: 1) a comprehensive vector annotation system with over 1.8 million instances of 10 typical objects (e.g., buildings, roads, rivers), ensuring semantic and spatial accuracy; 2) an intelligent annotation workflow combining manual and AI-based methods to improve efficiency and consistency; 3) global coverage across 79 regions in six continents, totaling over 1,000 km; and 4) multi-task adaptability for tasks like pixel-level classification, building outline extraction, road centerline extraction, and panoramic segmentation. IRSAMap provides a standardized benchmark for the shift from pixel-based to object-based approaches, advancing geographic feature automation and collaborative modeling. It is valuable for global geographic information updates and digital twin construction. The dataset is publicly available at https://github.com/ucas-dlg/IRSAMap",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "CuMoLoS-MAE: A Masked Autoencoder for Remote Sensing Data Reconstruction",
    "url": "http://arxiv.org/abs/2508.14957v1",
    "authors": [
      "Anurup Naskar",
      "Nathanael Zhixin Wong",
      "Sara Shamekh"
    ],
    "published": "2025-08-20",
    "abstract": "Accurate atmospheric profiles from remote sensing instruments such as Doppler Lidar, Radar, and radiometers are frequently corrupted by low-SNR (Signal to Noise Ratio) gates, range folding, and spurious discontinuities. Traditional gap filling blurs fine-scale structures, whereas deep models lack confidence estimates. We present CuMoLoS-MAE, a Curriculum-Guided Monte Carlo Stochastic Ensemble Masked Autoencoder designed to (i) restore fine-scale features such as updraft and downdraft cores, shear lines, and small vortices, (ii) learn a data-driven prior over atmospheric fields, and (iii) quantify pixel-wise uncertainty. During training, CuMoLoS-MAE employs a mask-ratio curriculum that forces a ViT decoder to reconstruct from progressively sparser context. At inference, we approximate the posterior predictive by Monte Carlo over random mask realisations, evaluating the MAE multiple times and aggregating the outputs to obtain the posterior predictive mean reconstruction together with a finely resolved per-pixel uncertainty map. Together with high-fidelity reconstruction, this novel deep learning-based workflow enables enhanced convection diagnostics, supports real-time data assimilation, and improves long-term climate reanalysis.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives",
    "url": "http://arxiv.org/abs/2508.14558v1",
    "authors": [
      "Juepeng Zheng",
      "Zi Ye",
      "Yibin Wen",
      "Jianxi Huang",
      "Zhiwei Zhang",
      "Qingmei Li",
      "Qiong Hu",
      "Baodong Xu",
      "Lingyuan Zhao",
      "Haohuan Fu"
    ],
    "published": "2025-08-20",
    "abstract": "Powered by advances in multiple remote sensing sensors, the production of high spatial resolution images provides great potential to achieve cost-efficient and high-accuracy agricultural inventory and analysis in an automated way. Lots of studies that aim at providing an inventory of the level of each agricultural parcel have generated many methods for Agricultural Parcel and Boundary Delineation (APBD). This review covers APBD methods for detecting and delineating agricultural parcels and systematically reviews the past and present of APBD-related research applied to remote sensing images. With the goal to provide a clear knowledge map of existing APBD efforts, we conduct a comprehensive review of recent APBD papers to build a meta-data analysis, including the algorithm, the study site, the crop type, the sensor type, the evaluation method, etc. We categorize the methods into three classes: (1) traditional image processing methods (including pixel-based, edge-based and region-based); (2) traditional machine learning methods (such as random forest, decision tree); and (3) deep learning-based methods. With deep learning-oriented approaches contributing to a majority, we further discuss deep learning-based methods like semantic segmentation-based, object detection-based and Transformer-based methods. In addition, we discuss five APBD-related issues to further comprehend the APBD domain using remote sensing data, such as multi-sensor data in APBD task, comparisons between single-task learning and multi-task learning in the APBD domain, comparisons among different algorithms and different APBD tasks, etc. Finally, this review proposes some APBD-related applications and a few exciting prospects and potential hot topics in future APBD research. We hope this review help researchers who involved in APBD domain to keep track of its development and tendency.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning",
    "url": "http://arxiv.org/abs/2508.09555v1",
    "authors": [
      "Ahmet \u00d6ztel",
      "\u0130smet Karaca"
    ],
    "published": "2025-08-13",
    "abstract": "Objective - This study presents a biometric identification method based on topological invariants from 2D iris images, representing iris texture via formally defined digital homology and evaluating classification performance.\n  Methods - Each normalized iris image (48x482 pixels) is divided into grids (e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their ratio using a recent algorithm for homology groups in 2D digital images. The resulting invariants form a feature matrix used with logistic regression, KNN, and SVM (with PCA and 100 randomized repetitions). A convolutional neural network (CNN) is trained on raw images for comparison.\n  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy, outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The topological features showed high accuracy with low variance.\n  Conclusion - This is the first use of topological invariants from formal digital homology for iris recognition. The method offers a compact, interpretable, and accurate alternative to deep learning, useful when explainability or limited data is important. Beyond iris recognition, it can apply to other biometrics, medical imaging, materials science, remote sensing, and interpretable AI. It runs efficiently on CPU-only systems and produces robust, explainable features valuable for security-critical domains.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification",
      "Recognition",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion",
    "url": "http://arxiv.org/abs/2508.06485v2",
    "authors": [
      "Sofiane Bouaziz",
      "Adel Hafiane",
      "Raphael Canals",
      "Rachid Nedjai"
    ],
    "published": "2025-08-08",
    "abstract": "Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a weakly-supervised generative network for daily 10 m LST estimation via spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.05% and improves SSIM by 4.22%. Furthermore, WGAST effectively captures fine-scale thermal patterns, as validated against near-surface air temperature measurements from 33 near-ground sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.05271v1",
    "authors": [
      "Xiaoyang Zhang",
      "Guodong Fan",
      "Guang-Yong Chen",
      "Zhen Hua",
      "Jinjiang Li",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "published": "2025-08-07",
    "abstract": "Change detection in remote sensing imagery plays a vital role in various engineering applications, such as natural disaster monitoring, urban expansion tracking, and infrastructure management. Despite the remarkable progress of deep learning in recent years, most existing methods still rely on spatial-domain modeling, where the limited diversity of feature representations hinders the detection of subtle change regions. We observe that frequency-domain feature modeling particularly in the wavelet domain an amplify fine-grained differences in frequency components, enhancing the perception of edge changes that are challenging to capture in the spatial domain. Thus, we propose a method called Wavelet-Guided Dual-Frequency Encoding (WGDF). Specifically, we first apply Discrete Wavelet Transform (DWT) to decompose the input images into high-frequency and low-frequency components, which are used to model local details and global structures, respectively. In the high-frequency branch, we design a Dual-Frequency Feature Enhancement (DFFE) module to strengthen edge detail representation and introduce a Frequency-Domain Interactive Difference (FDID) module to enhance the modeling of fine-grained changes. In the low-frequency branch, we exploit Transformers to capture global semantic relationships and employ a Progressive Contextual Difference Module (PCDM) to progressively refine change regions, enabling precise structural semantic characterization. Finally, the high- and low-frequency features are synergistically fused to unify local sensitivity with global discriminability. Extensive experiments on multiple remote sensing datasets demonstrate that WGDF significantly alleviates edge ambiguity and achieves superior detection accuracy and robustness compared to state-of-the-art methods. The code will be available at https://github.com/boshizhang123/WGDF.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Deep learning framework for crater detection and identification on the Moon and Mars",
    "url": "http://arxiv.org/abs/2508.03920v1",
    "authors": [
      "Yihan Ma",
      "Zeyang Yu",
      "Rohitash Chandra"
    ],
    "published": "2025-08-05",
    "abstract": "Impact craters are among the most prominent geomorphological features on planetary surfaces and are of substantial significance in planetary science research. Their spatial distribution and morphological characteristics provide critical information on planetary surface composition, geological history, and impact processes. In recent years, the rapid advancement of deep learning models has fostered significant interest in automated crater detection. In this paper, we apply advancements in deep learning models for impact crater detection and identification. We use novel models, including Convolutional Neural Networks (CNNs) and variants such as YOLO and ResNet. We present a framework that features a two-stage approach where the first stage features crater identification using simple classic CNN, ResNet-50 and YOLO. In the second stage, our framework employs YOLO-based detection for crater localisation. Therefore, we detect and identify different types of craters and present a summary report with remote sensing data for a selected region. We consider selected regions for craters and identification from Mars and the Moon based on remote sensing data. Our results indicate that YOLO demonstrates the most balanced crater detection performance, while ResNet-50 excels in identifying large craters with high precision.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "ResNet"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Probabilistic Emissivity Retrieval from Hyperspectral Data via Physics-Guided Variational Inference",
    "url": "http://arxiv.org/abs/2508.08291v2",
    "authors": [
      "Joshua R. Tempelman",
      "Kevin Mitchell",
      "Adam J. Wachtor",
      "Eric B. Flynn"
    ],
    "published": "2025-08-05",
    "abstract": "Recent research has proven neural networks to be a powerful tool for performing hyperspectral imaging (HSI) target identification. However, many deep learning frameworks deliver a single material class prediction and operate on a per-pixel basis; such approaches are limited in their interpretability and restricted to predicting materials that are accessible in available training libraries. In this work, we present an inverse modeling approach in the form of a physics-conditioned generative model.A probabilistic latent-variable model learns the underlying distribution of HSI radiance measurements and produces the conditional distribution of the emissivity spectrum. Moreover, estimates of the HSI scene's atmosphere and background are used as a physically relevant conditioning mechanism to contextualize a given radiance measurement during the encoding and decoding processes. Furthermore, we employ an in-the-loop augmentation scheme and physics-based loss criteria to avoid bias towards a predefined training material set and to encourage the model to learn physically consistent inverse mappings. Monte-Carlo sampling of the model's conditioned posterior delivers a sought emissivity distribution and allows for interpretable uncertainty quantification. Moreover, a distribution-based material matching scheme is presented to return a set of likely material matches for an inferred emissivity distribution. Hence, we present a strategy to incorporate contextual information about a given HSI scene, capture the possible variation of underlying material spectra, and provide interpretable probability measures of a candidate material accounting for given remotely-sensed radiance measurement.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling",
    "url": "http://arxiv.org/abs/2508.03774v1",
    "authors": [
      "Rui Zhu",
      "Yuexing Peng",
      "Peng Wang",
      "George C. Alexandropoulos",
      "Wenbo Wang",
      "Wei Xiang"
    ],
    "published": "2025-08-05",
    "abstract": "Electromagnetic (EM) scattering modeling is critical for radar remote sensing, however, its inherent complexity introduces significant computational challenges. Traditional numerical solvers offer high accuracy, but suffer from scalability issues and substantial computational costs. Pure data-driven deep learning approaches, while efficient, lack physical constraints embedding during training and require extensive labeled data, limiting their applicability and generalization. To overcome these limitations, we propose a U-shaped Physics-Informed Network (U-PINet), the first fully deep-learning-based, physics-informed hierarchical framework for computational EM designed to ensure physical consistency while maximizing computational efficiency. Motivated by the hierarchical decomposition strategy in EM solvers and the inherent sparsity of local EM coupling, the U-PINet models the decomposition and coupling of near- and far-field interactions through a multiscale processing neural network architecture, while employing a physics-inspired sparse graph representation to efficiently model both self- and mutual- coupling among mesh elements of complex $3$-Dimensional (3D) objects. This principled approach enables end-to-end multiscale EM scattering modeling with improved efficiency, generalization, and physical consistency. Experimental results showcase that the U-PINet accurately predicts surface current distributions, achieving close agreement with traditional solver, while significantly reducing computational time and outperforming conventional deep learning baselines in both accuracy and robustness. Furthermore, our evaluations on radar cross section prediction tasks confirm the feasibility of the U-PINet for downstream EM scattering applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MGCR-Net:Multimodal Graph-Conditioned Vision-Language Reconstruction Network for Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2508.01555v1",
    "authors": [
      "Chengming Wang",
      "Guodong Fan",
      "Jinjiang Li",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "published": "2025-08-03",
    "abstract": "With the advancement of remote sensing satellite technology and the rapid progress of deep learning, remote sensing change detection (RSCD) has become a key technique for regional monitoring. Traditional change detection (CD) methods and deep learning-based approaches have made significant contributions to change analysis and detection, however, many outstanding methods still face limitations in the exploration and application of multimodal data. To address this, we propose the multimodal graph-conditioned vision-language reconstruction network (MGCR-Net) to further explore the semantic interaction capabilities of multimodal data. Multimodal large language models (MLLM) have attracted widespread attention for their outstanding performance in computer vision, particularly due to their powerful visual-language understanding and dialogic interaction capabilities. Specifically, we design a MLLM-based optimization strategy to generate multimodal textual data from the original CD images, which serve as textual input to MGCR. Visual and textual features are extracted through a dual encoder framework. For the first time in the RSCD task, we introduce a multimodal graph-conditioned vision-language reconstruction mechanism, which is integrated with graph attention to construct a semantic graph-conditioned reconstruction module (SGCM), this module generates vision-language (VL) tokens through graph-based conditions and enables cross-dimensional interaction between visual and textual features via multihead attention. The reconstructed VL features are then deeply fused using the language vision transformer (LViT), achieving fine-grained feature alignment and high-level semantic interaction. Experimental results on four public datasets demonstrate that MGCR achieves superior performance compared to mainstream CD methods. Our code is available on https://github.com/cn-xvkong/MGCR",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "CGCCE-Net:Change-Guided Cross Correlation Enhancement Network for Remote Sensing Building Change Detection",
    "url": "http://arxiv.org/abs/2508.01549v1",
    "authors": [
      "ChengMing Wang"
    ],
    "published": "2025-08-03",
    "abstract": "Change detection encompasses a variety of task types, and the goal of building change detection (BCD) tasks is to accurately locate buildings and distinguish changed building areas. In recent years, various deep learning-based BCD methods have achieved significant success in detecting difference regions by using different change information enhancement techniques, effectively improving the precision of BCD tasks. To address the issue of BCD with special colors, we propose the change-guided cross correlation enhancement network (CGCCE-Net). We design the change-guided residual refinement (CGRR) Branch, which focuses on extending shallow texture features to multiple scale features obtained from PVT, enabling early attention and acquisition of special colors. Then, channel spatial attention is used in the deep features to achieve independent information enhancement. Additionally, we construct the global cross correlation module (GCCM) to facilitate semantic information interaction between bi-temporal images, establishing building and target recognition relationships between different images. Further semantic feature enhancement is achieved through the semantic cognitive enhancement module (SCEM), and finally, the cross fusion decoder (CFD) is used for change information fusion and image reconstruction. Extensive experiments on three public datasets demonstrate that our CGCCE-Net outperforms mainstream BCD methods with outstanding performance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "SCANet: Split Coordinate Attention Network for Building Footprint Extraction",
    "url": "http://arxiv.org/abs/2507.20809v1",
    "authors": [
      "Chunshi Wang",
      "Bin Zhao",
      "Shuxue Ding"
    ],
    "published": "2025-07-28",
    "abstract": "Building footprint extraction holds immense significance in remote sensing image analysis and has great value in urban planning, land use, environmental protection and disaster assessment. Despite the progress made by conventional and deep learning approaches in this field, they continue to encounter significant challenges. This paper introduces a novel plug-and-play attention module, Split Coordinate Attention (SCA), which ingeniously captures spatially remote interactions by employing two spatial range of pooling kernels, strategically encoding each channel along x and y planes, and separately performs a series of split operations for each feature group, thus enabling more efficient semantic feature extraction. By inserting into a 2D CNN to form an effective SCANet, our SCANet outperforms recent SOTA methods on the public Wuhan University (WHU) Building Dataset and Massachusetts Building Dataset in terms of various metrics. Particularly SCANet achieves the best IoU, 91.61% and 75.49% for the two datasets. Our code is available at https://github.com/AiEson/SCANet",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit",
    "url": "http://arxiv.org/abs/2507.20623v1",
    "authors": [
      "Yang Zhao",
      "Shusheng Li",
      "Xueshang Feng"
    ],
    "published": "2025-07-28",
    "abstract": "As the development of lightweight deep learning algorithms, various deep neural network (DNN) models have been proposed for the remote sensing scene classification (RSSC) application. However, it is still challenging for these RSSC models to achieve optimal performance among model accuracy, inference latency, and energy consumption on resource-constrained edge devices. In this paper, we propose a lightweight RSSC framework, which includes a distilled global filter network (GFNet) model and an early-exit mechanism designed for edge devices to achieve state-of-the-art performance. Specifically, we first apply frequency domain distillation on the GFNet model to reduce model size. Then we design a dynamic early-exit model tailored for DNN models on edge devices to further improve model inference efficiency. We evaluate our E3C model on three edge devices across four datasets. Extensive experimental results show that it achieves an average of 1.3x speedup on model inference and over 40% improvement on energy efficiency, while maintaining high classification accuracy.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges",
    "url": "http://arxiv.org/abs/2507.18376v6",
    "authors": [
      "Xing Hu",
      "Haodong Chen",
      "Qianqian Duan",
      "Dawei Zhang"
    ],
    "published": "2025-07-24",
    "abstract": "With the global population increasing and arable land resources becoming increasingly limited, smart and precision agriculture have emerged as essential directions for sustainable agricultural development. Artificial intelligence (AI), particularly deep learning models, has been widely adopted in applications such as crop monitoring, pest detection, and yield prediction. Among recent generative models, diffusion models have demonstrated considerable potential in agricultural image processing, data augmentation, and remote sensing analysis. Compared to traditional generative adversarial networks (GANs), diffusion models exhibit greater training stability and superior image generation quality, effectively addressing challenges such as limited annotated datasets and imbalanced sample distributions in agricultural scenarios. This paper reviews recent advancements in the application of diffusion models within agriculture, focusing on their roles in crop disease and pest detection, remote sensing image enhancement, crop growth prediction, and agricultural resource management. Diffusion models have been found useful in improving tasks like image generation, denoising, and data augmentation in agriculture, especially when environmental noise or variability is present. While their high computational requirements and limited generalizability across domains remain concerns, the approach is gradually proving effective in real-world applications such as precision crop monitoring. As research progresses, these models may help support sustainable agriculture and address emerging challenges in food systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN",
      "Diffusion Models"
    ],
    "applications": [
      "Detection",
      "Image Generation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels for Building Detection",
    "url": "http://arxiv.org/abs/2507.16657v1",
    "authors": [
      "Shuang Song",
      "Yang Tang",
      "Rongjun Qin"
    ],
    "published": "2025-07-22",
    "abstract": "Deep learning has significantly advanced building segmentation in remote sensing, yet models struggle to generalize on data of diverse geographic regions due to variations in city layouts and the distribution of building types, sizes and locations. However, the amount of time-consuming annotated data for capturing worldwide diversity may never catch up with the demands of increasingly data-hungry models. Thus, we propose a novel approach: re-training models at test time using synthetic data tailored to the target region's city layout. This method generates geo-typical synthetic data that closely replicates the urban structure of a target area by leveraging geospatial data such as street network from OpenStreetMap. Using procedural modeling and physics-based rendering, very high-resolution synthetic images are created, incorporating domain randomization in building shapes, materials, and environmental illumination. This enables the generation of virtually unlimited training samples that maintain the essential characteristics of the target environment. To overcome synthetic-to-real domain gaps, our approach integrates geo-typical data into an adversarial domain adaptation framework for building segmentation. Experiments demonstrate significant performance enhancements, with median improvements of up to 12%, depending on the domain gap. This scalable and cost-effective method blends partial geographic knowledge with synthetic imagery, providing a promising solution to the \"model collapse\" issue in purely synthetic datasets. It offers a practical pathway to improving generalization in remote sensing building segmentation without extensive real-world annotations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing",
    "url": "http://arxiv.org/abs/2507.16228v1",
    "authors": [
      "Shreelekha Revankar",
      "Utkarsh Mall",
      "Cheng Perng Phoo",
      "Kavita Bala",
      "Bharath Hariharan"
    ],
    "published": "2025-07-22",
    "abstract": "Natural disasters cause devastating damage to communities and infrastructure every year. Effective disaster response is hampered by the difficulty of accessing affected areas during and after events. Remote sensing has allowed us to monitor natural disasters in a remote way. More recently there have been advances in computer vision and deep learning that help automate satellite imagery analysis, However, they remain limited by their narrow focus on specific disaster types, reliance on manual expert interpretation, and lack of datasets with sufficient temporal granularity or natural language annotations for tracking disaster progression. We present MONITRS, a novel multimodal dataset of more than 10,000 FEMA disaster events with temporal satellite imagery and natural language annotations from news articles, accompanied by geotagged locations, and question-answer pairs. We demonstrate that fine-tuning existing MLLMs on our dataset yields significant performance improvements for disaster monitoring tasks, establishing a new benchmark for machine learning-assisted disaster response systems. Code can be found at: https://github.com/ShreelekhaR/MONITRS",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery",
    "url": "http://arxiv.org/abs/2507.16849v1",
    "authors": [
      "Yi-Shan Chu",
      "Hsuan-Cheng Wei"
    ],
    "published": "2025-07-21",
    "abstract": "We propose a vision transformer (ViT)-based deep learning framework to refine disaster-affected area segmentation from remote sensing imagery, aiming to support and enhance the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The process starts with a small set of manually annotated regions. We then apply principal component analysis (PCA)-based feature space analysis and construct a confidence index (CI) to expand these labels, producing a weakly supervised training set. These expanded labels are then used to train ViT-based encoder-decoder models with multi-band inputs from Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder variants and multi-stage loss strategies to improve performance under limited supervision. During the evaluation, model predictions are compared with higher-resolution EVAP output to assess spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate that our framework improves the smoothness and reliability of segmentation results, offering a scalable approach for disaster mapping when accurate ground truth is unavailable.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
    "url": "http://arxiv.org/abs/2507.13852v1",
    "authors": [
      "Luigi Russo",
      "Francesco Mauro",
      "Babak Memar",
      "Alessandro Sebastianelli",
      "Silvia Liberata Ullo",
      "Paolo Gamba"
    ],
    "published": "2025-07-18",
    "abstract": "Building segmentation in urban areas is essential in fields such as urban planning, disaster response, and population mapping. Yet accurately segmenting buildings in dense urban regions presents challenges due to the large size and high resolution of satellite images. This study investigates the use of a Quanvolutional pre-processing to enhance the capability of the Attention U-Net model in the building segmentation. Specifically, this paper focuses on the urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR) imagery. In this work, Quanvolution was used to extract more informative feature maps that capture essential structural details in radar imagery, proving beneficial for accurate building segmentation. Preliminary results indicate that proposed methodology achieves comparable test accuracy to the standard Attention U-Net model while significantly reducing network parameters. This result aligns with findings from previous works, confirming that Quanvolution not only maintains model accuracy but also increases computational efficiency. These promising outcomes highlight the potential of quantum-assisted Deep Learning frameworks for large-scale building segmentation in urban environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "A Deep-Learning Framework for Land-Sliding Classification from Remote Sensing Image",
    "url": "http://arxiv.org/abs/2507.12939v1",
    "authors": [
      "Hieu Tang",
      "Truong Vo",
      "Dong Pham",
      "Toan Nguyen",
      "Lam Pham",
      "Truong Nguyen"
    ],
    "published": "2025-07-17",
    "abstract": "The use of satellite imagery combined with deep learning to support automatic landslide detection is becoming increasingly widespread. However, selecting an appropriate deep learning architecture to optimize performance while avoiding overfitting remains a critical challenge. To address these issues, we propose a deep-learning based framework for landslide detection from remote sensing image in this paper. The proposed framework presents an effective combination of the online an offline data augmentation to tackle the imbalanced data, a backbone EfficientNet\\_Large deep learning model for extracting robust embedding features, and a post-processing SVM classifier to balance and enhance the classification performance. The proposed model achieved an F1-score of 0.8938 on the public test set of the Zindi challenge.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Precision Spatio-Temporal Feature Fusion for Robust Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2507.11523v1",
    "authors": [
      "Buddhi Wijenayake",
      "Athulya Ratnayake",
      "Praveen Sumanasekara",
      "Nichula Wasalathilaka",
      "Mathivathanan Piratheepan",
      "Roshan Godaliyadda",
      "Mervyn Ekanayake",
      "Vijitha Herath"
    ],
    "published": "2025-07-15",
    "abstract": "Remote sensing change detection is vital for monitoring environmental and urban transformations but faces challenges like manual feature extraction and sensitivity to noise. Traditional methods and early deep learning models, such as convolutional neural networks (CNNs), struggle to capture long-range dependencies and global context essential for accurate change detection in complex scenes. While Transformer-based models mitigate these issues, their computational complexity limits their applicability in high-resolution remote sensing. Building upon ChangeMamba architecture, which leverages state space models for efficient global context modeling, this paper proposes precision fusion blocks to capture channel-wise temporal variations and per-pixel differences for fine-grained change detection. An enhanced decoder pipeline, incorporating lightweight channel reduction mechanisms, preserves local details with minimal computational cost. Additionally, an optimized loss function combining Cross Entropy, Dice and Lovasz objectives addresses class imbalance and boosts Intersection-over-Union (IoU). Evaluations on SYSU-CD, LEVIR-CD+, and WHU-CD datasets demonstrate superior precision, recall, F1 score, IoU, and overall accuracy compared to state-of-the-art methods, highlighting the approach's robustness for remote sensing change detection. For complete transparency, the codes and pretrained models are accessible at https://github.com/Buddhi19/MambaCD.git",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images",
    "url": "http://arxiv.org/abs/2507.11143v1",
    "authors": [
      "Lam Pham",
      "Cam Le",
      "Hieu Tang",
      "Khang Truong",
      "Truong Nguyen",
      "Jasmin Lampert",
      "Alexander Schindler",
      "Martin Boyer",
      "Son Phan"
    ],
    "published": "2025-07-15",
    "abstract": "In recent years, landslide disasters have reported frequently due to the extreme weather events of droughts, floods , storms, or the consequence of human activities such as deforestation, excessive exploitation of natural resources. However, automatically observing landslide is challenging due to the extremely large observing area and the rugged topography such as mountain or highland. This motivates us to propose an end-to-end deep-learning-based model which explores the remote sensing images for automatically observing landslide events. By considering remote sensing images as the input data, we can obtain free resource, observe large and rough terrains by time. To explore the remote sensing images, we proposed a novel neural network architecture which is for two tasks of landslide detection and landslide segmentation. We evaluated our proposed model on three different benchmark datasets of LandSlide4Sense, Bijie, and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23, 93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense, Nepal datasets. These experimental results prove potential to integrate our proposed model into real-life landslide observation systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks",
    "url": "http://arxiv.org/abs/2507.10381v1",
    "authors": [
      "Aaryam Sharma"
    ],
    "published": "2025-07-14",
    "abstract": "Topological data analysis (TDA) is a relatively new field that is gaining rapid adoption due to its robustness and ability to effectively describe complex datasets by quantifying geometric information. In imaging contexts, TDA typically models data as filtered cubical complexes from which we can extract discriminative features using persistence homology. Meanwhile, convolutional neural networks (CNNs) have been shown to be biased towards texture based local features. To address this limitation, we propose a TDA feature engineering pipeline and a simple method to integrate topological features with deep learning models on remote sensing classification. Our method improves the performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving 99.33% accuracy, which surpasses all previously reported single-model accuracies, including those with larger architectures, such as ResNet50 (2x larger) and XL Vision Transformers (197x larger). We additionally show that our method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45 dataset. To our knowledge, this is the first application of TDA features in satellite scene classification with deep learning. This demonstrates that TDA features can be integrated with deep learning models, even on datasets without explicit topological structures, thereby increasing the applicability of TDA. A clean implementation of our method will be made publicly available upon publication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection",
    "url": "http://arxiv.org/abs/2507.09541v1",
    "authors": [
      "Zihao Xiong",
      "Fei Zhou",
      "Fengyi Wu",
      "Shuai Yuan",
      "Maixia Fu",
      "Zhenming Peng",
      "Jian Yang",
      "Yimian Dai"
    ],
    "published": "2025-07-13",
    "abstract": "Infrared small target detection plays a vital role in remote sensing, industrial monitoring, and various civilian applications. Despite recent progress powered by deep learning, many end-to-end convolutional models tend to pursue performance by stacking increasingly complex architectures, often at the expense of interpretability, parameter efficiency, and generalization. These models typically overlook the intrinsic sparsity prior of infrared small targets--an essential cue that can be explicitly modeled for both performance and efficiency gains. To address this, we revisit the model-based paradigm of Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network (DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware prior into a learnable architecture. Unlike conventional deep unfolding methods that rely on static, globally learned parameters, DRPCA-Net introduces a dynamic unfolding mechanism via a lightweight hypernetwork. This design enables the model to adaptively generate iteration-wise parameters conditioned on the input scene, thereby enhancing its robustness and generalization across diverse backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to better capture contextual variations within the background, leading to more accurate low-rank estimation and improved separation of small targets. Extensive experiments on multiple public infrared datasets demonstrate that DRPCA-Net significantly outperforms existing state-of-the-art methods in detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer",
    "url": "http://arxiv.org/abs/2507.08741v2",
    "authors": [
      "Tianlong Ai",
      "Tianzhu Liu",
      "Haochen Jiang",
      "Yanfeng Gu"
    ],
    "published": "2025-07-11",
    "abstract": "Hierarchical land cover and land use (LCLU) classification aims to assign pixel-wise labels with multiple levels of semantic granularity to remote sensing (RS) imagery. However, existing deep learning-based methods face two major challenges: 1) They predominantly adopt a flat classification paradigm, which limits their ability to generate end-to-end multi-granularity hierarchical predictions aligned with tree-structured hierarchies used in practice. 2) Most cross-domain studies focus on performance degradation caused by sensor or scene variations, with limited attention to transferring LCLU models to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop classification). These limitations hinder the flexibility and generalization of LCLU models in practical applications. To address these challenges, we propose HieraRS, a novel hierarchical interpretation paradigm that enables multi-granularity predictions and supports the efficient transfer of LCLU models to cross-domain tasks with heterogeneous tree-structured hierarchies. We introduce the Bidirectional Hierarchical Consistency Constraint Mechanism (BHCCM), which can be seamlessly integrated into mainstream flat classification models to generate hierarchical predictions, while improving both semantic consistency and classification accuracy. Furthermore, we present TransLU, a dual-branch cross-domain transfer framework comprising two key components: Cross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment (CDSA). TransLU supports dynamic category expansion and facilitates the effective adaptation of LCLU models to heterogeneous hierarchies. In addition, we construct MM-5B, a large-scale multi-modal hierarchical land use dataset featuring pixel-wise annotations. The code and MM-5B dataset will be released at: https://github.com/AI-Tianlong/HieraRS.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Ecological Legacies of Pre-Columbian Settlements Evident in Palm Clusters of Neotropical Mountain Forests",
    "url": "http://arxiv.org/abs/2507.06949v2",
    "authors": [
      "Sebastian Fajardo",
      "Sina Mohammadi",
      "Jonas Gregorio de Souza",
      "C\u00e9sar Ardila",
      "Alan Tapscott Baltar",
      "Shaddai Heidgen",
      "Maria Isabel Mayorga Hern\u00e1ndez",
      "Sylvia Mota de Oliveira",
      "Fernando Montejo",
      "Marco Moderato",
      "Vinicius Peripato",
      "Katy Puche",
      "Carlos Reina",
      "Juan Carlos Vargas",
      "Frank W. Takes",
      "Marco Madella"
    ],
    "published": "2025-07-09",
    "abstract": "Ancient populations markedly transformed Neotropical forests, yet the spatial extent of their ecological influence remains underexplored at high resolution. Here we present a deep learning and remote sensing based approach to estimate areas of pre-Columbian forest modification based on modern vegetation. We apply this method to high-resolution satellite imagery from the Sierra Nevada de Santa Marta, Colombia, as a demonstration of a scalable approach, to evaluate palm tree distributions in relation to archaeological infrastructure. Palms were significantly more abundant near archaeological sites with large infrastructure investment. The extent of the largest palm cluster indicates that ancient human-managed areas linked to major infrastructure sites may be up to two orders of magnitude bigger than indicated by current archaeological evidence alone. Our findings suggest that pre-Columbian populations influenced vegetation, fostering conditions conducive to palm proliferation, leaving a lasting ecological footprint. This may have lowered the logistical costs of establishing infrastructure-heavy settlements in less accessible locations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery",
    "url": "http://arxiv.org/abs/2507.01747v1",
    "authors": [
      "Nora Gourmelon",
      "Marcel Dreier",
      "Martin Mayr",
      "Thorsten Seehaus",
      "Dakota Pyles",
      "Matthias Braun",
      "Andreas Maier",
      "Vincent Christlein"
    ],
    "published": "2025-07-02",
    "abstract": "Glaciers are losing ice mass at unprecedented rates, increasing the need for accurate, year-round monitoring to understand frontal ablation, particularly the factors driving the calving process. Deep learning models can extract calving front positions from Synthetic Aperture Radar imagery to track seasonal ice losses at the calving fronts of marine- and lake-terminating glaciers. The current state-of-the-art model relies on ImageNet-pretrained weights. However, they are suboptimal due to the domain shift between the natural images in ImageNet and the specialized characteristics of remote sensing imagery, in particular for Synthetic Aperture Radar imagery. To address this challenge, we propose two novel self-supervised multimodal pretraining techniques that leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14 Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the dataset. Additionally, we introduce a novel hybrid model architecture that combines a Swin Transformer encoder with a residual Convolutional Neural Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean distance error of 293 m on the \"CAlving Fronts and where to Find thEm\" (CaFFe) benchmark dataset, outperforming the prior best model by 67 m. Evaluating an ensemble of the proposed model on a multi-annotator study of the benchmark dataset reveals a mean distance error of 75 m, approaching the human performance of 38 m. This advancement enables precise monitoring of seasonal changes in glacier calving fronts.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images",
    "url": "http://arxiv.org/abs/2507.01502v1",
    "authors": [
      "Ozan Durgut",
      "Beril Kallfelz-Sirmacek",
      "Cem Unsalan"
    ],
    "published": "2025-07-02",
    "abstract": "Global warming, loss of biodiversity, and air pollution are among the most significant problems facing Earth. One of the primary challenges in addressing these issues is the lack of monitoring forests to protect them. To tackle this problem, it is important to leverage remote sensing and computer vision methods to automate monitoring applications. Hence, automatic tree crown detection algorithms emerged based on traditional and deep learning methods. In this study, we first introduce two different tree crown detection methods based on these approaches. Then, we form a novel rule-based approach that integrates these two methods to enhance robustness and accuracy of tree crown detection results. While traditional methods are employed for feature extraction and segmentation of forested areas, deep learning methods are used to detect tree crowns in our method. With the proposed rule-based approach, we post-process these results, aiming to increase the number of detected tree crowns through neighboring trees and localized operations. We compare the obtained results with the proposed method in terms of the number of detected tree crowns and report the advantages, disadvantages, and areas for improvement of the obtained outcomes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions",
    "url": "http://arxiv.org/abs/2507.01123v1",
    "authors": [
      "Rahul A. Burange",
      "Harsh K. Shinde",
      "Omkar Mutyalwar"
    ],
    "published": "2025-07-01",
    "abstract": "Landslides pose severe threats to infrastructure, economies, and human lives, necessitating accurate detection and predictive mapping across diverse geographic regions. With advancements in deep learning and remote sensing, automated landslide detection has become increasingly effective. This study presents a comprehensive approach integrating multi-source satellite imagery and deep learning models to enhance landslide identification and prediction. We leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and Digital Elevation Model (DEM) layers to capture critical environmental features influencing landslide occurrences. Various geospatial analysis techniques are employed to assess the impact of terra in characteristics, vegetation cover, and rainfall on detection accuracy. Additionally, we evaluate the performance of multiple stateof-the-art deep learning segmentation models, including U-Net, DeepLabV3+, and Res-Net, to determine their effectiveness in landslide detection. The proposed framework contributes to the development of reliable early warning systems, improved disaster risk management, and sustainable land-use planning. Our findings provide valuable insights into the potential of deep learning and multi-source remote sensing in creating robust, scalable, and transferable landslide prediction models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Farm-Level, In-Season Crop Identification for India",
    "url": "http://arxiv.org/abs/2507.02972v1",
    "authors": [
      "Ishan Deshpande",
      "Amandeep Kaur Reehal",
      "Chandan Nath",
      "Renu Singh",
      "Aayush Patel",
      "Aishwarya Jayagopal",
      "Gaurav Singh",
      "Gaurav Aggarwal",
      "Amit Agarwal",
      "Prathmesh Bele",
      "Sridhar Reddy",
      "Tanya Warrier",
      "Kinjal Singh",
      "Ashish Tendulkar",
      "Luis Pazos Outon",
      "Nikita Saxena",
      "Agata Dondzik",
      "Dinesh Tewari",
      "Shruti Garg",
      "Avneet Singh",
      "Harsh Dhand",
      "Vaibhav Rajan",
      "Alok Talekar"
    ],
    "published": "2025-06-30",
    "abstract": "Accurate, timely, and farm-level crop type information is paramount for national food security, agricultural policy formulation, and economic planning, particularly in agriculturally significant nations like India. While remote sensing and machine learning have become vital tools for crop monitoring, existing approaches often grapple with challenges such as limited geographical scalability, restricted crop type coverage, the complexities of mixed-pixel and heterogeneous landscapes, and crucially, the robust in-season identification essential for proactive decision-making.\n  We present a framework designed to address the critical data gaps for targeted data driven decision making which generates farm-level, in-season, multi-crop identification at national scale (India) using deep learning. Our methodology leverages the strengths of Sentinel-1 and Sentinel-2 satellite imagery, integrated with national-scale farm boundary data. The model successfully identifies 12 major crops (which collectively account for nearly 90% of India's total cultivated area showing an agreement with national crop census 2023-24 of 94% in winter, and 75% in monsoon season). Our approach incorporates an automated season detection algorithm, which estimates crop sowing and harvest periods. This allows for reliable crop identification as early as two months into the growing season and facilitates rigorous in-season performance evaluation. Furthermore, we have engineered a highly scalable inference pipeline, culminating in what is, to our knowledge, the first pan-India, in-season, farm-level crop type data product. The system's effectiveness and scalability are demonstrated through robust validation against national agricultural statistics, showcasing its potential to deliver actionable, data-driven insights for transformative agricultural monitoring and management across India.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data",
    "url": "http://arxiv.org/abs/2506.22939v1",
    "authors": [
      "Ghufran A. Omran",
      "Wassan Saad Abduljabbar Hayale",
      "Ahmad AbdulQadir AlRababah",
      "Israa Ibraheem Al-Barazanchi",
      "Ravi Sekhar",
      "Pritesh Shah",
      "Sushma Parihar",
      "Harshavardhan Reddy Penubadi"
    ],
    "published": "2025-06-28",
    "abstract": "Scene categorization (SC) in remotely acquired images is an important subject with broad consequences in different fields, including catastrophe control, ecological observation, architecture for cities, and more. Nevertheless, its several apps, reaching a high degree of accuracy in SC from distant observation data has demonstrated to be difficult. This is because traditional conventional deep learning models require large databases with high variety and high levels of noise to capture important visual features. To address these problems, this investigation file introduces an innovative technique referred to as the Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type of scenes in remote sensing data. The investigation compares the execution of CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory (CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF), Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional Neural Networks Data Augmentation (CNN-DA). The results demonstrate that CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%, MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance of physical confirmation to ensure the efficiency of satellite data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake",
    "url": "http://arxiv.org/abs/2506.22338v1",
    "authors": [
      "Luigi Russo",
      "Deodato Tapete",
      "Silvia Liberata Ullo",
      "Paolo Gamba"
    ],
    "published": "2025-06-27",
    "abstract": "Building damage identification shortly after a disaster is crucial for guiding emergency response and recovery efforts. Although optical satellite imagery is commonly used for disaster mapping, its effectiveness is often hampered by cloud cover or the absence of pre-event acquisitions. To overcome these challenges, we introduce a novel multimodal deep learning (DL) framework for detecting building damage using single-date very high resolution (VHR) Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI) COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data. Our method integrates SAR image patches, OpenStreetMap (OSM) building footprints, digital surface model (DSM) data, and structural and exposure attributes from the Global Earthquake Model (GEM) to improve detection accuracy and contextual interpretation. Unlike existing approaches that depend on pre and post event imagery, our model utilizes only post event data, facilitating rapid deployment in critical scenarios. The framework effectiveness is demonstrated using a new dataset from the 2023 earthquake in Turkey, covering multiple cities with diverse urban settings. Results highlight that incorporating geospatial features significantly enhances detection performance and generalizability to previously unseen areas. By combining SAR imagery with detailed vulnerability and exposure information, our approach provides reliable and rapid building damage assessments without the dependency from available pre-event data. Moreover, the automated and scalable data generation process ensures the framework's applicability across diverse disaster-affected regions, underscoring its potential to support effective disaster management and recovery efforts. Code and data will be made available upon acceptance of the paper.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images",
    "url": "http://arxiv.org/abs/2506.21945v1",
    "authors": [
      "Naftaly Wambugu",
      "Ruisheng Wang",
      "Bo Guo",
      "Tianshu Yu",
      "Sheng Xu",
      "Mohammed Elhassan"
    ],
    "published": "2025-06-27",
    "abstract": "Land cover maps generated from semantic segmentation of high-resolution remotely sensed images have drawn mucon in the photogrammetry and remote sensing research community. Currently, massive fine-resolution remotely sensed (FRRS) images acquired by improving sensing and imaging technologies become available. However, accurate semantic segmentation of such FRRS images is greatly affected by substantial class disparities, the invisibility of key ground objects due to occlusion, and object size variation. Despite the extraordinary potential in deep convolutional neural networks (DCNNs) in image feature learning and representation, extracting sufficient features from FRRS images for accurate semantic segmentation is still challenging. These challenges demand the deep learning models to learn robust features and generate sufficient feature descriptors. Specifically, learning multi-contextual features to guarantee adequate coverage of varied object sizes from the ground scene and harnessing global-local contexts to overcome class disparities challenge even profound networks. Deeper networks significantly lose spatial details due to gradual downsampling processes resulting in poor segmentation results and coarse boundaries. This article presents a stacked deep residual network (SDRNet) for semantic segmentation from FRRS images. The proposed framework utilizes two stacked encoder-decoder networks to harness long-range semantics yet preserve spatial information and dilated residual blocks (DRB) between each encoder and decoder network to capture sufficient global dependencies thus improving segmentation performance. Our experimental results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate that the SDRNet performs effectively and competitively against current DCNNs in semantic segmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer",
    "url": "http://arxiv.org/abs/2506.21880v2",
    "authors": [
      "Yuansheng Li",
      "Yunhao Zou",
      "Linwei Chen",
      "Ying Fu"
    ],
    "published": "2025-06-27",
    "abstract": "Interferometric Hyperspectral Imaging (IHI) is a critical technique for large-scale remote sensing tasks due to its advantages in flux and spectral resolution. However, IHI is susceptible to complex errors arising from imaging steps, and its quality is limited by existing signal processing-based reconstruction algorithms. Two key challenges hinder performance enhancement: 1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific degradation components through learning-based methods. To address these challenges, we propose a novel IHI reconstruction pipeline. First, based on imaging physics and radiometric calibration data, we establish a simplified yet accurate IHI degradation model and a parameter estimation method. This model enables the synthesis of realistic IHI training datasets from hyperspectral images (HSIs), bridging the gap between IHI reconstruction and deep learning. Second, we design the Interferometric Hyperspectral Reconstruction Unfolding Transformer (IHRUT), which achieves effective spectral correction and detail restoration through a stripe-pattern enhancement mechanism and a spatial-spectral transformer architecture. Experimental results demonstrate the superior performance and generalization capability of our method.The code and are available at https://github.com/bit1120203554/IHRUT.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2506.21109v2",
    "authors": [
      "Luosheng Xu",
      "Dalin Zhang",
      "Zhaohui Song"
    ],
    "published": "2025-06-26",
    "abstract": "Remote sensing change detection is essential for monitoring urban expansion, disaster assessment, and resource management, offering timely, accurate, and large-scale insights into dynamic landscape transformations. While deep learning has revolutionized change detection, the increasing complexity and computational demands of modern models have not necessarily translated into significant accuracy gains. Instead of following this trend, this study explores a more efficient approach, focusing on lightweight models that maintain high accuracy while minimizing resource consumption, which is an essential requirement for on-satellite processing. To this end, we propose FlickCD, which means quick flick then get great results, pushing the boundaries of the performance-resource trade-off. FlickCD introduces an Enhanced Difference Module (EDM) to amplify critical feature differences between temporal phases while suppressing irrelevant variations such as lighting and weather changes, thereby reducing computational costs in the subsequent change decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion Blocks, leveraging Shifted Window Self-Attention (SWSA) and Efficient Global Self-Attention (EGSA) to effectively capture semantic information at multiple scales, preserving both coarse- and fine-grained changes. Extensive experiments on four benchmark datasets demonstrate that FlickCD reduces computational and storage overheads by more than an order of magnitude while achieving state-of-the-art (SOTA) performance or incurring only a minor (<1% F1) accuracy trade-off. The implementation code is publicly available at https://github.com/xulsh8/FlickCD.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Photon Absorption Remote Sensing (PARS): Comprehensive Absorption Imaging Enabling Label-Free Biomolecule Characterization and Mapping",
    "url": "http://arxiv.org/abs/2506.20069v1",
    "authors": [
      "Benjamin R. Ecclestone",
      "James A. Tummon Simmons",
      "James E. D. Tweel",
      "Deepak Dinakaran",
      "Parsin Haji Reza"
    ],
    "published": "2025-06-25",
    "abstract": "Label-free optical absorption microscopy techniques continue to evolve as promising tools for label-free histopathological imaging of cells and tissues. However, critical challenges relating to specificity and contrast, as compared to current gold-standard methods continue to hamper adoption. This work introduces Photon Absorption Remote Sensing (PARS), a new absorption microscope modality, which simultaneously captures the dominant de-excitation processes following an absorption event. In PARS, radiative (auto-fluorescence) and non-radiative (photothermal and photoacoustic) relaxation processes are collected simultaneously, providing enhanced specificity to a range of biomolecules. As an example, a multiwavelength PARS system featuring UV (266 nm) and visible (532 nm) excitation is applied to imaging human skin, and murine brain tissue samples. It is shown that PARS can directly characterize, differentiate, and unmix, clinically relevant biomolecules inside complex tissues samples using established statistical processing methods. Gaussian mixture models (GMM) are used to characterize clinically relevant biomolecules (e.g., white, and gray matter) based on their PARS signals, while non-negative least squares (NNLS) is applied to map the biomolecule abundance in murine brain tissues, without stained ground truth images or deep-learning methods. PARS unmixing and abundance estimates are directly validated and compared against chemically stained ground truth images, and deep learning based-image transforms. Overall, it is found that the PARS unique and rich contrast may provide comprehensive, and otherwise inaccessible, label-free characterization of molecular pathology, representing a new source of data to develop AI and machine learning methods for diagnostics and visualization.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Video Compression for Spatiotemporal Earth System Data",
    "url": "http://arxiv.org/abs/2506.19656v1",
    "authors": [
      "Oscar J. Pellicer-Valero",
      "Cesar Aybar",
      "Gustau Camps Valls"
    ],
    "published": "2025-06-24",
    "abstract": "Large-scale Earth system datasets, from high-resolution remote sensing imagery to spatiotemporal climate model outputs, exhibit characteristics analogous to those of standard videos. Their inherent spatial, temporal, and spectral redundancies can thus be readily exploited by established video compression techniques. Here, we present xarrayvideo, a Python library for compressing multichannel spatiotemporal datasets by encoding them as videos. Our approach achieves compression ratios of up to 250x while maintaining high fidelity by leveraging standard, well-optimized video codecs through ffmpeg. We demonstrate the library's effectiveness on four real-world multichannel spatiotemporal datasets: DynamicEarthNet (very high resolution Planet images), DeepExtremeCubes (high resolution Sentinel-2 images), ERA5 (weather reanalysis data), and the SimpleS2 dataset (high resolution multichannel Sentinel-2 images), achieving Peak Signal-to-Noise Ratios (PSNRs) of 55.86, 40.60, 46.58, and 43.23 dB at 0.1 bits per pixel per band (bpppb) and 65.91, 54.28, 62.90, and 55.04 dB at 1 bpppb. We are redistributing two of these datasets, DeepExtremeCubes (2.3 Tb) and DynamicEarthNet (525 Gb), in the machine-learning-ready and cloud-ready TACO format through HuggingFace at significantly reduced sizes (270 Gb and 8.5 Gb, respectively) without compromising quality (PSNR 55.77-56.65 and 60.15). No performance loss is observed when the compressed versions of these datasets are used in their respective deep learning-based downstream tasks (next step reflectance prediction and landcover segmentation). In conclusion, xarrayvideo presents an efficient solution for handling the rapidly growing size of Earth observation datasets, making advanced compression techniques accessible and practical to the Earth science community. The library is available for use at https://github.com/IPL-UV/xarrayvideo",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MambaOutRS: A Hybrid CNN-Fourier Architecture for Remote Sensing Image Classification",
    "url": "http://arxiv.org/abs/2506.19561v1",
    "authors": [
      "Minjong Cheon",
      "Changbae Mun"
    ],
    "published": "2025-06-24",
    "abstract": "Recent advances in deep learning for vision tasks have seen the rise of State Space Models (SSMs) like Mamba, celebrated for their linear scalability. However, their adaptation to 2D visual data often necessitates complex modifications that may diminish efficiency. In this paper, we introduce MambaOutRS, a novel hybrid convolutional architecture for remote sensing image classification that re-evaluates the necessity of recurrent SSMs. MambaOutRS builds upon stacked Gated CNN blocks for local feature extraction and introduces a novel Fourier Filter Gate (FFG) module that operates in the frequency domain to capture global contextual information efficiently. Our architecture employs a four-stage hierarchical design and was extensively evaluated on challenging remote sensing datasets: UC Merced, AID, NWPU-RESISC45, and EuroSAT. MambaOutRS consistently achieved state-of-the-art (SOTA) performance across these benchmarks. Notably, our MambaOutRS-t variant (24.0M parameters) attained the highest F1-scores of 98.41\\% on UC Merced and 95.99\\% on AID, significantly outperforming existing baselines, including larger transformer models and Mamba-based architectures, despite using considerably fewer parameters. An ablation study conclusively demonstrates the critical role of the Fourier Filter Gate in enhancing the model's ability to capture global spatial patterns, leading to robust and accurate classification. These results strongly suggest that the complexities of recurrent SSMs can be effectively superseded by a judicious combination of gated convolutions for spatial mixing and frequency-based gates for spectral global context. Thus, MambaOutRS provides a compelling and efficient paradigm for developing high-performance deep learning models in remote sensing and other vision domains, particularly where computational efficiency is paramount.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Mapping Farmed Landscapes from Remote Sensing",
    "url": "http://arxiv.org/abs/2506.13993v2",
    "authors": [
      "Michelangelo Conserva",
      "Alex Wilson",
      "Charlotte Stanton",
      "Vishal Batchu",
      "Varun Gulshan"
    ],
    "published": "2025-06-16",
    "abstract": "Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\\%) and farmed land (95\\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data",
    "url": "http://arxiv.org/abs/2506.13505v1",
    "authors": [
      "Vasiliki Balaska",
      "Ioannis Tsampikos Papapetros",
      "Katerina Maria Oikonomou",
      "Loukas Bampis",
      "Antonios Gasteratos"
    ],
    "published": "2025-06-16",
    "abstract": "The mining sector increasingly adopts digital tools to improve operational efficiency, safety, and data-driven decision-making. One of the key challenges remains the reliable acquisition of high-resolution, geo-referenced spatial information to support core activities such as extraction planning and on-site monitoring. This work presents an integrated system architecture that combines UAV-based sensing, LiDAR terrain modeling, and deep learning-based object detection to generate spatially accurate information for open-pit mining environments. The proposed pipeline includes geo-referencing, 3D reconstruction, and object localization, enabling structured spatial outputs to be integrated into an industrial digital twin platform. Unlike traditional static surveying methods, the system offers higher coverage and automation potential, with modular components suitable for deployment in real-world industrial contexts. While the current implementation operates in post-flight batch mode, it lays the foundation for real-time extensions. The system contributes to the development of AI-enhanced remote sensing in mining by demonstrating a scalable and field-validated geospatial data workflow that supports situational awareness and infrastructure safety.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images",
    "url": "http://arxiv.org/abs/2506.09553v1",
    "authors": [
      "Ligao Deng",
      "Yupeng Deng",
      "Yu Meng",
      "Jingbo Chen",
      "Zhihao Xi",
      "Diyou Liu",
      "Qifeng Chu"
    ],
    "published": "2025-06-11",
    "abstract": "Road networks are crucial for mapping, autonomous driving, and disaster response. While manual annotation is costly, deep learning offers efficient extraction. Current methods include postprocessing (prone to errors), global parallel (fast but misses nodes), and local iterative (accurate but slow). We propose GLD-Road, a two-stage model combining global efficiency and local precision. First, it detects road nodes and connects them via a Connect Module. Then, it iteratively refines broken roads using local searches, drastically reducing computation. Experiments show GLD-Road outperforms state-of-the-art methods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also reduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++ (local). The experimental results are available at https://github.com/ucas-dlg/GLD-Road.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery",
    "url": "http://arxiv.org/abs/2506.06667v1",
    "authors": [
      "Yu-Hsuan Ho",
      "Ali Mostafavi"
    ],
    "published": "2025-06-07",
    "abstract": "Most post-disaster damage classifiers succeed only when destructive forces leave clear spectral or structural signatures -- conditions rarely present after inundation. Consequently, existing models perform poorly at identifying flood-related building damages. The model presented in this study, Flood-DamageSense, addresses this gap as the first deep-learning framework purpose-built for building-level flood-damage assessment. The architecture fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical basemaps and an inherent flood-risk layer that encodes long-term exposure probabilities, guiding the network toward plausibly affected structures even when compositional change is minimal. A multimodal Mamba backbone with a semi-Siamese encoder and task-specific decoders jointly predicts (1) graded building-damage states, (2) floodwater extent, and (3) building footprints. Training and evaluation on Hurricane Harvey (2017) imagery from Harris County, Texas -- supported by insurance-derived property-damage extents -- show a mean F1 improvement of up to 19 percentage points over state-of-the-art baselines, with the largest gains in the frequently misclassified \"minor\" and \"moderate\" damage categories. Ablation studies identify the inherent-risk feature as the single most significant contributor to this performance boost. An end-to-end post-processing pipeline converts pixel-level outputs to actionable, building-scale damage maps within minutes of image acquisition. By combining risk-aware modeling with SAR's all-weather capability, Flood-DamageSense delivers faster, finer-grained, and more reliable flood-damage intelligence to support post-disaster decision-making and resource allocation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation",
    "url": "http://arxiv.org/abs/2506.05444v1",
    "authors": [
      "Marwane Kzadri",
      "Franco Alberto Cardillo",
      "Nan\u00e9e Chahinian",
      "Carole Delenne",
      "Renaud Hostache",
      "Jamal Riffi"
    ],
    "published": "2025-06-05",
    "abstract": "Segmenting Synthetic Aperture Radar (SAR) images is crucial for many remote sensing applications, particularly water body detection. However, deep learning-based segmentation models often face challenges related to convergence speed and stability, mainly due to the complex statistical distribution of this type of data. In this study, we evaluate the impact of mode normalization on two widely used semantic segmentation models, U-Net and SegNet. Specifically, we integrate mode normalization, to reduce convergence time while maintaining the performance of the baseline models. Experimental results demonstrate that mode normalization significantly accelerates convergence. Furthermore, cross-validation results indicate that normalized models exhibit increased stability in different zones. These findings highlight the effectiveness of normalization in improving computational efficiency and generalization in SAR image segmentation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Image Restoration Learning via Noisy Supervision in the Fourier Domain",
    "url": "http://arxiv.org/abs/2506.00564v1",
    "authors": [
      "Haosen Liu",
      "Jiahao Liu",
      "Shan Tan",
      "Edmund Y. Lam"
    ],
    "published": "2025-05-31",
    "abstract": "Noisy supervision refers to supervising image restoration learning with noisy targets. It can alleviate the data collection burden and enhance the practical applicability of deep learning techniques. However, existing methods suffer from two key drawbacks. Firstly, they are ineffective in handling spatially correlated noise commonly observed in practical applications such as low-light imaging and remote sensing. Secondly, they rely on pixel-wise loss functions that only provide limited supervision information. This work addresses these challenges by leveraging the Fourier domain. We highlight that the Fourier coefficients of spatially correlated noise exhibit sparsity and independence, making them easier to handle. Additionally, Fourier coefficients contain global information, enabling more significant supervision. Motivated by these insights, we propose to establish noisy supervision in the Fourier domain. We first prove that Fourier coefficients of a wide range of noise converge in distribution to the Gaussian distribution. Exploiting this statistical property, we establish the equivalence between using noisy targets and clean targets in the Fourier domain. This leads to a unified learning framework applicable to various image restoration tasks, diverse network architectures, and different noise models. Extensive experiments validate the outstanding performance of this framework in terms of both quantitative indices and perceptual quality.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Segmenting France Across Four Centuries",
    "url": "http://arxiv.org/abs/2505.24824v1",
    "authors": [
      "Marta L\u00f3pez-Rauhut",
      "Hongyu Zhou",
      "Mathieu Aubry",
      "Loic Landrieu"
    ],
    "published": "2025-05-30",
    "abstract": "Historical maps offer an invaluable perspective into territory evolution across past centuries--long before satellite or remote sensing technologies existed. Deep learning methods have shown promising results in segmenting historical maps, but publicly available datasets typically focus on a single map type or period, require extensive and costly annotations, and are not suited for nationwide, long-term analyses. In this paper, we introduce a new dataset of historical maps tailored for analyzing large-scale, long-term land use and land cover evolution with limited annotations. Spanning metropolitan France (548,305 km^2), our dataset contains three map collections from the 18th, 19th, and 20th centuries. We provide both comprehensive modern labels and 22,878 km^2 of manually annotated historical labels for the 18th and 19th century maps. Our dataset illustrates the complexity of the segmentation task, featuring stylistic inconsistencies, interpretive ambiguities, and significant landscape changes (e.g., marshlands disappearing in favor of forests). We assess the difficulty of these challenges by benchmarking three approaches: a fully-supervised model trained with historical labels, and two weakly-supervised models that rely only on modern annotations. The latter either use the modern labels directly or first perform image-to-image translation to address the stylistic gap between historical and contemporary maps. Finally, we discuss how these methods can support long-term environment monitoring, offering insights into centuries of landscape transformation. Our official project repository is publicly available at https://github.com/Archiel19/FRAx4.git.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing",
    "url": "http://arxiv.org/abs/2505.24489v1",
    "authors": [
      "Anasse Boutayeb",
      "Iyad Lahsen-cherif",
      "Ahmed El Khadimi"
    ],
    "published": "2025-05-30",
    "abstract": "Object detection has recently seen an interesting trend in terms of the most innovative research work, this task being of particular importance in the field of remote sensing, given the consistency of these images in terms of geographical coverage and the objects present. Furthermore, Deep Learning (DL) models, in particular those based on Transformers, are especially relevant for visual computing tasks in general, and target detection in particular. Thus, the present work proposes an application of Deformable-DETR model, a specific architecture using deformable attention mechanisms, on remote sensing images in two different modes, especially optical and Synthetic Aperture Radar (SAR). To achieve this objective, two datasets are used, one optical, which is Pleiades Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset (SSDD). The results of a 10-fold stratified validation showed that the proposed model performed particularly well, obtaining an F1 score of 95.12% for the optical dataset and 94.54% for SSDD, while comparing these results with several models detections, especially those based on CNNs and transformers, as well as those specifically designed to detect different object classes in remote sensing images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising",
    "url": "http://arxiv.org/abs/2505.24327v1",
    "authors": [
      "Jingjing Liu",
      "Jiashun Jin",
      "Xianchao Xiu",
      "Jianhua Zhang",
      "Wanquan Liu"
    ],
    "published": "2025-05-30",
    "abstract": "Remote sensing image (RSI) denoising is an important topic in the field of remote sensing. Despite the impressive denoising performance of RSI denoising methods, most current deep learning-based approaches function as black boxes and lack integration with physical information models, leading to limited interpretability. Additionally, many methods may struggle with insufficient attention to non-local self-similarity in RSI and require tedious tuning of regularization parameters to achieve optimal performance, particularly in conventional iterative optimization approaches. In this paper, we first propose a novel RSI denoising method named sparse tensor-aided representation network (STAR-Net), which leverages a low-rank prior to effectively capture the non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a sparse variant called STAR-Net-S to deal with the interference caused by non-Gaussian noise in original RSI for the purpose of improving robustness. Different from conventional iterative optimization, we develop an alternating direction method of multipliers (ADMM)-guided deep unrolling network, in which all regularization parameters can be automatically learned, thus inheriting the advantages of both model-based and deep learning-based approaches and successfully addressing the above-mentioned shortcomings. Comprehensive experiments on synthetic and real-world datasets demonstrate that STAR-Net and STAR-Net-S outperform state-of-the-art RSI denoising methods.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation",
    "url": "http://arxiv.org/abs/2505.23597v1",
    "authors": [
      "Georgios Voulgaris"
    ],
    "published": "2025-05-29",
    "abstract": "The accurate semantic segmentation of tree crowns within remotely sensed data is crucial for scientific endeavours such as forest management, biodiversity studies, and carbon sequestration quantification. However, precise segmentation remains challenging due to complexities in the forest canopy, including shadows, intricate backgrounds, scale variations, and subtle spectral differences among tree species. Compared to the traditional methods, Deep Learning models improve accuracy by extracting informative and discriminative features, but often fall short in capturing the aforementioned complexities.\n  To address these challenges, we propose PerceptiveNet, a novel model incorporating a Logarithmic Gabor-parameterised convolutional layer with trainable filter parameters, alongside a backbone that extracts salient features while capturing extensive context and spatial information through a wider receptive field. We investigate the impact of Log-Gabor, Gabor, and standard convolutional layers on semantic segmentation performance through extensive experimentation. Additionally, we conduct an ablation study to assess the contributions of individual layers and their combinations to overall model performance, and we evaluate PerceptiveNet as a backbone within a novel hybrid CNN-Transformer model. Our results outperform state-of-the-art models, demonstrating significant performance improvements on a tree crown dataset while generalising across domains, including two benchmark aerial scene semantic segmentation datasets with varying complexities.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Collaborative Learning for Unsupervised Multimodal Remote Sensing Image Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based Image Translation",
    "url": "http://arxiv.org/abs/2505.22000v1",
    "authors": [
      "Xiaochen Wei",
      "Weiwei Guo",
      "Wenxian Yu"
    ],
    "published": "2025-05-28",
    "abstract": "The substantial modality-induced variations in radiometric, texture, and structural characteristics pose significant challenges for the accurate registration of multimodal images. While supervised deep learning methods have demonstrated strong performance, they often rely on large-scale annotated datasets, limiting their practical application. Traditional unsupervised methods usually optimize registration by minimizing differences in feature representations, yet often fail to robustly capture geometric discrepancies, particularly under substantial spatial and radiometric variations, thus hindering convergence stability. To address these challenges, we propose a Collaborative Learning framework for Unsupervised Multimodal Image Registration, named CoLReg, which reformulates unsupervised registration learning into a collaborative training paradigm comprising three components: (1) a cross-modal image translation network, MIMGCD, which employs a learnable Maximum Index Map (MIM) guided conditional diffusion model to synthesize modality-consistent image pairs; (2) a self-supervised intermediate registration network which learns to estimate geometric transformations using accurate displacement labels derived from MIMGCD outputs; (3) a distilled cross-modal registration network trained with pseudo-label predicted by the intermediate network. The three networks are jointly optimized through an alternating training strategy wherein each network enhances the performance of the others. This mutual collaboration progressively reduces modality discrepancies, enhances the quality of pseudo-labels, and improves registration accuracy. Extensive experimental results on multiple datasets demonstrate that our ColReg achieves competitive or superior performance compared to state-of-the-art unsupervised approaches and even surpasses several supervised baselines.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Machine learning assisted speckle and OAM spectrum analysis for enhanced turbulence characterisation",
    "url": "http://arxiv.org/abs/2505.21878v3",
    "authors": [
      "Wenjie Jiang",
      "Mingjian Cheng",
      "Lixin Guo",
      "Xiang Yi",
      "Jiangting Li",
      "Junli Wang",
      "Andrew Forbes"
    ],
    "published": "2025-05-28",
    "abstract": "Atmospheric turbulence degrades the performance of free-space optical (FSO) communication and remote sensing systems by introducing phase and intensity distortions. While a majority of research focuses on mitigating these effects to ensure robust signal transmission, an underexplored alternative is to leverage the transformation of structured light to characterize the turbulent medium itself. Here, we introduce a deep learning framework that fuses post-propagation intensity speckle patterns and orbital angular momentum (OAM) spectral data for atmospheric turbulence parameter inference. Our architecture, based on a modified InceptionNet backbone, is optimized to extract and integrate multi-scale features from these distinct optical modalities. This multimodal approach achieves validation accuracies exceeding 80%, substantially outperforming conventional single-modality baselines. The framework demonstrates high inference accuracy and enhanced training stability across a broad range of simulated turbulent conditions, quantified by varying Fried parameters (r0) and Reynolds numbers (Re). This work presents a scalable and data-efficient method for turbulence characterization, offering a pathway toward robust environmental sensing and the optimization of dynamic FSO systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Fast Kernel-Space Diffusion for Remote Sensing Pansharpening",
    "url": "http://arxiv.org/abs/2505.18991v2",
    "authors": [
      "Hancong Jin",
      "Zihan Cao",
      "Liang-jian Deng",
      "Jingjing Li"
    ],
    "published": "2025-05-25",
    "abstract": "Pansharpening seeks to fuse high-resolution panchromatic (PAN) and low-resolution multispectral (LRMS) images into a single image with both fine spatial and rich spectral detail. Despite progress in deep learning-based approaches, existing methods often fail to capture global priors inherent in remote sensing data distributions. Diffusion-based models have recently emerged as promising solutions due to their powerful distribution mapping capabilities, however, they suffer from heavy inference latency. We introduce KSDiff, a fast kernel-space diffusion framework that generates convolutional kernels enriched with global context to enhance pansharpening quality and accelerate inference. Specifically, KSDiff constructs these kernels through the integration of a low-rank core tensor generator and a unified factor generator, orchestrated by a structure-aware multi-head attention mechanism. We further introduce a two-stage training strategy tailored for pansharpening, facilitating integration into existing pansharpening architectures. Experiments show that KSDiff achieves superior performance compared to recent promising methods, and with over $500 \\times$ faster inference than diffusion-based pansharpening baselines. Ablation studies, visualizations and further evaluations substantiate the effectiveness of our approach. Code will be released upon possible acceptance.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method",
    "url": "http://arxiv.org/abs/2505.18021v1",
    "authors": [
      "Yao Sun",
      "Sining Chen",
      "Yifan Tian",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-05-23",
    "abstract": "Accurate information on the number of building floors, or above-ground storeys, is essential for household estimation, utility provision, risk assessment, evacuation planning, and energy modeling. Yet large-scale floor-count data are rarely available in cadastral and 3D city databases. This study proposes an end-to-end deep learning framework that infers floor numbers directly from unrestricted, crowdsourced street-level imagery, avoiding hand-crafted features and generalizing across diverse facade styles. To enable benchmarking, we release the Munich Building Floor Dataset, a public set of over 6800 geo-tagged images collected from Mapillary and targeted field photography, each paired with a verified storey label. On this dataset, the proposed classification-regression network attains 81.2% exact accuracy and predicts 97.9% of buildings within +/-1 floor. The method and dataset together offer a scalable route to enrich 3D city models with vertical information and lay a foundation for future work in urban informatics, remote sensing, and geographic information science. Source code and data will be released under an open license at https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Wildfire spread forecasting with Deep Learning",
    "url": "http://arxiv.org/abs/2505.17556v1",
    "authors": [
      "Nikolaos Anastasiou",
      "Spyros Kondylatos",
      "Ioannis Papoutsis"
    ],
    "published": "2025-05-23",
    "abstract": "Accurate prediction of wildfire spread is crucial for effective risk management, emergency response, and strategic resource allocation. In this study, we present a deep learning (DL)-based framework for forecasting the final extent of burned areas, using data available at the time of ignition. We leverage a spatio-temporal dataset that covers the Mediterranean region from 2006 to 2022, incorporating remote sensing data, meteorological observations, vegetation maps, land cover classifications, anthropogenic factors, topography data, and thermal anomalies. To evaluate the influence of temporal context, we conduct an ablation study examining how the inclusion of pre- and post-ignition data affects model performance, benchmarking the temporal-aware DL models against a baseline trained exclusively on ignition-day inputs. Our results indicate that multi-day observational data substantially improve predictive accuracy. Particularly, the best-performing model, incorporating a temporal window of four days before to five days after ignition, improves both the F1 score and the Intersection over Union by almost 5% in comparison to the baseline on the test dataset. We publicly release our dataset and models to enhance research into data-driven approaches for wildfire modeling and response.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning",
    "url": "http://arxiv.org/abs/2505.17483v1",
    "authors": [
      "Yiqing Guo",
      "Nagur Cherukuru",
      "Eric Lehmann",
      "S. L. Kesav Unnithan",
      "Gemma Kerrisk",
      "Tim Malthus",
      "Faisal Islam"
    ],
    "published": "2025-05-23",
    "abstract": "Nitrate ($\\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived primarily from anthropogenic sources. The recent increase in river-discharged nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR) lagoon. Although nitrate is an optically inactive (i.e., colourless) constituent, previous studies have demonstrated there is an indirect, non-causal relationship between water surface nitrate and water-leaving reflectance that is mediated through optically active water quality parameters such as total suspended solids and coloured dissolved organic matter. This work aims to advance our understanding of this relationship with an effort to measure time-series nitrate and simultaneous hyperspectral reflectance at the Fitzroy River estuary, Queensland, Australia. Time-series observations revealed periodic cycles in nitrate loads due to the tidal influence in the estuarine study site. The water surface nitrate loads were predicted from hyperspectral reflectance and water salinity measurements, with hyperspectral reflectance indicating the concentrations of optically active variables and salinity indicating the mixing of river water and seawater proportions. The accuracy assessment of model-predicted nitrate against in-situ measured nitrate values showed that the predicted nitrate values correlated well with the ground-truth data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work demonstrates the feasibility of predicting water surface nitrate from hyperspectral reflectance and salinity measurements.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation",
    "url": "http://arxiv.org/abs/2505.15077v1",
    "authors": [
      "Alessandro dos Santos Ferreira",
      "Ana Paula Marques Ramos",
      "Jos\u00e9 Marcato Junior",
      "Wesley Nunes Gon\u00e7alves"
    ],
    "published": "2025-05-21",
    "abstract": "Urban forests play a key role in enhancing environmental quality and supporting biodiversity in cities. Mapping and monitoring these green spaces are crucial for urban planning and conservation, yet accurately detecting trees is challenging due to complex landscapes and the variability in image resolution caused by different satellite sensors or UAV flight altitudes. While deep learning architectures have shown promise in addressing these challenges, their effectiveness remains strongly dependent on the availability of large and manually labeled datasets, which are often expensive and difficult to obtain in sufficient quantity. In this work, we propose a novel pipeline that integrates domain adaptation with GANs and Diffusion models to enhance the quality of low-resolution aerial images. Our proposed pipeline enhances low-resolution imagery while preserving semantic content, enabling effective tree segmentation without requiring large volumes of manually annotated data. Leveraging models such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we generate realistic and structurally consistent synthetic samples that expand the training dataset and unify scale across domains. This approach not only improves the robustness of segmentation models across different acquisition conditions but also provides a scalable and replicable solution for remote sensing scenarios with scarce annotation resources. Experimental results demonstrated an improvement of over 50% in IoU for low-resolution images, highlighting the effectiveness of our method compared to traditional pipelines.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN",
      "Diffusion Models"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry",
    "url": "http://arxiv.org/abs/2505.13088v1",
    "authors": [
      "Zhaoyi Wang",
      "Shengyu Huang",
      "Jemil Avers Butt",
      "Yuanzhou Cai",
      "Matej Varga",
      "Andreas Wieser"
    ],
    "published": "2025-05-19",
    "abstract": "Point cloud registration has seen significant advancements with the application of deep learning techniques. However, existing approaches often overlook the potential of integrating radiometric information from RGB images. This limitation reduces their effectiveness in aligning point clouds pairs, especially in regions where geometric data alone is insufficient. When used effectively, radiometric information can enhance the registration process by providing context that is missing from purely geometric data. In this paper, we propose CoFF, a novel Cross-modal Feature Fusion method that utilizes both point cloud geometry and RGB images for pairwise point cloud registration. Assuming that the co-registration between point clouds and RGB images is available, CoFF explicitly addresses the challenges where geometric information alone is unclear, such as in regions with symmetric similarity or planar structures, through a two-stage fusion of 3D point cloud features and 2D image features. It incorporates a cross-modal feature fusion module that assigns pixel-wise image features to 3D input point clouds to enhance learned 3D point features, and integrates patch-wise image features with superpoint features to improve the quality of coarse matching. This is followed by a coarse-to-fine matching module that accurately establishes correspondences using the fused features. We extensively evaluate CoFF on four common datasets: 3DMatch, 3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In addition, we assess CoFF on specific subset datasets containing geometrically ambiguous cases. Our experimental results demonstrate that CoFF achieves state-of-the-art registration performance across all benchmarks, including remarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch and 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Spatial-Temporal-Spectral Unified Modeling for Remote Sensing Dense Prediction",
    "url": "http://arxiv.org/abs/2505.12280v3",
    "authors": [
      "Sijie Zhao",
      "Feng Liu",
      "Enzhuo Zhang",
      "Yiqing Guo",
      "Pengfeng Xiao",
      "Lei Bai",
      "Xueliang Zhang",
      "Hao Chen"
    ],
    "published": "2025-05-18",
    "abstract": "The proliferation of multi-source remote sensing data has propelled the development of deep learning for dense prediction, yet significant challenges in data and task unification persist. Current deep learning architectures for remote sensing are fundamentally rigid. They are engineered for fixed input-output configurations, restricting their adaptability to the heterogeneous spatial, temporal, and spectral dimensions inherent in real-world data. Furthermore, these models neglect the intrinsic correlations among semantic segmentation, binary change detection, and semantic change detection, necessitating the development of distinct models or task-specific decoders. This paradigm is also constrained to a predefined set of output semantic classes, where any change to the classes requires costly retraining. To overcome these limitations, we introduce the Spatial-Temporal-Spectral Unified Network (STSUN) for unified modeling. STSUN can adapt to input and output data with arbitrary spatial sizes, temporal lengths, and spectral bands by leveraging their metadata for a unified representation. Moreover, STSUN unifies disparate dense prediction tasks within a single architecture by conditioning the model on trainable task embeddings. Similarly, STSUN facilitates flexible prediction across multiple set of semantic categories by integrating trainable category embeddings as metadata. Extensive experiments on multiple datasets with diverse Spatial-Temporal-Spectral configurations in multiple scenarios demonstrate that a single STSUN model effectively adapts to heterogeneous inputs and outputs, unifying various dense prediction tasks and diverse semantic class predictions. The proposed approach consistently achieves state-of-the-art performance, highlighting its robustness and generalizability for complex remote sensing applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples",
    "url": "http://arxiv.org/abs/2505.12069v1",
    "authors": [
      "Shenzhou Liu",
      "Di Wang",
      "Haonan Guo",
      "Chengxi Han",
      "Wenzhi Zeng"
    ],
    "published": "2025-05-17",
    "abstract": "Accurate and fine-grained crop yield prediction plays a crucial role in advancing global agriculture. However, the accuracy of pixel-level yield estimation based on satellite remote sensing data has been constrained by the scarcity of ground truth data. To address this challenge, we propose a novel approach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This framework introduces an effective multi-task feature-sharing strategy, where features extracted from a shared backbone network are simultaneously utilized by both crop yield prediction decoders and crop classification decoders with the ability to fuse information between them. This design allows MT-CYP-Net to be trained with extremely sparse crop yield point labels and crop type labels, while still generating detailed pixel-level crop yield maps. Concretely, we collected 1,859 yield point labels along with corresponding crop type labels and satellite images from eight farms in Heilongjiang Province, China, in 2023, covering soybean, maize, and rice crops, and constructed a sparse crop yield label dataset. MT-CYP-Net is compared with three classical machine learning and deep learning benchmark methods in this dataset. Experimental results not only indicate the superiority of MT-CYP-Net compared to previous methods on multiple types of crops but also demonstrate the potential of deep networks on precise pixel-level crop yield prediction, especially with limited data labels.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Beluga Whale Detection from Satellite Imagery with Point Labels",
    "url": "http://arxiv.org/abs/2505.12066v1",
    "authors": [
      "Yijie Zheng",
      "Jinxuan Yang",
      "Yu Chen",
      "Yaxuan Wang",
      "Yihang Lu",
      "Guoqing Li"
    ],
    "published": "2025-05-17",
    "abstract": "Very high-resolution (VHR) satellite imagery has emerged as a powerful tool for monitoring marine animals on a large scale. However, existing deep learning-based whale detection methods usually require manually created, high-quality bounding box annotations, which are labor-intensive to produce. Moreover, existing studies often exclude ``uncertain whales'', individuals that have ambiguous appearances in satellite imagery, limiting the applicability of these models in real-world scenarios. To address these limitations, this study introduces an automated pipeline for detecting beluga whales and harp seals in VHR satellite imagery. The pipeline leverages point annotations and the Segment Anything Model (SAM) to generate precise bounding box annotations, which are used to train YOLOv8 for multiclass detection of certain whales, uncertain whales, and harp seals. Experimental results demonstrated that SAM-generated annotations significantly improved detection performance, achieving higher $\\text{F}_\\text{1}$-scores compared to traditional buffer-based annotations. YOLOv8 trained on SAM-labeled boxes achieved an overall $\\text{F}_\\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals, with superior performance in dense scenes. The proposed approach not only reduces the manual effort required for annotation but also enhances the detection of uncertain whales, offering a more comprehensive solution for marine animal monitoring. This method holds great potential for extending to other species, habitats, and remote sensing platforms, as well as for estimating whale biometrics, thereby advancing ecological monitoring and conservation efforts. The codes for our label and detection pipeline are publicly available at http://github.com/voyagerxvoyagerx/beluga-seeker .",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection",
    "url": "http://arxiv.org/abs/2505.11793v1",
    "authors": [
      "Jianing Wang",
      "Siying Guo",
      "Zheng Hua",
      "Runhu Huang",
      "Jinyu Hu",
      "Maoguo Gong"
    ],
    "published": "2025-05-17",
    "abstract": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral image (HSI) processing fields, and most existing deep learning (DL)-based algorithms indicate dramatic potential for detecting anomaly samples through specific training process under current scenario. However, the limited prior information and the catastrophic forgetting problem indicate crucial challenges for existing DL structure in open scenarios cross-domain detection. In order to improve the detection performance, a novel continual learning-based capsule differential generative adversarial network (CL-CaGAN) is proposed to elevate the cross-scenario learning performance for facilitating the real application of DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule structure with adversarial learning network is constructed to estimate the background distribution for surmounting the deficiency of prior information. To mitigate the catastrophic forgetting phenomenon, clustering-based sample replay strategy and a designed extra self-distillation regularization are integrated for merging the history and future knowledge in continual AD task, while the discriminative learning ability from previous detection scenario to current scenario is retained by the elaborately designed structure with continual learning (CL) strategy. In addition, the differentiable enhancement is enforced to augment the generation performance of the training data. This further stabilizes the training process with better convergence and efficiently consolidates the reconstruction ability of background samples. To verify the effectiveness of our proposed CL-CaGAN, we conduct experiments on several real HSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher detection performance and continuous learning capacity for mitigating the catastrophic forgetting under cross-domain scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis",
    "url": "http://arxiv.org/abs/2505.10751v1",
    "authors": [
      "Francisco Raverta Capua",
      "Pablo De Cristoforis"
    ],
    "published": "2025-05-15",
    "abstract": "Although the use of remote sensing technologies for monitoring forested environments has gained increasing attention, publicly available point cloud datasets remain scarce due to the high costs, sensor requirements, and time-intensive nature of their acquisition. Moreover, as far as we are aware, there are no public annotated datasets generated through Structure From Motion (SfM) algorithms applied to imagery, which may be due to the lack of SfM algorithms that can map semantic segmentation information into an accurate point cloud, especially in a challenging environment like forests.\n  In this work, we present a novel pipeline for generating semantically segmented point clouds of forest environments. Using a custom-built forest simulator, we generate realistic RGB images of diverse forest scenes along with their corresponding semantic segmentation masks. These labeled images are then processed using modified open-source SfM software capable of preserving semantic information during 3D reconstruction. The resulting point clouds provide both geometric and semantic detail, offering a valuable resource for training and evaluating deep learning models aimed at segmenting real forest point clouds obtained via SfM.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction",
    "url": "http://arxiv.org/abs/2505.10027v2",
    "authors": [
      "Shijie Lyu"
    ],
    "published": "2025-05-15",
    "abstract": "With the rapid advancement of remote sensing technology, super-resolution image reconstruction is of great research and practical significance. Existing deep learning methods have made progress but still face limitations in handling complex scenes and preserving image details. This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution. The method constructs a reinforcement learning environment with states, actions, and rewards, optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model. Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes. The results demonstrate the method's effectiveness in enhancing super-resolution quality and adaptability across scenes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing",
    "url": "http://arxiv.org/abs/2505.08302v2",
    "authors": [
      "Oishee Bintey Hoque",
      "Nibir Chandra Mandal",
      "Abhijin Adiga",
      "Samarth Swarup",
      "Sayjro Kossi Nouwakpo",
      "Amanda Wilson",
      "Madhav Marathe"
    ],
    "published": "2025-05-13",
    "abstract": "Accurate mapping of irrigation methods is crucial for sustainable agricultural practices and food systems. However, existing models that rely solely on spectral features from satellite imagery are ineffective due to the complexity of agricultural landscapes and limited training data, making this a challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a novel Swin-Transformer based approach that uses (i) a specialized projection matrix to encode crop to irrigation probability, (ii) a spatial attention map to identify agricultural lands from non-agricultural lands, (iii) bi-directional cross-attention to focus complementary information from different modalities, and (iv) a weighted ensemble for combining predictions from images and crop information. Our experimentation on five states in the US shows up to 22.9\\% (IoU) improvement over baseline with a 71.4% (IoU) improvement for hard-to-classify drip irrigation. In addition, we propose a two-phase transfer learning approach to enhance cross-state irrigation mapping, achieving a 51% IoU boost in a state with limited labeled data. The ability to achieve baseline performance with only 40% of the training data highlights its efficiency, reducing the dependency on extensive manual labeling efforts and making large-scale, automated irrigation mapping more feasible and cost-effective.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction",
    "url": "http://arxiv.org/abs/2505.06905v3",
    "authors": [
      "Jian Song",
      "Hongruixuan Chen",
      "Naoto Yokoya"
    ],
    "published": "2025-05-11",
    "abstract": "Monocular height estimation (MHE) from very-high-resolution (VHR) optical imagery remains challenging due to limited structural cues and the high cost and geographic constraints of conventional elevation data such as airborne LiDAR and multi-view stereo. Although recent MHE and monocular depth estimation (MDE) models show strong performance, their robustness under varied illumination and scene conditions is still limited. We introduce a fully automated correction pipeline that integrates sparse, imperfect global LiDAR measurements from ICESat-2 with deep learning predictions to enhance accuracy and stability. The workflow relies entirely on publicly available models and data and requires only a single georeferenced optical image to produce corrected height maps, enabling low-cost and globally scalable deployment. We also establish the first benchmark for this task, evaluating two random forest based approaches, four parameter efficient fine tuning methods, and full fine tuning. Experiments across six diverse regions at 0.5 m resolution (297 km2), covering the urban cores of Tokyo, Paris, and Sao Paulo as well as suburban and forested areas, show substantial gains. The best method reduces the MHE model's mean absolute error (MAE) by 30.9 percent and improves its F1HE score by 44.2 percent. For the MDE model, MAE improves by 24.1 percent and the F1HE score by 25.1 percent. These results validate the effectiveness of our correction pipeline and demonstrate how sparse global LiDAR can systematically strengthen both MHE and MDE models, enabling scalable and widely accessible 3D height mapping.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery",
    "url": "http://arxiv.org/abs/2505.05321v1",
    "authors": [
      "Chintan B. Maniyar",
      "Minakshi Kumar",
      "Gengchen Mai"
    ],
    "published": "2025-05-08",
    "abstract": "Accurate building segmentation from high-resolution RGB imagery remains challenging due to spectral similarity with non-building features, shadows, and irregular building geometries. In this study, we present a comprehensive deep learning framework for multiscale building segmentation using RGB aerial and satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate a diverse, multi-sensor dataset and introduce feature-augmented inputs by deriving secondary representations including Principal Component Analysis (PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index (MBI), and Sobel edge filters from RGB channels. These features guide a Res-U-Net architecture in learning complex spatial patterns more effectively. We also propose training policies incorporating layer freezing, cyclical learning rates, and SuperConvergence to reduce training time and resource usage. Evaluated on a held-out WorldView-3 image, our model achieves an overall accuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of 0.80, outperforming existing RGB-based benchmarks. This study demonstrates the effectiveness of combining multi-resolution imagery, feature augmentation, and optimized training strategies for robust building segmentation in remote sensing applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks",
    "url": "http://arxiv.org/abs/2505.03522v2",
    "authors": [
      "Haotong Cheng",
      "Zhiqi Zhang",
      "Hao Li",
      "Xinshang Zhang"
    ],
    "published": "2025-05-06",
    "abstract": "Deep learning has substantially advanced the field of Single Image Super-Resolution (SISR). However, existing research has predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. In this paper, we introduce the concept of \"Universality\" and its associated definitions, which extend the traditional notion of \"Generalization\" to encompass the ease of transferability of modules. We then propose the Universality Assessment Equation (UAE), a metric that quantifies how readily a given module can be transplanted across models and reveals the combined influence of multiple existing metrics on transferability. Guided by the UAE results of standard residual blocks and other plug-and-play modules, we further design two optimized modules: the Cycle Residual Block (CRB) and the Depth-Wise Cycle Residual Block (DCRB). Through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, and other low-level tasks, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-art methods, achieving a PSNR improvement of up to 0.83 dB or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity. Similar optimization approaches could be applied to a broader range of basic modules, offering a new paradigm for the design of plug-and-play modules.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning",
    "url": "http://arxiv.org/abs/2505.03327v1",
    "authors": [
      "Jos\u00e9-Luis Bueso-Bello",
      "Benjamin Chauvel",
      "Daniel Carcereri",
      "Philipp Posovszky",
      "Pietro Milillo",
      "Jennifer Ruiz",
      "Juan-Carlos Fern\u00e1ndez-Diaz",
      "Carolina Gonz\u00e1lez",
      "Michele Martone",
      "Ronny H\u00e4nsch",
      "Paola Rizzoli"
    ],
    "published": "2025-05-06",
    "abstract": "Deep learning models have shown encouraging capabilities for mapping accurately forests at medium resolution with TanDEM-X interferometric SAR data. Such models, as most of current state-of-the-art deep learning techniques in remote sensing, are trained in a fully-supervised way, which requires a large amount of labeled data for training and validation. In this work, our aim is to exploit the high-resolution capabilities of the TanDEM-X mission to map forests at 6 m. The goal is to overcome the intrinsic limitations posed by midresolution products, which affect, e.g., the detection of narrow roads within vegetated areas and the precise delineation of forested regions contours. To cope with the lack of extended reliable reference datasets at such a high resolution, we investigate self-supervised learning techniques for extracting highly informative representations from the input features, followed by a supervised training step with a significantly smaller number of reliable labels. A 1 m resolution forest/non-forest reference map over Pennsylvania, USA, allows for comparing different training approaches for the development of an effective forest mapping framework with limited labeled samples. We select the best-performing approach over this test region and apply it in a real-case forest mapping scenario over the Amazon rainforest, where only very few labeled data at high resolution are available. In this challenging scenario, the proposed self-supervised framework significantly enhances the classification accuracy with respect to fully-supervised methods, trained using the same amount of labeled data, representing an extremely promising starting point for large-scale, very high-resolution forest mapping with TanDEM-X data.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Sampling Kantorovich operators for speckle noise reduction using a Down-Up scaling approach and gap filling in remote sensing images",
    "url": "http://arxiv.org/abs/2505.02422v2",
    "authors": [
      "Danilo Costarelli",
      "Mariarosaria Natale"
    ],
    "published": "2025-05-05",
    "abstract": "In the literature, several approaches have been proposed for restoring and enhancing remote sensing images, including methods based on interpolation, filtering, and deep learning. In this paper, we investigate the application of multivariate sampling Kantorovich (SK) operators for image reconstruction, with a particular focus on gap filling and speckle noise reduction. To understand the accuracy performances of the proposed algorithms, we first derive a quantitative estimate in $C(\\R^n)$ for the error of approximation using the Euler-Maclaurin summation formula, under weak regularity conditions. We also establish a convergence result and a quantitative estimate with respect to the dissimilarity index measured by the continuous SSIM for functions in Lebesgue spaces. Additionally, we prove a multidimensional linear prediction result, which is used to design a new SK-based reconstruction algorithm to handle missing data, that we call LP-SK algorithm. To address speckle noise, we integrate SK operators into a newly proposed Down-Up scaling approach. Numerical tests are presented on synthetic and real SAR images to validate the proposed methods. Performance is assessed using similarity metrics such as SSIM and PSNR, along with speckle-specific indexes. Comparative analysis with state-of-the-art techniques highlights the effectiveness of the proposed approaches.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Core-Set Selection for Data-efficient Land Cover Segmentation",
    "url": "http://arxiv.org/abs/2505.01225v2",
    "authors": [
      "Keiller Nogueira",
      "Akram Zaytar",
      "Wanli Ma",
      "Ribana Roscher",
      "Ronny H\u00e4nsch",
      "Caleb Robinson",
      "Anthony Ortiz",
      "Simone Nsutezo",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres",
      "Oktay Karaku\u015f",
      "Paul L. Rosin"
    ],
    "published": "2025-05-02",
    "abstract": "The increasing accessibility of remotely sensed data and the potential of such data to inform large-scale decision-making has driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models must be trained on large datasets. However, the common assumption that broadly larger datasets lead to better outcomes tends to overlook the complexities of the data distribution, the potential for introducing biases and noise, and the computational resources required for processing and storing vast datasets. Therefore, effective solutions should consider both the quantity and quality of data. In this paper, we propose six novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets that rely on imagery only, labels only, and a combination of each. We benchmark these approaches against a random-selection baseline on three commonly used land cover classification datasets: DFC2022, Vaihingen, and Potsdam. In each of the datasets, we demonstrate that training on a subset of samples outperforms the random baseline, and some approaches outperform training on all available data. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at https://github.com/keillernogueira/data-centric-rs-classification/.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data",
    "url": "http://arxiv.org/abs/2505.03808v1",
    "authors": [
      "Ioannis Nasios"
    ],
    "published": "2025-05-02",
    "abstract": "Harmful algal blooms are a growing threat to inland water quality and public health worldwide, creating an urgent need for efficient, accurate, and cost-effective detection methods. This research introduces a high-performing methodology that integrates multiple open-source remote sensing data with advanced artificial intelligence models. Key data sources include Copernicus Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently retrieved using platforms like Google Earth Engine (GEE) and Microsoft Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the altitude from the elevation model, the temperature and wind from NOAA as well as the longitude and latitude were the most important features. The approach combines two types of machine learning models, tree-based models and a neural network, into an ensemble for classifying algal bloom severity. While the tree models performed strongly on their own, incorporating a neural network added robustness and demonstrated how deep learning models can effectively use diverse remote sensing inputs. The method leverages high-resolution satellite imagery and AI-driven analysis to monitor algal blooms dynamically, and although initially developed for a NASA competition in the U.S., it shows potential for global application. The complete code is available for further adaptation and practical implementation, illustrating the convergence of remote sensing data and AI to address critical environmental challenges (https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook",
    "url": "http://arxiv.org/abs/2505.00630v2",
    "authors": [
      "Muyi Bao",
      "Shuchang Lyu",
      "Zhaoyang Xu",
      "Huiyu Zhou",
      "Jinchang Ren",
      "Shiming Xiang",
      "Xiangtai Li",
      "Guangliang Cheng"
    ],
    "published": "2025-05-01",
    "abstract": "Deep learning has profoundly transformed remote sensing, yet prevailing architectures like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) remain constrained by critical trade-offs: CNNs suffer from limited receptive fields, while ViTs grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. State Space Models (SSMs), particularly the recently proposed Mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. This survey presents a comprehensive review of Mamba-based methodologies in remote sensing, systematically analyzing about 120 Mamba-based remote sensing studies to construct a holistic taxonomy of innovations and applications. Our contributions are structured across five dimensions: (i) foundational principles of vision Mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid SSM formulations, (iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. By bridging the gap between SSM theory and remote sensing practice, this survey establishes Mamba as a transformative framework for remote sensing analysis. To our knowledge, this paper is the first systematic review of Mamba architectures in remote sensing. Our work provides a structured foundation for advancing research in remote sensing systems through SSM-based methods. We curate an open-source repository (https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster community-driven advancements.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Field-scale soil moisture estimated from Sentinel-1 SAR data using a knowledge-guided deep learning approach",
    "url": "http://arxiv.org/abs/2505.00265v1",
    "authors": [
      "Yi Yu",
      "Patrick Filippi",
      "Thomas F. A. Bishop"
    ],
    "published": "2025-05-01",
    "abstract": "Soil moisture (SM) estimation from active microwave data remains challenging due to the complex interactions between radar backscatter and surface characteristics. While the water cloud model (WCM) provides a semi-physical approach for understanding these interactions, its empirical component often limits performance across diverse agricultural landscapes. This research presents preliminary efforts for developing a knowledge-guided deep learning approach, which integrates WCM principles into a long short-term memory (LSTM) model, to estimate field SM using Sentinel-1 Synthetic Aperture Radar (SAR) data. Our proposed approach leverages LSTM's capacity to capture spatiotemporal dependencies while maintaining physical consistency through a modified dual-component loss function, including a WCM-based semi-physical component and a boundary condition regularisation. The proposed approach is built upon the soil backscatter coefficients isolated from the total backscatter, together with Landsat-resolution vegetation information and surface characteristics. A four-fold spatial cross-validation was performed against in-situ SM data to assess the model performance. Results showed the proposed approach reduced SM retrieval uncertainties by 0.02 m$^3$/m$^3$ and achieved correlation coefficients (R) of up to 0.64 in areas with varying vegetation cover and surface conditions, demonstrating the potential to address the over-simplification in WCM.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies",
    "url": "http://arxiv.org/abs/2504.20203v1",
    "authors": [
      "Vladyslav Polushko",
      "Damjan Hatic",
      "Ronald R\u00f6sch",
      "Thomas M\u00e4rz",
      "Markus Rauhut",
      "Andreas Weinmann"
    ],
    "published": "2025-04-28",
    "abstract": "Floods cause serious problems around the world. Responding quickly and effectively requires accurate and timely information about the affected areas. The effective use of Remote Sensing images for accurate flood detection requires specific detection methods. Typically, Deep Neural Networks are employed, which are trained on specific datasets. For the purpose of river flood detection in RGB imagery, we use the BlessemFlood21 dataset. We here explore the use of different augmentation strategies, ranging from basic approaches to more complex techniques, including optical distortion. By identifying effective strategies, we aim to refine the training process of state-of-the-art Deep Learning segmentation networks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection",
    "url": "http://arxiv.org/abs/2504.19598v1",
    "authors": [
      "Dou Quan",
      "Rufan Zhou",
      "Shuang Wang",
      "Ning Huyan",
      "Dong Zhao",
      "Yunan Li",
      "Licheng Jiao"
    ],
    "published": "2025-04-28",
    "abstract": "Deep learning methods have shown promising performances in remote sensing image change detection (CD). However, existing methods usually train a dataset-specific deep network for each dataset. Due to the significant differences in the data distribution and labeling between various datasets, the trained dataset-specific deep network has poor generalization performances on other datasets. To solve this problem, this paper proposes a change adapter network (CANet) for a more universal and generalized CD. CANet contains dataset-shared and dataset-specific learning modules. The former explores the discriminative features of images, and the latter designs a lightweight adapter model, to deal with the characteristics of different datasets in data distribution and labeling. The lightweight adapter can quickly generalize the deep network for new CD tasks with a small computation cost. Specifically, this paper proposes an interesting change region mask (ICM) in the adapter, which can adaptively focus on interested change objects and decrease the influence of labeling differences in various datasets. Moreover, CANet adopts a unique batch normalization layer for each dataset to deal with data distribution differences. Compared with existing deep learning methods, CANet can achieve satisfactory CD performances on various datasets simultaneously. Experimental results on several public datasets have verified the effectiveness and advantages of the proposed CANet on CD. CANet has a stronger generalization ability, smaller training costs (merely updating 4.1%-7.7% parameters), and better performances under limited training datasets than other deep learning methods, which also can be flexibly inserted with existing deep models.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Photon Absorption Remote Sensing Virtual Histopathology: Diagnostic Equivalence to Gold-Standard H&E Staining in Skin Cancer Excisional Biopsies",
    "url": "http://arxiv.org/abs/2504.18737v1",
    "authors": [
      "Benjamin R. Ecclestone",
      "James E. D. Tweel",
      "Marie Abi Daoud",
      "Hager Gaouda",
      "Deepak Dinakaran",
      "Michael P. Wallace",
      "Ally Khan Somani",
      "Gilbert Bigras",
      "John R. Mackey",
      "Parsin Haji Reza"
    ],
    "published": "2025-04-25",
    "abstract": "Photon Absorption Remote Sensing (PARS) enables label-free imaging of subcellular morphology by observing biomolecule specific absorption interactions. Coupled with deep-learning, PARS produces label-free virtual Hematoxylin and Eosin (H&E) stained images in unprocessed tissues. This study evaluates the diagnostic performance of these PARS-derived virtual H&E images in benign and malignant excisional skin biopsies, including Squamous (SCC), Basal (BCC) Cell Carcinoma, and normal skin. Sixteen unstained formalin-fixed paraffin-embedded skin excisions were PARS imaged, virtually H&E stained, then chemically stained and imaged at 40x. Seven fellowship trained dermatopathologists assessed all 32 images in a masked randomized fashion. Concordance analysis indicates 95.5% agreement between primary diagnoses rendered on PARS versus H&E images (Cohen's k=0.93). Inter-rater reliability was near-perfect for both image types (Fleiss' k=0.89 for PARS, k=0.80 for H&E). For subtype classification, agreement was near-perfect 91% (k=0.73) for SCC and was perfect for BCC. When assessing malignancy confinement (e.g., cancer margins), agreement was 92% between PARS and H&E (k=0.718). During assessment dermatopathologists could not reliably distinguish image origin (PARS vs. H&E), and diagnostic confidence was equivalent between the modalities. Inter-rater reliability for PARS virtual H&E was consistent with reported benchmarks for histologic evaluation. These results indicate that PARS virtual histology may be diagnostically equivalent to traditional H&E staining in dermatopathology diagnostics, while enabling assessment directly from unlabeled, or unprocessed slides. In turn, the label-free PARS virtual H&E imaging workflow may preserve tissue for downstream analysis while producing data well-suited for AI integration potentially accelerating and enhancing the accuracy of skin cancer diagnostics.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition",
    "url": "http://arxiv.org/abs/2504.16467v1",
    "authors": [
      "Qishan He",
      "Lingjun Zhao",
      "Ru Luo",
      "Siqian Zhang",
      "Lin Lei",
      "Kefeng Ji",
      "Gangyao Kuang"
    ],
    "published": "2025-04-23",
    "abstract": "Aircraft recognition in synthetic aperture radar (SAR) imagery is a fundamental mission in both military and civilian applications. Recently deep learning (DL) has emerged a dominant paradigm for its explosive performance on extracting discriminative features. However, current classification algorithms focus primarily on learning decision hyperplane without enough comprehension on aircraft structural knowledge. Inspired by the fined aircraft annotation methods for optical remote sensing images (RSI), we first introduce a structure-based SAR aircraft annotations approach to provide structural and compositional supplement information. On this basis, we propose a multi-task structure guided learning (MTSGL) network for robust and interpretable SAR aircraft recognition. Besides the classification task, MTSGL includes a structural semantic awareness (SSA) module and a structural consistency regularization (SCR) module. The SSA is designed to capture structure semantic information, which is conducive to gain human-like comprehension of aircraft knowledge. The SCR helps maintain the geometric consistency between the aircraft structure in SAR imagery and the proposed annotation. In this process, the structural attribute can be disentangled in a geometrically meaningful manner. In conclusion, the MTSGL is presented with the expert-level aircraft prior knowledge and structure guided learning paradigm, aiming to comprehend the aircraft concept in a way analogous to the human cognitive process. Extensive experiments are conducted on a self-constructed multi-task SAR aircraft recognition dataset (MT-SARD) and the effective results illustrate the superiority of robustness and interpretation ability of the proposed MTSGL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "DAM-Net: Domain Adaptation Network with Micro-Labeled Fine-Tuning for Change Detection",
    "url": "http://arxiv.org/abs/2504.13748v1",
    "authors": [
      "Hongjia Chen",
      "Xin Xu",
      "Fangling Pu"
    ],
    "published": "2025-04-18",
    "abstract": "Change detection (CD) in remote sensing imagery plays a crucial role in various applications such as urban planning, damage assessment, and resource management. While deep learning approaches have significantly advanced CD performance, current methods suffer from poor domain adaptability, requiring extensive labeled data for retraining when applied to new scenarios. This limitation severely restricts their practical applications across different datasets. In this work, we propose DAM-Net: a Domain Adaptation Network with Micro-Labeled Fine-Tuning for CD. Our network introduces adversarial domain adaptation to CD for, utilizing a specially designed segmentation-discriminator and alternating training strategy to enable effective transfer between domains. Additionally, we propose a novel Micro-Labeled Fine-Tuning approach that strategically selects and labels a minimal amount of samples (less than 1%) to enhance domain adaptation. The network incorporates a Multi-Temporal Transformer for feature fusion and optimized backbone structure based on previous research. Experiments conducted on the LEVIR-CD and WHU-CD datasets demonstrate that DAM-Net significantly outperforms existing domain adaptation methods, achieving comparable performance to semi-supervised approaches that require 10% labeled data while using only 0.3% labeled samples. Our approach significantly advances cross-dataset CD applications and provides a new paradigm for efficient domain adaptation in remote sensing. The source code of DAM-Net will be made publicly available upon publication.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation",
    "url": "http://arxiv.org/abs/2504.11949v1",
    "authors": [
      "Jie Wang",
      "Chen Ye Gan",
      "Caoqi Wei",
      "Jiangtao Wen",
      "Yuxing Han"
    ],
    "published": "2025-04-16",
    "abstract": "Feature matching across video streams remains a cornerstone challenge in computer vision. Increasingly, robust multimodal matching has garnered interest in robotics, surveillance, remote sensing, and medical imaging. While traditional rely on detecting and matching spatial features, they break down when faced with noisy, misaligned, or cross-modal data. Recent deep learning methods have improved robustness through learned representations, but remain constrained by their dependence on extensive training data and computational demands. We present Flow Intelligence, a paradigm-shifting approach that moves beyond spatial features by focusing on temporal motion patterns exclusively. Instead of detecting traditional keypoints, our method extracts motion signatures from pixel blocks across consecutive frames and extract temporal motion signatures between videos. These motion-based descriptors achieve natural invariance to translation, rotation, and scale variations while remaining robust across different imaging modalities. This novel approach also requires no pretraining data, eliminates the need for spatial feature detection, enables cross-modal matching using only temporal motion, and it outperforms existing methods in challenging scenarios where traditional approaches fail. By leveraging motion rather than appearance, Flow Intelligence enables robust, real-time video feature matching in diverse environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps",
    "url": "http://arxiv.org/abs/2504.11416v1",
    "authors": [
      "Panagiotis Agrafiotis",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-04-15",
    "abstract": "Accurate, detailed, and high-frequent bathymetry is crucial for shallow seabed areas facing intense climatological and anthropogenic pressures. Current methods utilizing airborne or satellite optical imagery to derive bathymetry primarily rely on either SfM-MVS with refraction correction or Spectrally Derived Bathymetry (SDB). However, SDB methods often require extensive manual fieldwork or costly reference data, while SfM-MVS approaches face challenges even after refraction correction. These include depth data gaps and noise in environments with homogeneous visual textures, which hinder the creation of accurate and complete Digital Surface Models (DSMs) of the seabed. To address these challenges, this work introduces a methodology that combines the high-fidelity 3D reconstruction capabilities of the SfM-MVS methods with state-of-the-art refraction correction techniques, along with the spectral analysis capabilities of a new deep learning-based method for bathymetry prediction. This integration enables a synergistic approach where SfM-MVS derived DSMs with data gaps are used as training data to generate complete bathymetric maps. In this context, we propose Swin-BathyUNet that combines U-Net with Swin Transformer self-attention layers and a cross-attention mechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve bathymetric accuracy by capturing long-range spatial relationships and can also function as a standalone solution for standard SDB with various training depth data, independent of the SfM-MVS output. Experimental results in two completely different test sites in the Mediterranean and Baltic Seas demonstrate the effectiveness of the proposed approach through extensive experiments that demonstrate improvements in bathymetric accuracy, detail, coverage, and noise reduction in the predicted DSM. The code is available at https://github.com/pagraf/Swin-BathyUNet.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "YOLO-RS: Remote Sensing Enhanced Crop Detection Methods",
    "url": "http://arxiv.org/abs/2504.11165v2",
    "authors": [
      "Linlin Xiao",
      "Zhang Tiancong",
      "Yutong Jia",
      "Xinyu Nie",
      "Mengyao Wang",
      "Xiaohang Shao"
    ],
    "published": "2025-04-15",
    "abstract": "With the rapid development of remote sensing technology, crop classification and health detection based on deep learning have gradually become a research hotspot. However, the existing target detection methods show poor performance when dealing with small targets in remote sensing images, especially in the case of complex background and image mixing, which is difficult to meet the practical application requirementsite. To address this problem, a novel target detection model YOLO-RS is proposed in this paper. The model is based on the latest Yolov11 which significantly enhances the detection of small targets by introducing the Context Anchor Attention (CAA) mechanism and an efficient multi-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional feature fusion strategy in the feature fusion process, which effectively enhances the model's performance in the detection of small targets. Small target detection. Meanwhile, the ACmix module at the end of the model backbone network solves the category imbalance problem by adaptively adjusting the contrast and sample mixing, thus enhancing the detection accuracy in complex scenes. In the experiments on the PDT remote sensing crop health detection dataset and the CWC crop classification dataset, YOLO-RS improves both the recall and the mean average precision (mAP) by about 2-3\\% or so compared with the existing state-of-the-art methods, while the F1-score is also significantly improved. Moreover, the computational complexity of the model only increases by about 5.2 GFLOPs, indicating its significant advantages in both performance and efficiency. The experimental results validate the effectiveness and application potential of YOLO-RS in the task of detecting small targets in remote sensing images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion",
    "url": "http://arxiv.org/abs/2504.10974v3",
    "authors": [
      "Zhisheng Zhang",
      "Peng Zhang",
      "Fengxiang Wang",
      "Liangli Ma",
      "Fuchun Sun"
    ],
    "published": "2025-04-15",
    "abstract": "Enhancing forward-looking sonar images is critical for accurate underwater target detection. Current deep learning methods mainly rely on supervised training with simulated data, but the difficulty in obtaining high-quality real-world paired data limits their practical use and generalization. Although self-supervised approaches from remote sensing partially alleviate data shortages, they neglect the cross-modal degradation gap between sonar and remote sensing images. Directly transferring pretrained weights often leads to overly smooth sonar images, detail loss, and insufficient brightness. To address this, we propose a feature-space transformation that maps sonar images from the pixel domain to a robust feature domain, effectively bridging the degradation gap. Additionally, our self-supervised multi-frame fusion strategy leverages complementary inter-frame information to naturally remove speckle noise and enhance target-region brightness. Experiments on three self-collected real-world forward-looking sonar datasets show that our method significantly outperforms existing approaches, effectively suppressing noise, preserving detailed edges, and substantially improving brightness, demonstrating strong potential for underwater target detection applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "LightFormer: A lightweight and efficient decoder for remote sensing image segmentation",
    "url": "http://arxiv.org/abs/2504.10834v1",
    "authors": [
      "Sihang Chen",
      "Lijun Yun",
      "Ze Liu",
      "JianFeng Zhu",
      "Jie Chen",
      "Hui Wang",
      "Yueping Nie"
    ],
    "published": "2025-04-15",
    "abstract": "Deep learning techniques have achieved remarkable success in the semantic segmentation of remote sensing images and in land-use change detection. Nevertheless, their real-time deployment on edge platforms remains constrained by decoder complexity. Herein, we introduce LightFormer, a lightweight decoder for time-critical tasks that involve unstructured targets, such as disaster assessment, unmanned aerial vehicle search-and-rescue, and cultural heritage monitoring. LightFormer employs a feature-fusion and refinement module built on channel processing and a learnable gating mechanism to aggregate multi-scale, multi-range information efficiently, which drastically curtails model complexity. Furthermore, we propose a spatial information selection module (SISM) that integrates long-range attention with a detail preservation branch to capture spatial dependencies across multiple scales, thereby substantially improving the recognition of unstructured targets in complex scenes. On the ISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9% vs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters, thus achieving an excellent accuracy-efficiency trade-off. Consistent results on LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its robustness and superior perception of unstructured objects. These findings highlight LightFormer as a practical solution for remote sensing applications where both computational economy and high-precision segmentation are imperative.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "A Model-Guided Neural Network Method for the Inverse Scattering Problem",
    "url": "http://arxiv.org/abs/2512.10123v1",
    "authors": [
      "Olivia Tsang",
      "Owen Melia",
      "Vasileios Charisopoulos",
      "Jeremy Hoskins",
      "Yuehaw Khoo",
      "Rebecca Willett"
    ],
    "published": "2025-12-10",
    "abstract": "Inverse medium scattering is an ill-posed, nonlinear wave-based imaging problem arising in medical imaging, remote sensing, and non-destructive testing. Machine learning (ML) methods offer increased inference speed and flexibility in capturing prior knowledge of imaging targets relative to classical optimization-based approaches; however, they perform poorly in regimes where the scattering behavior is highly nonlinear. A key limitation is that ML methods struggle to incorporate the physics governing the scattering process, which are typically inferred implicitly from the training data or loosely enforced via architectural design. In this paper, we present a method that endows a machine learning framework with explicit knowledge of problem physics, in the form of a differentiable solver representing the forward model. The proposed method progressively refines reconstructions of the scattering potential using measurements at increasing wave frequencies, following a classical strategy to stabilize recovery. Empirically, we find that our method provides high-quality reconstructions at a fraction of the computational or sampling costs of competing approaches.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A roadmap of geospatial soil quality analysis systems",
    "url": "http://arxiv.org/abs/2512.09817v1",
    "authors": [
      "Habiba BEN ABDERRAHMANE",
      "Slimane Oulad-Naoui",
      "Benameur ZIANI"
    ],
    "published": "2025-12-10",
    "abstract": "Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection",
    "url": "http://arxiv.org/abs/2512.07078v1",
    "authors": [
      "Bo Gao",
      "Jingcheng Tong",
      "Xingsheng Chen",
      "Han Yu",
      "Zichen Li"
    ],
    "published": "2025-12-08",
    "abstract": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.\n  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.\n  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models",
    "url": "http://arxiv.org/abs/2511.13891v1",
    "authors": [
      "Seyed Mohamad Ali Tousi",
      "John A. Lory",
      "G. N. DeSouza"
    ],
    "published": "2025-11-17",
    "abstract": "Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data",
    "url": "http://arxiv.org/abs/2511.10461v1",
    "authors": [
      "Simon Donike",
      "Cesar Aybar",
      "Julio Contreras",
      "Luis G\u00f3mez-Chova"
    ],
    "published": "2025-11-13",
    "abstract": "We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery",
    "url": "http://arxiv.org/abs/2511.10387v3",
    "authors": [
      "Prince Mensah",
      "Pelumi Victor Aderinto",
      "Ibrahim Salihu Yusuf",
      "Arnu Pretorius"
    ],
    "published": "2025-11-13",
    "abstract": "Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.",
    "categories": [
      "fish_plankton",
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "Autoencoder",
      "PINN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP",
    "url": "http://arxiv.org/abs/2511.11680v1",
    "authors": [
      "Udaya Bhasker Cheerala",
      "Varun Teja Chirukuri",
      "Venkata Akhil Kumar Gummadi",
      "Jintu Moni Bhuyan",
      "Praveen Damacharla"
    ],
    "published": "2025-11-12",
    "abstract": "Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps With Sub-Meter Imagery",
    "url": "http://arxiv.org/abs/2511.07231v3",
    "authors": [
      "Kyeongjin Ahn",
      "YongHun Suh",
      "Sungwon Han",
      "Jeasurk Yang",
      "Hannes Taubenb\u00f6ck",
      "Meeyoung Cha"
    ],
    "published": "2025-11-10",
    "abstract": "Access to Water, Sanitation, and Hygiene (WASH) services remains a major public health concern in refugee camps. This study introduces a remote sensing-driven framework to quantify WASH accessibility-specifically to water pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one of the world's most densely populated displacement settings. Detecting refugee shelters in such emergent camps presents substantial challenges, primarily due to their dense spatial configuration and irregular geometric patterns. Using sub-meter satellite images, we develop a semi-supervised segmentation framework that achieves an F1-score of 76.4% in detecting individual refugee shelters. Applying the framework across multi-year data reveals declining WASH accessibility, driven by rapid refugee population growth and reduced facility availability, rising from 25 people per facility in 2022 to 29.4 in 2025. Gender-disaggregated analysis further shows that women and girls experience reduced accessibility, in scenarios with inadequate safety-related segregation in WASH facilities. These findings suggest the importance of demand-responsive allocation strategies that can identify areas with under-served populations-such as women and girls-and ensure that limited infrastructure serves the greatest number of people in settings with fixed or shrinking budgets. We also discuss the value of high-resolution remote sensing and machine learning to detect inequality and inform equitable resource planning in complex humanitarian environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Spectral-Convergent Decentralized Machine Learning: Theory and Application in Space Networks",
    "url": "http://arxiv.org/abs/2511.03291v1",
    "authors": [
      "Zhiyuan Zhai",
      "Shuyan Hu",
      "Wei Ni",
      "Xiaojun Yuan",
      "Xin Wang"
    ],
    "published": "2025-11-05",
    "abstract": "Decentralized machine learning (DML) supports collaborative training in large-scale networks with no central server. It is sensitive to the quality and reliability of inter-device communications that result in time-varying and stochastic topologies. This paper studies the impact of unreliable communication on the convergence of DML and establishes a direct connection between the spectral properties of the mixing process and the global performance. We provide rigorous convergence guarantees under random topologies and derive bounds that characterize the impact of the expected mixing matrix's spectral properties on learning. We formulate a spectral optimization problem that minimizes the spectral radius of the expected second-order mixing matrix to enhance the convergence rate under probabilistic link failures. To solve this non-smooth spectral problem in a fully decentralized manner, we design an efficient subgradient-based algorithm that integrates Chebyshev-accelerated eigenvector estimation with local update and aggregation weight adjustment, while ensuring symmetry and stochasticity constraints without central coordination. Experiments on a realistic low Earth orbit (LEO) satellite constellation with time-varying inter-satellite link models and real-world remote sensing data demonstrate the feasibility and effectiveness of the proposed method. The method significantly improves classification accuracy and convergence efficiency compared to existing baselines, validating its applicability in satellite and other decentralized systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "SAAIPAA: Optimizing aspect-angles-invariant physical adversarial attacks on SAR target recognition models",
    "url": "http://arxiv.org/abs/2511.03192v1",
    "authors": [
      "Isar Lemeire",
      "Yee Wei Law",
      "Sang-Heon Lee",
      "Will Meakin",
      "Tat-Jun Chin"
    ],
    "published": "2025-11-05",
    "abstract": "Synthetic aperture radar (SAR) enables versatile, all-time, all-weather remote sensing. Coupled with automatic target recognition (ATR) leveraging machine learning (ML), SAR is empowering a wide range of Earth observation and surveillance applications. However, the surge of attacks based on adversarial perturbations against the ML algorithms underpinning SAR ATR is prompting the need for systematic research into adversarial perturbation mechanisms. Research in this area began in the digital (image) domain and evolved into the physical (signal) domain, resulting in physical adversarial attacks (PAAs) that strategically exploit corner reflectors as attack vectors to evade ML-based ATR. This paper proposes a novel framework called SAR Aspect-Angles-Invariant Physical Adversarial Attack (SAAIPAA) for physics-based modelling of reflector-actuated adversarial perturbations, which improves on the rigor of prior work. A unique feature of SAAIPAA is its ability to remain effective even when the attacker lacks knowledge of the SAR platform's aspect angles, by deploying at least one reflector in each azimuthal quadrant and optimizing reflector orientations. The resultant physical evasion attacks are efficiently realizable and optimal over the considered range of aspect angles between a SAR platform and a target, achieving state-of-the-art fooling rates (over 80% for DenseNet-121 and ResNet50) in the white-box setting. When aspect angles are known to the attacker, an average fooling rate of 99.2% is attainable. In black-box settings, although the attack efficacy of SAAIPAA transfers well between some models (e.g., from ResNet50 to DenseNet121), the transferability to some models (e.g., MobileNetV2) can be improved. A useful outcome of using the MSTAR dataset for the experiments in this article, a method for generating bounding boxes for densely sampled azimuthal SAR datasets is introduced.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies",
    "url": "http://arxiv.org/abs/2511.03173v1",
    "authors": [
      "Arsalan Muhammad",
      "Wasiu Akande Ahmed",
      "Omada Friday Ojonugwa",
      "Paul Puspendu Biswas"
    ],
    "published": "2025-11-05",
    "abstract": "The rapid growth of cislunar activities, including lunar landings, the Lunar Gateway, and in-space refueling stations, requires advances in cost-efficient trajectory design and reliable integration of navigation and remote sensing. Traditional Earth-Moon transfers suffer from rigid launch windows and high propellant demands, while Earth-based GNSS systems provide little to no coverage beyond geostationary orbit. This limits autonomy and environmental awareness in cislunar space. This review compares four major transfer strategies by evaluating velocity requirements, flight durations, and fuel efficiency, and by identifying their suitability for both crewed and robotic missions. The emerging role of artificial intelligence and machine learning is highlighted: convolutional neural networks support automated crater recognition and digital terrain model generation, while deep reinforcement learning enables adaptive trajectory refinement during descent and landing to reduce risk and decision latency. The study also examines how GNSS-Reflectometry and advanced Positioning, Navigation, and Timing architectures can extend navigation capabilities beyond current limits. GNSS-R can act as a bistatic radar for mapping lunar ice, soil properties, and surface topography, while PNT systems support autonomous rendezvous, Lagrange point station-keeping, and coordinated satellite swarm operations. Combining these developments establishes a scalable framework for sustainable cislunar exploration and long-term human and robotic presence.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Recognition",
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Overspecified Mixture Discriminant Analysis: Exponential Convergence, Statistical Guarantees, and Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2510.27056v1",
    "authors": [
      "Arman Bolatov",
      "Alan Legg",
      "Igor Melnykov",
      "Amantay Nurlanuly",
      "Maxat Tezekbayev",
      "Zhenisbek Assylbekov"
    ],
    "published": "2025-10-30",
    "abstract": "This study explores the classification error of Mixture Discriminant Analysis (MDA) in scenarios where the number of mixture components exceeds those present in the actual data distribution, a condition known as overspecification. We use a two-component Gaussian mixture model within each class to fit data generated from a single Gaussian, analyzing both the algorithmic convergence of the Expectation-Maximization (EM) algorithm and the statistical classification error. We demonstrate that, with suitable initialization, the EM algorithm converges exponentially fast to the Bayes risk at the population level. Further, we extend our results to finite samples, showing that the classification error converges to Bayes risk with a rate $n^{-1/2}$ under mild conditions on the initial parameter estimates and sample size. This work provides a rigorous theoretical framework for understanding the performance of overspecified MDA, which is often used empirically in complex data settings, such as image and text classification. To validate our theory, we conduct experiments on remote sensing datasets.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Robust variable selection for spatial point processes observed with noise",
    "url": "http://arxiv.org/abs/2510.25550v1",
    "authors": [
      "Dominik Sturm",
      "Ivo F. Sbalzarini"
    ],
    "published": "2025-10-29",
    "abstract": "We propose a method for variable selection in the intensity function of spatial point processes that combines sparsity-promoting estimation with noise-robust model selection. As high-resolution spatial data becomes increasingly available through remote sensing and automated image analysis, identifying spatial covariates that influence the localization of events is crucial to understand the underlying mechanism. However, results from automated acquisition techniques are often noisy, for example due to measurement uncertainties or detection errors, which leads to spurious displacements and missed events. We study the impact of such noise on sparse point-process estimation across different models, including Poisson and Thomas processes. To improve noise robustness, we propose to use stability selection based on point-process subsampling and to incorporate a non-convex best-subset penalty to enhance model-selection performance. In extensive simulations, we demonstrate that such an approach reliably recovers true covariates under diverse noise scenarios and improves both selection accuracy and stability. We then apply the proposed method to a forestry data set, analyzing the distribution of trees in relation to elevation and soil nutrients in a tropical rain forest. This shows the practical utility of the method, which provides a systematic framework for robust variable selection in spatial point-process models under noise, without requiring additional knowledge of the process.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2510.24242v1",
    "authors": [
      "Zihan Li",
      "Jiahao Yang",
      "Yuxin Zhang",
      "Zhe Chen",
      "Yue Gao"
    ],
    "published": "2025-10-28",
    "abstract": "Large vision-language models (LVLMs) have recently demonstrated great potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by low Earth orbit (LEO) satellites. However, their deployment in real-world LEO satellite systems remains largely unexplored, hindered by limited onboard computing resources and brief satellite-ground contacts. We propose Grace, a satellite-ground collaborative system designed for near-realtime LVLM inference in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime inference, but larger ones on ground stations (GSs) to guarantee end-to-end performance. Grace is comprised of two main phases that are asynchronous satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch algorithm. Firstly, we still the knowledge archive of GS RAG to satellite archive with tailored adaptive update algorithm during limited satellite-ground data exchange period. Secondly, propose a confidence-based test algorithm that either processes the task onboard the satellite or offloads it to the GS. Extensive experiments based on real-world satellite orbital data show that Grace reduces the average latency by 76-95% compared to state-of-the-art methods, without compromising inference accuracy.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval",
    "url": "http://arxiv.org/abs/2510.20486v1",
    "authors": [
      "Fangjian Zhang",
      "Xiaoyong Zhuge",
      "Wenlan Wang",
      "Haixia Xiao",
      "Yuying Zhu",
      "Siyang Cheng"
    ],
    "published": "2025-10-23",
    "abstract": "Artificial intelligence has advanced quantitative remote sensing, yet its effectiveness is constrained by imbalanced label distribution. This imbalance leads conventionally trained models to favor common samples, which in turn degrades retrieval performance for rare ones. Rainfall retrieval exemplifies this issue, with performance particularly compromised for heavy rain. This study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework. Following a divide-and-conquer strategy, imbalance in the rain distribution is decomposed into two components: zero inflation, defined by the predominance of non-rain samples; and long tail, defined by the disproportionate abundance of light-rain samples relative to heavy-rain samples. A hurdle model is adopted to handle the zero inflation, while IMDL is proposed to address the long tail by transforming the learning object into an unbiased ideal inverse model. Comprehensive evaluation via statistical metrics and case studies investigating rainy weather in eastern China confirms Hurdle-IMDL's superiority over conventional, cost-sensitive, generative, and multi-task learning methods. Its key advancements include effective mitigation of systematic underestimation and a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a generalizable approach for addressing imbalance in distributions of environmental variables, enabling enhanced retrieval of rare yet high-impact events.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals",
    "url": "http://arxiv.org/abs/2510.19917v1",
    "authors": [
      "Trajan Murphy",
      "Akshunna S. Dogra",
      "Hanfeng Gu",
      "Caleb Meredith",
      "Mark Kon",
      "Julio Enrique Castrillion-Candas"
    ],
    "published": "2025-10-22",
    "abstract": "''Noisy'' datasets (regimes with low signal to noise ratios, small sample sizes, faulty data collection, etc) remain a key research frontier for classification methods with both theoretical and practical implications. We introduce FINDER, a rigorous framework for analyzing generic classification problems, with tailored algorithms for noisy datasets. FINDER incorporates fundamental stochastic analysis ideas into the feature learning and inference stages to optimally account for the randomness inherent to all empirical datasets. We construct ''stochastic features'' by first viewing empirical datasets as realizations from an underlying random field (without assumptions on its exact distribution) and then mapping them to appropriate Hilbert spaces. The Kosambi-Karhunen-Lo\u00e9ve expansion (KLE) breaks these stochastic features into computable irreducible components, which allow classification over noisy datasets via an eigen-decomposition: data from different classes resides in distinct regions, identified by analyzing the spectrum of the associated operators. We validate FINDER on several challenging, data-deficient scientific domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease stage classification, (ii) Remote sensing detection of deforestation. We end with a discussion on when FINDER is expected to outperform existing methods, its failure modes, and other limitations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Uncertainty evaluation of segmentation models for Earth observation",
    "url": "http://arxiv.org/abs/2510.19586v1",
    "authors": [
      "Melanie Rey",
      "Andriy Mnih",
      "Maxim Neumann",
      "Matt Overlan",
      "Drew Purves"
    ],
    "published": "2025-10-22",
    "abstract": "This paper investigates methods for estimating uncertainty in semantic segmentation predictions derived from satellite imagery. Estimating uncertainty for segmentation presents unique challenges compared to standard image classification, requiring scalable methods producing per-pixel estimates. While most research on this topic has focused on scene understanding or medical imaging, this work benchmarks existing methods specifically for remote sensing and Earth observation applications. Our evaluation focuses on the practical utility of uncertainty measures, testing their ability to identify prediction errors and noise-corrupted input image regions. Experiments are conducted on two remote sensing datasets, PASTIS and ForTy, selected for their differences in scale, geographic coverage, and label confidence. We perform an extensive evaluation featuring several models, such as Stochastic Segmentation Networks and ensembles, in combination with a number of neural architectures and uncertainty metrics. We make a number of practical recommendations based on our findings.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration",
    "url": "http://arxiv.org/abs/2510.19579v1",
    "authors": [
      "Francisco Mena",
      "Dino Ienco",
      "Cassio F. Dantas",
      "Roberto Interdonato",
      "Andreas Dengel"
    ],
    "published": "2025-10-22",
    "abstract": "Multi-modal co-learning is emerging as an effective paradigm in machine learning, enabling models to collaboratively learn from different modalities to enhance single-modality predictions. Earth Observation (EO) represents a quintessential domain for multi-modal data analysis, wherein diverse remote sensors collect data to sense our planet. This unprecedented volume of data introduces novel challenges. Specifically, the access to the same sensor modalities at both training and inference stages becomes increasingly complex based on real-world constraints affecting remote sensing platforms. In this context, multi-modal co-learning presents a promising strategy to leverage the vast amount of sensor-derived data available at the training stage to improve single-modality models for inference-time deployment. Most current research efforts focus on designing customized solutions for either particular downstream tasks or specific modalities available at the inference stage. To address this, we propose a novel multi-modal co-learning framework capable of generalizing across various tasks without targeting a specific modality for inference. Our approach combines contrastive and modality discriminative learning together to guide single-modality models to structure the internal model manifold into modality-shared and modality-specific information. We evaluate our framework on four EO benchmarks spanning classification and regression tasks across different sensor modalities, where only one of the modalities available during training is accessible at inference time. Our results demonstrate consistent predictive improvements over state-of-the-art approaches from the recent machine learning and computer vision literature, as well as EO-specific methods. The obtained findings validate our framework in the single-modality inference scenarios across a diverse range of EO applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Dimensionality Reduction for Remote Sensing Data Analysis: A Systematic Review of Methods and Applications",
    "url": "http://arxiv.org/abs/2510.18935v1",
    "authors": [
      "Nathan Mankovich",
      "Kai-Hendrik Cohrs",
      "Homer Durand",
      "Vasileios Sitokonstantinou",
      "Tristan Williams",
      "Gustau Camps-Valls"
    ],
    "published": "2025-10-21",
    "abstract": "Earth observation involves collecting, analyzing, and processing an ever-growing mass of data. Automatically harvesting information is crucial for addressing significant societal, economic, and environmental challenges, ranging from environmental monitoring to urban planning and disaster management. However, the high dimensionality of these data poses challenges in terms of sparsity, inefficiency, and the curse of dimensionality, which limits the effectiveness of machine learning models. Dimensionality reduction (DR) techniques, specifically feature extraction, address these challenges by preserving essential data properties while reducing complexity and enhancing tasks such as data compression, cleaning, fusion, visualization, anomaly detection, and prediction. This review provides a handbook for leveraging DR across the RS data value chain and identifies opportunities for under-explored DR algorithms and their application in future research.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network",
    "url": "http://arxiv.org/abs/2510.17756v1",
    "authors": [
      "Younghyun Koo",
      "Maryam Rahnemoonfar"
    ],
    "published": "2025-10-20",
    "abstract": "As an increasing amount of remote sensing data becomes available in the Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely used to predict sea ice velocity (SIV) and sea ice concentration (SIC). However, fully data-driven ML models have limitations in generalizability and physical consistency due to their excessive reliance on the quantity and quality of training data. In particular, as Arctic sea ice entered a new phase with thinner ice and accelerated melting, there is a possibility that an ML model trained with historical sea ice data cannot fully represent the dynamically changing sea ice conditions in the future. In this study, we develop physics-informed neural network (PINN) strategies to integrate physical knowledge of sea ice into the ML model. Based on the Hierarchical Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics loss function and the activation function to produce physically plausible SIV and SIC outputs. Our PINN model outperforms the fully data-driven model in the daily predictions of SIV and SIC, even when trained with a small number of samples. The PINN approach particularly improves SIC predictions in melting and early freezing seasons and near fast-moving ice regions.",
    "categories": [
      "ocean",
      "remote_sensing"
    ],
    "architectures": [
      "UNET",
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory",
    "url": "http://arxiv.org/abs/2510.16676v1",
    "authors": [
      "Anindya Sarkar",
      "Binglin Ji",
      "Yevgeniy Vorobeychik"
    ],
    "published": "2025-10-19",
    "abstract": "In many scientific and engineering fields, where acquiring high-quality data is expensive--such as medical imaging, environmental monitoring, and remote sensing--strategic sampling of unobserved regions based on prior observations is crucial for maximizing discovery rates within a constrained budget. The rise of powerful generative models, such as diffusion models, has enabled active target discovery in partially observable environments by leveraging learned priors--probabilistic representations that capture underlying structure from data. With guidance from sequentially gathered task-specific observations, these models can progressively refine exploration and efficiently direct queries toward promising regions. However, in domains where learning a strong prior is infeasible due to extremely limited data or high sampling cost (such as rare species discovery, diagnostics for emerging diseases, etc.), these methods struggle to generalize. To overcome this limitation, we propose a novel approach that enables effective active target discovery even in settings with uninformative priors, ensuring robust exploration and adaptability in complex real-world scenarios. Our framework is theoretically principled and draws inspiration from neuroscience to guide its design. Unlike black-box policies, our approach is inherently interpretable, providing clear insights into decision-making. Furthermore, it guarantees a strong, monotonic improvement in prior estimates with each new observation, leading to increasingly accurate sampling and reinforcing both reliability and adaptability in dynamic settings. Through comprehensive experiments and ablation studies across various domains, including species distribution modeling and remote sensing, we demonstrate that our method substantially outperforms baseline approaches.",
    "categories": [
      "fish_plankton",
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models",
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery",
    "url": "http://arxiv.org/abs/2510.14709v1",
    "authors": [
      "Caleb Robinson",
      "Kimberly T. Goetz",
      "Christin B. Khan",
      "Meredith Sackett",
      "Kathleen Leonard",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ],
    "published": "2025-10-16",
    "abstract": "Effective monitoring of whale populations is critical for conservation, but traditional survey methods are expensive and difficult to scale. While prior work has shown that whales can be identified in very high-resolution (VHR) satellite imagery, large-scale automated detection remains challenging due to a lack of annotated imagery, variability in image quality and environmental conditions, and the cost of building robust machine learning pipelines over massive remote sensing archives. We present a semi-automated approach for surfacing possible whale detections in VHR imagery using a statistical anomaly detection method that flags spatial outliers, i.e. \"interesting points\". We pair this detector with a web-based labeling interface designed to enable experts to quickly annotate the interesting points. We evaluate our system on three benchmark scenes with known whale annotations and achieve recalls of 90.3% to 96.4%, while reducing the area requiring expert inspection by up to 99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method does not rely on labeled training data and offers a scalable first step toward future machine-assisted marine mammal monitoring from space. We have open sourced this pipeline at https://github.com/microsoft/whales.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models",
    "url": "http://arxiv.org/abs/2510.13993v1",
    "authors": [
      "Jia Yun Chua",
      "Argyrios Zolotas",
      "Miguel Arana-Catania"
    ],
    "published": "2025-10-15",
    "abstract": "Remote sensing has become a vital tool across sectors such as urban planning, environmental monitoring, and disaster response. While the volume of data generated has increased significantly, traditional vision models are often constrained by the requirement for extensive domain-specific labelled data and their limited ability to understand the context within complex environments. Vision Language Models offer a complementary approach by integrating visual and textual data; however, their application to remote sensing remains underexplored, particularly given their generalist nature. This work investigates the combination of vision models and VLMs to enhance image analysis in remote sensing, with a focus on aircraft detection and scene understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and Gemini aims to achieve more accurate and contextually aware image interpretation. Performance is evaluated on both labelled and unlabelled remote sensing data, as well as degraded image scenarios which are crucial for remote sensing. The findings show an average MAE improvement of 48.46% across models in the accuracy of aircraft detection and counting, especially in challenging conditions, in both raw and degraded scenarios. A 6.17% improvement in CLIPScore for comprehensive understanding of remote sensing images is obtained. The proposed approach combining traditional vision models and VLMs paves the way for more advanced and efficient remote sensing image analysis, especially in few-shot learning scenarios.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Dedelayed: Deleting remote inference delay via on-device correction",
    "url": "http://arxiv.org/abs/2510.13714v2",
    "authors": [
      "Dan Jacobellis",
      "Mateen Ulhaq",
      "Fabien Racap\u00e9",
      "Hyomin Choi",
      "Neeraja J. Yadwadkar"
    ],
    "published": "2025-10-15",
    "abstract": "Video comprises the vast majority of bits that are generated daily, and is the primary signal driving current innovations in robotics, remote sensing, and wearable technology. Yet, the most powerful video understanding models are too expensive for the resource-constrained platforms used in these applications. One approach is to offload inference to the cloud; this gives access to GPUs capable of processing high-resolution videos in real time. But even with reliable, high-bandwidth communication channels, the combined latency of video encoding, model inference, and round-trip communication prohibits use for certain real-time applications. The alternative is to use fully local inference; but this places extreme constraints on computational and power costs, requiring smaller models and lower resolution, leading to degraded accuracy. To address these challenges, we propose Dedelayed, a real-time inference system that divides computation between a remote model operating on delayed video frames and a local model with access to the current frame. The remote model is trained to make predictions on anticipated future frames, which the local model incorporates into its prediction for the current frame. The local and remote models are jointly optimized with an autoencoder that limits the transmission bitrate required by the available downlink communication channel. We evaluate Dedelayed on the task of real-time streaming video segmentation using the BDD100k driving dataset. For a round trip delay of 100 ms, Dedelayed improves performance by 6.4 mIoU compared to fully local inference and 9.8 mIoU compared to remote inference -- an equivalent improvement to using a model ten times larger.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach",
    "url": "http://arxiv.org/abs/2510.16015v1",
    "authors": [
      "Qian Sun",
      "Graham Hults",
      "Susu Xu"
    ],
    "published": "2025-10-15",
    "abstract": "Timely and reliable decision-making is vital for flood emergency response, yet it remains severely hindered by limited and imprecise situational awareness due to various budget and data accessibility constraints. Traditional flood management systems often rely on in-situ sensors to calibrate remote sensing-based large-scale flood depth forecasting models, and further take flood depth estimates to optimize flood response decisions. However, these approaches often take fixed, decision task-agnostic strategies to decide where to put in-situ sensors (e.g., maximize overall information gain) and train flood forecasting models (e.g., minimize average forecasting errors), but overlook that systems with the same sensing gain and average forecasting errors may lead to distinct decisions. To address this, we introduce a novel decision-focused framework that strategically selects locations for in-situ sensor placement and optimize spatio-temporal flood forecasting models to optimize downstream flood response decision regrets. Our end-to-end pipeline integrates four components: a contextual scoring network, a differentiable sensor selection module under hard budget constraints, a spatio-temporal flood reconstruction and forecasting model, and a differentiable decision layer tailored to task-specific objectives. Central to our approach is the incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable gradient-based learning over discrete sensor configurations, and probabilistic decision heads to enable differentiable approximation to various constrained disaster response tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Knowledge-Guided Machine Learning Models to Upscale Evapotranspiration in the U.S. Midwest",
    "url": "http://arxiv.org/abs/2510.11505v1",
    "authors": [
      "Aleksei Rozanov",
      "Samikshya Subedi",
      "Vasudha Sharma",
      "Bryan C. Runck"
    ],
    "published": "2025-10-13",
    "abstract": "Evapotranspiration (ET) plays a critical role in the land-atmosphere interactions, yet its accurate quantification across various spatiotemporal scales remains a challenge. In situ measurement approaches, like eddy covariance (EC) or weather station-based ET estimation, allow for measuring ET at a single location. Agricultural uses of ET require estimates for each field over broad areas, making it infeasible to deploy sensing systems at each location. This study integrates tree-based and knowledge-guided machine learning (ML) techniques with multispectral remote sensing data, griddled meteorology and EC data to upscale ET across the Midwest United States. We compare four tree-based models - Random Forest, CatBoost, XGBoost, LightGBM - and a simple feed-forward artificial neural network in combination with features engineered using knowledge-guided ML principles. Models were trained and tested on EC towers located in the Midwest of the United States using k-fold cross validation with k=5 and site-year, biome stratified train-test split to avoid data leakage. Results show that LightGBM with knowledge-guided features outperformed other methods with an R2=0.86, MSE=14.99 W m^-2 and MAE = 8.82 W m^-2 according to grouped k-fold validation (k=5). Feature importance analysis shows that knowledge-guided features were most important for predicting evapotranspiration. Using the best performing model, we provide a data product at 500 m spatial and one-day temporal resolution for gridded ET for the period of 2019-2024. Intercomparison between the new gridded product and state-level weather station-based ET estimates show best-in-class correspondence.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter",
    "url": "http://arxiv.org/abs/2510.15954v1",
    "authors": [
      "Hongzheng Shi",
      "Yuhang Wang",
      "Xiao Liu"
    ],
    "published": "2025-10-10",
    "abstract": "As wildfires become increasingly destructive and expensive to control, effective management of active wildfires requires accurate, real-time fire spread predictions. To enhance the forecasting accuracy of active fires, data assimilation plays a vital role by integrating observations (such as remote-sensing data) and fire predictions generated from numerical models. This paper provides a comprehensive investigation on the application of a recently proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter (EnSF) -- to the data assimilation problem for real-time active wildfire spread predictions. Leveraging a score-based generative diffusion model, EnSF has been shown to have superior accuracy for high-dimensional nonlinear filtering problems, making it an ideal candidate for the filtering problems of wildfire spread models. Technical details are provided, and our numerical investigations demonstrate that EnSF provides superior accuracy, stability, and computational efficiency, establishing it as a robust and practical method for wildfire data assimilation. Our code has been made publicly available.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling",
    "url": "http://arxiv.org/abs/2510.09676v1",
    "authors": [
      "Shayan Mohajer Hamidi",
      "En-Hui Yang",
      "Ben Liang"
    ],
    "published": "2025-10-08",
    "abstract": "Inverse problems, where the goal is to recover an unknown signal from noisy or incomplete measurements, are central to applications in medical imaging, remote sensing, and computational biology. Diffusion models have recently emerged as powerful priors for solving such problems. However, existing methods either rely on projection-based techniques that enforce measurement consistency through heuristic updates, or they approximate the likelihood $p(\\boldsymbol{y} \\mid \\boldsymbol{x})$, often resulting in artifacts and instability under complex or high-noise conditions. To address these limitations, we propose a novel framework called \\emph{coupled data and measurement space diffusion posterior sampling} (C-DPS), which eliminates the need for constraint tuning or likelihood approximation. C-DPS introduces a forward stochastic process in the measurement space $\\{\\boldsymbol{y}_t\\}$, evolving in parallel with the data-space diffusion $\\{\\boldsymbol{x}_t\\}$, which enables the derivation of a closed-form posterior $p(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t, \\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive sampling based on a well-defined posterior distribution. Empirical results demonstrate that C-DPS consistently outperforms existing baselines, both qualitatively and quantitatively, across multiple inverse problem benchmarks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping",
    "url": "http://arxiv.org/abs/2510.06769v1",
    "authors": [
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ],
    "published": "2025-10-08",
    "abstract": "The quantity and the quality of the training labels are central problems in high-resolution land-cover mapping with machine-learning-based solutions. In this context, weak labels can be gathered in large quantities by leveraging on existing low-resolution or obsolete products. In this paper, we address the problem of training land-cover classifiers using high-resolution imagery (e.g., Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived land-cover maps). Inspired by recent works in Deep Multiple Instance Learning (DMIL), we propose a method that trains pixel-level multi-class classifiers and predicts low-resolution labels (i.e., patch-level classification), where the actual high-resolution labels are learned implicitly without direct supervision. This is achieved with flexible pooling layers that are able to link the semantics of the pixels in the high-resolution imagery to the low-resolution reference labels. Then, the Multiple Instance Learning (MIL) problem is re-framed in a multi-class and in a multi-label setting. In the former, the low-resolution annotation represents the majority of the pixels in the patch. In the latter, the annotation only provides us information on the presence of one of the land-cover classes in the patch and thus multiple labels can be considered valid for a patch at a time, whereas the low-resolution labels provide us only one label. Therefore, the classifier is trained with a Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020 IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed framework compared to standard training strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Do Superpixel Segmentation Methods Influence Deforestation Image Classification?",
    "url": "http://arxiv.org/abs/2510.04645v1",
    "authors": [
      "Hugo Resende",
      "Fabio A. Faria",
      "Eduardo B. Neto",
      "Isabela Borlido",
      "Victor Sundermann",
      "Silvio Jamil F. Guimar\u00e3es",
      "\u00c1lvaro L. Fazenda"
    ],
    "published": "2025-10-06",
    "abstract": "Image segmentation is a crucial step in various visual applications, including environmental monitoring through remote sensing. In the context of the ForestEyes project, which combines citizen science and machine learning to detect deforestation in tropical forests, image segments are used for labeling by volunteers and subsequent model training. Traditionally, the Simple Linear Iterative Clustering (SLIC) algorithm is adopted as the segmentation method. However, recent studies have indicated that other superpixel-based methods outperform SLIC in remote sensing image segmentation, and might suggest that they are more suitable for the task of detecting deforested areas. In this sense, this study investigated the impact of the four best segmentation methods, together with SLIC, on the training of classifiers for the target application. Initially, the results showed little variation in performance among segmentation methods, even when selecting the top five classifiers using the PyCaret AutoML library. However, by applying a classifier fusion approach (ensemble of classifiers), noticeable improvements in balanced accuracy were observed, highlighting the importance of both the choice of segmentation method and the combination of machine learning-based models for deforestation detection tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications",
    "url": "http://arxiv.org/abs/2509.19087v1",
    "authors": [
      "Ganesh Mallya",
      "Yotam Gigi",
      "Dahun Kim",
      "Maxim Neumann",
      "Genady Beryozkin",
      "Tomer Shekel",
      "Anelia Angelova"
    ],
    "published": "2025-09-23",
    "abstract": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models' understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data.",
    "categories": [
      "geo_reasoning",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Can multimodal representation learning by alignment preserve modality-specific information?",
    "url": "http://arxiv.org/abs/2509.17943v1",
    "authors": [
      "Romain Thoreau",
      "Jessie Levillain",
      "Dawa Derksen"
    ],
    "published": "2025-09-22",
    "abstract": "Combining multimodal data is a key issue in a wide range of machine learning tasks, including many remote sensing problems. In Earth observation, early multimodal data fusion methods were based on specific neural network architectures and supervised learning. Ever since, the scarcity of labeled data has motivated self-supervised learning techniques. State-of-the-art multimodal representation learning techniques leverage the spatial alignment between satellite data from different modalities acquired over the same geographic area in order to foster a semantic alignment in the latent space. In this paper, we investigate how this methods can preserve task-relevant information that is not shared across modalities. First, we show, under simplifying assumptions, when alignment strategies fundamentally lead to an information loss. Then, we support our theoretical insight through numerical experiments in more realistic settings. With those theoretical and empirical evidences, we hope to support new developments in contrastive learning for the combination of multimodal satellite data. Our code and data is publicly available at https://github.com/Romain3Ch216/alg_maclean_25.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Remote Sensing-Oriented World Model",
    "url": "http://arxiv.org/abs/2509.17808v3",
    "authors": [
      "Yuxi Lu",
      "Biao Wu",
      "Zhidong Li",
      "Kunqi Li",
      "Chenya Huang",
      "Huacan Wang",
      "Qizhen Lan",
      "Ronghao Chen",
      "Ling Chen",
      "Bin Liang"
    ],
    "published": "2025-09-22",
    "abstract": "World models have shown potential in artificial intelligence by predicting and reasoning about world states beyond direct observations. However, existing approaches are predominantly evaluated in synthetic environments or constrained scene settings, limiting their validation in real-world contexts with broad spatial coverage and complex semantics. Meanwhile, remote sensing applications urgently require spatial reasoning capabilities for disaster response and urban planning. This paper bridges these gaps by introducing the first framework for world modeling in remote sensing. We formulate remote sensing world modeling as direction-conditioned spatial extrapolation, where models generate semantically consistent adjacent image tiles given a central observation and directional instruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing World-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks across four scenarios: general, flood, urban, and rural. RSWISE combines visual fidelity assessment with instruction compliance evaluation using GPT-4o as a semantic judge, ensuring models genuinely perform spatial reasoning rather than simple replication. Afterwards, we present RemoteBAGEL, a unified multimodal model fine-tuned on remote sensing data for spatial extrapolation tasks. Extensive experiments demonstrate that RemoteBAGEL consistently outperforms state-of-the-art baselines on RSWISE.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation",
    "url": "http://arxiv.org/abs/2509.17206v1",
    "authors": [
      "Gunner Stone",
      "Sushmita Sarker",
      "Alireza Tavakkoli"
    ],
    "published": "2025-09-21",
    "abstract": "Generating realistic 3D point clouds is a fundamental problem in computer vision with applications in remote sensing, robotics, and digital object modeling. Existing generative approaches primarily capture geometry, and when semantics are considered, they are typically imposed post hoc through external segmentation or clustering rather than integrated into the generative process itself. We propose a diffusion-based framework that embeds per-point semantic conditioning directly within generation. Each point is associated with a conditional variable corresponding to its semantic label, which guides the diffusion dynamics and enables the joint synthesis of geometry and semantics. This design produces point clouds that are both structurally coherent and segmentation-aware, with object parts explicitly represented during synthesis. Through a comparative analysis of guided and unguided diffusion processes, we demonstrate the significant impact of conditional variables on diffusion dynamics and generation quality. Extensive experiments validate the efficacy of our approach, producing detailed and accurate 3D point clouds tailored to specific parts and features.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation",
    "url": "http://arxiv.org/abs/2509.13229v1",
    "authors": [
      "Hugo Carlesso",
      "Josiane Mothe",
      "Radu Tudor Ionescu"
    ],
    "published": "2025-09-16",
    "abstract": "Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data, e.g. cloud-covered areas. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data complexity during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at https://github.com/hugocarlesso/CMTSSL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji",
    "url": "http://arxiv.org/abs/2509.13388v2",
    "authors": [
      "Yadvendra Gurjar",
      "Ruoni Wan",
      "Ehsan Farahbakhsh",
      "Rohitash Chandra"
    ],
    "published": "2025-09-16",
    "abstract": "As a developing country, Fiji is facing rapid urbanisation, which is visible in the massive development projects that include housing, roads, and civil works. In this study, we present machine learning and remote sensing frameworks to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The ultimate goal of this study is to provide technical support in land cover/land use modelling and change detection. We used Landsat-8 satellite image for the study region and created our training dataset with labels for supervised machine learning. We used Google Earth Engine and unsupervised machine learning via k-means clustering to generate the land cover map. We used convolutional neural networks to classify the selected regions' land cover types. We present a visualisation of change detection, highlighting urban area changes over time to monitor changes in the map.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Vessel Detection and Localization Using Distributed Acoustic Sensing in Submarine Optical Fiber Cables",
    "url": "http://arxiv.org/abs/2509.11614v3",
    "authors": [
      "Erick Eduardo Ramirez-Torres",
      "Javier Macias-Guarasa",
      "Daniel Pizarro-Perez",
      "Javier Tejedor",
      "Sira Elena Palazuelos-Cagigas",
      "Pedro J. Vidal-Moreno",
      "Sonia Martin-Lopez",
      "Miguel Gonzalez-Herraez",
      "Roel Vanthillo"
    ],
    "published": "2025-09-15",
    "abstract": "Submarine cables play a critical role in global internet connectivity, energy transmission, and communication but remain vulnerable to accidental damage and sabotage. Recent incidents in the Baltic Sea highlighted the need for enhanced monitoring to protect this vital infrastructure. Traditional vessel detection methods, such as synthetic aperture radar, video surveillance, and multispectral satellite imagery, face limitations in real-time processing, adverse weather conditions, and coverage range. This paper explores Distributed Acoustic Sensing (DAS) as an alternative by repurposing submarine telecommunication cables as large-scale acoustic sensor arrays. DAS offers continuous real-time monitoring, operates independently of cooperative systems like the \"Automatic Identification System\" (AIS), being largely unaffected by lighting or weather conditions. However, existing research on DAS for vessel tracking is limited in scale and lacks validation under real-world conditions. To address these gaps, a general and systematic methodology is presented for vessel detection and distance estimation using DAS. Advanced machine learning models are applied to improve detection and localization accuracy in dynamic maritime environments. The approach is evaluated over a continuous ten-day period, covering diverse ship and operational conditions, representing one of the largest-scale DAS-based vessel monitoring studies to date, and for which we release the full evaluation dataset. Results demonstrate DAS as a practical tool for maritime surveillance, with an overall F1-score of over 90% in vessel detection, and a mean average error of 141 m for vessel distance estimation, bridging the gap between experimental research and real-world deployment.",
    "categories": [
      "remote_sensing",
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping",
    "url": "http://arxiv.org/abs/2509.09408v2",
    "authors": [
      "Jonas Schmidinger",
      "Viacheslav Barkov",
      "Sebastian Vogel",
      "Martin Atzmueller",
      "Gerard B M Heuvelink"
    ],
    "published": "2025-09-11",
    "abstract": "Machine learning and geostatistics are two fundamentally different frameworks for predicting and spatially mapping soil properties. Geostatistics leverages the spatial structure of soil properties, while machine learning captures the relationship between available environmental features and soil properties. We propose a hybrid framework that enriches ML with spatial context through engineering of 'spatial lag' features from ordinary kriging. We call this approach 'kriging prior regression' (KpR), as it follows the inverse logic of regression kriging. To evaluate this approach, we assessed both the point and probabilistic prediction performance of KpR, using the TabPFN model across six fieldscale datasets from LimeSoDa. These datasets included soil organic carbon, clay content, and pH, along with features derived from remote sensing and in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable uncertainty estimates and more accurate predictions in comparison to several other spatial techniques (e.g., regression/residual kriging with TabPFN), as well as to established non-spatial machine learning algorithms (e.g., random forest). Most notably, it significantly improved the average R2 by around 30% compared to machine learning algorithms without spatial context. This improvement was due to the strong prediction performance of the TabPFN algorithm itself and the complementary spatial information provided by KpR features. TabPFN is particularly effective for prediction tasks with small sample sizes, common in precision agriculture, whereas KpR can compensate for weak relationships between sensing features and soil properties when proximal soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a very robust and versatile modelling framework for digital soil mapping in precision agriculture.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Physics-informed waveform inversion using pretrained wavefield neural operators",
    "url": "http://arxiv.org/abs/2509.08967v2",
    "authors": [
      "Xinquan Huang",
      "Fu Wang",
      "Tariq Alkhalifah"
    ],
    "published": "2025-09-10",
    "abstract": "Full waveform inversion (FWI) is crucial for reconstructing high-resolution subsurface models, but it is often hindered, considering the limited data, by its null space resulting in low-resolution models, and more importantly, by its computational cost, especially if needed for real-time applications. Recent attempts to accelerate FWI using learned wavefield neural operators have shown promise in efficiency and differentiability, but typically suffer from noisy and unstable inversion performance. To address these limitations, we introduce a novel physics-informed FWI framework to enhance the inversion in accuracy while maintaining the efficiency of neural operator-based FWI. Instead of relying only on the L2 norm objective function via automatic differentiation, resulting in noisy model reconstruction, we integrate a physics constraint term in the loss function of FWI, improving the quality of the inverted velocity models. Specifically, starting with an initial model to simulate wavefields and then evaluating the loss over how much the resulting wavefield obeys the physical laws (wave equation) and matches the recorded data, we achieve a reduction in noise and artifacts. Numerical experiments using the OpenFWI and Overthrust models demonstrate our method's superior performance, offering cleaner and more accurate subsurface velocity than vanilla approaches. Considering the efficiency of the approach compared to FWI, this advancement represents a significant step forward in the practical application of FWI for real-time subsurface monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Benchmark Dataset for Satellite-Based Estimation and Detection of Rain",
    "url": "http://arxiv.org/abs/2509.08816v2",
    "authors": [
      "Simon Pfreundschuh",
      "Malarvizhi Arulraj",
      "Ali Behrangi",
      "Linda Bogerd",
      "Alan James Peixoto Calheiros",
      "Daniele Casella",
      "Neda Dolatabadi",
      "Clement Guilloteau",
      "Jie Gong",
      "Christian D. Kummerow",
      "Pierre Kirstetter",
      "Gyuwon Lee",
      "Maximilian Maahn",
      "Lisa Milani",
      "Giulia Panegrossi",
      "Rayana Palharini",
      "Veljko Petkovi\u0107",
      "Soorok Ryu",
      "Paolo San\u00f2",
      "Jackson Tan"
    ],
    "published": "2025-09-10",
    "abstract": "Accurately tracking the global distribution and evolution of precipitation is essential for both research and operational meteorology. Satellite observations remain the only means of achieving consistent, global-scale precipitation monitoring. While machine learning has long been applied to satellite-based precipitation retrieval, the absence of a standardized benchmark dataset has hindered fair comparisons between methods and limited progress in algorithm development.\n  To address this gap, the International Precipitation Working Group has developed SatRain, the first AI-ready benchmark dataset for satellite-based detection and estimation of rain, snow, graupel, and hail. SatRain includes multi-sensor satellite observations representative of the major platforms currently used in precipitation remote sensing, paired with high-quality reference estimates from ground-based radars corrected using rain gauge measurements. It offers a standardized evaluation protocol to enable robust and reproducible comparisons across machine learning approaches.\n  In addition to supporting algorithm evaluation, the diversity of sensors and inclusion of time-resolved geostationary observations make SatRain a valuable foundation for developing next-generation AI models to deliver more accurate, detailed, and globally consistent precipitation estimates.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Self-supervised Learning for Hyperspectral Images of Trees",
    "url": "http://arxiv.org/abs/2509.05630v1",
    "authors": [
      "Moqsadur Rahman",
      "Saurav Kumar",
      "Santosh S. Palmate",
      "M. Shahriar Hossain"
    ],
    "published": "2025-09-06",
    "abstract": "Aerial remote sensing using multispectral and RGB imagers has provided a critical impetus to precision agriculture. Analysis of the hyperspectral images with limited or no labels is challenging. This paper focuses on self-supervised learning to create neural network embeddings reflecting vegetation properties of trees from aerial hyperspectral images of crop fields. Experimental results demonstrate that a constructed tree representation, using a vegetation property-related embedding space, performs better in downstream machine learning tasks compared to the direct use of hyperspectral vegetation properties as tree representations.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena",
    "url": "http://arxiv.org/abs/2509.03500v1",
    "authors": [
      "Itai Zilberstein",
      "Alberto Candela",
      "Steve Chien"
    ],
    "published": "2025-09-03",
    "abstract": "Advancements in onboard computing mean remote sensing agents can employ state-of-the-art computer vision and machine learning at the edge. These capabilities can be leveraged to unlock new rare, transient, and pinpoint measurements of dynamic science phenomena. In this paper, we present an automated workflow that synthesizes the detection of these dynamic events in look-ahead satellite imagery with autonomous trajectory planning for a follow-up high-resolution sensor to obtain pinpoint measurements. We apply this workflow to the use case of observing volcanic plumes. We analyze classification approaches including traditional machine learning algorithms and convolutional neural networks. We present several trajectory planning algorithms that track the morphological features of a plume and integrate these algorithms with the classifiers. We show through simulation an order of magnitude increase in the utility return of the high-resolution instrument compared to baselines while maintaining efficient runtimes.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Invariant Features for Global Crop Type Classification",
    "url": "http://arxiv.org/abs/2509.03497v2",
    "authors": [
      "Xin-Yi Tong",
      "Sherrie Wang"
    ],
    "published": "2025-09-03",
    "abstract": "Accurately obtaining crop type and its spatial distribution at a global scale is critical for food security, agricultural policy-making, and sustainable development. Remote sensing offers an efficient solution for large-scale crop classification, but the limited availability of reliable ground samples in many regions constrains applicability across geographic areas. To address performance declines under geospatial shifts, this study identifies remote sensing features that are invariant to geographic variation and proposes strategies to enhance cross-regional generalization. We construct CropGlobe, a global crop type dataset with 300,000 pixel-level samples from eight countries across five continents, covering six major food and industrial crops (corn, soybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage, CropGlobe enables a systematic evaluation under cross-country, cross-continent, and cross-hemisphere transfer. We compare the transferability of temporal multi-spectral features (Sentinel-2-based 1D/2D median features and harmonic coefficients) and hyperspectral features (from EMIT). To improve generalization under spectral and phenological shifts, we design CropNet, a lightweight and robust CNN tailored for pixel-level crop classification, coupled with temporal data augmentation (time shift, time scale, and magnitude warping) that simulates realistic cross-regional phenology. Experiments show that 2D median temporal features from Sentinel-2 consistently exhibit the strongest invariance across all transfer scenarios, and augmentation further improves robustness, particularly when training data diversity is limited. Overall, the work identifies more invariant feature representations that enhance geographic transferability and suggests a promising path toward scalable, low-cost crop type applications across globally diverse regions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Solving Imaging Inverse Problems Using Plug-and-Play Denoisers: Regularization and Optimization Perspectives",
    "url": "http://arxiv.org/abs/2509.03475v2",
    "authors": [
      "Hong Ye Tan",
      "Subhadip Mukherjee",
      "Junqi Tang"
    ],
    "published": "2025-09-03",
    "abstract": "Inverse problems lie at the heart of modern imaging science, with broad applications in areas such as medical imaging, remote sensing, and microscopy. Recent years have witnessed a paradigm shift in solving imaging inverse problems, where data-driven regularizers are used increasingly, leading to remarkably high-fidelity reconstruction. A particularly notable approach for data-driven regularization is to use learned image denoisers as implicit priors in iterative image reconstruction algorithms. This chapter presents a comprehensive overview of this powerful and emerging class of algorithms, commonly referred to as plug-and-play (PnP) methods. We begin by providing a brief background on image denoising and inverse problems, followed by a short review of traditional regularization strategies. We then explore how proximal splitting algorithms, such as the alternating direction method of multipliers (ADMM) and proximal gradient descent (PGD), can naturally accommodate learned denoisers in place of proximal operators, and under what conditions such replacements preserve convergence. The role of Tweedie's formula in connecting optimal Gaussian denoisers and score estimation is discussed, which lays the foundation for regularization-by-denoising (RED) and more recent diffusion-based posterior sampling methods. We discuss theoretical advances regarding the convergence of PnP algorithms, both within the RED and proximal settings, emphasizing the structural assumptions that the denoiser must satisfy for convergence, such as non-expansiveness, Lipschitz continuity, and local homogeneity. We also address practical considerations in algorithm design, including choices of denoiser architecture and acceleration strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories",
    "url": "http://arxiv.org/abs/2508.21570v1",
    "authors": [
      "Bo Li",
      "Yingqi Feng",
      "Ming Jin",
      "Xin Zheng",
      "Yufei Tang",
      "Laurent Cherubin",
      "Alan Wee-Chung Liew",
      "Can Wang",
      "Qinghua Lu",
      "Jingwei Yao",
      "Shirui Pan",
      "Hong Zhang",
      "Xingquan Zhu"
    ],
    "published": "2025-08-29",
    "abstract": "Ocean salinity plays a vital role in circulation, climate, and marine ecosystems, yet its measurement is often sparse, irregular, and noisy, especially in drifter-based datasets. Traditional approaches, such as remote sensing and optimal interpolation, rely on linearity and stationarity, and are limited by cloud cover, sensor drift, and low satellite revisit rates. While machine learning models offer flexibility, they often fail under severe sparsity and lack principled ways to incorporate physical covariates without specialized sensors. In this paper, we introduce the OceAn Salinity Imputation System (OASIS), a novel diffusion adversarial framework designed to address these challenges.",
    "categories": [
      "ocean",
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing",
    "url": "http://arxiv.org/abs/2508.21402v1",
    "authors": [
      "Jakub Straka",
      "Ivan Gruber"
    ],
    "published": "2025-08-29",
    "abstract": "Self-supervised learning has emerged as a powerful tool for remote sensing, where large amounts of unlabeled data are available. In this work, we investigate the use of DINO, a contrastive self-supervised method, for pretraining on remote sensing imagery. We introduce SatDINO, a model tailored for representation learning in satellite imagery. Through extensive experiments on multiple datasets in multiple testing setups, we demonstrate that SatDINO outperforms other state-of-the-art methods based on much more common masked autoencoders (MAE) and achieves competitive results in multiple benchmarks.\n  We also provide a rigorous ablation study evaluating SatDINO's individual components. Finally, we propose a few novel enhancements, such as a new way to incorporate ground sample distance (GSD) encoding and adaptive view sampling. These enhancements can be used independently on our SatDINO model. Our code and trained models are available at: https://github.com/strakaj/SatDINO.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Inferring geometry and material properties from Mueller matrices with machine learning",
    "url": "http://arxiv.org/abs/2508.19713v1",
    "authors": [
      "Lars Doorenbos",
      "C. H. Lucas Patty",
      "Raphael Sznitman",
      "Pablo M\u00e1rquez-Neila"
    ],
    "published": "2025-08-27",
    "abstract": "Mueller matrices (MMs) encode information on geometry and material properties, but recovering both simultaneously is an ill-posed problem. We explore whether MMs contain sufficient information to infer surface geometry and material properties with machine learning. We use a dataset of spheres of various isotropic materials, with MMs captured over the full angular domain at five visible wavelengths (450-650 nm). We train machine learning models to predict material properties and surface normals using only these MMs as input. We demonstrate that, even when the material type is unknown, surface normals can be predicted and object geometry reconstructed. Moreover, MMs allow models to identify material types correctly. Further analyses show that diagonal elements are key for material characterization, and off-diagonal elements are decisive for normal estimation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts",
    "url": "http://arxiv.org/abs/2508.11349v1",
    "authors": [
      "Angela John",
      "Selvyn Allotey",
      "Till Koebe",
      "Alexandra Tyukavina",
      "Ingmar Weber"
    ],
    "published": "2025-08-15",
    "abstract": "Afforestation and reforestation are popular strategies for mitigating climate change by enhancing carbon sequestration. However, the effectiveness of these efforts is often self-reported by project developers, or certified through processes with limited external validation. This leads to concerns about data reliability and project integrity. In response to increasing scrutiny of voluntary carbon markets, this study presents a dataset on global afforestation and reforestation efforts compiled from primary (meta-)information and augmented with time-series satellite imagery and other secondary data. Our dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years. Since any remote sensing-based validation effort relies on the integrity of a planting site's geographic boundary, this dataset introduces a standardized assessment of the provided site-level location information, which we summarize in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity Score. We find that approximately 79\\% of the georeferenced planting sites monitored fail on at least 1 out of 10 LDIS indicators, while 15\\% of the monitored projects lack machine-readable georeferenced data in the first place. In addition to enhancing accountability in the voluntary carbon market, the presented dataset also holds value as training data for e.g. computer vision-related tasks with millions of linked Sentinel-2 and Planetscope satellite images.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Can Multitask Learning Enhance Model Explainability?",
    "url": "http://arxiv.org/abs/2508.06966v1",
    "authors": [
      "Hiba Najjar",
      "Bushra Alshbib",
      "Andreas Dengel"
    ],
    "published": "2025-08-09",
    "abstract": "Remote sensing provides satellite data in diverse types and formats. The usage of multimodal learning networks exploits this diversity to improve model performance, except that the complexity of such networks comes at the expense of their interpretability. In this study, we explore how modalities can be leveraged through multitask learning to intrinsically explain model behavior. In particular, instead of additional inputs, we use certain modalities as additional targets to be predicted along with the main task. The success of this approach relies on the rich information content of satellite data, which remains as input modalities. We show how this modeling context provides numerous benefits: (1) in case of data scarcity, the additional modalities do not need to be collected for model inference at deployment, (2) the model performance remains comparable to the multimodal baseline performance, and in some cases achieves better scores, (3) prediction errors in the main task can be explained via the model behavior in the auxiliary task(s). We demonstrate the efficiency of our approach on three datasets, including segmentation, classification, and regression tasks. Code available at git.opendfki.de/hiba.najjar/mtl_explainability/.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets",
    "url": "http://arxiv.org/abs/2508.02871v1",
    "authors": [
      "J. Alex Hurt",
      "Trevor M. Bajkowski",
      "Grant J. Scott",
      "Curt H. Davis"
    ],
    "published": "2025-08-04",
    "abstract": "In 2012, AlexNet established deep convolutional neural networks (DCNNs) as the state-of-the-art in CV, as these networks soon led in visual tasks for many domains, including remote sensing. With the publication of Visual Transformers, we are witnessing the second modern leap in computational vision, and as such, it is imperative to understand how various transformer-based neural networks perform on satellite imagery. While transformers have shown high levels of performance in natural language processing and CV applications, they have yet to be compared on a large scale to modern remote sensing data. In this paper, we explore the use of transformer-based neural networks for object detection in high-resolution electro-optical satellite imagery, demonstrating state-of-the-art performance on a variety of publicly available benchmark data sets. We compare eleven distinct bounding-box detection and localization algorithms in this study, of which seven were published since 2020, and all eleven since 2015. The performance of five transformer-based architectures is compared with six convolutional networks on three state-of-the-art opensource high-resolution remote sensing imagery datasets ranging in size and complexity. Following the training and evaluation of thirty-three deep neural models, we then discuss and analyze model performance across various feature extraction methodologies and detection algorithms.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2508.01941v1",
    "authors": [
      "Andrea Dosi",
      "Semanto Mondal",
      "Rajib Chandra Ghosh",
      "Massimo Brescia",
      "Giuseppe Longo"
    ],
    "published": "2025-08-03",
    "abstract": "This work presents the results of a methodological transfer from remote sensing to healthcare, adapting AMBER -- a transformer-based model originally designed for multiband images, such as hyperspectral data -- to the task of 3D medical datacube segmentation. In this study, we use the AMBER architecture with Adaptive Fourier Neural Operators (AFNO) in place of the multi-head self-attention mechanism. While existing models rely on various forms of attention to capture global context, AMBER-AFNO achieves this through frequency-domain mixing, enabling a drastic reduction in model complexity. This design reduces the number of trainable parameters by over 80% compared to UNETR++, while maintaining a FLOPs count comparable to other state-of-the-art architectures. Model performance is evaluated on two benchmark 3D medical datasets -- ACDC and Synapse -- using standard metrics such as Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO achieves competitive or superior accuracy with significant gains in training efficiency, inference speed, and memory usage.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation",
    "url": "http://arxiv.org/abs/2508.00750v1",
    "authors": [
      "Prerana Ramkumar"
    ],
    "published": "2025-08-01",
    "abstract": "Generative Adversarial Networks (GANs) have achieved realistic super-resolution (SR) of images however, they lack semantic consistency and per-pixel confidence, limiting their credibility in critical remote sensing applications such as disaster response, urban planning and agriculture. This paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first SR framework designed for satellite imagery to integrate the ESRGAN, segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results (PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This novel model is valuable in satellite systems or UAVs that use wide field-of-view (FoV) cameras, trading off spatial resolution for coverage. The modular design allows integration in UAV data pipelines for on-board or post-processing SR to enhance imagery resulting due to motion blur, compression and sensor limitations. Further, the model is fine-tuned to evaluate its performance on cross domain applications. The tests are conducted on two drone based datasets which differ in altitude and imaging perspective. Performance evaluation of the fine-tuned models show a stronger adaptation to the Aerial Maritime Drone Dataset, whose imaging characteristics align with the training data, highlighting the importance of domain-aware training in SR-applications.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Segmentation",
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool",
    "url": "http://arxiv.org/abs/2508.00506v1",
    "authors": [
      "Tulsi Patel",
      "Mark W. Jones",
      "Thomas Redfern"
    ],
    "published": "2025-08-01",
    "abstract": "Machine learning for remote sensing imaging relies on up-to-date and accurate labels for model training and testing. Labelling remote sensing imagery is time and cost intensive, requiring expert analysis. Previous labelling tools rely on pre-labelled data for training in order to label new unseen data. In this work, we define an unsupervised pipeline for finding and labelling geographical areas of similar context and content within Sentinel-2 satellite imagery. Our approach removes limitations of previous methods by utilising segmentation with convolutional and graph neural networks to encode a more robust feature space for image comparison. Unlike previous approaches we segment the image into homogeneous regions of pixels that are grouped based on colour and spatial similarity. Graph neural networks are used to aggregate information about the surrounding segments enabling the feature representation to encode the local neighbourhood whilst preserving its own local information. This reduces outliers in the labelling tool, allows users to label at a granular level, and allows a rotationally invariant semantic relationship at the image level to be formed within the encoding space.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "XFMNet: Decoding Cross-Site and Nonstationary Water Patterns via Stepwise Multimodal Fusion for Long-Term Water Quality Forecasting",
    "url": "http://arxiv.org/abs/2508.08279v1",
    "authors": [
      "Ziqi Wang",
      "Hailiang Zhao",
      "Cheng Bao",
      "Wenzhuo Qian",
      "Yuhao Yang",
      "Xueqiang Sun",
      "Shuiguang Deng"
    ],
    "published": "2025-08-01",
    "abstract": "Long-term time-series forecasting is critical for environmental monitoring, yet water quality prediction remains challenging due to complex periodicity, nonstationarity, and abrupt fluctuations induced by ecological factors. These challenges are further amplified in multi-site scenarios that require simultaneous modeling of temporal and spatial dynamics. To tackle this, we introduce XFMNet, a stepwise multimodal fusion network that integrates remote sensing precipitation imagery to provide spatial and environmental context in river networks. XFMNet first aligns temporal resolutions between water quality series and remote sensing inputs via adaptive downsampling, followed by locally adaptive decomposition to disentangle trend and cycle components. A cross-attention gated fusion module dynamically integrates temporal patterns with spatial and ecological cues, enhancing robustness to nonstationarity and site-specific anomalies. Through progressive and recursive fusion, XFMNet captures both long-term trends and short-term fluctuations. Extensive experiments on real-world datasets demonstrate substantial improvements over state-of-the-art baselines, highlighting the effectiveness of XFMNet for spatially distributed time series prediction.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations",
    "url": "http://arxiv.org/abs/2507.23154v1",
    "authors": [
      "Sofiane Bouaziz",
      "Adel Hafiane",
      "Raphael Canals",
      "Rachid Nedjai"
    ],
    "published": "2025-07-30",
    "abstract": "Urban heatwaves, droughts, and land degradation are pressing and growing challenges in the context of climate change. A valuable approach to studying them requires accurate spatio-temporal information on land surface conditions. One of the most important variables for assessing and understanding these phenomena is Land Surface Temperature (LST), which is derived from satellites and provides essential information about the thermal state of the Earth's surface. However, satellite platforms inherently face a trade-off between spatial and temporal resolutions. To bridge this gap, we propose FuseTen, a novel generative framework that produces daily LST observations at a fine 10 m spatial resolution by fusing spatio-temporal observations derived from Sentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative architecture trained using an averaging-based supervision strategy grounded in physical principles. It incorporates attention and normalization modules within the fusion process and uses a PatchGAN discriminator to enforce realism. Experiments across multiple dates show that FuseTen outperforms linear baselines, with an average 32.06% improvement in quantitative metrics and 31.42% in visual fidelity. To the best of our knowledge, this is the first non-linear method to generate daily LST estimates at such fine spatial resolution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "SpecBPP: A Self-Supervised Learning Approach for Hyperspectral Representation and Soil Organic Carbon Estimation",
    "url": "http://arxiv.org/abs/2507.19781v1",
    "authors": [
      "Daniel La'ah Ayuba",
      "Jean-Yves Guillemaut",
      "Belen Marti-Cardona",
      "Oscar Mendez Maldonado"
    ],
    "published": "2025-07-26",
    "abstract": "Self-supervised learning has revolutionized representation learning in vision and language, but remains underexplored for hyperspectral imagery (HSI), where the sequential structure of spectral bands offers unique opportunities. In this work, we propose Spectral Band Permutation Prediction (SpecBPP), a novel self-supervised learning framework that leverages the inherent spectral continuity in HSI. Instead of reconstructing masked bands, SpecBPP challenges a model to recover the correct order of shuffled spectral segments, encouraging global spectral understanding. We implement a curriculum-based training strategy that progressively increases permutation difficulty to manage the factorial complexity of the permutation space. Applied to Soil Organic Carbon (SOC) estimation using EnMAP satellite data, our method achieves state-of-the-art results, outperforming both masked autoencoder (MAE) and joint-embedding predictive (JEPA) baselines. Fine-tuned on limited labeled samples, our model yields an $R^2$ of 0.9456, RMSE of 1.1053%, and RPD of 4.19, significantly surpassing traditional and self-supervised benchmarks. Our results demonstrate that spectral order prediction is a powerful pretext task for hyperspectral understanding, opening new avenues for scientific representation learning in remote sensing and beyond.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "JEPA",
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover",
    "url": "http://arxiv.org/abs/2507.18099v1",
    "authors": [
      "Naman Srivastava",
      "Joel D Joy",
      "Yash Dixit",
      "Swarup E",
      "Rakshit Ramesh"
    ],
    "published": "2025-07-24",
    "abstract": "Land Use Land Cover (LULC) mapping is essential for urban and resource planning, and is one of the key elements in developing smart and sustainable cities.This study evaluates advanced LULC mapping techniques, focusing on Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat Multispectral (MX) sensor images, followed by supervised and semi-supervised learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo Supervision (CPS). The CPS model is further refined with dynamic weighting, enhancing pseudo-label reliability during training. This comprehensive approach analyses the accuracy and utility of LULC mapping techniques for various urban planning applications. A case study of Hyderabad, India, illustrates significant land use changes due to rapid urbanization. By analyzing Cartosat MX images over time, we highlight shifts such as urban sprawl, shrinking green spaces, and expanding industrial areas. This demonstrates the practical utility of these techniques for urban planners and policymakers.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images",
    "url": "http://arxiv.org/abs/2507.13120v2",
    "authors": [
      "Xiaozheng Jiang",
      "Wei Zhang",
      "Xuerui Mao"
    ],
    "published": "2025-07-17",
    "abstract": "Detecting tiny objects in remote sensing (RS) imagery has been a long-standing challenge due to their extremely limited spatial information, weak feature representations, and dense distributions across complex backgrounds. Despite numerous efforts devoted, mainstream detectors still underperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a multi-stage feature fusion and enhancement model explicitly tailored for RS tiny object detection in various RS scenarios. RS-TinyNet comes with two novel designs: tiny object saliency modeling and feature integrity reconstruction. Guided by these principles, we design three step-wise feature enhancement modules. Among them, the multi-dimensional collaborative attention (MDCA) module employs multi-dimensional attention to enhance the saliency of tiny objects. Additionally, the auxiliary reversible branch (ARB) and a progressive fusion detection head (PFDH) module are introduced to preserve information flow and fuse multi-level features to bridge semantic gaps and retain structural detail. Comprehensive experiments on public RS dataset AI-TOD show that our RS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and 6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior detection performance in diverse RS scenarios. These results demonstrate that the proposed multi-stage feature fusion strategy offers an effective and practical solution for tiny object detection in complex RS environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "From Time-series Generation, Model Selection to Transfer Learning: A Comparative Review of Pixel-wise Approaches for Large-scale Crop Mapping",
    "url": "http://arxiv.org/abs/2507.12590v2",
    "authors": [
      "Judy Long",
      "Tao Liu",
      "Sean Alexander Woznicki",
      "Miljana Markovi\u0107",
      "Oskar Marko",
      "Molly Sears"
    ],
    "published": "2025-07-16",
    "abstract": "Crop mapping involves identifying and classifying crop types using spatial data, primarily derived from remote sensing imagery. This study presents the first comprehensive review of large-scale, pixel-wise crop mapping workflows, encompassing both conventional supervised methods and emerging transfer learning approaches. To identify the optimal time-series generation approaches and supervised crop mapping models, we conducted systematic experiments, comparing six widely adopted satellite image-based preprocessing methods, alongside eleven supervised pixel-wise classification models. Additionally, we assessed the synergistic impact of varied training sample sizes and variable combinations. Moreover, we identified optimal transfer learning techniques for different magnitudes of domain shift. The evaluation of optimal methods was conducted across five diverse agricultural sites. Landsat 8 served as the primary satellite data source. Labels come from CDL trusted pixels and field surveys.\n  Our findings reveal three key insights. First, fine-scale interval preprocessing paired with Transformer models consistently delivered optimal performance for both supervised and transferable workflows. RF offered rapid training and competitive performance in conventional supervised learning and direct transfer to similar domains. Second, transfer learning techniques enhanced workflow adaptability, with UDA being effective for homogeneous crop classes while fine-tuning remains robust across diverse scenarios. Finally, workflow choice depends heavily on the availability of labeled samples. With a sufficient sample size, supervised training typically delivers more accurate and generalizable results. Below a certain threshold, transfer learning that matches the level of domain shift is a viable alternative to achieve crop mapping. All code is publicly available to encourage reproducibility practice.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery",
    "url": "http://arxiv.org/abs/2507.13385v1",
    "authors": [
      "Arjun Rao",
      "Esther Rolf"
    ],
    "published": "2025-07-15",
    "abstract": "A large variety of geospatial data layers is available around the world ranging from remotely-sensed raster data like satellite imagery, digital elevation models, predicted land cover maps, and human-annotated data, to data derived from environmental sensors such as air temperature or wind speed data. A large majority of machine learning models trained on satellite imagery (SatML), however, are designed primarily for optical input modalities such as multi-spectral satellite imagery. To better understand the value of using other input modalities alongside optical imagery in supervised learning settings, we generate augmented versions of SatML benchmark tasks by appending additional geographic data layers to datasets spanning classification, regression, and segmentation. Using these augmented datasets, we find that fusing additional geographic inputs with optical imagery can significantly improve SatML model performance. Benefits are largest in settings where labeled data are limited and in geographic out-of-sample settings, suggesting that multi-modal inputs may be especially valuable for data-efficiency and out-of-sample performance of SatML models. Surprisingly, we find that hard-coded fusion strategies outperform learned variants, with interesting implications for future work.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening",
    "url": "http://arxiv.org/abs/2507.10461v3",
    "authors": [
      "Tao Tang",
      "Chengxu Yang"
    ],
    "published": "2025-07-14",
    "abstract": "Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area",
    "url": "http://arxiv.org/abs/2507.10084v2",
    "authors": [
      "Haonan Chen",
      "Xin Tong"
    ],
    "published": "2025-07-14",
    "abstract": "The Tibetan Plateau, known as the Asian Water Tower, faces significant water security challenges due to its high sensitivity to climate change. Advancing Earth observation for sustainable water monitoring is thus essential for building climate resilience in this region. This study proposes a two-stage transfer learning strategy using the SegFormer model to overcome domain shift and data scarcit--key barriers in developing robust AI for climate-sensitive applications. After pre-training on a diverse source domain, our model was fine-tuned for the arid Zhada Tulin area. Experimental results show a substantial performance boost: the Intersection over Union (IoU) for water body segmentation surged from 25.50% (direct transfer) to 64.84%. This AI-driven accuracy is crucial for disaster risk reduction, particularly in monitoring flash flood-prone systems. More importantly, the high-precision map reveals a highly concentrated spatial distribution of water, with over 80% of the water area confined to less than 20% of the river channel length. This quantitative finding provides crucial evidence for understanding hydrological processes and designing targeted water management and climate adaptation strategies. Our work thus demonstrates an effective technical solution for monitoring arid plateau regions and contributes to advancing AI-powered Earth observation for disaster preparedness in critical transboundary river headwaters.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction",
    "url": "http://arxiv.org/abs/2507.07388v1",
    "authors": [
      "Zesheng Liu",
      "Maryam Rahnemoonfar"
    ],
    "published": "2025-07-10",
    "abstract": "Gaining a deeper understanding of the thickness and variability of internal ice layers in Radar imagery is essential in monitoring the snow accumulation, better evaluating ice dynamics processes, and minimizing uncertainties in climate models. Radar sensors, capable of penetrating ice, capture detailed radargram images of internal ice layers. In this work, we introduce GRIT, graph transformer for ice layer thickness. GRIT integrates an inductive geometric graph learning framework with an attention mechanism, designed to map the relationships between shallow and deeper ice layers. Compared to baseline graph neural networks, GRIT demonstrates consistently lower prediction errors. These results highlight the attention mechanism's effectiveness in capturing temporal changes across ice layers, while the graph transformer combines the strengths of transformers for learning long-range dependencies with graph neural networks for capturing spatial patterns, enabling robust modeling of complex spatiotemporal dynamics.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "GNN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction",
    "url": "http://arxiv.org/abs/2507.06806v3",
    "authors": [
      "Eya Cherif",
      "Arthur Ouaknine",
      "Luke A. Brown",
      "Phuong D. Dao",
      "Kyle R. Kovach",
      "Bing Lu",
      "Daniel Mederer",
      "Hannes Feilhauer",
      "Teja Kattenborn",
      "David Rolnick"
    ],
    "published": "2025-07-09",
    "abstract": "Plant traits such as leaf carbon content and leaf mass are essential variables in the study of biodiversity and climate change. However, conventional field sampling cannot feasibly cover trait variation at ecologically meaningful spatial scales. Machine learning represents a valuable solution for plant trait prediction across ecosystems, leveraging hyperspectral data from remote sensing. Nevertheless, trait prediction from hyperspectral data is challenged by label scarcity and substantial domain shifts (\\eg across sensors, ecological distributions), requiring robust cross-domain methods. Here, we present GreenHyperSpectra, a pretraining dataset encompassing real-world cross-sensor and cross-ecosystem samples designed to benchmark trait prediction with semi- and self-supervised methods. We adopt an evaluation framework encompassing in-distribution and out-of-distribution scenarios. We successfully leverage GreenHyperSpectra to pretrain label-efficient multi-output regression models that outperform the state-of-the-art supervised baseline. Our empirical analyses demonstrate substantial improvements in learning spectral representations for trait prediction, establishing a comprehensive methodological framework to catalyze research at the intersection of representation learning and plant functional traits assessment. All code and data are available at: https://github.com/echerif18/HyspectraSSL.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing",
    "url": "http://arxiv.org/abs/2507.04842v2",
    "authors": [
      "Colin Laganier",
      "Liam Fletcher",
      "Elim Kwan",
      "Richard Walters",
      "Victoria Nockles"
    ],
    "published": "2025-07-07",
    "abstract": "Rapid analysis of satellite imagery within minutes-to-hours of acquisition is increasingly vital for many remote sensing applications, and is an essential component for developing next-generation autonomous and distributed satellite systems. On-satellite machine learning (ML) has the potential for such rapid analysis, by overcoming latency associated with intermittent satellite connectivity to ground stations or relay satellites, but state-of-the-art models are often too large or power-hungry for on-board deployment. Vessel detection using Synthetic Aperture Radar (SAR) is a critical time-sensitive application in maritime security that exemplifies this challenge. SAR vessel detection has previously been demonstrated only by ML models that either are too large for satellite deployment, have not been developed for sufficiently low-power hardware, or have only been tested on small SAR datasets that do not sufficiently represent the difficulty of the real-world task. Here we systematically explore a suite of architectural adaptations to develop a novel YOLOv8 architecture optimized for this task and FPGA-based processing. We deploy our model on a Kria KV260 MPSoC, and show it can analyze a ~700 megapixel SAR image in less than a minute, within common satellite power constraints (<10W). Our model has detection and classification performance only ~2% and 3% lower than values from state-of-the-art GPU-based models on the largest and most diverse open SAR vessel dataset, xView3-SAR, despite being ~50 and ~2500 times more computationally efficient. This work represents a key contribution towards on-satellite ML for time-critical SAR analysis, and more autonomous, scalable satellites.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions",
    "url": "http://arxiv.org/abs/2507.01547v1",
    "authors": [
      "Ubada El Joulani",
      "Tatiana Kalganova",
      "Stergios-Aristoteles Mitoulis",
      "Sotirios Argyroudis"
    ],
    "published": "2025-07-02",
    "abstract": "Critical infrastructure, such as transport networks, underpins economic growth by enabling mobility and trade. However, ageing assets, climate change impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging from natural disasters to cyber attacks and conflicts pose growing risks to their resilience and functionality. This review paper explores how emerging digital technologies, specifically Artificial Intelligence (AI), can enhance damage assessment and monitoring of transport infrastructure. A systematic literature review examines existing AI models and datasets for assessing damage in roads, bridges, and other critical infrastructure impacted by natural disasters. Special focus is given to the unique challenges and opportunities associated with bridge damage detection due to their structural complexity and critical role in connectivity. The integration of SAR (Synthetic Aperture Radar) data with AI models is also discussed, with the review revealing a critical research gap: a scarcity of studies applying AI models to SAR data for comprehensive bridge damage assessment. Therefore, this review aims to identify the research gaps and provide foundations for AI-driven solutions for assessing and monitoring critical transport infrastructures.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Advancements in Weed Mapping: A Systematic Review",
    "url": "http://arxiv.org/abs/2507.01269v1",
    "authors": [
      "Mohammad Jahanbakht",
      "Alex Olsen",
      "Ross Marchant",
      "Emilie Fillols",
      "Mostafa Rahimi Azghadi"
    ],
    "published": "2025-07-02",
    "abstract": "Weed mapping plays a critical role in precision management by providing accurate and timely data on weed distribution, enabling targeted control and reduced herbicide use. This minimizes environmental impacts, supports sustainable land management, and improves outcomes across agricultural and natural environments. Recent advances in weed mapping leverage ground-vehicle Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The resulting data are processed using advanced techniques including big data analytics and machine learning, significantly improving the spatial and temporal resolution of weed maps and enabling site-specific management decisions. Despite a growing body of research in this domain, there is a lack of comprehensive literature reviews specifically focused on weed mapping. In particular, the absence of a structured analysis spanning the entire mapping pipeline, from data acquisition to processing techniques and mapping tools, limits progress in the field. This review addresses these gaps by systematically examining state-of-the-art methods in data acquisition (sensor and platform technologies), data processing (including annotation and modelling), and mapping techniques (such as spatiotemporal analysis and decision support tools). Following PRISMA guidelines, we critically evaluate and synthesize key findings from the literature to provide a holistic understanding of the weed mapping landscape. This review serves as a foundational reference to guide future research and support the development of efficient, scalable, and sustainable weed management systems.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution",
    "url": "http://arxiv.org/abs/2506.23566v1",
    "authors": [
      "Luigi Sigillo",
      "Renato Giamba",
      "Danilo Comminiello"
    ],
    "published": "2025-06-30",
    "abstract": "The acquisition of high-resolution satellite imagery is often constrained by the spatial and temporal limitations of satellite sensors, as well as the high costs associated with frequent observations. These challenges hinder applications such as environmental monitoring, disaster response, and agricultural management, which require fine-grained and high-resolution data. In this paper, we propose MWT-Diff, an innovative framework for satellite image super-resolution (SR) that combines latent diffusion models with wavelet transforms to address these challenges. At the core of the framework is a novel metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates embeddings that capture metadata attributes, multi-scale frequency information, and temporal relationships. The embedded feature representations steer the hierarchical diffusion dynamics, through which the model progressively reconstructs high-resolution satellite imagery from low-resolution inputs. This process preserves critical spatial characteristics including textural patterns, boundary discontinuities, and high-frequency spectral components essential for detailed remote sensing analysis. The comparative analysis of MWT-Diff across multiple datasets demonstrated favorable performance compared to recent approaches, as measured by standard perceptual quality metrics including FID and LPIPS.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Diffusion Models"
    ],
    "applications": [
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification",
    "url": "http://arxiv.org/abs/2506.23462v1",
    "authors": [
      "Manaswi Kulahara",
      "Gautam Siddharth Kashyap",
      "Nipun Joshi",
      "Arpita Soni"
    ],
    "published": "2025-06-30",
    "abstract": "Effective disaster management requires timely and accurate insights, yet traditional methods struggle to integrate multimodal data such as images, weather records, and textual reports. To address this, we propose DisasterNet-LLM, a specialized Large Language Model (LLM) designed for comprehensive disaster analysis. By leveraging advanced pretraining, cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM excels in disaster classification. Experimental results demonstrate its superiority over state-of-the-art models, achieving higher accuracy of 89.5%, an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal disaster classification tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Weakly Supervised Object Segmentation by Background Conditional Divergence",
    "url": "http://arxiv.org/abs/2506.22505v2",
    "authors": [
      "Hassan Baker",
      "Matthew S. Emigh",
      "Austin J. Brockmeier"
    ],
    "published": "2025-06-25",
    "abstract": "As a computer vision task, automatic object segmentation remains challenging in specialized image domains without massive labeled data, such as synthetic aperture sonar images, remote sensing, biomedical imaging, etc. In any domain, obtaining pixel-wise segmentation masks is expensive. In this work, we propose a method for training a masking network to perform binary object segmentation using weak supervision in the form of image-wise presence or absence of an object of interest, which provides less information but may be obtained more quickly from manual or automatic labeling. A key step in our method is that the segmented objects can be placed into background-only images to create realistic images of the objects with counterfactual backgrounds. To create a contrast between the original and counterfactual background images, we propose to first cluster the background-only images and then, during learning, create counterfactual images that blend objects segmented from their original source backgrounds to backgrounds chosen from a targeted cluster. One term in the training loss is the divergence between these counterfactual images and the real object images with backgrounds of the target cluster. The other term is a supervised loss for background-only images. While an adversarial critic could provide the divergence, we use sample-based divergences. We conduct experiments on side-scan and synthetic aperture sonar in which our approach succeeds compared to previous unsupervised segmentation baselines that were only tested on natural images. Furthermore, to show generality we extend our experiments to natural images, obtaining reasonable performance with our method that avoids pretrained networks, generative networks, and adversarial critics. The code for this work can be found at \\href{GitHub}{https://github.com/bakerhassan/WSOS}.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Vision Transformer-Based Time-Series Image Reconstruction for Cloud-Filling Applications",
    "url": "http://arxiv.org/abs/2506.19591v1",
    "authors": [
      "Lujun Li",
      "Yiqun Wang",
      "Radu State"
    ],
    "published": "2025-06-24",
    "abstract": "Cloud cover in multispectral imagery (MSI) poses significant challenges for early season crop mapping, as it leads to missing or corrupted spectral information. Synthetic aperture radar (SAR) data, which is not affected by cloud interference, offers a complementary solution, but lack sufficient spectral detail for precise crop mapping. To address this, we propose a novel framework, Time-series MSI Image Reconstruction using Vision Transformer (ViT), to reconstruct MSI data in cloud-covered regions by leveraging the temporal coherence of MSI and the complementary information from SAR from the attention mechanism. Comprehensive experiments, using rigorous reconstruction evaluation metrics, demonstrate that Time-series ViT framework significantly outperforms baselines that use non-time-series MSI and SAR or time-series MSI without SAR, effectively enhancing MSI image reconstruction in cloud-covered regions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing",
    "url": "http://arxiv.org/abs/2506.18587v1",
    "authors": [
      "Antoine Saget",
      "Baptiste Lafabregue",
      "Antoine Cornu\u00e9jols",
      "Pierre Gan\u00e7arski"
    ],
    "published": "2025-06-23",
    "abstract": "Given the abundance of unlabeled Satellite Image Time Series (SITS) and the scarcity of labeled data, contrastive self-supervised pretraining emerges as a natural tool to leverage this vast quantity of unlabeled data. However, designing effective data augmentations for contrastive learning remains challenging for time series. We introduce a novel resampling-based augmentation strategy that generates positive pairs by upsampling time series and extracting disjoint subsequences while preserving temporal coverage. We validate our approach on multiple agricultural classification benchmarks using Sentinel-2 imagery, showing that it outperforms common alternatives such as jittering, resizing, and masking. Further, we achieve state-of-the-art performance on the S2-Agri100 dataset without employing spatial information or temporal encodings, surpassing more complex masked-based SSL frameworks. Our method offers a simple, yet effective, contrastive learning augmentation for remote sensing time series.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder",
    "url": "http://arxiv.org/abs/2506.14906v1",
    "authors": [
      "Robert Czupryniak",
      "Abhishek Chakraborty",
      "Andrew N. Jordan",
      "John C. Howell"
    ],
    "published": "2025-06-17",
    "abstract": "We apply machine learning methods to demonstrate range superresolution in remote sensing radar detection. Specifically, we implement a denoising autoencoder to estimate the distance between two equal intensity scatterers in the subwavelength regime. The machine learning models are trained on waveforms subject to a bandlimit constraint such that ranges much smaller than the inverse bandlimit are optimized in their precision. The autoencoder achieves effective dimensionality reduction, with the bottleneck layer exhibiting a strong and consistent correlation with the true scatterer separation. We confirm reproducibility across different training sessions and network initializations by analyzing the scaled encoder outputs and their robustness to noise. We investigate the behavior of the bottleneck layer for the following types of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse, and a theoretically near-optimal pulse created from a spherical Bessel function basis. The Bessel signal performs best, followed by the triangle wave, with the sinc signal performing worst, highlighting the crucial role of signal design in the success of machine-learning-based range resolution.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "AgriPotential: A Novel Multi-Spectral and Multi-Temporal Remote Sensing Dataset for Agricultural Potentials",
    "url": "http://arxiv.org/abs/2506.11740v3",
    "authors": [
      "Mohammad El Sakka",
      "Caroline De Pourtales",
      "Lotfi Chaari",
      "Josiane Mothe"
    ],
    "published": "2025-06-13",
    "abstract": "Remote sensing has emerged as a critical tool for large-scale Earth monitoring and land management. In this paper, we introduce AgriPotential, a novel benchmark dataset composed of Sentinel-2 satellite imagery captured over multiple months. The dataset provides pixel-level annotations of agricultural potentials for three major crop types - viticulture, market gardening, and field crops - across five ordinal classes. AgriPotential supports a broad range of machine learning tasks, including ordinal regression, multi-label classification, and spatio-temporal modeling. The data cover diverse areas in Southern France, offering rich spectral information. AgriPotential is the first public dataset designed specifically for agricultural potential prediction, aiming to improve data-driven approaches to sustainable land use planning. The dataset and the code are freely accessible at: https://zenodo.org/records/15551829",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Data-driven approaches to inverse problems",
    "url": "http://arxiv.org/abs/2506.11732v1",
    "authors": [
      "Carola-Bibiane Sch\u00f6nlieb",
      "Zakhar Shumaylov"
    ],
    "published": "2025-06-13",
    "abstract": "Inverse problems are concerned with the reconstruction of unknown physical quantities using indirect measurements and are fundamental across diverse fields such as medical imaging, remote sensing, and material sciences. These problems serve as critical tools for visualizing internal structures beyond what is visible to the naked eye, enabling quantification, diagnosis, prediction, and discovery. However, most inverse problems are ill-posed, necessitating robust mathematical treatment to yield meaningful solutions. While classical approaches provide mathematically rigorous and computationally stable solutions, they are constrained by the ability to accurately model solution properties and implement them efficiently.\n  A more recent paradigm considers deriving solutions to inverse problems in a data-driven manner. Instead of relying on classical mathematical modeling, this approach utilizes highly over-parameterized models, typically deep neural networks, which are adapted to specific inverse problems using carefully selected training data. Current approaches that follow this new paradigm distinguish themselves through solution accuracy paired with computational efficiency that was previously inconceivable.\n  These notes offer an introduction to this data-driven paradigm for inverse problems. The first part of these notes will provide an introduction to inverse problems, discuss classical solution strategies, and present some applications. The second part will delve into modern data-driven approaches, with a particular focus on adversarial regularization and provably convergent linear plug-and-play denoisers. Throughout the presentation of these methodologies, their theoretical properties will be discussed, and numerical examples will be provided. The lecture series will conclude with a discussion of open problems and future perspectives in the field.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "A Comparative Study of U-Net Architectures for Change Detection in Satellite Images",
    "url": "http://arxiv.org/abs/2506.07925v1",
    "authors": [
      "Yaxita Amin",
      "Naimisha S Trivedi",
      "Rashmi Bhattad"
    ],
    "published": "2025-06-09",
    "abstract": "Remote sensing change detection is essential for monitoring the everchanging landscapes of the Earth. The U-Net architecture has gained popularity for its capability to capture spatial information and perform pixel-wise classification. However, their application in the Remote sensing field remains largely unexplored. Therefore, this paper fill the gap by conducting a comprehensive analysis of 34 papers. This study conducts a comparison and analysis of 18 different U-Net variations, assessing their potential for detecting changes in remote sensing. We evaluate both benefits along with drawbacks of each variation within the framework of this particular application. We emphasize variations that are explicitly built for change detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture. The analysis highlights the significance of aspects such as managing data from different time periods and collecting relationships over a long distance to enhance the precision of change detection. This study provides valuable insights for researchers and practitioners that choose U-Net versions for remote sensing change detection tasks.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "SDS-Net: Shallow-Deep Synergism-detection Network for infrared small target detection",
    "url": "http://arxiv.org/abs/2506.06042v1",
    "authors": [
      "Taoran Yue",
      "Xiaojin Lu",
      "Jiaxi Cai",
      "Yuanping Chen",
      "Shibing Chu"
    ],
    "published": "2025-06-06",
    "abstract": "Current CNN-based infrared small target detection(IRSTD) methods generally overlook the heterogeneity between shallow and deep features, leading to inefficient collaboration between shallow fine grained structural information and deep high-level semantic representations. Additionally, the dependency relationships and fusion mechanisms across different feature hierarchies lack systematic modeling, which fails to fully exploit the complementarity of multilevel features. These limitations hinder IRSTD performance while incurring substantial computational costs. To address these challenges, this paper proposes a shallow-deep synergistic detection network (SDS-Net) that efficiently models multilevel feature representations to increase both the detection accuracy and computational efficiency in IRSTD tasks. SDS-Net introduces a dual-branch architecture that separately models the structural characteristics and semantic properties of features, effectively preserving shallow spatial details while capturing deep semantic representations, thereby achieving high-precision detection with significantly improved inference speed. Furthermore, the network incorporates an adaptive feature fusion module to dynamically model cross-layer feature correlations, enhancing overall feature collaboration and representation capability. Comprehensive experiments on three public datasets (NUAA-SIRST, NUDT-SIRST, and IRSTD-1K) demonstrate that SDS-Net outperforms state-of-the-art IRSTD methods while maintaining low computational complexity and high inference efficiency, showing superior detection performance and broad application prospects. Our code will be made public at https://github.com/PhysiLearn/SDS-Net.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review",
    "url": "http://arxiv.org/abs/2506.03938v1",
    "authors": [
      "C\u00e9dric L\u00e9onard",
      "Dirk Stober",
      "Martin Schulz"
    ],
    "published": "2025-06-04",
    "abstract": "New UAV technologies and the NewSpace era are transforming Earth Observation missions and data acquisition. Numerous small platforms generate large data volume, straining bandwidth and requiring onboard decision-making to transmit high-quality information in time. While Machine Learning allows real-time autonomous processing, FPGAs balance performance with adaptability to mission-specific requirements, enabling onboard deployment. This review systematically analyzes 66 experiments deploying ML models on FPGAs for Remote Sensing applications. We introduce two distinct taxonomies to capture both efficient model architectures and FPGA implementation strategies. For transparency and reproducibility, we follow PRISMA 2020 guidelines and share all data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Validating remotely sensed biomass estimates with forest inventory data in the western US",
    "url": "http://arxiv.org/abs/2506.03120v1",
    "authors": [
      "Xiuyu Cao",
      "Joseph O. Sexton",
      "Panshi Wang",
      "Dimitrios Gounaridis",
      "Neil H. Carter",
      "Kai Zhu"
    ],
    "published": "2025-06-03",
    "abstract": "Monitoring aboveground biomass (AGB) and its density (AGBD) at high resolution is essential for carbon accounting and ecosystem management. While NASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission provides globally distributed reference measurements for AGBD estimation, the majority of commercial remote sensing products based on GEDI remain without rigorous or independent validation. Here, we present an independent regional validation of an AGBD dataset offered by terraPulse, Inc., based on independent reference data from the US Forest Service Forest Inventory and Analysis (FIA) program. Aggregated to 64,000-hectare hexagons and US counties across the US states of Utah, Nevada, and Washington, we found very strong agreement between terraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE = 26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale, agreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95. Spatial and statistical analyses indicated that terraPulse AGBD values tended to exceed FIA estimates in non-forest areas, likely due to FIA's limited sampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited lower values in high-biomass forests, likely due to saturation effects in its optical remote-sensing covariates. This study advances operational carbon monitoring by delivering a scalable framework for comprehensive AGBD validation using independent FIA data, as well as a benchmark validation of a new commercial dataset for global biomass monitoring.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery",
    "url": "http://arxiv.org/abs/2506.03114v1",
    "authors": [
      "Michelle Chen",
      "David Russell",
      "Amritha Pallavoor",
      "Derek Young",
      "Jane Wu"
    ],
    "published": "2025-06-03",
    "abstract": "Large-scale delineation of individual trees from remote sensing imagery is crucial to the advancement of ecological research, particularly as climate change and other environmental factors rapidly transform forest landscapes across the world. Current RGB tree segmentation methods rely on training specialized machine learning models with labeled tree datasets. While these learning-based approaches can outperform manual data collection when accurate, the existing models still depend on training data that's hard to scale. In this paper, we investigate the efficacy of using a state-of-the-art image segmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for individual tree detection and segmentation. We evaluate a pretrained SAM2 model on two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot transfer by using predictions from an existing tree detection model as prompts. Our results suggest that SAM2 not only has impressive generalization capabilities, but also can form a natural synergy with specialized methods trained on in-domain labeled data. We find that applying large pretrained models to problems in remote sensing is a promising avenue for future progress. We make our code available at: https://github.com/open-forest-observatory/tree-detection-framework.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Multi-Platform Methane Plume Detection via Model and Domain Adaptation",
    "url": "http://arxiv.org/abs/2506.06348v1",
    "authors": [
      "Vassiliki Mancoridis",
      "Brian Bue",
      "Jake H. Lee",
      "Andrew K. Thorpe",
      "Daniel Cusworth",
      "Alana Ayasse",
      "Philip G. Brodrick",
      "Riley Duren"
    ],
    "published": "2025-06-02",
    "abstract": "Prioritizing methane for near-term climate action is crucial due to its significant impact on global warming. Previous work used columnwise matched filter products from the airborne AVIRIS-NG imaging spectrometer to detect methane plume sources; convolutional neural networks (CNNs) discerned anthropogenic methane plumes from false positive enhancements. However, as an increasing number of remote sensing platforms are used for methane plume detection, there is a growing need to address cross-platform alignment. In this work, we describe model- and data-driven machine learning approaches that leverage airborne observations to improve spaceborne methane plume detection, reconciling the distributional shifts inherent with performing the same task across platforms. We develop a spaceborne methane plume classifier using data from the EMIT imaging spectroscopy mission. We refine classifiers trained on airborne imagery from AVIRIS-NG campaigns using transfer learning, outperforming the standalone spaceborne model. Finally, we use CycleGAN, an unsupervised image-to-image translation technique, to align the data distributions between airborne and spaceborne contexts. Translating spaceborne EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne classifiers directly yields the best plume detection results. This methodology is useful not only for data simulation, but also for direct data alignment. Though demonstrated on the task of methane plume detection, our work more broadly demonstrates a data-driven approach to align related products obtained from distinct remote sensing instruments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment",
    "url": "http://arxiv.org/abs/2506.00238v1",
    "authors": [
      "Ehsan Karimi",
      "Maryam Rahnemoonfar"
    ],
    "published": "2025-05-30",
    "abstract": "Natural disasters usually affect vast areas and devastate infrastructures. Performing a timely and efficient response is crucial to minimize the impact on affected communities, and data-driven approaches are the best choice. Visual question answering (VQA) models help management teams to achieve in-depth understanding of damages. However, recently published models do not possess the ability to answer open-ended questions and only select the best answer among a predefined list of answers. If we want to ask questions with new additional possible answers that do not exist in the predefined list, the model needs to be fin-tuned/retrained on a new collected and annotated dataset, which is a time-consuming procedure. In recent years, large-scale Vision-Language Models (VLMs) have earned significant attention. These models are trained on extensive datasets and demonstrate strong performance on both unimodal and multimodal vision/language downstream tasks, often without the need for fine-tuning. In this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and investigate the performance of on post-disaster FloodNet dataset. Since the proposed method takes advantage of zero-shot learning, it can be applied on new datasets without fine-tuning. In addition, ZeShot-VQA is able to process and generate answers that has been not seen during the training procedure, which demonstrates its flexibility.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation",
    "url": "http://arxiv.org/abs/2506.17232v1",
    "authors": [
      "Zelin Zang",
      "Fei Wang",
      "Liangyu Li",
      "Jinlin Wu",
      "Chunshui Zhao",
      "Zhen Lei",
      "Baigui Sun"
    ],
    "published": "2025-05-27",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Recent UDA methods based on Vision Transformers (ViTs) have achieved strong performance through attention-based feature alignment. However, we identify a key limitation: foreground object mismatch, where the discrepancy in foreground object size and spatial distribution across domains weakens attention consistency and hampers effective domain alignment. To address this issue, we propose the Progressive Focus Cross-Attention Mechanism (PCaM), which progressively filters out background information during cross-attention, allowing the model to focus on and fuse discriminative foreground semantics across domains. We further introduce an attentional guidance loss that explicitly directs attention toward task-relevant regions, enhancing cross-domain attention consistency. PCaM is lightweight, architecture-agnostic, and easy to integrate into existing ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet, VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly improves adaptation performance and achieves new state-of-the-art results, validating the effectiveness of attention-guided foreground fusion for domain adaptation.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia",
    "url": "http://arxiv.org/abs/2506.06309v1",
    "authors": [
      "Mohamed Kefi",
      "Tien Dat Pham",
      "Thin Nguyen",
      "Mark G. Tjoelker",
      "Viola Devasirvatham",
      "Kenichi Kashiwagi"
    ],
    "published": "2025-05-26",
    "abstract": "Olive production is an important tree crop in Mediterranean climates. However, olive yield varies significantly due to climate change. Accurately estimating yield using remote sensing and machine learning remains a complex challenge. In this study, we developed a streamlined pipeline for olive yield estimation in the Kairouan and Sousse governorates of Tunisia. We extracted features from multispectral reflectance bands, vegetation indices derived from Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital elevation model data. These spatial features were combined with ground-based field survey data to form a structured tabular dataset. We then developed an automated ensemble learning framework, implemented using AutoGluon to train and evaluate multiple machine learning models, select optimal combinations through stacking, and generate robust yield predictions using five-fold cross-validation. The results demonstrate strong predictive performance from both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This study highlights a scalable, cost-effective, and accurate method for olive yield estimation, with potential applicability across diverse agricultural regions globally.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development",
    "url": "http://arxiv.org/abs/2505.17959v1",
    "authors": [
      "Nguyen Duc",
      "Yan-Ling Lai",
      "Patrick Madlindl",
      "Xinyuan Zhu",
      "Benedikt Schwab",
      "Olaf Wysocki",
      "Ludwig Hoegner",
      "Thomas H. Kolbe"
    ],
    "published": "2025-05-23",
    "abstract": "Owing to the typical long-tail data distribution issues, simulating domain-gap-free synthetic data is crucial in robotics, photogrammetry, and computer vision research. The fundamental challenge pertains to credibly measuring the difference between real and simulated data. Such a measure is vital for safety-critical applications, such as automated driving, where out-of-domain samples may impact a car's perception and cause fatal accidents. Previous work has commonly focused on simulating data on one scene and analyzing performance on a different, real-world scene, hampering the disjoint analysis of domain gap coming from networks' deficiencies, class definitions, and object representation. In this paper, we propose a novel approach to measuring the domain gap between the real world sensor observations and simulated data representing the same location, enabling comprehensive domain gap analysis. To measure such a domain gap, we introduce a novel metric DoGSS-PCL and evaluation assessing the geometric and semantic quality of the simulated point cloud. Our experiments corroborate that the introduced approach can be used to measure the domain gap. The tests also reveal that synthetic semantic point clouds may be used for training deep neural networks, maintaining the performance at the 50/50 real-to-synthetic ratio. We strongly believe that this work will facilitate research on credible data simulation and allow for at-scale deployment in automated driving testing and digital twinning.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Geometric Shape Modelling and Volume Estimation of Dry Bulk Cargo Piles using a Single Image",
    "url": "http://arxiv.org/abs/2505.17896v1",
    "authors": [
      "Debanshu Ratha",
      "Madhu Koirala",
      "P\u00e5l Gunnar Ellingsen"
    ],
    "published": "2025-05-23",
    "abstract": "Volume estimation of onshore cargo piles is of economic importance for shipping and mining companies as well as public authorities for real-time planning of logistics, business intelligence, transport services by land or sea and governmental oversight. In remote sensing literature, the volume of pile is estimated by relying on the illumination property of object to construct the geometric shape from a single image, alternatively, stereographic imaging for construction of a digital elevation model from pairs of images. In a fresh perspective, we propose a novel approach for estimating volume from a single optical image in this work where we use the material property, which relates the base dimensions of the pile to its height through the critical angle of repose. In materials literature, often this is well-studied for fixed base and their \\textit{in situ} volume estimation for different materials. In this work, however, we mathematically model the geometric shape of the pile through a fixed height model. This is appropriate because the unloading crane arm that forms the pile can rise only up to a certain height and generally moved in the horizontal plane during unloading of the material. After mathematically modelling the geometric shape of regular piles for fixed heights under rectilinear motion of unloader, we provide closed form formula to estimate their volume. Apart from laying the mathematical foundations, we also test it on real optical remote sensing data of an open bulk cargo storage facility for silica sand and present the results. We obtain an accuracy of $95\\%$ in estimating the total bulk storage volume of the storage facility. This is a first demonstration study and will be integrated with applied machine learning approaches or current state-of-art approaches in the future for more complex scenarios for estimating dry bulk cargo pile volume.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization",
    "url": "http://arxiv.org/abs/2505.17881v2",
    "authors": [
      "Wenjin Qin",
      "Hailin Wang",
      "Hao Shu",
      "Feng Zhang",
      "Jianjun Wang",
      "Xiangyong Cao",
      "Xi-Le Zhao",
      "Gemine Vivone"
    ],
    "published": "2025-05-23",
    "abstract": "In recent years, tensor decomposition-based approaches for hyperspectral anomaly detection (HAD) have gained significant attention in the field of remote sensing. However, existing methods often fail to fully leverage both the global correlations and local smoothness of the background components in hyperspectral images (HSIs), which exist in both the spectral and spatial domains. This limitation results in suboptimal detection performance. To mitigate this critical issue, we put forward a novel HAD method named HAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR) factors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first decomposed into background and anomaly components. The TR decomposition is then employed to capture the spatial-spectral correlations within the background component. Additionally, we introduce a unified and efficient nonconvex regularizer, induced by tensor singular value decomposition (TSVD), to simultaneously encode the low-rankness and sparsity of the 3-D gradient TR factors into a unique concise form. The above characterization scheme enables the interpretable gradient TR factors to inherit the low-rankness and smoothness of the original background. To further enhance anomaly detection, we design a generalized nonconvex regularization term to exploit the group sparsity of the anomaly component. To solve the resulting doubly nonconvex model, we develop a highly efficient optimization algorithm based on the alternating direction method of multipliers (ADMM) framework. Experimental results on several benchmark datasets demonstrate that our proposed method outperforms existing state-of-the-art (SOTA) approaches in terms of detection accuracy.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors",
    "url": "http://arxiv.org/abs/2505.14122v1",
    "authors": [
      "Ehsan Masoudian",
      "Ali Mirzaei",
      "Hossein Bagheri"
    ],
    "published": "2025-05-20",
    "abstract": "This study investigates the multifaceted factors influencing wildfire risk in Iran, focusing on the interplay between climatic conditions and human activities. Utilizing advanced remote sensing, geospatial information system (GIS) processing techniques such as cloud computing, and machine learning algorithms, this research analyzed the impact of climatic parameters, topographic features, and human-related factors on wildfire susceptibility assessment and prediction in Iran. Multiple scenarios were developed for this purpose based on the data sampling strategy. The findings revealed that climatic elements such as soil moisture, temperature, and humidity significantly contribute to wildfire susceptibility, while human activities-particularly population density and proximity to powerlines-also played a crucial role. Furthermore, the seasonal impact of each parameter was separately assessed during warm and cold seasons. The results indicated that human-related factors, rather than climatic variables, had a more prominent influence during the seasonal analyses. This research provided new insights into wildfire dynamics in Iran by generating high-resolution wildfire susceptibility maps using advanced machine learning classifiers. The generated maps identified high risk areas, particularly in the central Zagros region, the northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting the urgent need for effective fire management strategies.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction",
    "url": "http://arxiv.org/abs/2505.14747v1",
    "authors": [
      "Fatemeh Chajaei",
      "Hossein Bagheri"
    ],
    "published": "2025-05-20",
    "abstract": "Three-dimensional reconstruction of buildings, particularly at Level of Detail 1 (LOD1), plays a crucial role in various applications such as urban planning, urban environmental studies, and designing optimized transportation networks. This study focuses on assessing the potential of LiDAR data for accurate 3D building reconstruction at LOD1 and extracting morphological features from these models. Four deep semantic segmentation models, U-Net, Attention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning to extract building footprints from LiDAR data. The results showed that U-Net3+ and Attention U-Net outperformed the others, achieving IoU scores of 0.833 and 0.814, respectively. Various statistical measures, including maximum, range, mode, median, and the 90th percentile, were used to estimate building heights, resulting in the generation of 3D models at LOD1. As the main contribution of the research, the impact of segmentation accuracy on the quality of 3D building modeling and the accuracy of morphological features like building area and external wall surface area was investigated. The results showed that the accuracy of building identification (segmentation performance) significantly affects the 3D model quality and the estimation of morphological features, depending on the height calculation method. Overall, the UNet3+ method, utilizing the 90th percentile and median measures, leads to accurate height estimation of buildings and the extraction of morphological features.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition",
    "url": "http://arxiv.org/abs/2512.11545v1",
    "authors": [
      "Sheng Feng",
      "Shuqing Ma",
      "Xiaoqian Zhu"
    ],
    "published": "2025-12-12",
    "abstract": "Underwater acoustic target recognition (UATR) is extremely challenging due to the complexity of ship-radiated noise and the variability of ocean environments. Although deep learning (DL) approaches have achieved promising results, most existing models implicitly assume that underwater acoustic data lie in a Euclidean space. This assumption, however, is unsuitable for the inherently complex topology of underwater acoustic signals, which exhibit non-stationary, non-Gaussian, and nonlinear characteristics. To overcome this limitation, this paper proposes the UATR-GTransformer, a non-Euclidean DL model that integrates Transformer architectures with graph neural networks (GNNs). The model comprises three key components: a Mel patchify block, a GTransformer block, and a classification head. The Mel patchify block partitions the Mel-spectrogram into overlapping patches, while the GTransformer block employs a Transformer Encoder to capture mutual information between split patches to generate Mel-graph embeddings. Subsequently, a GNN enhances these embeddings by modeling local neighborhood relationships, and a feed-forward network (FFN) further performs feature transformation. Experiments results based on two widely used benchmark datasets demonstrate that the UATR-GTransformer achieves performance competitive with state-of-the-art methods. In addition, interpretability analysis reveals that the proposed model effectively extracts rich frequency-domain information, highlighting its potential for applications in ocean engineering.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "GNN"
    ],
    "applications": [
      "Classification",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics",
    "url": "http://arxiv.org/abs/2512.11525v1",
    "authors": [
      "Hao Wu",
      "Yuan Gao",
      "Fan Xu",
      "Fan Zhang",
      "Guangliang Liu",
      "Yuxuan Liang",
      "Xiaomeng Huang"
    ],
    "published": "2025-12-12",
    "abstract": "High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Observation-driven correction of numerical weather prediction for marine winds",
    "url": "http://arxiv.org/abs/2512.03606v1",
    "authors": [
      "Matteo Peduto",
      "Qidong Yang",
      "Jonathan Giezendanner",
      "Devis Tuia",
      "Sherrie Wang"
    ],
    "published": "2025-12-03",
    "abstract": "Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Super-resolution of satellite-derived SST data via Generative Adversarial Networks",
    "url": "http://arxiv.org/abs/2511.22610v1",
    "authors": [
      "Claudia Fanelli",
      "Tiany Li",
      "Luca Biferale",
      "Bruno Buongiorno Nardelli",
      "Daniele Ciani",
      "Andrea Pisano",
      "Michele Buzzicotti"
    ],
    "published": "2025-11-27",
    "abstract": "In this work, we address the super-resolution problem of satellite-derived sea surface temperature (SST) using deep generative models. Although standard gap-filling techniques are effective in producing spatially complete datasets, they inherently smooth out fine-scale features that may be critical for a better understanding of the ocean dynamics. We investigate the use of deep learning models as Autoencoders (AEs) and generative models as Conditional-Generative Adversarial Networks (C-GANs), to reconstruct small-scale structures lost during interpolation. Our supervised -- model free -- training is based on SST observations of the Mediterranean Sea, with a focus on learning the conditional distribution of high-resolution fields given their low-resolution counterparts. We apply a tiling and merging strategy to deal with limited observational coverage and to ensure spatial continuity. Quantitative evaluations based on mean squared error metrics, spectral analysis, and gradient statistics show that while the AE reduces reconstruction error, it fails to recover high-frequency variability. In contrast, the C-GAN effectively restores the statistical properties of the true SST field at the cost of increasing the pointwise discrepancy with the ground truth observation. Our results highlight the potential of deep generative models to enhance the physical and statistical realism of gap-filled satellite data in oceanographic applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GAN",
      "Autoencoder"
    ],
    "applications": [
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "Extratropical Atmospheric Circulation Response to ENSO in Deep Learning Pacific Pacemaker Experiments",
    "url": "http://arxiv.org/abs/2511.20899v1",
    "authors": [
      "Zhanxiang Hua",
      "Christina Karamperidou",
      "Zilu Meng"
    ],
    "published": "2025-11-25",
    "abstract": "Coupled atmosphere-ocean deep learning (DL) climate emulators are a new frontier but are known to exhibit weak ENSO variability, raising questions about their ability to simulate teleconnections. Here, we present the first Pacific pacemaker (PACE) experiments using a coupled DL emulator (DLESyM) to bypass this weak variability and isolate the atmospheric response to observed ENSO forcing. We find that while the emulator realistically captures internal atmospheric variability, it produces a significantly amplified forced teleconnection response to ENSO. This amplified response leads to biases in simulating extremes, notably an overestimation of atmospheric blocking frequency and duration with the underestimation of peak intensity. Our findings underscore that coupled DL climate models require in-depth and physically-grounded validation, analogous to traditional numerical models, to build confidence in their use for physical climate analysis.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting",
    "url": "http://arxiv.org/abs/2511.18732v1",
    "authors": [
      "Haoming Jia",
      "Yi Han",
      "Xiang Wang",
      "Huizan Wang",
      "Wei Wu",
      "Jianming Zheng",
      "Peikun Xiao"
    ],
    "published": "2025-11-24",
    "abstract": "Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "An Ecologically-Informed Deep Learning Framework for Interpretable and Validatable Habitat Mapping",
    "url": "http://arxiv.org/abs/2511.17627v1",
    "authors": [
      "Iv\u00e1n Felipe Benavides-Mart\u00ednez",
      "Cristiam Victoriano Portilla-Cabrera",
      "Katherine E. Mills",
      "Claire Enterline",
      "Jos\u00e9 Garc\u00e9s-Vargas",
      "Andrew J. Allyn",
      "Auroop R Ganguly"
    ],
    "published": "2025-11-18",
    "abstract": "Benthic habitat is challenging due to the environmental complexity of the seafloor, technological limitations, and elevated operational costs, especially in under-explored regions. This generates knowledge gaps for the sustainable management of hydrobiological resources and their nexus with society. We developed ECOSAIC (Ecological Compression via Orthogonal Specialized Autoencoders for Interpretable Classification), an Artificial Intelligence framework for automatic classification of benthic habitats through interpretable latent representations using a customizable autoencoder. ECOSAIC compresses n-dimensional feature space by optimizing specialization and orthogonality between domain-informed features. We employed two domain-informed categories: biogeochemical and hydrogeomorphological, that together integrate biological, physicochemical, hydrological and geomorphological, features, whose constraints on habitats have been recognized in ecology for a century. We applied the model to the Colombian Pacific Ocean and the results revealed 16 benthic habitats, expanding from mangroves to deep rocky areas up to 1000 m depth. The candidate habitats exhibited a strong correspondence between their environmental constraints, represented in latent space, and their expected species composition. This correspondence reflected meaningful ecological associations rather than purely statistical correlations, where the habitat's environmental offerings align semantically with the species' requirements. This approach could improve the management and conservation of benthic habitats, facilitating the development of functional maps that support marine planning, biodiversity conservation and fish stock assessment. We also hope it provides new insights into how ecological principles can inform AI frameworks, particularly given the substantial data limitations that characterize ecological research.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Advancing Ocean State Estimation with efficient and scalable AI",
    "url": "http://arxiv.org/abs/2511.06041v1",
    "authors": [
      "Yanfei Xiang",
      "Yuan Gao",
      "Hao Wu",
      "Quan Zhang",
      "Ruiqi Shu",
      "Xiao Zhou",
      "Xi Wu",
      "Xiaomeng Huang"
    ],
    "published": "2025-11-08",
    "abstract": "Accurate and efficient global ocean state estimation remains a grand challenge for Earth system science, hindered by the dual bottlenecks of computational scalability and degraded data fidelity in traditional data assimilation (DA) and deep learning (DL) approaches. Here we present an AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly assimilates multi-source and multi-scale observations, ranging from sparse in-situ measurements to 4 km satellite swaths, without any interpolation or data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous mapping from heterogeneous inputs to ocean states, preserving native data fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\\circ$ mesoscale dynamics from coarse 1$^\\circ$ fields, which ensures both efficiency and scalability, with just 3.7\\% more parameters than the 1$^\\circ$ configuration. When coupled with a DL forecasting system, ADAF-Ocean extends global forecast skill by up to 20 days compared to baselines without assimilation. This framework establishes a computationally viable and scientifically rigorous pathway toward real-time, high-resolution Earth system monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Deep Learning-Driven Downscaling for Climate Risk Assessment of Projected Temperature Extremes in the Nordic Region",
    "url": "http://arxiv.org/abs/2511.03770v1",
    "authors": [
      "Parthiban Loganathan",
      "Elias Zea",
      "Ricardo Vinuesa",
      "Evelyn Otero"
    ],
    "published": "2025-11-05",
    "abstract": "Rapid changes and increasing climatic variability across the widely varied Koppen-Geiger regions of northern Europe generate significant needs for adaptation. Regional planning needs high-resolution projected temperatures. This work presents an integrative downscaling framework that incorporates Vision Transformer (ViT), Convolutional Long Short-Term Memory (ConvLSTM), and Geospatial Spatiotemporal Transformer with Attention and Imbalance-Aware Network (GeoStaNet) models. The framework is evaluated with a multicriteria decision system, Deep Learning-TOPSIS (DL-TOPSIS), for ten strategically chosen meteorological stations encompassing the temperate oceanic (Cfb), subpolar oceanic (Cfc), warm-summer continental (Dfb), and subarctic (Dfc) climate regions. Norwegian Earth System Model (NorESM2-LM) Coupled Model Intercomparison Project Phase 6 (CMIP6) outputs were bias-corrected during the 1951-2014 period and subsequently validated against earlier observations of day-to-day temperature metrics and diurnal range statistics. The ViT showed improved performance (Root Mean Squared Error (RMSE): 1.01 degrees C; R^2: 0.92), allowing for production of credible downscaled projections. Under the SSP5-8.5 scenario, the Dfc and Dfb climate zones are projected to warm by 4.8 degrees C and 3.9 degrees C, respectively, by 2100, with expansion in the diurnal temperature range by more than 1.5 degrees C. The Time of Emergence signal first appears in subarctic winter seasons (Dfc: approximately 2032), signifying an urgent need for adaptation measures. The presented framework offers station-based, high-resolution estimates of uncertainties and extremes, with direct uses for adaptation policy over high-latitude regions with fast environmental change.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Disentangling Internal Tides from Balanced Motions with Deep Learning and Surface Field Synergy",
    "url": "http://arxiv.org/abs/2511.03614v1",
    "authors": [
      "Han Wang",
      "Jeffrey Uncu",
      "Kaushik Srinivasan",
      "Nicolas Grisouard"
    ],
    "published": "2025-11-05",
    "abstract": "A fundamental challenge in ocean dynamics is the disentanglement of balanced motions and internal waves. Extracting internal tidal (IT) imprints on surface data is a central part of this challenge. For IT extraction, traditional harmonic analysis fails in the presence of strong incoherence and poor temporal sampling, as is common in global satellite observations. The advent of new wide-swath satellites, which provide two-dimensional spatial coverage, allows IT extraction to be reformulated as an image translation problem. Building on recent work where we developed a deep learning approach to extract IT signatures from sea surface height (SSH) in an idealized turbulent simulation, we show here that a simpler and computationally cheaper algorithm can perform equally well if the learning rate is annealed during training. Using this new, convenient algorithm, we experiment with different combinations of input surface fields -- SSH, surface temperature, and surface velocity. All fields contribute synergistically to disentanglement, with surface velocity by far the most informative. These findings underscore the value of coordinated multi-platform observational campaigns and highlight the critical importance of surface velocity observations for separating balanced motions and internal waves. Additional insights into the behavior of deep learning algorithm emerge: both wave signature and scattering medium aids IT extraction, and to exploit large-scale information in the scattering medium, the algorithm must be highly non-local. Residual errors of our algorithm concentrate at small spatial scales near mode-2 tidal wavelengths, likely arising from artifacts introduced during data preparation (e.g., Doppler shifts) as well as imperfections in the deep learning architecture.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets",
    "url": "http://arxiv.org/abs/2511.02887v1",
    "authors": [
      "Chaitanya Rele",
      "Aditya Rathod",
      "Kaustubh Natu",
      "Saurabh Kulkarni",
      "Ajay Koli",
      "Swapnali Makdey"
    ],
    "published": "2025-11-04",
    "abstract": "The North Indian Ocean, including the Arabian Sea and the Bay of Bengal, represents a vital source of livelihood for coastal communities, yet fishermen often face uncertainty in locating productive fishing grounds. To address this challenge, we present an AI-assisted framework for predicting Potential Fishing Zones (PFZs) using oceanographic parameters such as sea surface temperature and chlorophyll concentration. The approach is designed to enhance the accuracy of PFZ identification and provide region-specific insights for sustainable fishing practices. Preliminary results indicate that the framework can support fishermen by reducing search time, lowering fuel consumption, and promoting efficient resource utilization.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Framework for Hybrid Physics-AI Coupled Ocean Models",
    "url": "http://arxiv.org/abs/2510.22676v1",
    "authors": [
      "Laure Zanna",
      "William Gregory",
      "Pavel Perezhogin",
      "Aakash Sane",
      "Cheng Zhang",
      "Alistair Adcroft",
      "Mitch Bushuk",
      "Carlos Fernandez-Granda",
      "Brandon Reichl",
      "Dhruv Balwada",
      "Julius Busecke",
      "William Chapman",
      "Alex Connolly",
      "Danni Du",
      "Kelsey Everard",
      "Fabrizio Falasca",
      "Renaud Falga",
      "David Kamm",
      "Etienne Meunier",
      "Qi Liu",
      "Antoine Nasser",
      "Matthew Pudig",
      "Andrew Shao",
      "Julia L. Simpson",
      "Linus Vogt",
      "Jiarong Wu"
    ],
    "published": "2025-10-26",
    "abstract": "Climate simulations, at all grid resolutions, rely on approximations that encapsulate the forcing due to unresolved processes on resolved variables, known as parameterizations. Parameterizations often lead to inaccuracies in climate models, with significant biases in the physics of key climate phenomena. Advances in artificial intelligence (AI) are now directly enabling the learning of unresolved processes from data to improve the physics of climate simulations. Here, we introduce a flexible framework for developing and implementing physics- and scale-aware machine learning parameterizations within climate models. We focus on the ocean and sea-ice components of a state-of-the-art climate model by implementing a spectrum of data-driven parameterizations, ranging from complex deep learning models to more interpretable equation-based models. Our results showcase the viability of AI-driven parameterizations in operational models, advancing the capabilities of a new generation of hybrid simulations, and include prototypes of fully coupled atmosphere-ocean-sea-ice hybrid simulations. The tools developed are open source, accessible, and available to all.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline",
    "url": "http://arxiv.org/abs/2511.00022v1",
    "authors": [
      "Jules Gerard",
      "Leandro Di Bella",
      "Filip Huyghe",
      "Marc Kochzius"
    ],
    "published": "2025-10-24",
    "abstract": "Coral reef monitoring in the Western Indian Ocean is limited by the labor demands of underwater visual censuses. This work evaluates a YOLOv8-based deep learning pipeline for automating family-level fish identification from video transects collected in Kenya and Tanzania. A curated dataset of 24 families was tested under different configurations, providing the first region-specific benchmark for automated reef fish monitoring in the Western Indian Ocean. The best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families but weaker detection of rare or complex taxa. Results demonstrate the potential of deep learning as a scalable complement to traditional monitoring methods.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets",
    "url": "http://arxiv.org/abs/2511.00021v1",
    "authors": [
      "Julio Jerison E. Macrohon",
      "Gordon Hung"
    ],
    "published": "2025-10-24",
    "abstract": "Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "ResNet",
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters",
    "url": "http://arxiv.org/abs/2510.14250v1",
    "authors": [
      "Lianzi Jiang",
      "Jianxin Zhang",
      "Xinyu Han",
      "Huanhe Dong",
      "Xiangrong Wang"
    ],
    "published": "2025-10-16",
    "abstract": "Accurate motion response prediction for elastic Bragg breakwaters is critical for their structural safety and operational integrity in marine environments. However, conventional deep learning models often exhibit limited generalization capabilities when presented with unseen sea states. These deficiencies stem from the neglect of natural decay observed in marine systems and inadequate modeling of wave-structure interaction (WSI). To overcome these challenges, this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network (PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module incorporates a learnable temporal decay to assign higher weights to recent states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase differences guided bidirectional cross-attention (PDG-BCA) module explicitly captures the bidirectional interaction and phase relationship between waves and the structure using a cosine-based bias within a bidirectional cross-computation paradigm. These streams are synergistically integrated through a global context fusion (GCF) module. Finally, PhysAttnNet is trained with a hybrid time-frequency loss that jointly minimizes time-domain prediction errors and frequency-domain spectral discrepancies. Comprehensive experiments on wave flume datasets demonstrate that PhysAttnNet significantly outperforms mainstream models. Furthermore,cross-scenario generalization tests validate the model's robustness and adaptability to unseen environments, highlighting its potential as a framework to develop predictive models for complex systems in ocean engineering.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures",
    "url": "http://arxiv.org/abs/2509.17734v1",
    "authors": [
      "Pablo Rodr\u00edguez-Bocca",
      "Guillermo Pereira",
      "Diego Kiedanski",
      "Soledad Collazo",
      "Sebasti\u00e1n Basterrech",
      "Gerardo Rubino"
    ],
    "published": "2025-09-22",
    "abstract": "In recent years, great progress has been made in the field of forecasting meteorological variables. Recently, deep learning architectures have made a major breakthrough in forecasting the daily average temperature over a ten-day horizon. However, advances in forecasting events related to the maximum temperature over short horizons remain a challenge for the community. A problem that is even more complex consists in making predictions of the maximum daily temperatures in the short, medium, and long term. In this work, we focus on forecasting events related to the maximum daily temperature over medium-term periods (90 days). Therefore, instead of addressing the problem from a meteorological point of view, this article tackles it from a climatological point of view. Due to the complexity of this problem, a common approach is to frame the study as a temporal classification problem with the classes: maximum temperature \"above normal\", \"normal\" or \"below normal\". From a practical point of view, we created a large historical dataset (from 1981 to 2018) collecting information from weather stations located in South America. In addition, we also integrated exogenous information from the Pacific, Atlantic, and Indian Ocean basins. We applied the AutoGluonTS platform to solve the above-mentioned problem. This AutoML tool shows competitive forecasting performance with respect to large operational platforms dedicated to tackling this climatological problem; but with a \"relatively\" low computational cost in terms of time and resources.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations",
    "url": "http://arxiv.org/abs/2509.19384v1",
    "authors": [
      "Hongyuan Shi",
      "Yilin Zhai",
      "Ping Dong",
      "Zaijin You",
      "Chao Zhan",
      "Qing Wang"
    ],
    "published": "2025-09-21",
    "abstract": "Reconstructing high-resolution regional significant wave height fields from sparse and uneven buoy observations remains a core challenge for ocean monitoring and risk-aware operations. We introduce AUWave, a hybrid deep learning framework that fuses a station-wise sequence encoder (MLP) with a multi-scale U-Net enhanced by a bottleneck self-attention layer to recover 32$\\times$32 regional SWH fields. A systematic Bayesian hyperparameter search with Optuna identifies the learning rate as the dominant driver of generalization, followed by the scheduler decay and the latent dimension. Using NDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave attains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE distribution. Spatial errors are lowest near observation sites and increase with distance, reflecting identifiability limits under sparse sampling. Sensitivity experiments show that AUWave consistently outperforms a representative baseline in data-richer configurations, while the baseline is only marginally competitive in the most underdetermined single-buoy cases. The architecture's multi-scale and attention components translate into accuracy gains when minimal but non-trivial spatial anchoring is available. Error maps and buoy ablations reveal key anchor stations whose removal disproportionately degrades performance, offering actionable guidance for network design. AUWave provides a scalable pathway for gap filling, high-resolution priors for data assimilation, and contingency reconstruction.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Deep Learning Model of Lightning Stroke Density",
    "url": "http://arxiv.org/abs/2509.10399v1",
    "authors": [
      "Randall Jones",
      "Joel A. Thornton",
      "Chris J. Wright",
      "Robert Holzworth"
    ],
    "published": "2025-09-12",
    "abstract": "Lightning plays a crucial role in the Earth's climate system, yet existing parameterizations for use in forecasting and earth system models show room for improvement in capturing spatial and temporal variations in its frequency. This study develops deep learning-based parameterizations of lightning stroke density using meteorological variables from the ERA and IMERG datasets. Convolutional neural networks (CNNs) with U-Net architectures are trained using World Wide Lightning Location Network (WWLLN) data from 2010 to 2021 and evaluated on WWLLN lightning observations from 2022 and 2023. The CNNs reduce the average domain mean bias by an order of magnitude and produce significantly higher Fractions Skill Score (FSS) values across all lightning regimes compared to the multiplicative product of CAPE and precipitation. The CNNs show skill relative to previously published parameterizations over the oceans especially, with r2 values as high as 0.93 achieved between the best performing modeled and observed lightning stroke density climatologies. The CNNs are also able to accurately capture the 12-hourly evolution of lightning spatial patterns on an event-scale with high skill. These results show the potential for deep learning to improve on lightning parameterizations in weather and earth system models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction",
    "url": "http://arxiv.org/abs/2509.09128v1",
    "authors": [
      "Emam Hossain",
      "Md Osman Gani"
    ],
    "published": "2025-09-11",
    "abstract": "Conventional machine learning and deep learning models typically rely on correlation-based learning, which often fails to distinguish genuine causal relationships from spurious associations, limiting their robustness, interpretability, and ability to generalize. To overcome these limitations, we introduce a causality-aware deep learning framework that integrates Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily and monthly resolutions, the proposed method identifies causally influential predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary features, and enhances computational efficiency. Experimental results show that incorporating causal inputs leads to improved prediction accuracy and interpretability across varying lead times. While demonstrated on Arctic SIE forecasting, the framework is broadly applicable to other dynamic, high-dimensional domains, offering a scalable approach that advances both the theoretical foundations and practical performance of causality-informed predictive modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "DeepSeasons: a Deep Learning scale-selecting approach to Seasonal Forecasts",
    "url": "http://arxiv.org/abs/2509.10494v1",
    "authors": [
      "A. Navarra",
      "G. G. Navarra"
    ],
    "published": "2025-08-31",
    "abstract": "Seasonal forecasting remains challenging due to the inherent chaotic nature of atmospheric dynamics. This paper introduces DeepSeasons, a novel deep learning approach designed to enhance the accuracy and reliability of seasonal forecasts. Leveraging advanced neural network architectures and extensive historical climatic datasets, DeepSeasons identifies complex, nonlinear patterns and dependencies in climate variables with similar or improved skill respcet GCM-based forecasting methods, at a significant lower cost. The framework also allow tailored application to specific regions or variables, rather than the overall problem of predicting the entire atmosphere/ocean system. The proposed methods also allow for direct predictions of anomalies and time-means, opening a new approach to long-term forecasting and highlighting its potential for operational deployment in climate-sensitive sectors. This innovative methodology promises substantial improvements in managing climate-related risks and decision-making processes.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Ensembles of Neural Surrogates for Parametric Sensitivity in Ocean Modeling",
    "url": "http://arxiv.org/abs/2508.16489v2",
    "authors": [
      "Yixuan Sun",
      "Romain Egele",
      "Sri Hari Krishna Narayanan",
      "Luke Van Roekel",
      "Carmelo Gonzales",
      "Steven Brus",
      "Balu Nadiga",
      "Sandeep Madireddy",
      "Prasanna Balaprakash"
    ],
    "published": "2025-08-22",
    "abstract": "Accurate simulations of the oceans are crucial in understanding the Earth system. Despite their efficiency, simulations at lower resolutions must rely on various uncertain parameterizations to account for unresolved processes. However, model sensitivity to parameterizations is difficult to quantify, making it challenging to tune these parameterizations to reproduce observations. Deep learning surrogates have shown promise for efficient computation of the parametric sensitivities in the form of partial derivatives, but their reliability is difficult to evaluate without ground truth derivatives. In this work, we leverage large-scale hyperparameter search and ensemble learning to improve both forward predictions, autoregressive rollout, and backward adjoint sensitivity estimation. Particularly, the ensemble method provides epistemic uncertainty of function value predictions and their derivatives, providing improved reliability of the neural surrogates in decision making.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MedFormer: a data-driven model for forecasting the Mediterranean Sea",
    "url": "http://arxiv.org/abs/2509.00015v1",
    "authors": [
      "Italo Epicoco",
      "Davide Donno",
      "Gabriele Accarino",
      "Simone Norberti",
      "Alessandro Grandi",
      "Michele Giurato",
      "Ronan McAdam",
      "Donatello Elia",
      "Emanuela Clementi",
      "Paola Nassisi",
      "Enrico Scoccimarro",
      "Giovanni Coppini",
      "Silvio Gualdi",
      "Giovanni Aloisio",
      "Simona Masina",
      "Giulio Boccaletti",
      "Antonio Navarra"
    ],
    "published": "2025-08-16",
    "abstract": "Accurate ocean forecasting is essential for supporting a wide range of marine applications. Recent advances in artificial intelligence have highlighted the potential of data-driven models to outperform traditional numerical approaches, particularly in atmospheric weather forecasting. However, extending these methods to ocean systems remains challenging due to their inherently slower dynamics and complex boundary conditions. In this work, we present MedFormer, a fully data-driven deep learning model specifically designed for medium-range ocean forecasting in the Mediterranean Sea. MedFormer is based on a U-Net architecture augmented with 3D attention mechanisms and operates at a high horizontal resolution of 1/24\u00b0. The model is trained on 20 years of daily ocean reanalysis data and fine-tuned with high-resolution operational analyses. It generates 9-day forecasts using an autoregressive strategy. The model leverages both historical ocean states and atmospheric forcings, making it well-suited for operational use. We benchmark MedFormer against the state-of-the-art Mediterranean Forecasting System (MedFS), developed at Euro-Mediterranean Center on Climate Change (CMCC), using both analysis data and independent observations. The forecast skills, evaluated with the Root Mean Squared Difference and the Anomaly Correlation Coefficient, indicate that MedFormer consistently outperforms MedFS across key 3D ocean variables. These findings underscore the potential of data-driven approaches like MedFormer to complement, or even surpass, traditional numerical ocean forecasting systems in both accuracy and computational efficiency.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Bridging ocean wave physics and deep learning: Physics-informed neural operators for nonlinear wavefield reconstruction in real-time",
    "url": "http://arxiv.org/abs/2508.03315v1",
    "authors": [
      "Svenja Ehlers",
      "Merten Stender",
      "Norbert Hoffmann"
    ],
    "published": "2025-08-05",
    "abstract": "Accurate real-time prediction of phase-resolved ocean wave fields remains a critical yet largely unsolved problem, primarily due to the absence of practical data assimilation methods for reconstructing initial conditions from sparse or indirect wave measurements. While recent advances in supervised deep learning have shown potential for this purpose, they require large labelled datasets of ground truth wave data, which are infeasible to obtain in real-world scenarios. To overcome this limitation, we propose a Physics-Informed Neural Operator (PINO) framework for reconstructing spatially and temporally phase-resolved, nonlinear ocean wave fields from sparse measurements, without the need for ground truth data during training. This is achieved by embedding residuals of the free surface boundary conditions of ocean gravity waves into the loss function of the PINO, constraining the solution space in a soft manner. After training, we validate our approach using highly realistic synthetic wave data and demonstrate the accurate reconstruction of nonlinear wave fields from both buoy time series and radar snapshots. Our results indicate that PINOs enable accurate, real-time reconstruction and generalize robustly across a wide range of wave conditions, thereby paving the way for operational, data-driven wave reconstruction and prediction in realistic marine environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Data-driven global ocean model resolving ocean-atmosphere coupling dynamics",
    "url": "http://arxiv.org/abs/2508.10908v1",
    "authors": [
      "Jeong-Hwan Kim",
      "Daehyun Kang",
      "Young-Min Yang",
      "Jae-Heung Park",
      "Yoo-Geun Ham"
    ],
    "published": "2025-07-31",
    "abstract": "Artificial intelligence has advanced global weather forecasting, outperforming traditional numerical models in both accuracy and computational efficiency. Nevertheless, extending predictions beyond subseasonal timescales requires the development of deep learning (DL)-based ocean-atmosphere coupled models that can realistically simulate complex oceanic responses to atmospheric forcing. This study presents KIST-Ocean, a DL-based global three-dimensional ocean general circulation model using a U-shaped visual attention adversarial network architecture. KIST-Ocean integrates partial convolution, adversarial training, and transfer learning to address coastal complexity and predictive distribution drift in auto-regressive models. Comprehensive evaluations confirmed the model's robust ocean predictive skill and efficiency. Moreover, it accurately captures realistic ocean response, such as Kelvin and Rossby wave propagation in the tropical Pacific, and vertical motions induced by cyclonic and anticyclonic wind stress, demonstrating its ability to represent key ocean-atmosphere coupling mechanisms underlying climate phenomena, including the El Nino-Southern Oscillation. These findings reinforce confidence in DL-based global weather and climate models and their extending DL-based approaches to broader Earth system modeling, offering potential for enhancing climate prediction capabilities.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "GAN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents",
    "url": "http://arxiv.org/abs/2507.18067v2",
    "authors": [
      "Abdessamad El-Kabid",
      "Loubna Benabbou",
      "Redouane Lguensat",
      "Alex Hern\u00e1ndez-Garc\u00eda"
    ],
    "published": "2025-07-24",
    "abstract": "Accurate modeling of physical systems governed by partial differential equations is a central challenge in scientific computing. In oceanography, high-resolution current data are critical for coastal management, environmental monitoring, and maritime safety. However, available satellite products, such as Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and global ocean models, often lack the spatial granularity required for detailed local analyses. In this work, we (a) introduce a supervised deep learning framework based on neural operators for solving PDEs and providing arbitrary resolution solutions, and (b) propose downscaling models with an application to Copernicus ocean current data. Additionally, our method can model surrogate PDEs and predict solutions at arbitrary resolution, regardless of the input resolution. We evaluated our model on real-world Copernicus ocean current data and synthetic Navier-Stokes simulation datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Multimodal Data Fusion Generative Adversarial Network for Real Time Underwater Sound Speed Field Construction",
    "url": "http://arxiv.org/abs/2507.11812v1",
    "authors": [
      "Wei Huang",
      "Yuqiang Huang",
      "Yanan Wu",
      "Tianhe Xu",
      "Junting Wang",
      "Hao Zhang"
    ],
    "published": "2025-07-16",
    "abstract": "Sound speed profiles (SSPs) are essential parameters underwater that affects the propagation mode of underwater signals and has a critical impact on the energy efficiency of underwater acoustic communication and accuracy of underwater acoustic positioning. Traditionally, SSPs can be obtained by matching field processing (MFP), compressive sensing (CS), and deep learning (DL) methods. However, existing methods mainly rely on on-site underwater sonar observation data, which put forward strict requirements on the deployment of sonar observation systems. To achieve high-precision estimation of sound velocity distribution in a given sea area without on-site underwater data measurement, we propose a multi-modal data-fusion generative adversarial network model with residual attention block (MDF-RAGAN) for SSP construction. To improve the model's ability for capturing global spatial feature correlations, we embedded the attention mechanisms, and use residual modules for deeply capturing small disturbances in the deep ocean sound velocity distribution caused by changes of SST. Experimental results on real open dataset show that the proposed model outperforms other state-of-the-art methods, which achieves an accuracy with an error of less than 0.3m/s. Specifically, MDF-RAGAN not only outperforms convolutional neural network (CNN) and spatial interpolation (SITP) by nearly a factor of two, but also achieves about 65.8\\% root mean square error (RMSE) reduction compared to mean profile, which fully reflects the enhancement of overall profile matching by multi-source fusion and cross-modal attention.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "GAN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Generative Lagrangian data assimilation for ocean dynamics under extreme sparsity",
    "url": "http://arxiv.org/abs/2507.06479v1",
    "authors": [
      "Niloofar Asefi",
      "Leonard Lupin-Jimenez",
      "Tianning Wu",
      "Ruoying He",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-07-09",
    "abstract": "Reconstructing ocean dynamics from observational data is fundamentally limited by the sparse, irregular, and Lagrangian nature of spatial sampling, particularly in subsurface and remote regions. This sparsity poses significant challenges for forecasting key phenomena such as eddy shedding and rogue waves. Traditional data assimilation methods and deep learning models often struggle to recover mesoscale turbulence under such constraints. We leverage a deep learning framework that combines neural operators with denoising diffusion probabilistic models (DDPMs) to reconstruct high-resolution ocean states from extremely sparse Lagrangian observations. By conditioning the generative model on neural operator outputs, the framework accurately captures small-scale, high-wavenumber dynamics even at $99\\%$ sparsity (for synthetic data) and $99.9\\%$ sparsity (for real satellite observations). We validate our method on benchmark systems, synthetic float observations, and real satellite data, demonstrating robust performance under severe spatial sampling limitations as compared to other deep learning baselines.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Machine Learning in Acoustics: A Review and Open-Source Repository",
    "url": "http://arxiv.org/abs/2507.04419v1",
    "authors": [
      "Ryan A. McCarthy",
      "You Zhang",
      "Samuel A. Verburg",
      "William F. Jenkins",
      "Peter Gerstoft"
    ],
    "published": "2025-07-06",
    "abstract": "Acoustic data provide scientific and engineering insights in fields ranging from bioacoustics and communications to ocean and earth sciences. In this review, we survey recent advances and the transformative potential of machine learning (ML) in acoustics, including deep learning (DL). Using the Python high-level programming language, we demonstrate a broad collection of ML techniques to detect and find patterns for classification, regression, and generation in acoustics data automatically. We have ML examples including acoustic data classification, generative modeling for spatial audio, and physics-informed neural networks. This work includes AcousticsML, a set of practical Jupyter notebook examples on GitHub demonstrating ML benefits and encouraging researchers and practitioners to apply reproducible data-driven approaches to acoustic challenges.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Accurate Mediterranean Sea forecasting via graph-based deep learning",
    "url": "http://arxiv.org/abs/2506.23900v1",
    "authors": [
      "Daniel Holmberg",
      "Emanuela Clementi",
      "Italo Epicoco",
      "Teemu Roos"
    ],
    "published": "2025-06-30",
    "abstract": "Accurate ocean forecasting systems are essential for understanding marine dynamics, which play a crucial role in sectors such as shipping, aquaculture, environmental monitoring, and coastal risk management. Traditional numerical solvers, while effective, are computationally expensive and time-consuming. Recent advancements in machine learning have revolutionized weather forecasting, offering fast and energy-efficient alternatives. Building on these advancements, we introduce SeaCast, a neural network designed for high-resolution regional ocean forecasting. SeaCast employs a graph-based framework to effectively handle the complex geometry of ocean grids and integrates external forcing data tailored to the regional ocean context. Our approach is validated through experiments at a high horizontal resolution using the operational numerical forecasting system of the Mediterranean Sea, along with both numerical and data-driven atmospheric forcings. Results demonstrate that SeaCast consistently outperforms the operational model in forecast skill, marking a significant advancement in regional ocean prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research",
    "url": "http://arxiv.org/abs/2506.22174v1",
    "authors": [
      "Bavo Lesy",
      "Siemen Herremans",
      "Robin Kerstens",
      "Jan Steckel",
      "Walter Daems",
      "Siegfried Mercelis",
      "Ali Anwar"
    ],
    "published": "2025-06-27",
    "abstract": "The transport industry has recently shown significant interest in unmanned surface vehicles (USVs), specifically for port and inland waterway transport. These systems can improve operational efficiency and safety, which is especially relevant in the European Union, where initiatives such as the Green Deal are driving a shift towards increased use of inland waterways. At the same time, a shortage of qualified personnel is accelerating the adoption of autonomous solutions. However, there is a notable lack of open-source, high-fidelity simulation frameworks and datasets for developing and evaluating such solutions. To address these challenges, we introduce AirSim For Surface Vehicles (ASVSim), an open-source simulation framework specifically designed for autonomous shipping research in inland and port environments. The framework combines simulated vessel dynamics with marine sensor simulation capabilities, including radar and camera systems and supports the generation of synthetic datasets for training computer vision models and reinforcement learning agents. Built upon Cosys-AirSim, ASVSim provides a comprehensive platform for developing autonomous navigation algorithms and generating synthetic datasets. The simulator supports research of both traditional control methods and deep learning-based approaches. Through limited experiments, we demonstrate the potential of the simulator in these research areas. ASVSim is provided as an open-source project under the MIT license, making autonomous navigation research accessible to a larger part of the ocean engineering community.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting",
    "url": "http://arxiv.org/abs/2507.00036v1",
    "authors": [
      "Rohan Putatunda",
      "Sanjay Purushotham",
      "Ratnaksha Lele",
      "Vandana P. Janeja"
    ],
    "published": "2025-06-18",
    "abstract": "Drifting icebergs in the polar oceans play a key role in the Earth's climate system, impacting freshwater fluxes into the ocean and regional ecosystems while also posing a challenge to polar navigation. However, accurately forecasting iceberg trajectories remains a formidable challenge, primarily due to the scarcity of spatiotemporal data and the complex, nonlinear nature of iceberg motion, which is also impacted by environmental variables. The iceberg motion is influenced by multiple dynamic environmental factors, creating a highly variable system that makes trajectory identification complex. These limitations hinder the ability of deep learning models to effectively capture the underlying dynamics and provide reliable predictive outcomes. To address these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep learning model that combines an analytical formulation of iceberg drift physics, with an augmented residual learning model. The model learns the pattern of mismatch between the analytical solution and ground-truth observations, which is combined with a rotate-augmented spectral neural network that captures both global and local patterns from the data to forecast future iceberg drift positions. We compare IDRIFTNET model performance with state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings demonstrate that IDRIFTNET outperforms other models by achieving a lower Final Displacement Error (FDE) and Average Displacement Error (ADE) across a variety of time points. These results highlight IDRIFTNET's effectiveness in capturing the complex, nonlinear drift of icebergs for forecasting iceberg trajectories under limited data and dynamic environmental conditions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Forecast error diagnostics in neural weather models",
    "url": "http://arxiv.org/abs/2506.11987v2",
    "authors": [
      "Uros Perkan",
      "Ziga Zaplotnik",
      "Gregor Skok"
    ],
    "published": "2025-06-13",
    "abstract": "Deep-learning (DL) weather prediction models offer some notable advantages over traditional physics-based models, including auto-differentiability and low computational cost, enabling detailed diagnostics of forecast errors. Using our convolutional encoder-decoder model, ConvCastNet, we systematically relax selected subdomains of the forecast fields towards \"true\" weather states (ERA5 reanalyses) and monitor the forecast skill gain in other regions. Our results show that a medium-range mid-latitude forecast improves substantially when the stratosphere and boundary layer are relaxed, while relaxation of the tropical atmosphere has a negligible effect. This underscores the need for a more accurate representation of the stratosphere and the planetary boundary layer to improve medium-range weather predictability. Additionally, we investigate the relationship between the forecast error sensitivity to initial conditions and relaxation experiments. By utilising auto-differentiability, we identify overlapping regions of large error sensitivity and strong forecast skill improvement from relaxation. Average mid-latitude error sensitivity to initial conditions shows negligible influence from the tropics, corroborating the results of the tropical relaxation experiments. The error sensitivity shows a physically consistent influence of upstream weather dynamics and sea surface temperatures on forecast accuracy. The latter also highlights the importance of accurately representing the atmosphere--ocean coupling in numerical weather prediction models. This combined approach could provide valuable heuristics for diagnosing neural model errors and guiding targeted model improvements.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System",
    "url": "http://arxiv.org/abs/2505.24429v2",
    "authors": [
      "Giovanny A. Cuervo-Londo\u00f1o",
      "Javier S\u00e1nchez",
      "\u00c1ngel Rodr\u00edguez-Santana"
    ],
    "published": "2025-05-30",
    "abstract": "Oceanographic forecasting impacts various sectors of society by supporting environmental conservation and economic activities. Based on global circulation models, traditional forecasting methods are computationally expensive and slow, limiting their ability to provide rapid forecasts. Recent advances in deep learning offer faster and more accurate predictions, although these data-driven models are often trained with global data from numerical simulations, which may not reflect reality. The emergence of such models presents great potential for improving ocean prediction at a subregional domain. However, their ability to predict fine-scale ocean processes, like mesoscale structures, remains largely unknown. This work aims to adapt a graph neural network initially developed for global weather forecasting to improve subregional ocean prediction, specifically focusing on the Canary Current upwelling system. The model is trained with satellite data and compared to state-of-the-art physical ocean models to assess its performance in capturing ocean dynamics. Our results show that the deep learning model surpasses traditional methods in precision despite some challenges in upwelling areas. It demonstrated superior performance in reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis, particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape Bojador, and Cape Blanc. The model achieved improvements of up to 26.5% relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts compared to the GLORYS reanalysis at these critical locations, highlighting its enhanced capability to capture spatial variability and improve predictive accuracy in complex areas. These findings suggest the viability of adapting meteorological data-driven models for improving subregional medium-term ocean forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis",
    "url": "http://arxiv.org/abs/2505.14285v1",
    "authors": [
      "Eirini Panteli",
      "Paulo E. Santos",
      "Nabil Humphrey"
    ],
    "published": "2025-05-20",
    "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for preprocessing, denoising, classification, and novelty detection of underwater acoustic signals. Designed to operate effectively in noisy and dynamic marine environments, AquaSignal integrates state-of-the-art deep learning architectures to enhance the reliability and accuracy of acoustic signal analysis. The system is evaluated on a combined dataset from the Deepship and Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a ResNet18 convolutional neural network for classifying known acoustic events, and an AutoEncoder-based model for unsupervised detection of novel or anomalous signals. To our knowledge, this is the first comprehensive study to apply and evaluate this combination of techniques on maritime vessel acoustic data. Experimental results show that AquaSignal improves signal clarity and task performance, achieving 71% classification accuracy and 91% accuracy in novelty detection. Despite slightly lower classification performance compared to some state-of-the-art models, differences in data partitioning strategies limit direct comparisons. Overall, AquaSignal demonstrates strong potential for real-time underwater acoustic monitoring in scientific, environmental, and maritime domains.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting",
    "url": "http://arxiv.org/abs/2505.10894v1",
    "authors": [
      "Yishuo Wang",
      "Feng Zhou",
      "Muping Zhou",
      "Qicheng Meng",
      "Zhijun Hu",
      "Yi Wang"
    ],
    "published": "2025-05-16",
    "abstract": "This paper proposes CTP, a novel deep learning framework that integrates convolutional neural network(CNN), Transformer architectures, and physics-informed neural network(PINN) for ocean front prediction. Ocean fronts, as dynamic interfaces between distinct water masses, play critical roles in marine biogeochemical and physical processes. Existing methods such as LSTM, ConvLSTM, and AttentionConv often struggle to maintain spatial continuity and physical consistency over multi-step forecasts. CTP addresses these challenges by combining localized spatial encoding, long-range temporal attention, and physical constraint enforcement. Experimental results across south China sea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP achieves state-of-the-art(SOTA) performance in both single-step and multi-step predictions, significantly outperforming baseline models in accuracy, $F_1$ score, and temporal stability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer",
      "LSTM",
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting",
    "url": "http://arxiv.org/abs/2505.10191v1",
    "authors": [
      "Qingyu Zheng",
      "Qi Shao",
      "Guijun Han",
      "Wei Li",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2025-05-15",
    "abstract": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the ocean, and their impact on the energy cascade of the global ocean cannot be ignored. Eddy-resolving ocean forecasting is providing more reliable protection for fisheries and navigational safety, but also presents significant scientific challenges and high computational costs for traditional numerical models. Artificial intelligence (AI)-based weather and ocean forecasting systems are becoming powerful tools that balance forecast performance with computational efficiency. However, the complex multiscale features in the ocean dynamical system make AI models still face many challenges in mesoscale eddy forecasting (especially regional modelling). Here, we develop LanTu, a regional eddy-resolving ocean forecasting system based on dynamics-enhanced deep learning. We incorporate cross-scale interactions into LanTu and construct multiscale physical constraint for optimising LanTu guided by knowledge of eddy dynamics in order to improve the forecasting skill of LanTu for mesoscale evolution. The results show that LanTu outperforms the existing advanced operational numerical ocean forecasting system (NOFS) and AI-based ocean forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and current prediction, with a lead time of more than 10 days. Our study highlights that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for eddy-resolving ocean forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network",
    "url": "http://arxiv.org/abs/2505.00495v1",
    "authors": [
      "Nguyen Van Thanh",
      "Nguyen Dang Huynh",
      "Nguyen Ngoc Tan",
      "Nguyen Thai Minh",
      "Nguyen Nam Hoang"
    ],
    "published": "2025-05-01",
    "abstract": "A storm is a type of extreme weather. Therefore, forecasting the path of a storm is extremely important for protecting human life and property. However, storm forecasting is very challenging because storm trajectories frequently change. In this study, we propose an improved deep learning method using a Transformer network to predict the movement trajectory of a storm over the next 6 hours. The storm data used to train the model was obtained from the National Oceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that the proposed method is more accurate than traditional methods. Moreover, the proposed method is faster and more cost-effective",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal",
    "url": "http://arxiv.org/abs/2504.20620v1",
    "authors": [
      "Abhishek Pasula",
      "Deepak N. Subramani"
    ],
    "published": "2025-04-29",
    "abstract": "Climate change alters ocean conditions, notably temperature and sea level. In the Bay of Bengal, these changes influence monsoon precipitation and marine productivity, critical to the Indian economy. In Phase 6 of the Coupled Model Intercomparison Project (CMIP6), Global Climate Models (GCMs) use different shared socioeconomic pathways (SSPs) to obtain future climate projections. However, significant discrepancies are observed between these models and the reanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean square error (RMSE) between the climate model output and the Ocean Reanalysis System (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the dynamic sea level (DSL). We introduce a new data-driven deep learning model to correct for this bias. The deep neural model for each variable is trained using pairs of climatology-removed monthly climate projections as input and the corresponding month's ORAS5 as output. This model is trained with historical data (1950 to 2014), validated with future projection data from 2015 to 2020, and tested with future projections from 2021 to 2023. Compared to the conventional EquiDistant Cumulative Distribution Function (EDCDF) statistical method for bias correction in climate models, our approach decreases RMSE by 0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the projections for 2024-2100. A detailed analysis of the monthly, seasonal, and decadal means and variability is performed to underscore the implications of the novel dynamics uncovered in our corrected projections.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Global Climate Model Bias Correction Using Deep Learning",
    "url": "http://arxiv.org/abs/2504.19145v2",
    "authors": [
      "Abhishek Pasula",
      "Deepak N. Subramani"
    ],
    "published": "2025-04-27",
    "abstract": "Climate change affects ocean temperature, salinity and sea level, impacting monsoons and ocean productivity. Future projections by Global Climate Models based on shared socioeconomic pathways from the Coupled Model Intercomparison Project (CMIP) are widely used to understand the effects of climate change. However, CMIP models have significant bias compared to reanalysis in the Bay of Bengal for the time period when both projections and reanalysis are available. For example, there is a 1.5C root mean square error (RMSE) in the sea surface temperature (SST) projections of the climate model CNRM-CM6 compared to the Ocean Reanalysis System (ORAS5). We develop a suite of data-driven deep learning models for bias correction of climate model projections and apply it to correct SST projections of the Bay of Bengal. We propose the use of three different deep neural network architectures: convolutional encoder-decoder UNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression model and the Equi-Distant Cumulative Density Function (EDCDF) bias correction method for comparison and evaluating the impact of the new deep learning models. All bias correction models are trained using pairs of monthly CMIP6 projections and the corresponding month's ORAS5 as input and output. Historical data (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used for training and validation, including hyperparameter tuning. Testing is performed on future projection data from 2021 to 2024. Detailed analysis of the three deep neural models has been completed. We found that the UNet architecture trained using a climatology-removed CNRM-CM6 projection as input and climatology-removed ORAS5 as output gives the best bias-corrected projections. Our novel deep learning-based method for correcting CNRM-CM6 data has a 15% reduction in RMSE compared EDCDF.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET",
      "LSTM",
      "Deep Neural Network"
    ],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence",
    "url": "http://arxiv.org/abs/2504.19036v1",
    "authors": [
      "Henry Herzog",
      "Joshua Hansen",
      "Yawen Zhang",
      "Patrick Beukema"
    ],
    "published": "2025-04-26",
    "abstract": "Unsustainable exploitation of the oceans exacerbated by global warming is threatening coastal communities worldwide. Accurate and timely monitoring of maritime activity is an essential step to effective governance and to inform future policy. In support of this complex global-scale effort, we built Atlantes, a deep learning based system that provides the first-ever real-time view of vessel behavior at global scale. Atlantes leverages a series of bespoke transformers to distill a high volume, continuous stream of GPS messages emitted by hundreds of thousands of vessels into easily quantifiable behaviors. The combination of low latency and high performance enables operationally relevant decision-making and successful interventions on the high seas where illegal and exploitative activity is too common. Atlantes is already in use by hundreds of organizations worldwide. Here we provide an overview of the model and infrastructure that enables this system to function efficiently and cost-effectively at global-scale and in real-time.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Last-layer committee machines for uncertainty estimations of benthic imagery",
    "url": "http://arxiv.org/abs/2504.16952v1",
    "authors": [
      "H. Martin Gillis",
      "Isaac Xu",
      "Benjamin Misiuk",
      "Craig J. Brown",
      "Thomas Trappenberg"
    ],
    "published": "2025-04-22",
    "abstract": "Automating the annotation of benthic imagery (i.e., images of the seafloor and its associated organisms, habitats, and geological features) is critical for monitoring rapidly changing ocean ecosystems. Deep learning approaches have succeeded in this purpose; however, consistent annotation remains challenging due to ambiguous seafloor images, potential inter-user annotation disagreements, and out-of-distribution samples. Marine scientists implementing deep learning models often obtain predictions based on one-hot representations trained using a cross-entropy loss objective with softmax normalization, resulting with a single set of model parameters. While efficient, this approach may lead to overconfident predictions for context-challenging datasets, raising reliability concerns that present risks for downstream tasks such as benthic habitat mapping and marine spatial planning. In this study, we investigated classification uncertainty as a tool to improve the labeling of benthic habitat imagery. We developed a framework for two challenging sub-datasets of the recently publicly available BenthicNet dataset using Bayesian neural networks, Monte Carlo dropout inference sampling, and a proposed single last-layer committee machine. This approach resulted with a > 95% reduction of network parameters to obtain per-sample uncertainties while obtaining near-identical performance compared to computationally more expensive strategies such as Bayesian neural networks, Monte Carlo dropout, and deep ensembles. The method proposed in this research provides a strategy for obtaining prioritized lists of uncertain samples for human-in-the-loop interventions to identify ambiguous, mislabeled, out-of-distribution, and/or difficult images for enhancing existing annotation tools for benthic mapping and other applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping",
    "url": "http://arxiv.org/abs/2504.14372v1",
    "authors": [
      "Jose Marie Antonio Minoza"
    ],
    "published": "2025-04-19",
    "abstract": "Accurate ocean modeling and coastal hazard prediction depend on high-resolution bathymetric data; yet, current worldwide datasets are too coarse for exact numerical simulations. While recent deep learning advances have improved earth observation data resolution, existing methods struggle with the unique challenges of producing detailed ocean floor maps, especially in maintaining physical structure consistency and quantifying uncertainties. This work presents a novel uncertainty-aware mechanism using spatial blocks to efficiently capture local bathymetric complexity based on block-based conformal prediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE) architecture, the integration of this uncertainty quantification framework yields spatially adaptive confidence estimates while preserving topographical features via discrete latent representations. With smaller uncertainty widths in well-characterized areas and appropriately larger bounds in areas of complex seafloor structures, the block-based design adapts uncertainty estimates to local bathymetric complexity. Compared to conventional techniques, experimental results over several ocean regions show notable increases in both reconstruction quality and uncertainty estimation reliability. This framework increases the reliability of bathymetric reconstructions by preserving structural integrity while offering spatially adaptive uncertainty estimates, so opening the path for more solid climate modeling and coastal hazard assessment.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Joint Source-Environment Adaptation for Deep Learning-Based Underwater Acoustic Source Ranging",
    "url": "http://arxiv.org/abs/2503.23262v1",
    "authors": [
      "Dariush Kari",
      "Andrew C. Singer"
    ],
    "published": "2025-03-30",
    "abstract": "In this paper, we propose a method to adapt a pre-trained deep-learning-based model for underwater acoustic localization to a new environment. We use unsupervised domain adaptation to improve the generalization performance of the model, i.e., using an unsupervised loss, fine-tune the pre-trained network parameters without access to any labels of the target environment or any data used to pre-train the model. This method improves the pre-trained model prediction by coupling that with an almost independent estimation based on the received signal energy (that depends on the source). We show the effectiveness of this approach on Bellhop generated data in an environment similar to that of the SWellEx-96 experiment contaminated with real ocean noise from the KAM11 experiment.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Joint Source-Environment Adaptation of Data-Driven Underwater Acoustic Source Ranging Based on Model Uncertainty",
    "url": "http://arxiv.org/abs/2503.23258v2",
    "authors": [
      "Dariush Kari",
      "Hari Vishnu",
      "Andrew C. Singer"
    ],
    "published": "2025-03-30",
    "abstract": "Adapting pre-trained deep learning models to new and unknown environments remains a major challenge in underwater acoustic localization. We show that although the performance of pre-trained models suffers from mismatch between the training and test data, they generally exhibit a higher uncertainty in environments where there is more mismatch. Additionally, in the presence of environmental mismatch, spurious peaks can appear in the output of classification-based localization approaches, which inspires us to define and use a method to quantify the \"implied uncertainty\" based on the number of model output peaks. Leveraging this notion of implied uncertainty, we partition the test samples into sets with more certain and less certain samples, and implement a method to adapt the model to new environments by using the certain samples to improve the labeling for uncertain samples, which helps to adapt the model. Thus, using this efficient method for model uncertainty quantification, we showcase an innovative approach to adapt a pre-trained model to unseen underwater environments at test time. This eliminates the need for labeled data from the target environment or the original training data. This adaptation is enhanced by integrating an independent estimate based on the received signal energy. We validate the approach extensively using real experimental data, as well as synthetic data consisting of model-generated signals with real ocean noise. The results demonstrate significant improvements in model prediction accuracy, underscoring the potential of the method to enhance underwater acoustic localization in diverse, noisy, and unknown environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Simulation-informed deep learning for enhanced SWOT observations of fine-scale ocean dynamics",
    "url": "http://arxiv.org/abs/2503.21303v1",
    "authors": [
      "Eugenio Cutolo",
      "Carlos Granero-Belinchon",
      "Ptashanna Thiraux",
      "Jinbo Wang",
      "Ronan Fablet"
    ],
    "published": "2025-03-27",
    "abstract": "Oceanic processes at fine scales are crucial yet difficult to observe accurately due to limitations in satellite and in-situ measurements. The Surface Water and Ocean Topography (SWOT) mission provides high-resolution Sea Surface Height (SSH) data, though noise patterns often obscure fine scale structures. Current methods struggle with noisy data or require extensive supervised training, limiting their effectiveness on real-world observations. We introduce SIMPGEN (Simulation-Informed Metric and Prior for Generative Ensemble Networks), an unsupervised adversarial learning framework combining real SWOT observations with simulated reference data. SIMPGEN leverages wavelet-informed neural metrics to distinguish noisy from clean fields, guiding realistic SSH reconstructions. Applied to SWOT data, SIMPGEN effectively removes noise, preserving fine-scale features better than existing neural methods. This robust, unsupervised approach not only improves SWOT SSH data interpretation but also demonstrates strong potential for broader oceanographic applications, including data assimilation and super-resolution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years",
    "url": "http://arxiv.org/abs/2503.21211v2",
    "authors": [
      "Rixu Hao",
      "Yuxin Zhao",
      "Shaoqing Zhang",
      "Guihua Wang",
      "Xiong Deng"
    ],
    "published": "2025-03-27",
    "abstract": "El Ni\u00f1o-Southern Oscillation (ENSO) exerts global climate and societal impacts, but real-time prediction with lead times beyond one year remains challenging. Dynamical models suffer from large biases and uncertainties, while deep learning struggles with interpretability and multi-scale dynamics. Here, we introduce PTSTnet, an interpretable model that unifies dynamical processes and cross-scale spatiotemporal learning in an innovative neural-network framework with physics-encoding learning. PTSTnet produces interpretable predictions significantly outperforming state-of-the-art benchmarks with lead times beyond 24 months, providing physical insights into error propagation in ocean-atmosphere interactions. PTSTnet learns feature representations with physical consistency from sparse data to tackle inherent multi-scale and multi-physics challenges underlying ocean-atmosphere processes, thereby inherently enhancing long-term prediction skill. Our successful realizations mark substantial steps forward in interpretable insights into innovative neural ocean modelling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
    "url": "http://arxiv.org/abs/2503.19502v1",
    "authors": [
      "Qi Chen",
      "Yinghao Cui",
      "Guobin Hong",
      "Karumuri Ashok",
      "Yuchun Pu",
      "Xiaogu Zheng",
      "Xuanze Zhang",
      "Wei Zhong",
      "Peng Zhan",
      "Zhonglei Wang"
    ],
    "published": "2025-03-25",
    "abstract": "El Ni\u00f1o-Southern Oscillation (ENSO) is a prominent mode of interannual climate variability with far-reaching global impacts. Its evolution is governed by intricate air-sea interactions, posing significant challenges for long-term prediction. In this study, we introduce CTEFNet, a multivariate deep learning model that synergizes convolutional neural networks and transformers to enhance ENSO forecasting. By integrating multiple oceanic and atmospheric predictors, CTEFNet extends the effective forecast lead time to 20 months while mitigating the impact of the spring predictability barrier, outperforming both dynamical models and state-of-the-art deep learning approaches. Furthermore, CTEFNet offers physically meaningful and statistically significant insights through gradient-based sensitivity analysis, revealing the key precursor signals that govern ENSO dynamics, which align with well-established theories and reveal new insights about inter-basin interactions among the Pacific, Atlantic, and Indian Oceans. The CTEFNet's superior predictive skill and interpretable sensitivity assessments underscore its potential for advancing climate prediction. Our findings highlight the importance of multivariate coupling in ENSO evolution and demonstrate the promise of deep learning in capturing complex climate dynamics with enhanced interpretability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Deep learning in the abyss: a stratified Physics Informed Neural Network for data assimilation",
    "url": "http://arxiv.org/abs/2503.19160v1",
    "authors": [
      "Vadim Limousin",
      "Nelly Pustelnik",
      "Bruno Deremble",
      "Antoine Venaille"
    ],
    "published": "2025-03-24",
    "abstract": "The reconstruction of deep ocean currents is a major challenge in data assimilation due to the scarcity of interior data. In this work, we present a proof of concept for deep ocean flow reconstruction using a Physics-Informed Neural Network (PINN), a machine learning approach that offers an alternative to traditional data assimilation methods. We introduce an efficient algorithm called StrAssPINN (for Stratified Assimilation PINNs), which assigns a separate network to each layer of the ocean model while allowing them to interact during training. The neural network takes spatiotemporal coordinates as input and predicts the velocity field at those points. Using a SIREN architecture (a multilayer perceptron with sine activation functions), which has proven effective in various contexts, the network is trained using both available observational data and dynamical priors enforced at several collocation points. We apply this method to pseudo-observed ocean data generated from a 3-layer quasi-geostrophic model, where the pseudo-observations include surface-level data akin to SWOT observations of sea surface height, interior data similar to ARGO floats, and a limited number of deep ARGO-like measurements in the lower layers. Our approach successfully reconstructs ocean flows in both the interior and surface layers, demonstrating a strong ability to resolve key ocean mesoscale features, including vortex rings, eastward jets associated with potential vorticity fronts, and smoother Rossby waves. This work serves as a prelude to applying StrAssPINN to real-world observational data.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Observation-only learning of neural mapping schemes for gappy satellite-derived ocean colour parameters",
    "url": "http://arxiv.org/abs/2503.11532v1",
    "authors": [
      "Cl\u00e9ment Dorffer",
      "Fr\u00e9d\u00e9ric Jourdin",
      "Thi Thuy Nga Nguyen",
      "Rodolphe Devillers",
      "David Mouillot",
      "Ronan Fablet"
    ],
    "published": "2025-03-14",
    "abstract": "Monitoring optical properties of coastal and open ocean waters is crucial to assessing the health of marine ecosystems. Deep learning offers a promising approach to address these ecosystem dynamics, especially in scenarios where gap-free ground-truth data is lacking, which poses a challenge for designing effective training frameworks. Using an advanced neural variational data assimilation scheme (called 4DVarNet), we introduce a comprehensive training framework designed to effectively train directly on gappy data sets. Using the Mediterranean Sea as a case study, our experiments not only highlight the high performance of the chosen neural network in reconstructing gap-free images from gappy datasets but also demonstrate its superior performance over state-of-the-art algorithms such as DInEOF and Direct Inversion, whether using CNN or UNet architectures.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "UNET"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Open-Set Plankton Recognition",
    "url": "http://arxiv.org/abs/2503.11318v1",
    "authors": [
      "Joona Kareinen",
      "Annaliina Skytt\u00e4",
      "Tuomas Eerola",
      "Kaisa Kraft",
      "Lasse Lensu",
      "Sanna Suikkanen",
      "Maiju Lehtiniemi",
      "Heikki K\u00e4lvi\u00e4inen"
    ],
    "published": "2025-03-14",
    "abstract": "This paper considers open-set recognition (OSR) of plankton images. Plankton include a diverse range of microscopic aquatic organisms that have an important role in marine ecosystems as primary producers and as a base of food webs. Given their sensitivity to environmental changes, fluctuations in plankton populations offer valuable information about oceans' health and climate change motivating their monitoring. Modern automatic plankton imaging devices enable the collection of large-scale plankton image datasets, facilitating species-level analysis. Plankton species recognition can be seen as an image classification task and is typically solved using deep learning-based image recognition models. However, data collection in real aquatic environments results in imaging devices capturing a variety of non-plankton particles and plankton species not present in the training set. This creates a challenging fine-grained OSR problem, characterized by subtle differences between taxonomically close plankton species. We address this challenge by conducting extensive experiments on three OSR approaches using both phyto- and zooplankton images analyzing also on the effect of the rejection thresholds for OSR. The results demonstrate that high OSR accuracy can be obtained promoting the use of these methods in operational plankton research. We have made the data publicly available to the research community.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Correlation to Causation: A Causal Deep Learning Framework for Arctic Sea Ice Prediction",
    "url": "http://arxiv.org/abs/2503.02093v1",
    "authors": [
      "Emam Hossain",
      "Muhammad Hasan Ferdous",
      "Jianwu Wang",
      "Aneesh Subramanian",
      "Md Osman Gani"
    ],
    "published": "2025-03-03",
    "abstract": "Traditional machine learning and deep learning techniques rely on correlation-based learning, often failing to distinguish spurious associations from true causal relationships, which limits robustness, interpretability, and generalizability. To address these challenges, we propose a causality-driven deep learning framework that integrates Multivariate Granger Causality (MVGC) and PCMCI+ causal discovery algorithms with a hybrid deep learning architecture. Using 43 years (1979-2021) of daily and monthly Arctic Sea Ice Extent (SIE) and ocean-atmospheric datasets, our approach identifies causally significant factors, prioritizes features with direct influence, reduces feature overhead, and improves computational efficiency. Experiments demonstrate that integrating causal features enhances the deep learning model's predictive accuracy and interpretability across multiple lead times. Beyond SIE prediction, the proposed framework offers a scalable solution for dynamic, high-dimensional systems, advancing both theoretical understanding and practical applications in predictive modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "The Role, Trends, and Applications of Machine Learning in Undersea Communication: A Bangladesh Perspective",
    "url": "http://arxiv.org/abs/2503.00669v1",
    "authors": [
      "Yousuf Islam",
      "Sumon Chandra Das",
      "Md. Jalal Uddin Chowdhury"
    ],
    "published": "2025-03-01",
    "abstract": "The rapid evolution of machine learning (ML) has brought about groundbreaking developments in numerous industries, not the least of which is in the area of undersea communication. This domain is critical for applications like ocean exploration, environmental monitoring, resource management, and national security. Bangladesh, a maritime nation with abundant resources in the Bay of Bengal, can harness the immense potential of ML to tackle the unprecedented challenges associated with underwater communication. Beyond that, environmental conditions are unique to the region: in addition to signal attenuation, multipath propagation, noise interference, and limited bandwidth. In this study, we address the necessity to bring ML into communication via undersea; it investigates the latest technologies under the domain of ML in that respect, such as deep learning and reinforcement learning, especially concentrating on Bangladesh scenarios in the sense of implementation. This paper offers a contextualized regional perspective by incorporating region-specific needs, case studies, and recent research to propose a roadmap for deploying ML-driven solutions to improve safety at sea, promote sustainable resource use, and enhance disaster response systems. This research ultimately highlights the promise of ML-powered solutions for transforming undersea communication, leading to more efficient and cost-effective technologies that subsequently contribute to both economic growth and environmental sustainability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "CondensNet: Enabling stable long-term climate simulations via hybrid deep learning models with adaptive physical constraints",
    "url": "http://arxiv.org/abs/2502.13185v1",
    "authors": [
      "Xin Wang",
      "Juntao Yang",
      "Jeff Adie",
      "Simon See",
      "Kalli Furtado",
      "Chen Chen",
      "Troy Arcomano",
      "Romit Maulik",
      "Gianmarco Mengaldo"
    ],
    "published": "2025-02-18",
    "abstract": "Accurate and efficient climate simulations are crucial for understanding Earth's evolving climate. However, current general circulation models (GCMs) face challenges in capturing unresolved physical processes, such as cloud and convection. A common solution is to adopt cloud resolving models, that provide more accurate results than the standard subgrid parametrisation schemes typically used in GCMs. However, cloud resolving models, also referred to as super paramtetrizations, remain computationally prohibitive. Hybrid modeling, which integrates deep learning with equation-based GCMs, offers a promising alternative but often struggles with long-term stability and accuracy issues. In this work, we find that water vapor oversaturation during condensation is a key factor compromising the stability of hybrid models. To address this, we introduce CondensNet, a novel neural network architecture that embeds a self-adaptive physical constraint to correct unphysical condensation processes. CondensNet effectively mitigates water vapor oversaturation, enhancing simulation stability while maintaining accuracy and improving computational efficiency compared to super parameterization schemes.\n  We integrate CondensNet into a GCM to form PCNN-GCM (Physics-Constrained Neural Network GCM), a hybrid deep learning framework designed for long-term stable climate simulations in real-world conditions, including ocean and land. PCNN-GCM represents a significant milestone in hybrid climate modeling, as it shows a novel way to incorporate physical constraints adaptively, paving the way for accurate, lightweight, and stable long-term climate simulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Learning to generate physical ocean states: Towards hybrid climate modeling",
    "url": "http://arxiv.org/abs/2502.02499v1",
    "authors": [
      "Etienne Meunier",
      "David Kamm",
      "Guillaume Gachon",
      "Redouane Lguensat",
      "Julie Deshayes"
    ],
    "published": "2025-02-04",
    "abstract": "Ocean General Circulation Models require extensive computational resources to reach equilibrium states, while deep learning emulators, despite offering fast predictions, lack the physical interpretability and long-term stability necessary for climate scientists to understand climate sensitivity (to greenhouse gas emissions) and mechanisms of abrupt % variability such as tipping points. We propose to take the best from both worlds by leveraging deep generative models to produce physically consistent oceanic states that can serve as initial conditions for climate projections. We assess the viability of this hybrid approach through both physical metrics and numerical experiments, and highlight the benefits of enforcing physical constraints during generation. Although we train here on ocean variables from idealized numerical simulations, we claim that this hybrid approach, combining the computational efficiency of deep learning with the physical accuracy of numerical models, can effectively reduce the computational burden of running climate models to equilibrium, and reduce uncertainties in climate projections by minimizing drifts in baseline simulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Simultaneous emulation and downscaling with physically-consistent deep learning-based regional ocean emulators",
    "url": "http://arxiv.org/abs/2501.05058v1",
    "authors": [
      "Leonard Lupin-Jimenez",
      "Moein Darman",
      "Subhashis Hazarika",
      "Tianning Wu",
      "Michael Gray",
      "Ruyoing He",
      "Anthony Wong",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-01-09",
    "abstract": "Building on top of the success in AI-based atmospheric emulation, we propose an AI-based ocean emulation and downscaling framework focusing on the high-resolution regional ocean over Gulf of Mexico. Regional ocean emulation presents unique challenges owing to the complex bathymetry and lateral boundary conditions as well as from fundamental biases in deep learning-based frameworks, such as instability and hallucinations. In this paper, we develop a deep learning-based framework to autoregressively integrate ocean-surface variables over the Gulf of Mexico at $8$ Km spatial resolution without unphysical drifts over decadal time scales and simulataneously downscale and bias-correct it to $4$ Km resolution using a physics-constrained generative model. The framework shows both short-term skills as well as accurate long-term statistics in terms of mean and variability.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Deep learning selection of analogues for Mars landing sites in the Qaidam Basin, Qinghai-Tibet Plateau",
    "url": "http://arxiv.org/abs/2501.08584v1",
    "authors": [
      "Fanwei Meng",
      "Xiaopeng Wang",
      "Andr\u00e9 Antunes",
      "Jie Zhao",
      "Guoliang Zhou",
      "Biqiong Wu",
      "Tianqi Hao"
    ],
    "published": "2024-12-31",
    "abstract": "Remote sensing observations and Mars rover missions have recorded the presence of beaches, salt lakes, and wind erosion landforms in Martian sediments. All these observations indicate that Mars was hydrated in its early history. There used to be oceans on Mars, but they have now dried up. Therefore, signs of previous life on Mars could be preserved in the evaporites formed during this process. The study of evaporite regions has thus become a priority area for Mars' life exploration. This study proposes a method for training similarity metrics from surface land image data of Earth and Mars, which can be used for recognition or validation applications. The method will be applied in simulating tasks to select Mars landing sites using a selecting small-scale area of the Mars analaogue the evaporite region of Qaidam Basin, Qinghai-Tibet Plateau. This learning process minimizes discriminative loss function, which makes the similarity measure smaller for images from the same location and larger for images from different locations. This study selected a Convolutional Neural Networks (CNN) based model, which has been trained to explain various changes in image appearance and identify different landforms in Mars. By identifying different landforms, priority landing sites on Mars can be selected.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Generating Unseen Nonlinear Evolution in Sea Surface Temperature Using a Deep Learning-Based Latent Space Data Assimilation Framework",
    "url": "http://arxiv.org/abs/2412.13477v1",
    "authors": [
      "Qingyu Zheng",
      "Guijun Han",
      "Wei Li",
      "Lige Cao",
      "Gongfu Zhou",
      "Haowen Wu",
      "Qi Shao",
      "Ru Wang",
      "Xiaobo Wu",
      "Xudong Cui",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2024-12-18",
    "abstract": "Advances in data assimilation (DA) methods have greatly improved the accuracy of Earth system predictions. To fuse multi-source data and reconstruct the nonlinear evolution missing from observations, geoscientists are developing future-oriented DA methods. In this paper, we redesign a purely data-driven latent space DA framework (DeepDA) that employs a generative artificial intelligence model to capture the nonlinear evolution in sea surface temperature. Under variational constraints, DeepDA embedded with nonlinear features can effectively fuse heterogeneous data. The results show that DeepDA remains highly stable in capturing and generating nonlinear evolutions even when a large amount of observational information is missing. It can be found that when only 10% of the observation information is available, the error increase of DeepDA does not exceed 40%. Furthermore, DeepDA has been shown to be robust in the fusion of real observations and ensemble simulations. In particular, this paper provides a mechanism analysis of the nonlinear evolution generated by DeepDA from the perspective of physical patterns, which reveals the inherent explainability of our DL model in capturing multi-scale ocean signals.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Advancing Marine Heatwave Forecasts: An Integrated Deep Learning Approach",
    "url": "http://arxiv.org/abs/2412.04475v1",
    "authors": [
      "Ding Ning",
      "Varvara Vetrova",
      "Yun Sing Koh",
      "Karin R. Bryan"
    ],
    "published": "2024-11-19",
    "abstract": "Marine heatwaves (MHWs), an extreme climate phenomenon, pose significant challenges to marine ecosystems and industries, with their frequency and intensity increasing due to climate change. This study introduces an integrated deep learning approach to forecast short-to-long-term MHWs on a global scale. The approach combines graph representation for modeling spatial properties in climate data, imbalanced regression to handle skewed data distributions, and temporal diffusion to enhance forecast accuracy across various lead times. To the best of our knowledge, this is the first study that synthesizes three spatiotemporal anomaly methodologies to predict MHWs. Additionally, we introduce a method for constructing graphs that avoids isolated nodes and provide a new publicly available sea surface temperature anomaly graph dataset. We examine the trade-offs in the selection of loss functions and evaluation metrics for MHWs. We analyze spatial patterns in global MHW predictability by focusing on historical hotspots, and our approach demonstrates better performance compared to traditional numerical models in regions such as the middle south Pacific, equatorial Atlantic near Africa, south Atlantic, and high-latitude Indian Ocean. We highlight the potential of temporal diffusion to replace the conventional sliding window approach for long-term forecasts, achieving improved prediction up to six months in advance. These insights not only establish benchmarks for machine learning applications in MHW forecasting but also enhance understanding of general climate forecasting methodologies.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "FengWu-W2S: A deep learning model for seamless weather-to-subseasonal forecast of global atmosphere",
    "url": "http://arxiv.org/abs/2411.10191v2",
    "authors": [
      "Fenghua Ling",
      "Kang Chen",
      "Jiye Wu",
      "Tao Han",
      "Jing-Jia Luo",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "published": "2024-11-15",
    "abstract": "Seamless forecasting that produces warning information at continuum timescales based on only one system is a long-standing pursuit for weather-climate service. While the rapid advancement of deep learning has induced revolutionary changes in classical forecasting field, current efforts are still focused on building separate AI models for weather and climate forecasts. To explore the seamless forecasting ability based on one AI model, we propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the FengWu global weather forecast model and incorporates an ocean-atmosphere-land coupling structure along with a diverse perturbation strategy. FengWu-W2S can generate 6-hourly atmosphere forecasts extending up to 42 days through an autoregressive and seamless manner. Our hindcast results demonstrate that FengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead, enhancing predictive capabilities for global surface air temperature, precipitation, geopotential height and intraseasonal signals such as the Madden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover, our ablation experiments on forecast error growth from daily to seasonal timescales reveal potential pathways for developing AI-based integrated system for seamless weather-climate forecasting in the future.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "An Analysis of Deep Learning Parameterizations for Ocean Subgrid Eddy Forcing",
    "url": "http://arxiv.org/abs/2411.06604v1",
    "authors": [
      "Cem Gultekin",
      "Adam Subel",
      "Cheng Zhang",
      "Matan Leibovich",
      "Pavel Perezhogin",
      "Alistair Adcroft",
      "Carlos Fernandez-Granda",
      "Laure Zanna"
    ],
    "published": "2024-11-10",
    "abstract": "Due to computational constraints, climate simulations cannot resolve a range of small-scale physical processes, which have a significant impact on the large-scale evolution of the climate system. Parameterization is an approach to capture the effect of these processes, without resolving them explicitly. In recent years, data-driven parameterizations based on convolutional neural networks have obtained promising results. In this work, we provide an in-depth analysis of these parameterizations developed using data from ocean simulations. The parametrizations account for the effect of mesoscale eddies toward improving simulations of momentum, heat, and mass exchange in the ocean. Our results provide several insights into the properties of data-driven parameterizations based on neural networks. First, their performance can be substantially improved by increasing the geographic extent of the training data. Second, they learn nonlinear structure, since they are able to outperform a linear baseline. Third, they generalize robustly across different CO2 forcings, but not necessarily across different ocean depths. Fourth, they exploit a relatively small region of their input to generate their output. Our results will guide the further development of ocean mesoscale eddy parameterizations, and multiscale modeling more generally.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark",
    "url": "http://arxiv.org/abs/2410.16695v2",
    "authors": [
      "Yang Yu",
      "Yuezun Li",
      "Xin Sun",
      "Junyu Dong"
    ],
    "published": "2024-10-22",
    "abstract": "Phytoplankton are a crucial component of aquatic ecosystems, and effective monitoring of them can provide valuable insights into ocean environments and ecosystem changes. Traditional phytoplankton monitoring methods are often complex and lack timely analysis. Therefore, deep learning algorithms offer a promising approach for automated phytoplankton monitoring. However, the lack of large-scale, high-quality training samples has become a major bottleneck in advancing phytoplankton tracking. In this paper, we propose a challenging benchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse background information and variations in motion during observation. The dataset includes 27 species of phytoplankton and zooplankton, 14 different backgrounds to simulate diverse and complex underwater environments, and a total of 140 videos. To enable accurate real-time observation of phytoplankton, we introduce a multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion Tracker(DSFT), which addresses issues such as focus shifts during tracking and the loss of small target information when computing frame-to-frame similarity. Specifically, we introduce an additional feature extractor to predict the residuals of the standard feature extractor's output, and compute multi-scale frame-to-frame similarity based on features from different layers of the extractor. Extensive experiments on the MPT have demonstrated the validity of the dataset and the superiority of DSFT in tracking phytoplankton, providing an effective solution for phytoplankton monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Accelerate Coastal Ocean Circulation Model with AI Surrogate",
    "url": "http://arxiv.org/abs/2410.14952v2",
    "authors": [
      "Zelin Xu",
      "Jie Ren",
      "Yupu Zhang",
      "Jose Maria Gonzalez Ondina",
      "Maitane Olabarrieta",
      "Tingsong Xiao",
      "Wenchong He",
      "Zibo Liu",
      "Shigang Chen",
      "Kaleb Smith",
      "Zhe Jiang"
    ],
    "published": "2024-10-19",
    "abstract": "Nearly 900 million people live in low-lying coastal zones around the world and bear the brunt of impacts from more frequent and severe hurricanes and storm surges. Oceanographers simulate ocean current circulation along the coasts to develop early warning systems that save lives and prevent loss and damage to property from coastal hazards. Traditionally, such simulations are conducted using coastal ocean circulation models such as the Regional Ocean Modeling System (ROMS), which usually runs on an HPC cluster with multiple CPU cores. However, the process is time-consuming and energy expensive. While coarse-grained ROMS simulations offer faster alternatives, they sacrifice detail and accuracy, particularly in complex coastal environments. Recent advances in deep learning and GPU architecture have enabled the development of faster AI (neural network) surrogates. This paper introduces an AI surrogate based on a 4D Swin Transformer to simulate coastal tidal wave propagation in an estuary for both hindcast and forecast (up to 12 days). Our approach not only accelerates simulations but also incorporates a physics-based constraint to detect and correct inaccurate results, ensuring reliability while minimizing manual intervention. We develop a fully GPU-accelerated workflow, optimizing the model training and inference pipeline on NVIDIA DGX-2 A100 GPUs. Our experiments demonstrate that our AI surrogate reduces the time cost of 12-day forecasting of traditional ROMS simulations from 9,908 seconds (on 512 CPU cores) to 22 seconds (on one A100 GPU), achieving over 450$\\times$ speedup while maintaining high-quality simulation results. This work contributes to oceanographic modeling by offering a fast, accurate, and physically consistent alternative to traditional simulation models, particularly for real-time forecasting in rapid disaster response.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Improved deep learning of chaotic dynamical systems with multistep penalty losses",
    "url": "http://arxiv.org/abs/2410.05572v1",
    "authors": [
      "Dibyajyoti Chakraborty",
      "Seung Whan Chung",
      "Ashesh Chattopadhyay",
      "Romit Maulik"
    ],
    "published": "2024-10-08",
    "abstract": "Predicting the long-term behavior of chaotic systems remains a formidable challenge due to their extreme sensitivity to initial conditions and the inherent limitations of traditional data-driven modeling approaches. This paper introduces a novel framework that addresses these challenges by leveraging the recently proposed multi-step penalty (MP) optimization technique. Our approach extends the applicability of MP optimization to a wide range of deep learning architectures, including Fourier Neural Operators and UNETs. By introducing penalized local discontinuities in the forecast trajectory, we effectively handle the non-convexity of loss landscapes commonly encountered in training neural networks for chaotic systems. We demonstrate the effectiveness of our method through its application to two challenging use-cases: the prediction of flow velocity evolution in two-dimensional turbulence and ocean dynamics using reanalysis data. Our results highlight the potential of this approach for accurate and stable long-term prediction of chaotic dynamics, paving the way for new advancements in data-driven modeling of complex natural phenomena.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Anti-biofouling Lensless Camera System with Deep Learning based Image Reconstruction",
    "url": "http://arxiv.org/abs/2410.01365v1",
    "authors": [
      "Naoki Ide",
      "Tomohiro Kawahara",
      "Hiroshi Ueno",
      "Daiki Yanagidaira",
      "Susumu Takatsuka"
    ],
    "published": "2024-10-02",
    "abstract": "In recent years, there has been an increasing demand for underwater cameras that monitor the condition of offshore structures and check the number of individuals in aqua culture environments with long-period observation. One of the significant issues with this observation is that biofouling sticks to the aperture and lens densely and prevents cameras from capturing clear images. This study examines an underwater camera that applies material technologies with high inherent resistance to biofouling and computer vision technologies based on image reconstruction by deep learning to lens-less cameras. For this purpose, our prototype camera uses a coded aperture with 1k rectangular shape pinholes in a thin metal plate, such as copper, which hinder the growth of biofouling and keep the surface clean. Although images taken by lens-less cameras are usually not well formed due to lack of the traditional glass-based lens, a deep learning approach using ViT (Vision Transformer) has recently demonstrated reconstructing original photo images well and our study shows that using gated MLP (Multilayer Perceptron) also yields good results. On the other hand, a certain degree of thickness for bio-repellence materials is required to exhibit their effect the thickness of aperture is necessary to use apertures sufficiently thinner than the size of the pinholes to avoid unintentional reflection and absorption on the sidewalls. Therefore, we prepared a sufficiently thin plate for image reconstruction and now currently we conduct tests of the lens-less camera of the bio-repellence aperture with actual seawater environments to determine whether it can sufficiently demonstrate the biofouling effect compared with usual camera with only waterproof.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Multi-Sensor Deep Learning for Glacier Mapping",
    "url": "http://arxiv.org/abs/2409.12034v2",
    "authors": [
      "Codru\u0163-Andrei Diaconu",
      "Konrad Heidler",
      "Jonathan L. Bamber",
      "Harry Zekollari"
    ],
    "published": "2024-09-18",
    "abstract": "The more than 200,000 glaciers outside the ice sheets play a crucial role in our society by influencing sea-level rise, water resource management, natural hazards, biodiversity, and tourism. However, only a fraction of these glaciers benefit from consistent and detailed in-situ observations that allow for assessing their status and changes over time. This limitation can, in part, be overcome by relying on satellite-based Earth Observation techniques. Satellite-based glacier mapping applications have historically mainly relied on manual and semi-automatic detection methods, while recently, a fast and notable transition to deep learning techniques has started.\n  This chapter reviews how combining multi-sensor remote sensing data and deep learning allows us to better delineate (i.e. map) glaciers and detect their temporal changes. We explain how relying on deep learning multi-sensor frameworks to map glaciers benefits from the extensive availability of regional and global glacier inventories. We also analyse the rationale behind glacier mapping, the benefits of deep learning methodologies, and the inherent challenges in integrating multi-sensor earth observation data with deep learning algorithms.\n  While our review aims to provide a broad overview of glacier mapping efforts, we highlight a few setups where deep learning multi-sensor remote sensing applications have a considerable potential added value. This includes applications for debris-covered and rock glaciers that are visually difficult to distinguish from surroundings and for calving glaciers that are in contact with the ocean. These specific cases are illustrated through a series of visual imageries, highlighting some significant advantages and challenges when detecting glacier changes, including dealing with seasonal snow cover, changing debris coverage, and distinguishing glacier fronts from the surrounding sea ice.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling",
    "url": "http://arxiv.org/abs/2409.16313v2",
    "authors": [
      "Teerapong Panboonyuen"
    ],
    "published": "2024-09-14",
    "abstract": "Forecasting sea surface currents is essential for applications such as maritime navigation, environmental monitoring, and climate analysis, particularly in regions like the Gulf of Thailand and the Andaman Sea. This paper introduces SEA-ViT, an advanced deep learning model that integrates Vision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to capture spatio-temporal covariance for predicting sea surface currents (U, V) using high-frequency radar (HF) data. The name SEA-ViT is derived from ``Sea Surface Currents Forecasting using Vision Transformer,'' highlighting the model's emphasis on ocean dynamics and its use of the ViT architecture to enhance forecasting capabilities. SEA-ViT is designed to unravel complex dependencies by leveraging a rich dataset spanning over 30 years and incorporating ENSO indices (El Ni\u00f1o, La Ni\u00f1a, and neutral phases) to address the intricate relationship between geographic coordinates and climatic variations. This development enhances the predictive capabilities for sea surface currents, supporting the efforts of the Geo-Informatics and Space Technology Development Agency (GISTDA) in Thailand's maritime regions. The code and pretrained models are available at \\url{https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents}.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Multi-scale decomposition of sea surface height snapshots using machine learning",
    "url": "http://arxiv.org/abs/2409.17354v1",
    "authors": [
      "Jingwen Lyu",
      "Yue Wang",
      "Christian Pedersen",
      "Spencer Jones",
      "Dhruv Balwada"
    ],
    "published": "2024-09-11",
    "abstract": "Knowledge of ocean circulation is important for understanding and predicting weather and climate, and managing the blue economy. This circulation can be estimated through Sea Surface Height (SSH) observations, but requires decomposing the SSH into contributions from balanced and unbalanced motions (BMs and UBMs). This decomposition is particularly pertinent for the novel SWOT satellite, which measures SSH at an unprecedented spatial resolution. Specifically, the requirement, and the goal of this work, is to decompose instantaneous SSH into BMs and UBMs. While a few studies using deep learning (DL) approaches have shown promise in framing this decomposition as an image-to-image translation task, these models struggle to work well across a wide range of spatial scales and require extensive training data, which is scarce in this domain. These challenges are not unique to our task, and pervade many problems requiring multi-scale fidelity. We show that these challenges can be addressed by using zero-phase component analysis (ZCA) whitening and data augmentation; making this a viable option for SSH decomposition across scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "CAS-Canglong: A skillful 3D Transformer model for sub-seasonal to seasonal global sea surface temperature prediction",
    "url": "http://arxiv.org/abs/2409.05369v1",
    "authors": [
      "Longhao Wang",
      "Xuanze Zhang",
      "L. Ruby Leung",
      "Francis H. S. Chiew",
      "Amir AghaKouchak",
      "Kairan Ying",
      "Yongqiang Zhang"
    ],
    "published": "2024-09-09",
    "abstract": "Accurate prediction of global sea surface temperature at sub-seasonal to seasonal (S2S) timescale is critical for drought and flood forecasting, as well as for improving disaster preparedness in human society. Government departments or academic studies normally use physics-based numerical models to predict S2S sea surface temperature and corresponding climate indices, such as El Ni\u00f1o-Southern Oscillation. However, these models are hampered by computational inefficiencies, limited retention of ocean-atmosphere initial conditions, and significant uncertainty and biases. Here, we introduce a novel three-dimensional deep learning neural network to model the nonlinear and complex coupled atmosphere-ocean weather systems. This model incorporates climatic and temporal features and employs a self-attention mechanism to enhance the prediction of global S2S sea surface temperature pattern. Compared to the physics-based models, it shows significant computational efficiency and predictive capability, improving one to three months sea surface temperature predictive skill by 13.7% to 77.1% in seven ocean regions with dominant influence on S2S variability over land. This achievement underscores the significant potential of deep learning for largely improving forecasting skills at the S2S scale over land.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Generative Diffusion Model-based Downscaling of Observed Sea Surface Height over Kuroshio Extension since 2000",
    "url": "http://arxiv.org/abs/2408.12632v1",
    "authors": [
      "Qiuchang Han",
      "Xingliang Jiang",
      "Yang Zhao",
      "Xudong Wang",
      "Zhijin Li",
      "Renhe Zhang"
    ],
    "published": "2024-08-22",
    "abstract": "Satellite altimetry has been widely utilized to monitor global sea surface dynamics, enabling investigation of upper ocean variability from basin-scale to localized eddy ranges. However, the sparse spatial resolution of observational altimetry limits our understanding of oceanic submesoscale variability, prevalent at horizontal scales below 0.25o resolution. Here, we introduce a state-of-the-art generative diffusion model to train high-resolution sea surface height (SSH) reanalysis data and demonstrate its advantage in observational SSH downscaling over the eddy-rich Kuroshio Extension region. The diffusion-based model effectively downscales raw satellite-interpolated data from 0.25o resolution to 1/16o, corresponding to approximately 12-km wavelength. This model outperforms other high-resolution reanalysis datasets and neural network-based methods. Also, it successfully reproduces the spatial patterns and power spectra of satellite along-track observations. Our diffusion-based results indicate that eddy kinetic energy at horizontal scales less than 250 km has intensified significantly since 2004 in the Kuroshio Extension region. These findings underscore the great potential of deep learning in reconstructing satellite altimetry and enhancing our understanding of ocean dynamics at eddy scales.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Long-Range Vision-Based UAV-assisted Localization for Unmanned Surface Vehicles",
    "url": "http://arxiv.org/abs/2408.11429v1",
    "authors": [
      "Waseem Akram",
      "Siyuan Yang",
      "Hailiang Kuang",
      "Xiaoyu He",
      "Muhayy Ud Din",
      "Yihao Dong",
      "Defu Lin",
      "Lakmal Seneviratne",
      "Shaoming He",
      "Irfan Hussain"
    ],
    "published": "2024-08-21",
    "abstract": "The global positioning system (GPS) has become an indispensable navigation method for field operations with unmanned surface vehicles (USVs) in marine environments. However, GPS may not always be available outdoors because it is vulnerable to natural interference and malicious jamming attacks. Thus, an alternative navigation system is required when the use of GPS is restricted or prohibited. To this end, we present a novel method that utilizes an Unmanned Aerial Vehicle (UAV) to assist in localizing USVs in GNSS-restricted marine environments. In our approach, the UAV flies along the shoreline at a consistent altitude, continuously tracking and detecting the USV using a deep learning-based approach on camera images. Subsequently, triangulation techniques are applied to estimate the USV's position relative to the UAV, utilizing geometric information and datalink range from the UAV. We propose adjusting the UAV's camera angle based on the pixel error between the USV and the image center throughout the localization process to enhance accuracy. Additionally, visual measurements are integrated into an Extended Kalman Filter (EKF) for robust state estimation. To validate our proposed method, we utilize a USV equipped with onboard sensors and a UAV equipped with a camera. A heterogeneous robotic interface is established to facilitate communication between the USV and UAV. We demonstrate the efficacy of our approach through a series of experiments conducted during the ``Muhammad Bin Zayed International Robotic Challenge (MBZIRC-2024)'' in real marine environments, incorporating noisy measurements and ocean disturbances. The successful outcomes indicate the potential of our method to complement GPS for USV navigation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Forecasting seasonal rainfall in SE Australia using Empirical Orthogonal Functions and Neural Networks",
    "url": "http://arxiv.org/abs/2408.10550v1",
    "authors": [
      "Stjepan Marcelja"
    ],
    "published": "2024-08-20",
    "abstract": "Quantitative forecasting of average rainfall into the next season remains highly challenging, but in some favourable isolated cases may be possible with a series of relatively simple steps. We chose to explore predictions of austral springtime rainfall in SE Australia regions based on the surrounding ocean surface temperatures during the winter. In the first stage, we search for correlations between the target rainfall and both the standard ocean climate indicators as well as the time series of surface temperature data expanded in terms of Empirical Orthogonal Functions (EOFs). In the case of the Indian Ocean, during the winter the dominant EOF shows stronger correlation with the future rainfall than the commonly used Indian Ocean Dipole. Information sources with the strongest correlation to the historical rainfall data are then used as inputs into deep learning artificial neural networks. The resulting hindcasts appear accurate for September and October and less reliable for November. We also attempt to forecast the rainfall in several regions for the coming austral spring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "DUNE: A Machine Learning Deep UNet++ based Ensemble Approach to Monthly, Seasonal and Annual Climate Forecasting",
    "url": "http://arxiv.org/abs/2408.06262v1",
    "authors": [
      "Pratik Shukla",
      "Milton Halem"
    ],
    "published": "2024-08-12",
    "abstract": "Capitalizing on the recent availability of ERA5 monthly averaged long-term data records of mean atmospheric and climate fields based on high-resolution reanalysis, deep-learning architectures offer an alternative to physics-based daily numerical weather predictions for subseasonal to seasonal (S2S) and annual means. A novel Deep UNet++-based Ensemble (DUNE) neural architecture is introduced, employing multi-encoder-decoder structures with residual blocks. When initialized from a prior month or year, this architecture produced the first AI-based global monthly, seasonal, or annual mean forecast of 2-meter temperatures (T2m) and sea surface temperatures (SST). ERA5 monthly mean data is used as input for T2m over land, SST over oceans, and solar radiation at the top of the atmosphere for each month of 40 years to train the model. Validation forecasts are performed for an additional two years, followed by five years of forecast evaluations to account for natural annual variability. AI-trained inference forecast weights generate forecasts in seconds, enabling ensemble seasonal forecasts. Root Mean Squared Error (RMSE), Anomaly Correlation Coefficient (ACC), and Heidke Skill Score (HSS) statistics are presented globally and over specific regions. These forecasts outperform persistence, climatology, and multiple linear regression for all domains. DUNE forecasts demonstrate comparable statistical accuracy to NOAA's operational monthly and seasonal probabilistic outlook forecasts over the US but at significantly higher resolutions. RMSE and ACC error statistics for other recent AI-based daily forecasts also show superior performance for DUNE-based forecasts. The DUNE model's application to an ensemble data assimilation cycle shows comparable forecast accuracy with a single high-resolution model, potentially eliminating the need for retraining on extrapolated datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Riverbed litter monitoring using consumer-grade aerial-aquatic speedy scanner (AASS) and deep learning based super-resolution reconstruction and detection network",
    "url": "http://arxiv.org/abs/2408.03564v3",
    "authors": [
      "Fan Zhao",
      "Yongying Liu",
      "Jiaqi Wang",
      "Yijia Chen",
      "Dianhan Xi",
      "Xinlei Shao",
      "Shigeru Tabeta",
      "Katsunori Mizuno"
    ],
    "published": "2024-08-07",
    "abstract": "Underwater litter is widely spread across aquatic environments such as lakes, rivers, and oceans, significantly impacting natural ecosystems. Current monitoring technologies for detecting underwater litter face limitations in survey efficiency, cost, and environmental conditions, highlighting the need for efficient, consumer-grade technologies for automatic detection. This research introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with Super-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network. AASS enhances data acquisition efficiency over traditional methods, capturing high-quality images that accurately identify underwater waste. SRR improves image-resolution by mitigating motion blur and insufficient resolution, thereby enhancing detection tasks. Specifically, the RCAN model achieved the highest mean average precision (mAP) of 78.6% for detection accuracy on reconstructed images among the tested SRR models. With a magnification factor of 4, the SRR test set shows an improved mAP compared to the conventional bicubic set. These results demonstrate the effectiveness of the proposed method in detecting underwater litter.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "Introducing VaDA: Novel Image Segmentation Model for Maritime Object Segmentation Using New Dataset",
    "url": "http://arxiv.org/abs/2407.09005v1",
    "authors": [
      "Yongjin Kim",
      "Jinbum Park",
      "Sanha Kang",
      "Hanguen Kim"
    ],
    "published": "2024-07-12",
    "abstract": "The maritime shipping industry is undergoing rapid evolution driven by advancements in computer vision artificial intelligence (AI). Consequently, research on AI-based object recognition models for maritime transportation is steadily growing, leveraging advancements in sensor technology and computing performance. However, object recognition in maritime environments faces challenges such as light reflection, interference, intense lighting, and various weather conditions. To address these challenges, high-performance deep learning algorithms tailored to maritime imagery and high-quality datasets specialized for maritime scenes are essential. Existing AI recognition models and datasets have limited suitability for composing autonomous navigation systems. Therefore, in this paper, we propose a Vertical and Detail Attention (VaDA) model for maritime object segmentation and a new model evaluation method, the Integrated Figure of Calculation Performance (IFCP), to verify its suitability for the system in real-time. Additionally, we introduce a benchmark maritime dataset, OASIs (Ocean AI Segmentation Initiatives) to standardize model performance evaluation across diverse maritime environments. OASIs dataset and details are available at our website: https://www.navlue.com/dataset",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "A Computer Vision Approach to Estimate the Localized Sea State",
    "url": "http://arxiv.org/abs/2407.03755v2",
    "authors": [
      "Aleksandar Vorkapic",
      "Miran Pobar",
      "Marina Ivasic-Kos"
    ],
    "published": "2024-07-04",
    "abstract": "This research presents a novel application of computer vision (CV) and deep learning methods for real-time sea state recognition, aiming to contribute to improving the operational safety and energy efficiency of seagoing vessels, key factors in meeting the legislative carbon reduction targets. Our work focuses on utilizing sea images in operational envelopes captured by a single stationary camera mounted on the ship bridge. The collected images are used to train a deep learning model to automatically recognize the state of the sea based on the Beaufort scale. To recognize the sea state, we used 4 state-of-the-art deep neural networks with different characteristics that proved useful in various computer vision tasks: Resnet-101, NASNet, MobileNet_v2, and Transformer ViT-b32. Furthermore, we have defined a unique large-scale dataset, collected over a broad range of sea conditions from an ocean-going vessel prepared for machine learning. We used the transfer learning approach to fine-tune the models on our dataset. The obtained results demonstrate the potential for this approach to complement traditional methods, particularly where in-situ measurements are unfeasible or interpolated weather buoy data is insufficiently accurate. This study sets the groundwork for further development of sea state classification models to address recognized gaps in maritime research and enable safer and more efficient maritime operations.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet",
      "Transformer",
      "Deep Neural Network"
    ],
    "applications": [
      "Classification",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Advanced Framework for Animal Sound Classification With Features Optimization",
    "url": "http://arxiv.org/abs/2407.03440v1",
    "authors": [
      "Qiang Yang",
      "Xiuying Chen",
      "Changsheng Ma",
      "Carlos M. Duarte",
      "Xiangliang Zhang"
    ],
    "published": "2024-07-03",
    "abstract": "The automatic classification of animal sounds presents an enduring challenge in bioacoustics, owing to the diverse statistical properties of sound signals, variations in recording equipment, and prevalent low Signal-to-Noise Ratio (SNR) conditions. Deep learning models like Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) have excelled in human speech recognition but have not been effectively tailored to the intricate nature of animal sounds, which exhibit substantial diversity even within the same domain. We propose an automated classification framework applicable to general animal sound classification. Our approach first optimizes audio features from Mel-frequency cepstral coefficients (MFCC) including feature rearrangement and feature reduction. It then uses the optimized features for the deep learning model, i.e., an attention-based Bidirectional LSTM (Bi-LSTM), to extract deep semantic features for sound classification. We also contribute an animal sound benchmark dataset encompassing oceanic animals and birds1. Extensive experimentation with real-world datasets demonstrates that our approach consistently outperforms baseline methods by over 25% in precision, recall, and accuracy, promising advancements in animal sound classification.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Scale-Translation Equivariant Network for Oceanic Internal Solitary Wave Localization",
    "url": "http://arxiv.org/abs/2406.13060v1",
    "authors": [
      "Zhang Wan",
      "Shuo Wang",
      "Xudong Zhang"
    ],
    "published": "2024-06-18",
    "abstract": "Internal solitary waves (ISWs) are gravity waves that are often observed in the interior ocean rather than the surface. They hold significant importance due to their capacity to carry substantial energy, thus influence pollutant transport, oil platform operations, submarine navigation, etc. Researchers have studied ISWs through optical images, synthetic aperture radar (SAR) images, and altimeter data from remote sensing instruments. However, cloud cover in optical remote sensing images variably obscures ground information, leading to blurred or missing surface observations. As such, this paper aims at altimeter-based machine learning solutions to automatically locate ISWs. The challenges, however, lie in the following two aspects: 1) the altimeter data has low resolution, which requires a strong machine learner; 2) labeling data is extremely labor-intensive, leading to very limited data for training. In recent years, the grand progress of deep learning demonstrates strong learning capacity given abundant data. Besides, more recent studies on efficient learning and self-supervised learning laid solid foundations to tackle the aforementioned challenges. In this paper, we propose to inject prior knowledge to achieve a strong and efficient learner. Specifically, intrinsic patterns in altimetry data are efficiently captured using a scale-translation equivariant convolutional neural network (ST-ECNN). By considering inherent symmetries in neural network design, ST-ECNN achieves higher efficiency and better performance than baseline models. Furthermore, we also introduce prior knowledge from massive unsupervised data to enhance our solution using the SimCLR framework for pre-training. Our final solution achieves an overall better performance than baselines on our handcrafted altimetry dataset. Data and codes are available at https://github.com/ZhangWan-byte/Internal_Solitary_Wave_Localization .",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Reconstructing the Tropical Pacific Upper Ocean using Online Data Assimilation with a Deep Learning model",
    "url": "http://arxiv.org/abs/2406.07063v1",
    "authors": [
      "Zilu Meng",
      "Gregory J. Hakim"
    ],
    "published": "2024-06-11",
    "abstract": "A deep learning (DL) model, based on a transformer architecture, is trained on a climate-model dataset and compared with a standard linear inverse model (LIM) in the tropical Pacific. We show that the DL model produces more accurate forecasts compared to the LIM when tested on a reanalysis dataset. We then assess the ability of an ensemble Kalman filter to reconstruct the monthly-averaged upper ocean from a noisy set of 24 sea-surface temperature observations designed to mimic existing coral proxy measurements, and compare results for the DL model and LIM. Due to signal damping in the DL model, we implement a novel inflation technique by adding noise from hindcast experiments. Results show that assimilating observations with the DL model yields better reconstructions than the LIM for observation averaging times ranging from one month to one year. The improved reconstruction is due to the enhanced predictive capabilities of the DL model, which map the memory of past observations to future assimilation times.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Ocean Wave Forecasting with Deep Learning as Alternative to Conventional Models",
    "url": "http://arxiv.org/abs/2406.03848v4",
    "authors": [
      "Ziliang Zhang",
      "Huaming Yu",
      "Danqin Ren",
      "Chenyu Zhang",
      "Minghua Sun",
      "Xin Qi"
    ],
    "published": "2024-06-06",
    "abstract": "This study presents OceanCastNet (OCN), a machine learning approach for wave forecasting that incorporates wind and wave fields to predict significant wave height, mean wave period, and mean wave direction.We evaluate OCN's performance against the operational ECWAM model using two independent datasets: NDBC buoy and Jason-3 satellite observations. NDBC station validation indicates OCN performs better at 24 stations compared to ECWAM's 10 stations, and Jason-3 satellite validation confirms similar accuracy across 228-hour forecasts. OCN successfully captures wave patterns during extreme weather conditions, demonstrated through Typhoon Goni with prediction errors typically within $\\pm$0.5 m. The approach also offers computational efficiency advantages. The results suggest that machine learning approaches can achieve performance comparable to conventional wave forecasting systems for operational wave prediction applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Applications of Deep Learning parameterization of Ocean Momentum Forcing",
    "url": "http://arxiv.org/abs/2406.03659v1",
    "authors": [
      "Guosong Wang",
      "Min Hou",
      "Xinrong Wu",
      "Xidong Wang",
      "Zhigang Gao",
      "Hongli Fu",
      "Bo Dan",
      "Chunjian Sun",
      "Xiaoshuang Zhang"
    ],
    "published": "2024-06-06",
    "abstract": "Mesoscale eddies are of utmost importance in understanding ocean dynamics and the transport of heat, salt, and nutrients. Accurate representation of these eddies in ocean models is essential for improving model predictions. However, accurately representing these mesoscale features in numerical models is challenging due to their relatively small size. In this study, we propose a convolutional neural network (CNN) that combines data-driven techniques with physical principles to develop a robust and interpretable parameterization scheme for mesoscale eddies in ocean modeling. We first analyze a high-resolution reanalysis dataset to extract subgrid eddy momentum and use machine learning algorithms to identify patterns and correlations. To ensure physical consistency, we have introduced conservation of momentum constraints in our CNN parameterization scheme through soft and hard constraints. The interpretability analysis illustrate that the pre-trained CNN parameterization shows promising results in accurately solving the resolved mean velocity at the local scale and effectively capturing the representation of unresolved subgrid turbulence processes at the global scale. Furthermore, to validate the CNN parameterization scheme offline, we conduct simulations using the MITgcm ocean model. A series of experiments is conducted to compare the performance of the model with the CNN parameterization scheme and high-resolution simulations. The offline validation using MITgcm simulations demonstrates the effectiveness of the CNN parameterization scheme in improving the representation of mesoscale eddies in the ocean model. Incorporating the CNN parameterization scheme leads to better agreement with high-resolution simulations and a more accurate representation of the kinetic energy spectra.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Data-driven Global Ocean Modeling for Seasonal to Decadal Prediction",
    "url": "http://arxiv.org/abs/2405.15412v2",
    "authors": [
      "Zijie Guo",
      "Pumeng Lyu",
      "Fenghua Ling",
      "Lei Bai",
      "Jing-Jia Luo",
      "Niklas Boers",
      "Toshio Yamagata",
      "Takeshi Izumo",
      "Sophie Cravatte",
      "Antonietta Capotondi",
      "Wanli Ouyang"
    ],
    "published": "2024-05-24",
    "abstract": "Accurate ocean dynamics modeling is crucial for enhancing understanding of ocean circulation, predicting climate variability, and tackling challenges posed by climate change. Despite improvements in traditional numerical models, predicting global ocean variability over multi-year scales remains challenging. Here, we propose ORCA-DL (Oceanic Reliable foreCAst via Deep Learning), the first data-driven 3D ocean model for seasonal to decadal prediction of global ocean circulation. ORCA-DL accurately simulates three-dimensional ocean dynamics and outperforms state-of-the-art dynamical models in capturing extreme events, including El Ni\u00f1o-Southern Oscillation and upper ocean heatwaves. This demonstrates the high potential of data-driven models for efficient and accurate global ocean forecasting. Moreover, ORCA-DL stably emulates ocean dynamics at decadal timescales, demonstrating its potential even for skillful decadal predictions and climate projections.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Deep learning-based hyperspectral image reconstruction for quality assessment of agro-product",
    "url": "http://arxiv.org/abs/2405.12313v1",
    "authors": [
      "Md. Toukir Ahmed",
      "Ocean Monjur",
      "Mohammed Kamruzzaman"
    ],
    "published": "2024-05-20",
    "abstract": "Hyperspectral imaging (HSI) has recently emerged as a promising tool for many agricultural applications; however, the technology cannot be directly used in a real-time system due to the extensive time needed to process large volumes of data. Consequently, the development of a simple, compact, and cost-effective imaging system is not possible with the current HSI systems. Therefore, the overall goal of this study was to reconstruct hyperspectral images from RGB images through deep learning for agricultural applications. Specifically, this study used Hyperspectral Convolutional Neural Network - Dense (HSCNN-D) to reconstruct hyperspectral images from RGB images for predicting soluble solid content (SSC) in sweet potatoes. The algorithm accurately reconstructed the hyperspectral images from RGB images, with the resulting spectra closely matching the ground-truth. The partial least squares regression (PLSR) model based on reconstructed spectra outperformed the model using the full spectral range, demonstrating its potential for SSC prediction in sweet potatoes. These findings highlight the potential of deep learning-based hyperspectral image reconstruction as a low-cost, efficient tool for various agricultural uses.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "BharatBench: Dataset for data-driven weather forecasting over India",
    "url": "http://arxiv.org/abs/2405.07534v1",
    "authors": [
      "Animesh Choudhury",
      "Jagabandhu Panda",
      "Asmita Mukherjee"
    ],
    "published": "2024-05-13",
    "abstract": "Advanced weather and climate models use numerical techniques on grided meshes to simulate atmospheric and ocean dynamics, which are computationally expensive. Data-driven approaches are gaining popularity in weather and climate modeling, with a broad scope of applications. Although Machine Learning (ML) has been employed in this domain, significant progress has occurred in the past decade, leading to ML applications that are now competitive with traditional numerical methods. This study presents a user-friendly dataset for data-driven medium-range weather forecasting focused on India. The dataset is derived from IMDAA reanalysis datasets and optimized for ML applications. The study provides clear evaluation metrics and a few baseline scores from simple linear regression techniques and deep learning models. The dataset can be found at https://www.kaggle.com/datasets/maslab/bharatbench, while the codes are available at https://github.com/MASLABnitrkl/BharatBench. We hope this dataset will boost data-driven weather forecasting over India. We also address limitations in the current evaluation process and future challenges in data-driven weather forecasting.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "OXYGENERATOR: Reconstructing Global Ocean Deoxygenation Over a Century with Deep Learning",
    "url": "http://arxiv.org/abs/2405.07233v1",
    "authors": [
      "Bin Lu",
      "Ze Zhao",
      "Luyu Han",
      "Xiaoying Gan",
      "Yuntao Zhou",
      "Lei Zhou",
      "Luoyi Fu",
      "Xinbing Wang",
      "Chenghu Zhou",
      "Jing Zhang"
    ],
    "published": "2024-05-12",
    "abstract": "Accurately reconstructing the global ocean deoxygenation over a century is crucial for assessing and protecting marine ecosystem. Existing expert-dominated numerical simulations fail to catch up with the dynamic variation caused by global warming and human activities. Besides, due to the high-cost data collection, the historical observations are severely sparse, leading to big challenge for precise reconstruction. In this work, we propose OxyGenerator, the first deep learning based model, to reconstruct the global ocean deoxygenation from 1920 to 2023. Specifically, to address the heterogeneity across large temporal and spatial scales, we propose zoning-varying graph message-passing to capture the complex oceanographic correlations between missing values and sparse observations. Additionally, to further calibrate the uncertainty, we incorporate inductive bias from dissolved oxygen (DO) variations and chemical effects. Compared with in-situ DO observations, OxyGenerator significantly outperforms CMIP6 numerical simulations, reducing MAPE by 38.77%, demonstrating a promising potential to understand the \"breathless ocean\" in data-driven manner.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Continual Learning of Range-Dependent Transmission Loss for Underwater Acoustic using Conditional Convolutional Neural Net",
    "url": "http://arxiv.org/abs/2404.08091v1",
    "authors": [
      "Indu Kant Deo",
      "Akash Venkateshwaran",
      "Rajeev K. Jaiman"
    ],
    "published": "2024-04-11",
    "abstract": "There is a significant need for precise and reliable forecasting of the far-field noise emanating from shipping vessels. Conventional full-order models based on the Navier-Stokes equations are unsuitable, and sophisticated model reduction methods may be ineffective for accurately predicting far-field noise in environments with seamounts and significant variations in bathymetry. Recent advances in reduced-order models, particularly those based on convolutional and recurrent neural networks, offer a faster and more accurate alternative. These models use convolutional neural networks to reduce data dimensions effectively. However, current deep-learning models face challenges in predicting wave propagation over long periods and for remote locations, often relying on auto-regressive prediction and lacking far-field bathymetry information. This research aims to improve the accuracy of deep-learning models for predicting underwater radiated noise in far-field scenarios. We propose a novel range-conditional convolutional neural network that incorporates ocean bathymetry data into the input. By integrating this architecture into a continual learning framework, we aim to generalize the model for varying bathymetry worldwide. To demonstrate the effectiveness of our approach, we analyze our model on several test cases and a benchmark scenario involving far-field prediction over Dickin's seamount in the Northeast Pacific. Our proposed architecture effectively captures transmission loss over a range-dependent, varying bathymetry profile. This architecture can be integrated into an adaptive management system for underwater radiated noise, providing real-time end-to-end mapping between near-field ship noise sources and received noise at the marine mammal's location.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks",
    "url": "http://arxiv.org/abs/2404.06437v1",
    "authors": [
      "Dimitrios Michail",
      "Lefki-Ioanna Panagiotou",
      "Charalampos Davalas",
      "Ioannis Prapas",
      "Spyros Kondylatos",
      "Nikolaos Ioannis Bountos",
      "Ioannis Papoutsis"
    ],
    "published": "2024-04-09",
    "abstract": "With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we train deep learning models with different architectures that capture the spatio-temporal context leading to wildfires. Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models. Our findings demonstrate the great potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations",
    "url": "http://arxiv.org/abs/2404.06400v2",
    "authors": [
      "Maximilian Witte",
      "Fabricio Rodrigues Lapolli",
      "Philip Freese",
      "Sebastian G\u00f6tschel",
      "Daniel Ruprecht",
      "Peter Korn",
      "Christopher Kadow"
    ],
    "published": "2024-04-09",
    "abstract": "Using the nonlinear shallow water equations as benchmark, we demonstrate that a simulation with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a simulation with 10km resolution. The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h. Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow. We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution simulation. After 8 day of simulation, the $L_2$-error of the corrected run is similar to a simulation run on the finer mesh. While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Super-Resolution"
    ],
    "is_recent": false
  },
  {
    "title": "Downscaling GRACE-derived ocean bottom pressure anomalies using self-supervised data fusion",
    "url": "http://arxiv.org/abs/2404.05818v1",
    "authors": [
      "Junyang Gou",
      "Lara B\u00f6rger",
      "Michael Schindelegger",
      "Benedikt Soja"
    ],
    "published": "2024-04-08",
    "abstract": "The gravimetry measurements from the Gravity Recovery and Climate Experiment (GRACE) and its follow-on (GRACE-FO) satellite mission provide an essential way to monitor changes in ocean bottom pressure ($p_b$), which is a critical variable in understanding ocean circulation. However, the coarse spatial resolution of the GRACE(-FO) fields blurs important spatial details, such as $p_b$ gradients. In this study, we employ a self-supervised deep learning algorithm to downscale global monthly $p_b$ anomalies derived from GRACE(-FO) observations to an equal-angle $0.25^\\circ$ grid in the absence of high-resolution ground truth. The optimization process is realized by constraining the outputs to follow the large-scale mass conservation contained in the gravity field estimates while learning the spatial details from two ocean reanalysis products. The downscaled product agrees with GRACE(-FO) solutions over large ocean basins at the millimeter level in terms of equivalent water height and shows signs of outperforming them when evaluating short spatial scale variability. In particular, the downscaled $p_b$ product has more realistic signal content near the coast and exhibits better agreement with tide gauge measurements at around 80% of 465 globally distributed stations. Our method presents a novel way of combining the advantages of satellite measurements and ocean models at the product level, with potential downstream applications for studies of the large-scale ocean circulation, coastal sea level variability, and changes in global geodetic parameters.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A Multiobjective Hyperparameter and Architecture Optimization Approach",
    "url": "http://arxiv.org/abs/2404.05768v2",
    "authors": [
      "Yixuan Sun",
      "Ololade Sowunmi",
      "Romain Egele",
      "Sri Hari Krishna Narayanan",
      "Luke Van Roekel",
      "Prasanna Balaprakash"
    ],
    "published": "2024-04-07",
    "abstract": "Training an effective deep learning model to learn ocean processes involves careful choices of various hyperparameters. We leverage the advanced search algorithms for multiobjective optimization in DeepHyper, a scalable hyperparameter optimization software, to streamline the development of neural networks tailored for ocean modeling. The focus is on optimizing Fourier neural operators (FNOs), a data-driven model capable of simulating complex ocean behaviors. Selecting the correct model and tuning the hyperparameters are challenging tasks, requiring much effort to ensure model accuracy. DeepHyper allows efficient exploration of hyperparameters associated with data preprocessing, FNO architecture-related hyperparameters, and various model training strategies. We aim to obtain an optimal set of hyperparameters leading to the most performant model. Moreover, on top of the commonly used mean squared error for model training, we propose adopting the negative anomaly correlation coefficient as the additional loss term to improve model performance and investigate the potential trade-off between the two terms. The experimental results show that the optimal set of hyperparameters enhanced model performance in single timestepping forecasting and greatly exceeded the baseline configuration in the autoregressive rollout for long-horizon forecasting up to 30 days. Utilizing DeepHyper, we demonstrate an approach to enhance the use of FNOs in ocean dynamics forecasting, offering a scalable solution with improved precision.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Difference Learning for Air Quality Forecasting Transport Emulation",
    "url": "http://arxiv.org/abs/2402.14806v1",
    "authors": [
      "Reed River Chen",
      "Christopher Ribaudo",
      "Jennifer Sleeman",
      "Chace Ashcraft",
      "Collin Kofroth",
      "Marisa Hughes",
      "Ivanka Stajner",
      "Kevin Viner",
      "Kai Wang"
    ],
    "published": "2024-02-22",
    "abstract": "Human health is negatively impacted by poor air quality including increased risk for respiratory and cardiovascular disease. Due to a recent increase in extreme air quality events, both globally and locally in the United States, finer resolution air quality forecasting guidance is needed to effectively adapt to these events. The National Oceanic and Atmospheric Administration provides air quality forecasting guidance for the Continental United States. Their air quality forecasting model is based on a 15 km spatial resolution; however, the goal is to reach a three km spatial resolution. This is currently not feasible due in part to prohibitive computational requirements for modeling the transport of chemical species. In this work, we describe a deep learning transport emulator that is able to reduce computations while maintaining skill comparable with the existing numerical model. We show how this method maintains skill in the presence of extreme air quality events, making it a potential candidate for operational use. We also explore evaluating how well this model maintains the physical properties of the modeled transport for a given set of species.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Quantitative causality, causality-guided scientific discovery, and causal machine learning",
    "url": "http://arxiv.org/abs/2402.13427v1",
    "authors": [
      "X. San Liang",
      "Dake Chen",
      "Renhe Zhang"
    ],
    "published": "2024-02-20",
    "abstract": "It has been said, arguably, that causality analysis should pave a promising way to interpretable deep learning and generalization. Incorporation of causality into artificial intelligence (AI) algorithms, however, is challenged with its vagueness, non-quantitiveness, computational inefficiency, etc. During the past 18 years, these challenges have been essentially resolved, with the establishment of a rigorous formalism of causality analysis initially motivated from atmospheric predictability. This not only opens a new field in the atmosphere-ocean science, namely, information flow, but also has led to scientific discoveries in other disciplines, such as quantum mechanics, neuroscience, financial economics, etc., through various applications. This note provides a brief review of the decade-long effort, including a list of major theoretical results, a sketch of the causal deep learning framework, and some representative real-world applications in geoscience pertaining to this journal, such as those on the anthropogenic cause of global warming, the decadal prediction of El Ni\u00f1o Modoki, the forecasting of an extreme drought in China, among others.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Guiding the underwater acoustic target recognition with interpretable contrastive learning",
    "url": "http://arxiv.org/abs/2402.12658v1",
    "authors": [
      "Yuan Xie",
      "Jiawei Ren",
      "Ji Xu"
    ],
    "published": "2024-02-20",
    "abstract": "Recognizing underwater targets from acoustic signals is a challenging task owing to the intricate ocean environments and variable underwater channels. While deep learning-based systems have become the mainstream approach for underwater acoustic target recognition, they have faced criticism for their lack of interpretability and weak generalization performance in practical applications. In this work, we apply the class activation mapping (CAM) to generate visual explanations for the predictions of a spectrogram-based recognition system. CAM can help to understand the behavior of recognition models by highlighting the regions of the input features that contribute the most to the prediction. Our explorations reveal that recognition models tend to focus on the low-frequency line spectrum and high-frequency periodic modulation information of underwater signals. Based on the observation, we propose an interpretable contrastive learning (ICL) strategy that employs two encoders to learn from acoustic features with different emphases (line spectrum and modulation information). By imposing constraints between encoders, the proposed strategy can enhance the generalization performance of the recognition system. Our experiments demonstrate that the proposed contrastive learning approach can improve the recognition accuracy and bring significant improvements across various underwater databases.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Recognition",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NWP forecasts",
    "url": "http://arxiv.org/abs/2402.07851v2",
    "authors": [
      "Apoorva Narula",
      "Aastha Jain",
      "Jatin Batra",
      "MN Rajeevan",
      "Sandeep Juneja"
    ],
    "published": "2024-02-12",
    "abstract": "The Indian summer monsoon is a highly complex and critical weather system that directly affects the livelihoods of over a billion people across the Indian subcontinent. Accurate short-term forecasting remains a major scientific challenge due to the monsoon's intrinsic nonlinearity and its sensitivity to multi-scale drivers, including local land-atmosphere interactions and large-scale ocean-atmosphere phenomena. In this study, we address the problem of forecasting daily rainfall across India during the summer months, focusing on both one-day and three-day lead times. We use Autoformers - deep learning transformer-based architectures designed for time series forecasting. These are trained on historical gridded precipitation data from the Indian Meteorological Department (1901--2023) at spatial resolutions of $0.25^\\circ \\times 0.25^\\circ$, as well as $1^\\circ \\times 1^\\circ$. The models also incorporate auxiliary meteorological variables from ECMWFs reanalysis datasets, namely, cloud cover, humidity, temperature, soil moisture, vorticity, and wind speed. Forecasts at $0.25^\\circ \\times 0.25^\\circ$ are benchmarked against ECMWFs High-Resolution Ensemble System (HRES), widely regarded as the most accurate numerical weather predictor, and at $1^\\circ \\times 1^\\circ $ with those from National Centre for Environmental Prediction (NCEP). We conduct both nationwide evaluations and localized analyses for major Indian cities. Our results indicate that transformer-based deep learning models consistently outperform both HRES and NCEP, as well as other climatological baselines. Specifically, compared to our model, forecasts from HRES and NCEP model have about 22\\% and 43\\% higher error, respectively, for a single day prediction, and over 27\\% and 66\\% higher error respectively, for a three day prediction.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Climate Trends of Tropical Cyclone Intensity and Energy Extremes Revealed by Deep Learning",
    "url": "http://arxiv.org/abs/2402.00362v1",
    "authors": [
      "Buo-Fu Chen",
      "Boyo Chen",
      "Chun-Min Hsiao",
      "Hsu-Feng Teng",
      "Cheng-Shang Lee",
      "Hung-Chi Kuo"
    ],
    "published": "2024-02-01",
    "abstract": "Anthropogenic influences have been linked to tropical cyclone (TC) poleward migration, TC extreme precipitation, and an increased proportion of major hurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is critical for projecting future TC impacts on human society considering the changing climate [5]. However, past trends of TC structure/energy remain uncertain due to limited observations; subjective-analyzed and spatiotemporal-heterogeneous \"best-track\" datasets lead to reduced confidence in the assessed TC repose to climate change [6, 7]. Here, we use deep learning to reconstruct past \"observations\" and yield an objective global TC wind profile dataset during 1981 to 2020, facilitating a comprehensive examination of TC structure/energy. By training with uniquely labeled data integrating best tracks and numerical model analysis of 2004 to 2018 TCs, our model converts multichannel satellite imagery to a 0-750-km wind profile of axisymmetric surface winds. The model performance is verified to be sufficient for climate studies by comparing it to independent satellite-radar surface winds. Based on the new homogenized dataset, the major TC proportion has increased by ~13% in the past four decades. Moreover, the proportion of extremely high-energy TCs has increased by ~25%, along with an increasing trend (> one standard deviation of the 40-y variability) of the mean total energy of high-energy TCs. Although the warming ocean favors TC intensification, the TC track migration to higher latitudes and altered environments further affect TC structure/energy. This new deep learning method/dataset reveals novel trends regarding TC structure extremes and may help verify simulations/studies regarding TCs in the changing climate.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain",
    "url": "http://arxiv.org/abs/2512.08657v1",
    "authors": [
      "Renato Cordeiro Ferreira",
      "Aditya Dhinavahi",
      "Rowanne Trapmann",
      "Willem-Jan van den Heuvel"
    ],
    "published": "2025-12-09",
    "abstract": "ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Khalasi: Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields",
    "url": "http://arxiv.org/abs/2512.06912v2",
    "authors": [
      "Rushiraj Gadhvi",
      "Sandeep Manjanna"
    ],
    "published": "2025-12-07",
    "abstract": "For centuries, khalasi (Gujarati for sailor) have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics",
    "url": "http://arxiv.org/abs/2512.05765v1",
    "authors": [
      "Edward Y. Chang"
    ],
    "published": "2025-12-05",
    "abstract": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "NORi: An ML-Augmented Ocean Boundary Layer Parameterization",
    "url": "http://arxiv.org/abs/2512.04452v1",
    "authors": [
      "Xin Kai Lee",
      "Ali Ramadhan",
      "Andre Souza",
      "Gregory LeClaire Wagner",
      "Simone Silvestri",
      "John Marshall",
      "Raffaele Ferrari"
    ],
    "published": "2025-12-04",
    "abstract": "NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an \"a posteriori\" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition",
    "url": "http://arxiv.org/abs/2511.20612v1",
    "authors": [
      "Yujin Kim",
      "Sarah Dean"
    ],
    "published": "2025-11-25",
    "abstract": "Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Harmonic Extension for Multiscale Analysis and Modeling Near Boundaries, with an Ocean Application",
    "url": "http://arxiv.org/abs/2512.03051v1",
    "authors": [
      "Benjamin A. Storer",
      "Mehrnoush Kharghani",
      "Alistair Adcroft",
      "Hussein Aluie"
    ],
    "published": "2025-11-23",
    "abstract": "Treatment of fields near domain boundaries is a long-standing problem in signal processing that has come into renewed focus following recent efforts in convolution-based multiscale coarse-graining and in machine-learned parameterizations due to ocean boundary artifacts. Here, we propose a general method for extending fields beyond the domain boundaries by solving a Laplace boundary-value problem. Construction of the harmonic extension is well-posed, including uniqueness, and is consistent with the boundary conditions by design. The formulation applies to irregular boundaries such as discretized coastlines. The harmonic extension is physically desirable since it has minimum spatial variability among all admissible extensions satisfying the boundary conditions. The method is simple to implement using well-established numerical approaches, and is broadly applicable to extending oceanic variables over land boundaries. Other applications include machine learning parametrization and subgrid modeling of wall-bounded flows and multiphase flows. We demonstrate the method by extending sea-surface temperature (SST) over land using fixed temperature (Dirichlet) and no-flux (Neumann) boundary conditions: the land-filled solution is smooth with SST values between the coastal minimum and maximum.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Towards fully differentiable neural ocean model with Veros",
    "url": "http://arxiv.org/abs/2511.17427v1",
    "authors": [
      "Etienne Meunier",
      "Said Ouala",
      "Hugo Frezat",
      "Julien Le Sommer",
      "Ronan Fablet"
    ],
    "published": "2025-11-21",
    "abstract": "We present a differentiable extension of the VEROS ocean model, enabling automatic differentiation through its dynamical core. We describe the key modifications required to make the model fully compatible with JAX autodifferentiation framework and evaluate the numerical consistency of the resulting implementation. Two illustrative applications are then demonstrated: (i) the correction of an initial ocean state through gradient-based optimization, and (ii) the calibration of unknown physical parameters directly from model observations. These examples highlight how differentiable programming can facilitate end-to-end learning and parameter tuning in ocean modeling. Our implementation is available online.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "An Active Learning Interatomic Potential For Defect-Engineered CoCrFeMnNi High-Entropy Alloy",
    "url": "http://arxiv.org/abs/2511.12514v1",
    "authors": [
      "Manish Sahoo",
      "Akash Deshmukh",
      "Yash Kokane",
      "Jayaprakash H M",
      "Raghavan Ranganathan"
    ],
    "published": "2025-11-16",
    "abstract": "High-entropy alloys (HEAs) exhibit exceptional properties arising from a combination of thermodynamic, kinetic and structural factors and have found applications in numerous fields such as aerospace, energy, chemical industries, hydrogen storage, and ocean engineering. However, a large compositional space remains to be explored. Unlike conventional approaches, computational methods have shown accelerated discovery of novel alloys in a short time. However, the lack of interatomic potentials have posed a challenge in discovering new alloy compositions and property measurements. In the present work, we have developed a Moment Tensor Potential (MTP) trained by Machine Learning based approach using the BFGS unconstrained optimization algorithm for the CoCrFeMnNi High-entropy alloy. Our training set consists of various defects induced configurations such as vacancies, dislocations and stacking-faults. An active learning scheme to re-train the potential was undertaken to dynamically to add training data upon encountering extrapolative configurations during non-equilibrium simulations. A thorough investigation of the error metrics, equation of state, uniaxial tensile deformation, nano-indentation and solid-liquid interface stability for this alloy was carried out, and it is seen that the MTP potential outperforms the popular Modified Embedded Atom Method (MEAM) potential on physical properties prediction. The accuracy and high computational speed are discussed using scaling performance. The potential is prepared for public use by embedding it into the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) code.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction",
    "url": "http://arxiv.org/abs/2511.05629v1",
    "authors": [
      "Zheng Jiang",
      "Wei Wang",
      "Gaowei Zhang",
      "Yi Wang"
    ],
    "published": "2025-11-07",
    "abstract": "Sea Surface Temperature (SST) is crucial for understanding upper-ocean thermal dynamics and ocean-atmosphere interactions, which have profound economic and social impacts. While data-driven models show promise in SST prediction, their black-box nature often limits interpretability and overlooks key physical processes. Recently, physics-informed neural networks have been gaining momentum but struggle with complex ocean-atmosphere dynamics due to 1) inadequate characterization of seawater movement (e.g., coastal upwelling) and 2) insufficient integration of external SST drivers (e.g., turbulent heat fluxes). To address these challenges, we propose SSTODE, a physics-informed Neural Ordinary Differential Equations (Neural ODEs) framework for SST prediction. First, we derive ODEs from fluid transport principles, incorporating both advection and diffusion to model ocean spatiotemporal dynamics. Through variational optimization, we recover a latent velocity field that explicitly governs the temporal dynamics of SST. Building upon ODE, we introduce an Energy Exchanges Integrator (EEI)-inspired by ocean heat budget equations-to account for external forcing factors. Thus, the variations in the components of these factors provide deeper insights into SST dynamics. Extensive experiments demonstrate that SSTODE achieves state-of-the-art performances in global and regional SST forecasting benchmarks. Furthermore, SSTODE visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. These findings demonstrate the model's interpretability and physical consistency.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Towards a Unified Data-Driven Boundary Layer Momentum Flux Parameterization for Ocean and Atmosphere",
    "url": "http://arxiv.org/abs/2511.01766v1",
    "authors": [
      "Renaud Falga",
      "Sara Shamekh",
      "Laure Zanna"
    ],
    "published": "2025-11-03",
    "abstract": "Boundary layer turbulence, particularly the vertical fluxes of momentum, shapes the evolution of winds and currents and plays a critical role in weather, climate, and biogeochemical processes. In this work, a unified, data-driven parameterization of turbulent momentum fluxes is introduced for both the oceanic and atmospheric convective boundary layers. An artificial neural network (ANN) is trained offline on coarse-grained large-eddy simulation (LES) data representing a wide range of turbulent regimes in both fluids. By normalizing momentum flux profiles with their surface values, we exploit a self-similar structure across regimes and fluids, enabling joint training. The ANN learns to predict vertical profiles of subgrid momentum fluxes from mean wind or current profiles, capturing key physical features such as upgradient fluxes that are inaccessible to traditional first-order closure schemes. When implemented online in the Single Column Atmospheric Model (SCAM), the ANN parameterization consistently outperforms the SCAM baseline parameterization in replicating the evolution of the boundary layer wind profiles from the LES, especially under convective conditions, with errors reduced by a factor of 2-3 across regimes. ANN performance remains robust even when the surface momentum flux is biased up to 30\\%, and generalization is confirmed by testing on LES cases excluded from the training dataset. This work demonstrates the potential of machine learning to create unified and physically consistent parameterizations across boundary layer systems in climate models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",
    "url": "http://arxiv.org/abs/2511.01019v2",
    "authors": [
      "Bowen Chen",
      "Jayesh Gajbhar",
      "Gregory Dusek",
      "Rob Redmon",
      "Patrick Hogan",
      "Paul Liu",
      "DelWayne Bohnenstiehl",
      "Dongkuan Xu",
      "Ruoying He"
    ],
    "published": "2025-11-02",
    "abstract": "Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified \"hallucinations\" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as \"What was Boston Harbor's highest water level in 2024?\" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Curly Flow Matching for Learning Non-gradient Field Dynamics",
    "url": "http://arxiv.org/abs/2510.26645v1",
    "authors": [
      "Katarina Petrovi\u0107",
      "Lazar Atanackovic",
      "Viggo Moro",
      "Kacper Kapu\u015bniak",
      "\u0130smail \u0130lkan Ceylan",
      "Michael Bronstein",
      "Avishek Joey Bose",
      "Alexander Tong"
    ],
    "published": "2025-10-30",
    "abstract": "Modeling the transport dynamics of natural processes from population-level observations is a ubiquitous problem in the natural sciences. Such models rely on key assumptions about the underlying process in order to enable faithful learning of governing dynamics that mimic the actual system behavior. The de facto assumption in current approaches relies on the principle of least action that results in gradient field dynamics and leads to trajectories minimizing an energy functional between two probability measures. However, many real-world systems, such as cell cycles in single-cell RNA, are known to exhibit non-gradient, periodic behavior, which fundamentally cannot be captured by current state-of-the-art methods such as flow and bridge matching. In this paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is capable of learning non-gradient field dynamics by designing and solving a Schr\u00f6dinger bridge problem with a non-zero drift reference process -- in stark contrast to typical zero-drift reference processes -- which is constructed using inferred velocities in addition to population snapshot data. We showcase Curly-FM by solving the trajectory inference problems for single cells, computational fluid dynamics, and ocean currents with approximate velocities. We demonstrate that Curly-FM can learn trajectories that better match both the reference process and population marginals. Curly-FM expands flow matching models beyond the modeling of populations and towards the modeling of known periodic behavior in physical systems. Our code repository is accessible at: https://github.com/kpetrovicc/curly-flow-matching.git",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Psychlysis: Towards the Creation of a Questionnaire-based Machine Learning Tool to Analyze States of Mind",
    "url": "http://arxiv.org/abs/2512.08940v1",
    "authors": [
      "Hemakshi Jani",
      "Mitish Karia",
      "Meet Gohil",
      "Rahul Bhadja",
      "Aznam Yacoub",
      "Shafaq Khan"
    ],
    "published": "2025-10-27",
    "abstract": "This paper describes the development of Psychlysis, a work-in-progress questionnaire-based machine learning application analyzing the user's current state of mind and suggesting ways to improve their mood using Machine Learning. The application utilizes the OCEAN model to understand the user's personality traits and make customized suggestions to enhance their well-being. The proposed application focus on improving the user's mood rather than just detecting their emotions. Preliminary results of the model are presented, showing the potential of the application in predicting the user's mood and providing personalized recommendations. The paper concludes by highlighting the potential benefits of such an application for various societal segments, including doctors, individuals, and mental health organizations, in improving emotional well-being and reducing the negative impact of mental health issues on daily life.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Learning Coupled Earth System Dynamics with GraphDOP",
    "url": "http://arxiv.org/abs/2510.20416v1",
    "authors": [
      "Eulalie Boucher",
      "Mihai Alexe",
      "Peter Lean",
      "Ewan Pinnington",
      "Simon Lang",
      "Patrick Laloyaux",
      "Lorenzo Zampieri",
      "Patricia de Rosnay",
      "Niels Bormann",
      "Anthony McNally"
    ],
    "published": "2025-10-23",
    "abstract": "Interactions between different components of the Earth System (e.g. ocean, atmosphere, land and cryosphere) are a crucial driver of global weather patterns. Modern Numerical Weather Prediction (NWP) systems typically run separate models of the different components, explicitly coupled across their interfaces to additionally model exchanges between the different components. Accurately representing these coupled interactions remains a major scientific and technical challenge of weather forecasting. GraphDOP is a graph-based machine learning model that learns to forecast weather directly from raw satellite and in-situ observations, without reliance on reanalysis products or traditional physics-based NWP models. GraphDOP simultaneously embeds information from diverse observation sources spanning the full Earth system into a shared latent space. This enables predictions that implicitly capture cross-domain interactions in a single model without the need for any explicit coupling. Here we present a selection of case studies which illustrate the capability of GraphDOP to forecast events where coupled processes play a particularly key role. These include rapid sea-ice freezing in the Arctic, mixing-induced ocean surface cooling during Hurricane Ian and the severe European heat wave of 2022. The results suggest that learning directly from Earth System observations can successfully characterise and propagate cross-component interactions, offering a promising path towards physically consistent end-to-end data-driven Earth System prediction with a single model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Small Ensemble-based Data Assimilation: A Machine Learning-Enhanced Data Assimilation Method with Limited Ensemble Size",
    "url": "http://arxiv.org/abs/2510.15284v1",
    "authors": [
      "Zhilin Li",
      "Zhou Yao",
      "Xianglong Li",
      "Zeng Liu",
      "Zhaokuan Lu",
      "Shanlin Xu",
      "Seungnam Kim",
      "Guangyao Wang"
    ],
    "published": "2025-10-17",
    "abstract": "Ensemble-based data assimilation (DA) methods have become increasingly popular due to their inherent ability to address nonlinear dynamic problems. However, these methods often face a trade-off between analysis accuracy and computational efficiency, as larger ensemble sizes required for higher accuracy also lead to greater computational cost. In this study, we propose a novel machine learning-based data assimilation approach that combines the traditional ensemble Kalman filter (EnKF) with a fully connected neural network (FCNN). Specifically, our method uses a relatively small ensemble size to generate preliminary yet suboptimal analysis states via EnKF. A FCNN is then employed to learn and predict correction terms for these states, thereby mitigating the performance degradation induced by the limited ensemble size. We evaluate the performance of our proposed EnKF-FCNN method through numerical experiments involving Lorenz systems and nonlinear ocean wave field simulations. The results consistently demonstrate that the new method achieves higher accuracy than traditional EnKF with the same ensemble size, while incurring negligible additional computational cost. Moreover, the EnKF-FCNN method is adaptable to diverse applications through coupling with different models and the use of alternative ensemble-based DA methods.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis",
    "url": "http://arxiv.org/abs/2510.17852v1",
    "authors": [
      "Yuze Sun",
      "Wentao Luo",
      "Yanfei Xiang",
      "Jiancheng Pan",
      "Jiahao Li",
      "Quan Zhang",
      "Xiaomeng Huang"
    ],
    "published": "2025-10-14",
    "abstract": "With the growing role of artificial intelligence in climate and weather research, efficient model training and inference are in high demand. Current models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware independence, especially for Chinese domestic hardware and frameworks. To address this issue, we present a framework for migrating large-scale atmospheric and oceanic models from PyTorch to MindSpore and optimizing for Chinese chips, and evaluating their performance against GPUs. The framework focuses on software-hardware adaptation, memory optimization, and parallelism. Furthermore, the model's performance is evaluated across multiple metrics, including training speed, inference speed, model accuracy, and energy efficiency, with comparisons against GPU-based implementations. Experimental results demonstrate that the migration and optimization process preserves the models' original accuracy while significantly reducing system dependencies and improving operational efficiency by leveraging Chinese chips as a viable alternative for scientific computing. This work provides valuable insights and practical guidance for leveraging Chinese domestic chips and frameworks in atmospheric and oceanic AI model development, offering a pathway toward greater technological independence.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Principled Operator Learning in Ocean Dynamics: The Role of Temporal Structure",
    "url": "http://arxiv.org/abs/2510.09792v1",
    "authors": [
      "Vahidreza Jahanmard",
      "Ali Ramezani-Kebrya",
      "Robinson Hordoir"
    ],
    "published": "2025-10-10",
    "abstract": "Neural operators are becoming the default tools to learn solutions to governing partial differential equations (PDEs) in weather and ocean forecasting applications. Despite early promising achievements, significant challenges remain, including long-term prediction stability and adherence to physical laws, particularly for high-frequency processes. In this paper, we take a step toward addressing these challenges in high-resolution ocean prediction by incorporating temporal Fourier modes, demonstrating how this modification enhances physical fidelity. This study compares the standard Fourier Neural Operator (FNO) with its variant, FNOtD, which has been modified to internalize the dispersion relation while learning the solution operator for ocean PDEs. The results demonstrate that entangling space and time in the training of integral kernels enables the model to capture multiscale wave propagation and effectively learn ocean dynamics. FNOtD substantially improves long-term prediction stability and consistency with underlying physical dynamics in challenging high-frequency settings compared to the standard FNO. It also provides competitive predictive skill relative to a state-of-the-art numerical ocean model, while requiring significantly lower computational cost.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer",
    "url": "http://arxiv.org/abs/2510.03566v1",
    "authors": [
      "Ashwin Prabu",
      "Nhat Thanh Tran",
      "Guofa Zhou",
      "Jack Xin"
    ],
    "published": "2025-10-03",
    "abstract": "A variety of models have been developed to forecast dengue cases to date. However, it remains a challenge to predict major dengue outbreaks that need timely public warnings the most. In this paper, we introduce CrossLag, an environmentally informed attention that allows for the incorporation of lagging endogenous signals behind the significant events in the exogenous data into the architecture of the transformer at low parameter counts. Outbreaks typically lag behind major changes in climate and oceanic anomalies. We use TimeXer, a recent general-purpose transformer distinguishing exogenous-endogenous inputs, as the baseline for this study. Our proposed model outperforms TimeXer by a considerable margin in detecting and predicting major outbreaks in Singapore dengue data over a 24-week prediction window.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2510.03534v3",
    "authors": [
      "Nicol\u00f2 Dal Fabbro",
      "Milad Mesbahi",
      "Renato Mendes",
      "Jo\u00e3o Borges de Sousa",
      "George J. Pappas"
    ],
    "published": "2025-10-03",
    "abstract": "We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
    "url": "http://arxiv.org/abs/2509.26536v2",
    "authors": [
      "Yida Xue",
      "Mingjun Mao",
      "Xiangyuan Ru",
      "Yuqi Zhu",
      "Baochang Ren",
      "Shuofei Qiao",
      "Mengru Wang",
      "Shumin Deng",
      "Xinyu An",
      "Ningyu Zhang",
      "Ying Chen",
      "Huajun Chen"
    ],
    "published": "2025-09-30",
    "abstract": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields",
    "url": "http://arxiv.org/abs/2509.26005v1",
    "authors": [
      "Rui-Yang Zhang",
      "Henry B. Moss",
      "Lachlan Astfalck",
      "Edward Cripps",
      "David S. Leslie"
    ],
    "published": "2025-09-30",
    "abstract": "We introduce a formal active learning methodology for guiding the placement of Lagrangian observers to infer time-dependent vector fields -- a key task in oceanography, marine science, and ocean engineering -- using a physics-informed spatio-temporal Gaussian process surrogate model. The majority of existing placement campaigns either follow standard `space-filling' designs or relatively ad-hoc expert opinions. A key challenge to applying principled active learning in this setting is that Lagrangian observers are continuously advected through the vector field, so they make measurements at different locations and times. It is, therefore, important to consider the likely future trajectories of placed observers to account for the utility of candidate placement locations. To this end, we present BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories. We observe noticeable benefits of BALLAST-aided sequential observer placement strategies on both synthetic and high-fidelity ocean current models.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise",
    "url": "http://arxiv.org/abs/2509.25730v1",
    "authors": [
      "Indu Kant Deo",
      "Akash Venkateshwaran",
      "Rajeev K. Jaiman"
    ],
    "published": "2025-09-30",
    "abstract": "Ship traffic is an increasing source of underwater radiated noise in coastal waters, motivating real-time digital twins of ocean acoustics for operational noise mitigation. We present a physics-guided probabilistic framework to predict three-dimensional transmission loss in realistic ocean environments. As a case study, we consider the Salish Sea along shipping routes from the Pacific Ocean to the Port of Vancouver. A dataset of over 30 million source-receiver pairs was generated with a Gaussian beam solver across seasonal sound speed profiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We first assess sparse variational Gaussian processes (SVGP) and then incorporate physics-based mean functions combining spherical spreading with frequency-dependent absorption. To capture nonlinear effects, we examine deep sigma-point processes and stochastic variational deep kernel learning. The final framework integrates four components: (i) a learnable physics-informed mean that represents dominant propagation trends, (ii) a convolutional encoder for bathymetry along the source-receiver track, (iii) a neural encoder for source, receiver, and frequency coordinates, and (iv) a residual SVGP layer that provides calibrated predictive uncertainty. This probabilistic digital twin facilitates the construction of sound-exposure bounds and worst-case scenarios for received levels. We further demonstrate the application of the framework to ship speed optimization, where predicted transmission loss combined with near-field source models provides sound exposure level estimates for minimizing acoustic impacts on marine mammals. The proposed framework advances uncertainty-aware digital twins for ocean acoustics and illustrates how physics-guided machine learning can support sustainable maritime operations.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Meta-Learning Fourier Neural Operators for Hessian Inversion and Enhanced Variational Data Assimilation",
    "url": "http://arxiv.org/abs/2509.22949v1",
    "authors": [
      "Hamidreza Moazzami",
      "Asma Jamali",
      "Nicholas Kevlahan",
      "Rodrigo A. Vargas-Hern\u00e1ndez"
    ],
    "published": "2025-09-26",
    "abstract": "Data assimilation (DA) is crucial for enhancing solutions to partial differential equations (PDEs), such as those in numerical weather prediction, by optimizing initial conditions using observational data. Variational DA methods are widely used in oceanic and atmospheric forecasting, but become computationally expensive, especially when Hessian information is involved. To address this challenge, we propose a meta-learning framework that employs the Fourier Neural Operator (FNO) to approximate the inverse Hessian operator across a family of DA problems, thereby providing an effective initialization for the conjugate gradient (CG) method. Numerical experiments on a linear advection equation demonstrate that the resulting FNO-CG approach reduces the average relative error by $62\\%$ and the number of iterations by $17\\%$ compared to the standard CG. These improvements are most pronounced in ill-conditioned scenarios, highlighting the robustness and efficiency of FNO-CG for challenging DA problems.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations",
    "url": "http://arxiv.org/abs/2509.21477v1",
    "authors": [
      "Yuan Gao",
      "Hao Wu",
      "Qingsong Wen",
      "Kun Wang",
      "Xian Wu",
      "Xiaomeng Huang"
    ],
    "published": "2025-09-25",
    "abstract": "Reconstructing subsurface ocean dynamics, such as vertical velocity fields, from incomplete surface observations poses a critical challenge in Earth science, a field long hampered by the lack of standardized, analysis-ready benchmarks. To systematically address this issue and catalyze research, we first build and release KD48, a high-resolution ocean dynamics benchmark derived from petascale simulations and curated with expert-driven denoising. Building on this benchmark, we introduce VISION, a novel reconstruction paradigm based on Dynamic Prompting designed to tackle the core problem of missing data in real-world observations. The essence of VISION lies in its ability to generate a visual prompt on-the-fly from any available subset of observations, which encodes both data availability and the ocean's physical state. More importantly, we design a State-conditioned Prompting module that efficiently injects this prompt into a universal backbone, endowed with geometry- and scale-aware operators, to guide its adaptive adjustment of computational strategies. This mechanism enables VISION to precisely handle the challenges posed by varying input combinations. Extensive experiments on the KD48 benchmark demonstrate that VISION not only substantially outperforms state-of-the-art models but also exhibits strong generalization under extreme data missing scenarios. By providing a high-quality benchmark and a robust model, our work establishes a solid infrastructure for ocean science research under data uncertainty. Our codes are available at: https://github.com/YuanGao-YG/VISION.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling",
    "url": "http://arxiv.org/abs/2509.19032v2",
    "authors": [
      "Kashaf Ul Emaan"
    ],
    "published": "2025-09-23",
    "abstract": "Detection of credit card fraud is an acute issue of financial security because transaction datasets are highly lopsided, with fraud cases being only a drop in the ocean. Balancing datasets using the most popular methods of traditional oversampling such as the Synthetic Minority Oversampling Technique (SMOTE) generally create simplistic synthetic samples that are not readily applicable to complex fraud patterns. Recent industry advances that include Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular Variational Autoencoders (TVAE) have demonstrated increased efficiency in tabular synthesis, yet all these models still exhibit issues with high-dimensional dependence modelling. Now we will present our hybrid approach where we use a Generative Adversarial Network (GAN) with a Transformer encoder block to produce realistic fraudulent transactions samples. The GAN architecture allows training realistic generators adversarial, and the Transformer allows the model to learn rich feature interactions by self-attention. Such a hybrid strategy overcomes the limitations of SMOTE, CTGAN, and TVAE by producing a variety of high-quality synthetic minority classes samples. We test our algorithm on the publicly-available Credit Card Fraud Detection dataset and compare it to conventional and generative resampling strategies with a variety of classifiers, such as Logistic Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN shows substantial gains in Recall, F1-score and Area Under the Receiver Operating Characteristic Curve (AUC), which indicates that it is effective in overcoming the severe class imbalance inherent in the task of fraud detection.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "GAN",
      "Autoencoder"
    ],
    "applications": [
      "Detection",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Climate-Adaptive and Cascade-Constrained Machine Learning Prediction for Sea Surface Height under Greenhouse Warming",
    "url": "http://arxiv.org/abs/2509.18741v1",
    "authors": [
      "Tianmu Zheng",
      "Ru Chen",
      "Xin Su",
      "Gang Huang",
      "Bingzheng Yan"
    ],
    "published": "2025-09-23",
    "abstract": "Machine learning (ML) has achieved remarkable success in climate and marine science. Given that greenhouse warming fundamentally reshapes ocean conditions such as stratification, circulation patterns and eddy activity, evaluating the climate adaptability of the ML model is crucial. While physical constraints have been shown to enhance the performance of ML models, kinetic energy (KE) cascade has not been used as a constraint despite its importance in regulating multi-scale ocean motions. Here we develop two sea surface height (SSH) prediction models (with and without KE cascade constraint) and quantify their climate adaptability at the Kuroshio Extension. Our results demonstrate that both models exhibit only slight performance degradation under greenhouse warming conditions. Incorporating the KE cascade as a physical constraint significantly improve the model performance, reducing eddy kinetic energy errors by 14.7% in the present climate and 15.9% under greenhouse warming. This work presents the first application of the kinetic energy (KE) cascade as a physical constraint for ML based ocean state prediction and demonstrates its robust adaptability across climates, offering guidance for the further development of global ML models for both present and future conditions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning",
    "url": "http://arxiv.org/abs/2509.18553v1",
    "authors": [
      "Richa Rawat",
      "Faisal Ahmed"
    ],
    "published": "2025-09-23",
    "abstract": "Cancer is one of the leading health challenges for women, specifically breast and ovarian cancer. Early detection can help improve the survival rate through timely intervention and treatment. Traditional methods of detecting cancer involve manually examining mammograms, CT scans, ultrasounds, and other imaging types. However, this makes the process labor-intensive and requires the expertise of trained pathologists. Hence, making it both time-consuming and resource-intensive. In this paper, we introduce a novel vision transformer (ViT)-based method for detecting and classifying breast and ovarian cancer. We use a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both binary and multi-class classification tasks using publicly available histopathological image datasets. Further, we use a preprocessing pipeline that converts raw histophological images into standardized PyTorch tensors, which are compatible with the ViT architecture and also help improve the model performance. We evaluated the performance of our model on two benchmark datasets: the BreakHis dataset for binary classification and the UBC-OCEAN dataset for five-class classification without any data augmentation. Our model surpasses existing CNN, ViT, and topological data analysis-based approaches in binary classification. For multi-class classification, it is evaluated against recent topological methods and demonstrates superior performance. Our study highlights the effectiveness of Vision Transformer-based transfer learning combined with efficient preprocessing in oncological diagnostics.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Artificial neural networks ensemble methodology to predict significant wave height",
    "url": "http://arxiv.org/abs/2509.14020v1",
    "authors": [
      "Felipe Crivellaro Minuzzi",
      "Leandro Farina"
    ],
    "published": "2025-09-17",
    "abstract": "The forecast of wave variables are important for several applications that depend on a better description of the ocean state. Due to the chaotic behaviour of the differential equations which model this problem, a well know strategy to overcome the difficulties is basically to run several simulations, by for instance, varying the initial condition, and averaging the result of each of these, creating an ensemble. Moreover, in the last few years, considering the amount of available data and the computational power increase, machine learning algorithms have been applied as surrogate to traditional numerical models, yielding comparative or better results. In this work, we present a methodology to create an ensemble of different artificial neural networks architectures, namely, MLP, RNN, LSTM, CNN and a hybrid CNN-LSTM, which aims to predict significant wave height on six different locations in the Brazilian coast. The networks are trained using NOAA's numerical reforecast data and target the residual between observational data and the numerical model output. A new strategy to create the training and target datasets is demonstrated. Results show that our framework is capable of producing high efficient forecast, with an average accuracy of $80\\%$, that can achieve up to $88\\%$ in the best case scenario, which means $5\\%$ reduction in error metrics if compared to NOAA's numerical model, and a increasingly reduction of computational cost.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN",
      "LSTM",
      "RNN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "SamudrACE: Fast and Accurate Coupled Climate Modeling with 3D Ocean and Atmosphere Emulators",
    "url": "http://arxiv.org/abs/2509.12490v1",
    "authors": [
      "James P. C. Duncan",
      "Elynn Wu",
      "Surya Dheeshjith",
      "Adam Subel",
      "Troy Arcomano",
      "Spencer K. Clark",
      "Brian Henn",
      "Anna Kwa",
      "Jeremy McGibbon",
      "W. Andre Perkins",
      "William Gregory",
      "Carlos Fernandez-Granda",
      "Julius Busecke",
      "Oliver Watt-Meyer",
      "William J. Hurlin",
      "Alistair Adcroft",
      "Laure Zanna",
      "Christopher Bretherton"
    ],
    "published": "2025-09-15",
    "abstract": "Traditional numerical global climate models simulate the full Earth system by exchanging boundary conditions between separate simulators of the atmosphere, ocean, sea ice, land surface, and other geophysical processes. This paradigm allows for distributed development of individual components within a common framework, unified by a coupler that handles translation between realms via spatial or temporal alignment and flux exchange. Following a similar approach adapted for machine learning-based emulators, we present SamudrACE: a coupled global climate model emulator which produces centuries-long simulations at 1-degree horizontal, 6-hourly atmospheric, and 5-daily oceanic resolution, with 145 2D fields spanning 8 atmospheric and 19 oceanic vertical levels, plus sea ice, surface, and top-of-atmosphere variables. SamudrACE is highly stable and has low climate biases comparable to those of its components with prescribed boundary forcing, with realistic variability in coupled climate phenomena such as ENSO that is not possible to simulate in uncoupled mode.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Seasonal forecasting using the GenCast probabilistic machine learning model",
    "url": "http://arxiv.org/abs/2509.06457v1",
    "authors": [
      "Bobby Antonio",
      "Kristian Strommen",
      "Hannah M. Christensen"
    ],
    "published": "2025-09-08",
    "abstract": "Machine-learnt weather prediction (MLWP) models are now well established as being competitive with conventional numerical weather prediction (NWP) models in the medium range. However, there is still much uncertainty as to how this performance extends to longer timescales, where interactions with slower components of the earth system become important. We take GenCast, a state-of-the-art probabilistic MLWP model, and apply it to the task of seasonal forecasting with prescribed sea surface temperature (SST), by providing anomalies persisted over climatology (GenCast-Persisted) or forcing with observations (GenCast-Forced). The forecasts are compared to the European Centre for Medium-Range Weather Forecasts seasonal forecasting system, SEAS5. Our results indicate that, despite being trained at short timescales, GenCast-Persisted produces much of the correct precipitation patterns in response to El Ni\u00f1o and La Ni\u00f1a events, with several erroneous patterns in GenCast-Persisted corrected with GenCast-Forced. The uncertainty in precipitation response, as represented by the ensemble, compares favourably to SEAS5. Whilst SEAS5 achieves superior skill in the tropics for 2-metre temperature and mean sea level pressure (MSLP), GenCast-Persisted achieves significantly higher skill in some areas in higher latitudes, including mountainous areas, with notable improvements for MSLP in particular; this is reflected in a higher correlation with the observed NAO index. Reliability diagrams indicate that GenCast-Persisted is overconfident compared to SEAS5, whilst GenCast-Forced produces well-calibrated seasonal 2-metre temperature predictions. These results provide an indication of the potential of MLWP models similar to GenCast for the `full' seasonal forecasting problem, where the atmospheric model is coupled to ocean, land and cryosphere models.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "LUCIE-3D: A three-dimensional climate emulator for forced responses",
    "url": "http://arxiv.org/abs/2509.02061v1",
    "authors": [
      "Haiwen Guan",
      "Troy Arcomano",
      "Ashesh Chattopadhyay",
      "Romit Maulik"
    ],
    "published": "2025-09-02",
    "abstract": "We introduce LUCIE-3D, a lightweight three-dimensional climate emulator designed to capture the vertical structure of the atmosphere, respond to climate change forcings, and maintain computational efficiency with long-term stability. Building on the original LUCIE-2D framework, LUCIE-3D employs a Spherical Fourier Neural Operator (SFNO) backbone and is trained on 30 years of ERA5 reanalysis data spanning eight vertical \u03c3-levels. The model incorporates atmospheric CO2 as a forcing variable and optionally integrates prescribed sea surface temperature (SST) to simulate coupled ocean--atmosphere dynamics. Results demonstrate that LUCIE-3D successfully reproduces climatological means, variability, and long-term climate change signals, including surface warming and stratospheric cooling under increasing CO2 concentrations. The model further captures key dynamical processes such as equatorial Kelvin waves, the Madden--Julian Oscillation, and annular modes, while showing credible behavior in the statistics of extreme events. Despite requiring longer training than its 2D predecessor, LUCIE-3D remains efficient, training in under five hours on four GPUs. Its combination of stability, physical consistency, and accessibility makes it a valuable tool for rapid experimentation, ablation studies, and the exploration of coupled climate dynamics, with potential applications extending to paleoclimate research and future Earth system emulation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef",
    "url": "http://arxiv.org/abs/2509.01019v1",
    "authors": [
      "Scarlett Raine",
      "Benjamin Moshirian",
      "Tobias Fischer"
    ],
    "published": "2025-08-31",
    "abstract": "Coral reefs are on the brink of collapse, with climate change, ocean acidification, and pollution leading to a projected 70-90% loss of coral species within the next decade. Restoration efforts are crucial, but their success hinges on introducing automation to upscale efforts. We present automated deployment of coral re-seeding devices powered by artificial intelligence, computer vision, and robotics. Specifically, we perform automated substrate classification, enabling detection of areas of the seafloor suitable for coral growth, thus significantly reducing reliance on human experts and increasing the range and efficiency of restoration. Real-world testing of the algorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%, sub-image patch classification of 89.1%, and real-time model inference at 5.5 frames per second. Further, we present and publicly contribute a large collection of annotated substrate image data to foster future research in this area.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Efficient Probabilistic Visualization of Local Divergence of 2D Vector Fields with Independent Gaussian Uncertainty",
    "url": "http://arxiv.org/abs/2510.01190v1",
    "authors": [
      "Timbwaoga A. J. Ouermi",
      "Eric Li",
      "Kenneth Moreland",
      "Dave Pugmire",
      "Chris R. Johnson",
      "Tushar M. Athawale"
    ],
    "published": "2025-08-21",
    "abstract": "This work focuses on visualizing uncertainty of local divergence of two-dimensional vector fields. Divergence is one of the fundamental attributes of fluid flows, as it can help domain scientists analyze potential positions of sources (positive divergence) and sinks (negative divergence) in the flow. However, uncertainty inherent in vector field data can lead to erroneous divergence computations, adversely impacting downstream analysis. While Monte Carlo (MC) sampling is a classical approach for estimating divergence uncertainty, it suffers from slow convergence and poor scalability with increasing data size and sample counts. Thus, we present a two-fold contribution that tackles the challenges of slow convergence and limited scalability of the MC approach. (1) We derive a closed-form approach for highly efficient and accurate uncertainty visualization of local divergence, assuming independently Gaussian-distributed vector uncertainties. (2) We further integrate our approach into Viskores, a platform-portable parallel library, to accelerate uncertainty visualization. In our results, we demonstrate significantly enhanced efficiency and accuracy of our serial analytical (speed-up up to 1946X) and parallel Viskores (speed-up up to 19698X) algorithms over the classical serial MC approach. We also demonstrate qualitative improvements of our probabilistic divergence visualizations over traditional mean-field visualization, which disregards uncertainty. We validate the accuracy and efficiency of our methods on wind forecast and ocean simulation datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Generative AI models capture realistic sea-ice evolution from days to decades",
    "url": "http://arxiv.org/abs/2508.14984v2",
    "authors": [
      "Tobias Sebastian Finn",
      "Marc Bocquet",
      "Pierre Rampal",
      "Charlotte Durand",
      "Flavia Porro",
      "Alban Farchi",
      "Alberto Carrassi"
    ],
    "published": "2025-08-20",
    "abstract": "Sea ice plays an important role in stabilising the Earth system. Yet, representing its dynamics remains a major challenge for models, as the underlying processes are scale-invariant and highly anisotropic. This poses a dilemma: physics-based models that faithfully reproduce the observed dynamics are computationally costly, while efficient AI models sacrifice realism. Here, to resolve this dilemma, we introduce GenSIM, the first generative AI model to predict the evolution of the full Arctic sea-ice state at 12-hour increments. Trained for sub-daily forecasting on 20 years of sea-ice-ocean simulation data, GenSIM makes realistic predictions for 30 years, while reproducing the dynamical properties of sea ice with its leads and ridges and capturing long-term trends in the sea-ice volume. Notably, although solely driven by atmospheric reanalysis, GenSIM implicitly learns hidden signatures of multi-year ice-ocean interaction. Therefore, generative AI can extrapolate from sub-daily forecasts to decadal simulations, while retaining physical consistency.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Nonparametric Reaction Coordinate Optimization with Histories: A Framework for Rare Event Dynamics",
    "url": "http://arxiv.org/abs/2508.07326v1",
    "authors": [
      "Polina V. Banushkina",
      "Sergei V. Krivov"
    ],
    "published": "2025-08-10",
    "abstract": "Rare but critical events in complex systems, such as protein folding, chemical reactions, disease progression, and extreme weather or climate phenomena, are governed by complex, high-dimensional, stochastic dynamics. Identifying an optimal reaction coordinate (RC) that accurately captures the progress of these dynamics is crucial for understanding and simulating such processes. This work introduces a nonparametric RC optimization framework that incorporates trajectory histories, enabling robust analysis even for irregular or incomplete data. The power of the method is demonstrated through increasingly challenging analyses of protein folding dynamics, where it provides accurate committor estimates that pass a stringent validation test and yield high-resolution free energy profiles. Its generality is further illustrated through applications to dynamics in phase space, a conceptual ocean circulation model, and a longitudinal clinical dataset. These results demonstrate that rare event dynamics can be accurately characterized without exhaustive sampling of the configuration space, establishing a general, flexible, and robust framework for analyzing complex dynamical systems and longitudinal datasets.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models",
    "url": "http://arxiv.org/abs/2508.05260v1",
    "authors": [
      "Zhouyao Qian",
      "Yang Chen",
      "Baodian Li",
      "Shuyi Zhang",
      "Zhen Tian",
      "Gongsen Wang",
      "Tianyue Gu",
      "Xinyu Zhou",
      "Huilin Chen",
      "Xinyi Li",
      "Hao Zhu",
      "Shuyao Zhang",
      "Zongheng Li",
      "Siyuan Wang"
    ],
    "published": "2025-08-07",
    "abstract": "Marine chlorophyll concentration is an important indicator of ecosystem health and carbon cycle strength, and its accurate prediction is crucial for red tide warning and ecological response. In this paper, we propose a LSTM-RF hybrid model that combines the advantages of LSTM and RF, which solves the deficiencies of a single model in time-series modelling and nonlinear feature portrayal. Trained with multi-source ocean data(temperature, salinity, dissolved oxygen, etc.), the experimental results show that the LSTM-RF model has an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test set, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2 =0.4934) alone , respectively. The standardised treatment and sliding window approach improved the prediction accuracy of the model and provided an innovative solution for high-frequency prediction of marine ecological variables.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction",
    "url": "http://arxiv.org/abs/2508.00933v1",
    "authors": [
      "Hanchen Yang",
      "Jiaqi Wang",
      "Jiannong Cao",
      "Wengen Li",
      "Jialun Zheng",
      "Yangning Li",
      "Chunyu Miao",
      "Jihong Guan",
      "Shuigeng Zhou",
      "Philip S. Yu"
    ],
    "published": "2025-07-31",
    "abstract": "Sea surface temperature (SST) prediction is a critical task in ocean science, supporting various applications, such as weather forecasting, fisheries management, and storm tracking. While existing data-driven methods have demonstrated significant success, they often neglect to leverage the rich domain knowledge accumulated over the past decades, limiting further advancements in prediction accuracy. The recent emergence of large language models (LLMs) has highlighted the potential of integrating domain knowledge for downstream tasks. However, the application of LLMs to SST prediction remains underexplored, primarily due to the challenge of integrating ocean domain knowledge and numerical data. To address this issue, we propose Ocean Knowledge Graph-enhanced LLM (OKG-LLM), a novel framework for global SST prediction. To the best of our knowledge, this work presents the first systematic effort to construct an Ocean Knowledge Graph (OKG) specifically designed to represent diverse ocean knowledge for SST prediction. We then develop a graph embedding network to learn the comprehensive semantic and structural knowledge within the OKG, capturing both the unique characteristics of individual sea regions and the complex correlations between them. Finally, we align and fuse the learned knowledge with fine-grained numerical SST data and leverage a pre-trained LLM to model SST patterns for accurate prediction. Extensive experiments on the real-world dataset demonstrate that OKG-LLM consistently outperforms state-of-the-art methods, showcasing its effectiveness, robustness, and potential to advance SST prediction. The codes are available in the online repository.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Discrete Gaussian Vector Fields On Meshes",
    "url": "http://arxiv.org/abs/2507.20024v1",
    "authors": [
      "Michael Gillan",
      "Stefan Siegert",
      "Ben Youngman"
    ],
    "published": "2025-07-26",
    "abstract": "Though the underlying fields associated with vector-valued environmental data are continuous, observations themselves are discrete. For example, climate models typically output grid-based representations of wind fields or ocean currents, and these are often downscaled to a discrete set of points. By treating the area of interest as a two-dimensional manifold that can be represented as a triangular mesh and embedded in Euclidean space, this work shows that discrete intrinsic Gaussian processes for vector-valued data can be developed from discrete differential operators defined with respect to a mesh. These Gaussian processes account for the geometry and curvature of the manifold whilst also providing a flexible and practical formulation that can be readily applied to any two-dimensional mesh. We show that these models can capture harmonic flows, incorporate boundary conditions, and model non-stationary data. Finally, we apply these models to downscaling stationary and non-stationary gridded wind data on the globe, and to inference of ocean currents from sparse observations in bounded domains.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Discovering the dynamics of \\emph{Sargassum} rafts' centers of mass",
    "url": "http://arxiv.org/abs/2507.18771v1",
    "authors": [
      "Francisco J. Beron-Vera",
      "Gage Bonner"
    ],
    "published": "2025-07-24",
    "abstract": "Since 2011, rafts of floating \\emph{Sargassum} seaweed have frequently obstructed the coasts of the Intra-Americas Seas. The motion of the rafts is represented by a high-dimensional nonlinear dynamical system. Referred to as the eBOMB model, this builds on the Maxey--Riley equation by incorporating interactions between clumps of \\emph{Sargassum} forming a raft and the effects of Earth's rotation. The absence of a predictive law for the rafts' centers of mass suggests a need for machine learning. In this paper, we evaluate and contrast Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) and Sparse Identification of Nonlinear Dynamics (SINDy). In both cases, a physics-inspired closure modeling approach is taken rooted in eBOMB. Specifically, the LSTM model learns a mapping from a collection of eBOMB variables to the difference between raft center-of-mass and ocean velocities. The SINDy model's library of candidate functions is suggested by eBOMB variables and includes windowed velocity terms incorporating far-field effects of the carrying flow. Both LSTM and SINDy models perform most effectively in conditions with tightly bonded clumps, despite declining precision with rising complexity, such as with wind effects and when assessing loosely connected clumps. The LSTM model delivered the best results when designs were straightforward, with fewer neurons and hidden layers. While LSTM model serves as an opaque black-box model lacking interpretability, the SINDy model brings transparency by discerning explicit functional relationships through the function libraries. Integration of the windowed velocity terms enabled effective modeling of nonlocal interactions, particularly in datasets featuring sparsely connected rafts.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM",
      "RNN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Site-Specific Parameterization of Ocean Spectra for Power Estimates of Wave Energy Converters",
    "url": "http://arxiv.org/abs/2507.05440v1",
    "authors": [
      "Rafael Baez Ramirez",
      "Ethan J. Sloan",
      "Carlos Alejandro Michel\u00e9n Str\u00f6fer"
    ],
    "published": "2025-07-07",
    "abstract": "Estimating the mean annual power of a wave energy converter (WEC) through the method of bins relies on a parametric representation of all possible sea states. In practice, two-parameter spectra based on significant wave height and energy period are ubiquitous. Two-parameter spectra have been shown insufficient in capturing the range of spectral shapes that can occur in an actual ocean environment. Furthermore, through sensitivity analysis, these two-parameters have been shown to be insufficient for predicting power performance of WECs. Four parameter spectra, which expand the parameter space to include two additional shape parameters have been shown sufficient in capturing sea state variance, but their effect on mean power estimates has not been presented. This work directly looks at the effects of incorporating 4-parameter spectra into annual power estimates compared to using the traditional 2-parameter spectra. We use two different 4-parameter spectra: one from the literature and a novel machine learning-based autoencoder, presented here. Both are shown to improve the information retained when parameterizing spectra. The site-specific autoencoder performs consistently the best across two case studies of mean annual power prediction, achieving an error around 1% in each instance. The 2-parameter spectra resulted in less consistent predictive performance, with errors of -8% and 1% in the two case studies. For the case study where all three models performed well, it is shown that the low error in the 2-parameter model is attributable to a symmetrical distribution of large errors whereas both 4-parameter spectra result in relatively low errors throughout the parametric space. These results highlight the need for more sophisticated resource characterization methods for estimating the power performance of WECs and suggest site-specific machine learning-based spectra are an adequate option.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Physical Informed Neural Networks for modeling ocean pollutant",
    "url": "http://arxiv.org/abs/2507.08834v1",
    "authors": [
      "Karishma Battina",
      "Prathamesh Dinesh Joshi",
      "Raj Abhijit Dandekar",
      "Rajat Dandekar",
      "Sreedath Panat"
    ],
    "published": "2025-07-07",
    "abstract": "Traditional numerical methods often struggle with the complexity and scale of modeling pollutant transport across vast and dynamic oceanic domains. This paper introduces a Physics-Informed Neural Network (PINN) framework to simulate the dispersion of pollutants governed by the 2D advection-diffusion equation. The model achieves physically consistent predictions by embedding physical laws and fitting to noisy synthetic data, generated via a finite difference method (FDM), directly into the neural network training process. This approach addresses challenges such as non-linear dynamics and the enforcement of boundary and initial conditions. Synthetic data sets, augmented with varying noise levels, are used to capture real-world variability. The training incorporates a hybrid loss function including PDE residuals, boundary/initial condition conformity, and a weighted data fit term. The approach takes advantage of the Julia language scientific computing ecosystem for high-performance simulations, offering a scalable and flexible alternative to traditional solvers",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models",
    "url": "http://arxiv.org/abs/2506.22552v6",
    "authors": [
      "Fabrizio Falasca"
    ],
    "published": "2025-06-27",
    "abstract": "A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To explore the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and to probe causal mechanisms. We argue that the ability of emulators of multiscale systems to reproduce perturbed statistics depends critically on (i) the choice of an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. These insights highlight reduced-order models, tailored to specific goals, processes, and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application by developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. Overall, these results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory providing a principled framework to guide model design and enhance causal understanding.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks",
    "url": "http://arxiv.org/abs/2506.20181v1",
    "authors": [
      "Ronald Katende"
    ],
    "published": "2025-06-25",
    "abstract": "We develop a principled framework for discovering causal structure in partial differential equations (PDEs) using physics-informed neural networks and counterfactual perturbations. Unlike classical residual minimization or sparse regression methods, our approach quantifies operator-level necessity through functional interventions on the governing dynamics. We introduce causal sensitivity indices and structural deviation metrics to assess the influence of candidate differential operators within neural surrogates. Theoretically, we prove exact recovery of the causal operator support under restricted isometry or mutual coherence conditions, with residual bounds guaranteeing identifiability. Empirically, we validate the framework on both synthetic and real-world datasets across climate dynamics, tumor diffusion, and ocean flows. Our method consistently recovers governing operators even under noise, redundancy, and data scarcity, outperforming standard PINNs and DeepONets in structural fidelity. This work positions causal PDE discovery as a tractable and interpretable inference task grounded in structural causal models and variational residual analysis.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models",
    "url": "http://arxiv.org/abs/2506.18124v2",
    "authors": [
      "Shaoxiu Wei",
      "Mingchao Liang",
      "Florian Meyer"
    ],
    "published": "2025-06-22",
    "abstract": "Multiobject tracking (MOT) is an important task in applications including autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT methods are model-based and combine sequential Bayesian estimation with data association and an object birth model. More recent methods are fully data-driven and rely on the training of neural networks. Both approaches offer distinct advantages in specific settings. In particular, model-based methods are generally applicable across a wide range of scenarios, whereas data-driven MOT achieves superior performance in scenarios where abundant labeled data for training is available. A natural thought is whether a general framework can integrate the two approaches. This paper introduces a hybrid method that utilizes neural networks to enhance specific aspects of the statistical model in Bayesian MOT that have been identified as overly simplistic. By doing so, the performance of the prediction and update steps of Bayesian MOT is improved. To ensure tractable computation, our framework uses belief propagation to avoid high-dimensional operations combined with sequential Monte Carlo methods to perform low-dimensional operations efficiently. The resulting method combines the flexibility and robustness of model-based approaches with the capability to learn complex information from data of neural networks. We evaluate the performance of the proposed method based on the nuScenes autonomous driving dataset and demonstrate that it has state-of-the-art performance",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation",
    "url": "http://arxiv.org/abs/2506.17409v1",
    "authors": [
      "Quoc Thinh Vo",
      "Joe Woods",
      "Priontu Chowdhury",
      "David K. Han"
    ],
    "published": "2025-06-20",
    "abstract": "Localizing acoustic sound sources in the ocean is a challenging task due to the complex and dynamic nature of the environment. Factors such as high background noise, irregular underwater geometries, and varying acoustic properties make accurate localization difficult. To address these obstacles, we propose a multi-branch network architecture designed to accurately predict the distance between a moving acoustic source and a receiver, tested on real-world underwater signal arrays. The network leverages Convolutional Neural Networks (CNNs) for robust spatial feature extraction and integrates Conformers with self-attention mechanism to effectively capture temporal dependencies. Log-mel spectrogram and generalized cross-correlation with phase transform (GCC-PHAT) features are employed as input representations. To further enhance the model performance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively adjusts the amplitude of input features, ensuring consistent energy levels across varying ranges, signal strengths, and noise conditions. We assess the model's generalization capability by training it in one domain and testing it in a different domain, using only a limited amount of data from the test domain for fine-tuning. Our proposed method outperforms state-of-the-art (SOTA) approaches in similar settings, establishing new benchmarks for underwater sound localization.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Tale of Two Systems: Characterizing Architectural Complexity on Machine Learning-Enabled Systems",
    "url": "http://arxiv.org/abs/2506.11295v1",
    "authors": [
      "Renato Cordeiro Ferreira"
    ],
    "published": "2025-06-12",
    "abstract": "How can the complexity of ML-enabled systems be managed effectively? The goal of this research is to investigate how complexity affects ML-Enabled Systems (MLES). To address this question, this research aims to introduce a metrics-based architectural model to characterize the complexity of MLES. The goal is to support architectural decisions, providing a guideline for the inception and growth of these systems. This paper brings, side-by-side, the architecture representation of two systems that can be used as case studies for creating the metrics-based architectural model: the SPIRA and the Ocean Guard MLES.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion",
    "url": "http://arxiv.org/abs/2506.10391v1",
    "authors": [
      "Yuanyi Song",
      "Pumeng Lyu",
      "Ben Fei",
      "Fenghua Ling",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "published": "2025-06-12",
    "abstract": "Accurate reconstruction of ocean is essential for reflecting global climate dynamics and supporting marine meteorological research. Conventional methods face challenges due to sparse data, algorithmic complexity, and high computational costs, while increasing usage of machine learning (ML) method remains limited to reconstruction problems at the sea surface and local regions, struggling with issues like cloud occlusion. To address these limitations, this paper proposes ReconMOST, a data-driven guided diffusion model framework for multi-layer sea temperature reconstruction. Specifically, we first pre-train an unconditional diffusion model using a large collection of historical numerical simulation data, enabling the model to attain physically consistent distribution patterns of ocean temperature fields. During the generation phase, sparse yet high-accuracy in-situ observational data are utilized as guidance points for the reverse diffusion process, generating accurate reconstruction results. Importantly, in regions lacking direct observational data, the physically consistent spatial distribution patterns learned during pre-training enable implicitly guided and physically plausible reconstructions. Our method extends ML-based SST reconstruction to a global, multi-layer setting, handling over 92.5% missing data while maintaining reconstruction accuracy, spatial resolution, and superior generalization capability. We pre-train our model on CMIP6 numerical simulation data and conduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The results of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on reconstruction, and 0.633 on total, respectively, demonstrating the effectiveness and robustness of the proposed framework. Our source code is available at https://github.com/norsheep/ReconMOST.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "MLOps with Microservices: A Case Study on the Maritime Domain",
    "url": "http://arxiv.org/abs/2506.06202v2",
    "authors": [
      "Renato Cordeiro Ferreira",
      "Rowanne Trapmann",
      "Willem-Jan van den Heuvel"
    ],
    "published": "2025-06-06",
    "abstract": "This case study describes challenges and lessons learned on building Ocean Guard: a Machine Learning-Enabled System (MLES) for anomaly detection in the maritime domain. First, the paper presents the system's specification, and architecture. Ocean Guard was designed with a microservices' architecture to enable multiple teams to work on the project in parallel. Then, the paper discusses how the developers adapted contract-based design to MLOps for achieving that goal. As a MLES, Ocean Guard employs code, model, and data contracts to establish guidelines between its services. This case study hopes to inspire software engineers, machine learning engineers, and data scientists to leverage similar approaches for their systems.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates",
    "url": "http://arxiv.org/abs/2506.05513v1",
    "authors": [
      "Yunfei Huang",
      "David S. Greenberg"
    ],
    "published": "2025-06-05",
    "abstract": "Neural PDE surrogates can improve the cost-accuracy tradeoff of classical solvers, but often generalize poorly to new initial conditions and accumulate errors over time. Physical and symmetry constraints have shown promise in closing this performance gap, but existing techniques for imposing these inductive biases are incompatible with the staggered grids commonly used in computational fluid dynamics. Here we introduce novel input and output layers that respect physical laws and symmetries on the staggered grids, and for the first time systematically investigate how these constraints, individually and in combination, affect the accuracy of PDE surrogates. We focus on two challenging problems: shallow water equations with closed boundaries and decaying incompressible turbulence. Compared to strong baselines, symmetries and physical constraints consistently improve performance across tasks, architectures, autoregressive prediction steps, accuracy measures, and network sizes. Symmetries are more effective than physical constraints, but surrogates with both performed best, even compared to baselines with data augmentation or pushforward training, while themselves benefiting from the pushforward trick. Doubly-constrained surrogates also generalize better to initial conditions and durations beyond the range of the training data, and more accurately predict real-world ocean currents.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
    "url": "http://arxiv.org/abs/2506.03210v2",
    "authors": [
      "Qiusheng Huang",
      "Yuan Niu",
      "Xiaohui Zhong",
      "Anboyu Guo",
      "Lei Chen",
      "Dianjun Zhang",
      "Xuefeng Zhang",
      "Hao Li"
    ],
    "published": "2025-06-03",
    "abstract": "Accurate, high-resolution ocean forecasting is crucial for maritime operations and environmental monitoring. While traditional numerical models are capable of producing sub-daily, eddy-resolving forecasts, they are computationally intensive and face challenges in maintaining accuracy at fine spatial and temporal scales. In contrast, recent data-driven approaches offer improved computational efficiency and emerging potential, yet typically operate at daily resolution and struggle with sub-daily predictions due to error accumulation over time. We introduce FuXi-Ocean, the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12\u00b0 spatial resolution, reaching depths of up to 1500 meters. The model architecture integrates a context-aware feature extraction module with a predictive network employing stacked attention blocks. The core innovation is the Mixture-of-Time (MoT) module, which adaptively integrates predictions from multiple temporal contexts by learning variable-specific reliability , mitigating cumulative errors in sequential forecasting. Through comprehensive experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting key variables, including temperature, salinity, and currents, across multiple depths.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation",
    "url": "http://arxiv.org/abs/2505.21020v4",
    "authors": [
      "Yuan Gao",
      "Hao Wu",
      "Fan Xu",
      "Yanfei Xiang",
      "Ruijian Gou",
      "Ruiqi Shu",
      "Qingsong Wen",
      "Xian Wu",
      "Kun Wang",
      "Xiaomeng Huang"
    ],
    "published": "2025-05-27",
    "abstract": "Long-term, high-fidelity simulation of slow-changing physical systems, such as the ocean and climate, presents a fundamental challenge in scientific computing. Traditional autoregressive machine learning models often fail in these tasks as minor errors accumulate and lead to rapid forecast degradation. To address this problem, we propose NeuralOM, a general neural operator framework designed for simulating complex, slow-changing dynamics. NeuralOM's core consists of two key innovations: (1) a Progressive Residual Correction Framework that decomposes the forecasting task into a series of fine-grained refinement steps, effectively suppressing long-term error accumulation; and (2) a Physics-Guided Graph Network whose built-in adaptive messaging mechanism explicitly models multi-scale physical interactions, such as gradient-driven flows and multiplicative couplings, thereby enhancing physical consistency while maintaining computational efficiency. We validate NeuralOM on the challenging task of global Subseasonal-to-Seasonal (S2S) ocean simulation. Extensive experiments demonstrate that NeuralOM not only surpasses state-of-the-art models in forecast accuracy and long-term stability, but also excels in simulating extreme events. For instance, at a 60-day lead time, NeuralOM achieves a 13.3% lower RMSE compared to the best-performing baseline, offering a stable, efficient, and physically-aware paradigm for data-driven scientific computing. Code link: https://github.com/YuanGao-YG/NeuralOM.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Automated data curation for self-supervised learning in underwater acoustic analysis",
    "url": "http://arxiv.org/abs/2505.20066v1",
    "authors": [
      "Hilde I Hummel",
      "Sandjai Bhulai",
      "Burooj Ghani",
      "Rob van der Mei"
    ],
    "published": "2025-05-26",
    "abstract": "The sustainability of the ocean ecosystem is threatened by increased levels of sound pollution, making monitoring crucial to understand its variability and impact. Passive acoustic monitoring (PAM) systems collect a large amount of underwater sound recordings, but the large volume of data makes manual analysis impossible, creating the need for automation. Although machine learning offers a potential solution, most underwater acoustic recordings are unlabeled. Self-supervised learning models have demonstrated success in learning from large-scale unlabeled data in various domains like computer vision, Natural Language Processing, and audio. However, these models require large, diverse, and balanced datasets for training in order to generalize well. To address this, a fully automated self-supervised data curation pipeline is proposed to create a diverse and balanced dataset from raw PAM data. It integrates Automatic Identification System (AIS) data with recordings from various hydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw audio data is sampled and then combined with AIS samples to create a balanced and diverse dataset. The resulting curated dataset enables the development of self-supervised learning models, facilitating various tasks such as monitoring marine mammals and assessing sound pollution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Machine learning-based correlation analysis of decadal cyclone intensity with sea surface temperature: data and tutorial",
    "url": "http://arxiv.org/abs/2506.09254v2",
    "authors": [
      "Jingyang Wu",
      "Rohitash Chandra"
    ],
    "published": "2025-05-25",
    "abstract": "The rising number of extreme climate events in the past decades has motivated the need for a thorough consideration of tropical cyclone genesis and intensity, given the sea-surface temperature (SST). In this paper, we present an analysis of the relationship between the increasing global SST with cyclone genesis using linear regression machine learning models. We extract and curate a dataset of tropical cyclones across selected ocean basins with their associated SST over the past 40 years. We provide correlation analysis using linear regression and visualisation strategies. Our preliminary results show a strong positive correlation between SST and high wind speed across selected ocean basins via linear regression and machine learning models. Our dataset and available open-source code offer a novel perspective for the investigation of the genesis and intensity of tropical cyclones. Alongside the time and position of each cyclone, we also provide the related Saffir-Simpson category, season, wind speed, and SST for 15 days before and after the tropical cyclone genesis.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Advancing global sea ice prediction capabilities using a fully-coupled climate model with integrated machine learning",
    "url": "http://arxiv.org/abs/2505.18328v1",
    "authors": [
      "William Gregory",
      "Mitchell Bushuk",
      "Yong-Fei Zhang",
      "Alistair Adcroft",
      "Laure Zanna",
      "Colleen McHugh",
      "Liwei Jia"
    ],
    "published": "2025-05-23",
    "abstract": "We showcase a hybrid modeling framework which embeds machine learning (ML) inference into the GFDL SPEAR climate model, for online sea ice bias correction during a set of global fully-coupled 1-year retrospective forecasts. We compare two hybrid versions of SPEAR to understand the importance of exposing ML models to coupled ice-atmosphere-ocean feedbacks before implementation into fully-coupled simulations: Hybrid_CPL (with feedbacks) and Hybrid_IO (without feedbacks). Relative to SPEAR, Hybrid_CPL systematically reduces seasonal forecast errors in the Arctic and significantly reduces Antarctic errors for target months May-December, with >2x error reduction in 4-6-month lead forecasts of Antarctic winter sea ice extent. Meanwhile, Hybrid_IO suffers from out-of-sample behavior which can trigger a chain of Southern Ocean feedbacks, leading to ice-free Antarctic summers. Our results demonstrate that ML can significantly improve numerical sea ice prediction capabilities and that exposing ML models to coupled ice-atmosphere-ocean processes is essential for generalization in fully-coupled simulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Physics-Informed Neural Networks for Vessel Trajectory Prediction: Learning Time-Discretized Kinematic Dynamics via Finite Differences",
    "url": "http://arxiv.org/abs/2506.12029v1",
    "authors": [
      "Md Mahbub Alam",
      "Amilcar Soares",
      "Jos\u00e9 F. Rodrigues-Jr",
      "Gabriel Spadon"
    ],
    "published": "2025-05-22",
    "abstract": "Accurate vessel trajectory prediction is crucial for navigational safety, route optimization, traffic management, search and rescue operations, and autonomous navigation. Traditional data-driven models lack real-world physical constraints, leading to forecasts that disobey vessel motion dynamics, such as in scenarios with limited or noisy data where sudden course changes or speed variations occur due to external factors. To address this limitation, we propose a Physics-Informed Neural Network (PINN) approach for trajectory prediction that integrates a streamlined kinematic model for vessel motion into the neural network training process via a first- and second-order, finite difference physics-based loss function. This loss function, discretized using the first-order forward Euler method, Heun's second-order approximation, and refined with a midpoint approximation based on Taylor series expansion, enforces fidelity to fundamental physical principles by penalizing deviations from expected kinematic behavior. We evaluated PINN using real-world AIS datasets that cover diverse maritime conditions and compared it with state-of-the-art models. Our results demonstrate that the proposed method reduces average displacement errors by up to 32% across models and datasets while maintaining physical consistency. These results enhance model reliability and adherence to mission-critical maritime activities, where precision translates into better situational awareness in the oceans.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning",
    "url": "http://arxiv.org/abs/2505.12904v1",
    "authors": [
      "Hilde I. Hummel",
      "Arwin Gansekoele",
      "Sandjai Bhulai",
      "Rob van der Mei"
    ],
    "published": "2025-05-19",
    "abstract": "The increasing level of sound pollution in marine environments poses an increased threat to ocean health, making it crucial to monitor underwater noise. By monitoring this noise, the sources responsible for this pollution can be mapped. Monitoring is performed by passively listening to these sounds. This generates a large amount of data records, capturing a mix of sound sources such as ship activities and marine mammal vocalizations. Although machine learning offers a promising solution for automatic sound classification, current state-of-the-art methods implement supervised learning. This requires a large amount of high-quality labeled data that is not publicly available. In contrast, a massive amount of lower-quality unlabeled data is publicly available, offering the opportunity to explore unsupervised learning techniques. This research explores this possibility by implementing an unsupervised Contrastive Learning approach. Here, a Conformer-based encoder is optimized by the so-called Variance-Invariance-Covariance Regularization loss function on these lower-quality unlabeled data and the translation to the labeled data is made. Through classification tasks involving recognizing ship types and marine mammal vocalizations, our method demonstrates to produce robust and generalized embeddings. This shows to potential of unsupervised methods for various automatic underwater acoustic analysis tasks.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations",
    "url": "http://arxiv.org/abs/2505.09284v3",
    "authors": [
      "Panqi Chen",
      "Yifan Sun",
      "Lei Cheng",
      "Yang Yang",
      "Weichang Li",
      "Yang Liu",
      "Weiqing Liu",
      "Jiang Bian",
      "Shikai Fang"
    ],
    "published": "2025-05-14",
    "abstract": "Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations presents a fundamental challenge in scientific research. Recently, diffusion-based generative modeling shows promising potential for physical simulation. However, current approaches typically operate on on-grid data with preset spatiotemporal resolution, but struggle with the sparsely observed and continuous nature of real-world physical dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in Functional Tucker space, a novel framework that generates full-field evolution of physical dynamics from irregular sparse observations. SDIFT leverages the functional Tucker model as the latent space representer with proven universal approximation property, and represents observations as latent functions and Tucker core sequences. We then construct a sequential diffusion model with temporally augmented UNet in the functional Tucker space, denoising noise drawn from a Gaussian process to generate the sequence of core tensors.\n  At the posterior sampling stage, we propose a Message-Passing Posterior Sampling mechanism, enabling conditional generation of the entire sequence guided by observations at limited time steps. We validate SDIFT on three physical systems spanning astronomical (supernova explosions, light-year scale), environmental (ocean sound speed fields, kilometer scale), and molecular (organic liquid, millimeter scale) domains, demonstrating significant improvements in both reconstruction accuracy and computational efficiency compared to state-of-the-art approaches.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling",
    "url": "http://arxiv.org/abs/2505.05599v3",
    "authors": [
      "Seraj Al Mahmud Mostafa",
      "Chenxi Wang",
      "Jia Yue",
      "Yuta Hozumi",
      "Jianwu Wang"
    ],
    "published": "2025-05-08",
    "abstract": "Object localization in satellite imagery is particularly challenging due to the high variability of objects, low spatial resolution, and interference from noise and dominant features such as clouds and city lights. In this research, we focus on three satellite datasets: upper atmospheric Gravity Waves (GW), mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique challenges. These challenges include the variability in the scale and appearance of the main object patterns, where the size, shape, and feature extent of objects of interest can differ significantly. To address these challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed to improve object localization in these complex scenarios. YOLO-DCAP incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture multi-scale features at scale with varying dilation rates, and an Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant spatial regions, enhancing feature selection. These structural improvements help to better localize objects in satellite imagery. Experimental results demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model and state-of-the-art approaches, achieving an average improvement of 20.95% in mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively over state-of-the-art alternatives, consistently across all three satellite datasets. These consistent gains across all three satellite datasets highlight the robustness and generalizability of the proposed approach. Our code is open sourced at https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows",
    "url": "http://arxiv.org/abs/2505.05525v2",
    "authors": [
      "Selim Mecanna",
      "Aurore Loisy",
      "Christophe Eloy"
    ],
    "published": "2025-05-08",
    "abstract": "Navigating in a fluid flow while being carried by it, using only information accessible from on-board sensors, is a problem commonly faced by small planktonic organisms. It is also directly relevant to autonomous robots deployed in the oceans. In the last ten years, the fluid mechanics community has widely adopted reinforcement learning, often in the form of its simplest implementations, to address this challenge. But it is unclear how good are the strategies learned by these algorithms. In this paper, we perform a quantitative assessment of reinforcement learning methods applied to navigation in partially observable flows. We first introduce a well-posed problem of directional navigation for which a quasi-optimal policy is known analytically. We then report on the poor performance and robustness of commonly used algorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered in the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and two-dimensional turbulence. We show that they are vastly surpassed by PPO (Proximal Policy Optimization), a more advanced algorithm that has established dominance across a wide range of benchmarks in the reinforcement learning community. In particular, our custom implementation of PPO matches the theoretical quasi-optimal performance in turbulent flow and does so in a robust manner. Reaching this result required the use of several additional techniques, such as vectorized environments and generalized advantage estimation, as well as hyperparameter optimization. This study demonstrates the importance of algorithm selection, implementation details, and fine-tuning for discovering truly smart autonomous navigation strategies in complex flows.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Big Data Architecture for Large Organizations",
    "url": "http://arxiv.org/abs/2505.04717v1",
    "authors": [
      "Fathima Nuzla Ismail",
      "Abira Sengupta",
      "Shanika Amarasoma"
    ],
    "published": "2025-05-07",
    "abstract": "The exponential growth of big data has transformed how large organisations leverage information to drive innovation, optimise processes, and maintain competitive advantages. However, managing and extracting insights from vast, heterogeneous data sources requires a scalable, secure, and well-integrated big data architecture. This paper proposes a comprehensive big data framework that aligns with organisational objectives while ensuring flexibility, scalability, and governance. The architecture encompasses multiple layers, including data ingestion, transformation, storage, analytics, machine learning, and security, incorporating emerging technologies such as Generative AI (GenAI) and low-code machine learning. Cloud-based implementations across Google Cloud, AWS, and Microsoft Azure are analysed, highlighting their tools and capabilities. Additionally, this study explores advancements in big data architecture, including AI-driven automation, data mesh, and Data Ocean paradigms. By establishing a structured, adaptable framework, this research provides a foundational blueprint for large organisations to harness big data as a strategic asset effectively.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Reduced Cloud Cover Errors in a Hybrid AI-Climate Model Through Equation Discovery And Automatic Tuning",
    "url": "http://arxiv.org/abs/2505.04358v4",
    "authors": [
      "Arthur Grundner",
      "Tom Beucler",
      "Julien Savre",
      "Axel Lauer",
      "Manuel Schlund",
      "Veronika Eyring"
    ],
    "published": "2025-05-07",
    "abstract": "Cloud-related parameterizations remain a leading source of uncertainty in climate projections. Although machine learning holds promise for Earth system models (ESMs), many data-driven parameterizations lack interpretability, physical consistency, and smooth integration into ESMs. Here, a two-step method is presented to improve a climate model with data-driven parameterizations. First, we incorporate a physically consistent cloud cover parameterization -- derived from storm-resolving simulations via symbolic regression, preserving interpretability while enhancing accuracy -- into the ICON global atmospheric model. Second, we apply the gradient-free Nelder-Mead optimizer to automatically recalibrate the hybrid model against Earth observations, tuning in nested stages (2-, 7-, 30- and 365-day runs) to ensure stability and tractability. The tuned hybrid model substantially reduces long-standing biases in cloud cover -- particularly over the Southern Ocean (by 75%) and subtropical stratocumulus regions (by 44%) -- and remains robust under +4K surface warming. These results demonstrate that interpretable machine-learned parameterizations, paired with practical tuning, can efficiently and transparently strengthen ESM fidelity.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "A machine learning model for skillful climate system prediction",
    "url": "http://arxiv.org/abs/2505.06269v1",
    "authors": [
      "Chenguang Zhou",
      "Lei Chen",
      "Xiaohui Zhong",
      "Bo Lu",
      "Hao Li",
      "Libo Wu",
      "Jie Wu",
      "Jiahui Hu",
      "Zesheng Dou",
      "Pang-Chi Hsu",
      "Xiaoye Zhang"
    ],
    "published": "2025-05-06",
    "abstract": "Climate system models (CSMs), through integrating cross-sphere interactions among the atmosphere, ocean, land, and cryosphere, have emerged as pivotal tools for deciphering climate dynamics and improving forecasting capabilities. Recent breakthroughs in artificial intelligence (AI)-driven meteorological modeling have demonstrated remarkable success in single-sphere systems and partially spheres coupled systems. However, the development of a fully coupled AI-based climate system model encompassing atmosphere-ocean-land-sea ice interactions has remained an unresolved challenge. This paper introduces FengShun-CSM, an AI-based CSM model that provides 60-day global daily forecasts for 29 critical variables across atmospheric, oceanic, terrestrial, and cryospheric domains. The model significantly outperforms the European Centre for Medium-Range Weather Forecasts (ECMWF) subseasonal-to-seasonal (S2S) model in predicting most variables, particularly precipitation, land surface, and oceanic components. This enhanced capability is primarily attributed to its improved representation of intra-seasonal variability modes, most notably the Madden-Julian Oscillation (MJO). Remarkably, FengShun-CSM exhibits substantial potential in predicting subseasonal extreme events. Such breakthroughs will advance its applications in meteorological disaster mitigation, marine ecosystem conservation, and agricultural productivity enhancement. Furthermore, it validates the feasibility of developing AI-powered CSMs through machine learning technologies, establishing a transformative paradigm for next-generation Earth system modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "AI-Enhanced Automatic Design of Efficient Underwater Gliders",
    "url": "http://arxiv.org/abs/2505.00222v1",
    "authors": [
      "Peter Yichen Chen",
      "Pingchuan Ma",
      "Niklas Hagemann",
      "John Romanishin",
      "Wei Wang",
      "Daniela Rus",
      "Wojciech Matusik"
    ],
    "published": "2025-04-30",
    "abstract": "The development of novel autonomous underwater gliders has been hindered by limited shape diversity, primarily due to the reliance on traditional design tools that depend heavily on manual trial and error. Building an automated design framework is challenging due to the complexities of representing glider shapes and the high computational costs associated with modeling complex solid-fluid interactions. In this work, we introduce an AI-enhanced automated computational framework designed to overcome these limitations by enabling the creation of underwater robots with non-trivial hull shapes. Our approach involves an algorithm that co-optimizes both shape and control signals, utilizing a reduced-order geometry representation and a differentiable neural-network-based fluid surrogate model. This end-to-end design workflow facilitates rapid iteration and evaluation of hydrodynamic performance, leading to the discovery of optimal and complex hull shapes across various control settings. We validate our method through wind tunnel experiments and swimming pool gliding tests, demonstrating that our computationally designed gliders surpass manually designed counterparts in terms of energy efficiency. By addressing challenges in efficient shape representation and neural fluid surrogate models, our work paves the way for the development of highly efficient underwater gliders, with implications for long-range ocean exploration and environmental monitoring.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Digital Twin-based Out-of-Distribution Detection in Autonomous Vessels",
    "url": "http://arxiv.org/abs/2504.19816v1",
    "authors": [
      "Erblin Isaku",
      "Hassan Sartaj",
      "Shaukat Ali"
    ],
    "published": "2025-04-28",
    "abstract": "An autonomous vessel (AV) is a complex cyber-physical system (CPS) with software enabling many key functionalities, e.g., navigation software enables an AV to autonomously or semi-autonomously follow a path to its destination. Digital twins of such AVs enable advanced functionalities such as running what-if scenarios, performing predictive maintenance, and enabling fault diagnosis. Due to technological improvements, real-time analyses using continuous data from vessels' real-time operations have become increasingly possible. However, the literature has little explored developing advanced analyses in real-time data in AVs with digital twins built with machine learning techniques. To this end, we present a novel digital twin-based approach (ODDIT) to detect future out-of-distribution (OOD) states of an AV before reaching them, enabling proactive intervention. Such states may indicate anomalies requiring attention (e.g., manual correction by the ship master) and assist testers in scenario-centered testing. The digital twin consists of two machine-learning models predicting future vessel states and whether the predicted state will be OOD. We evaluated ODDIT with five vessels across waypoint and zigzag maneuvering under simulated conditions, including sensor and actuator noise and environmental disturbances i.e., ocean current. ODDIT achieved high accuracy in detecting OOD states, with AUROC and TNR@TPR95 scores reaching 99\\% across multiple vessels.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Unveiling 3D Ocean Biogeochemical Provinces in the North Atlantic: A Systematic Comparison and Validation of Clustering Methods",
    "url": "http://arxiv.org/abs/2504.18181v2",
    "authors": [
      "Yvonne Jenniges",
      "Maike Sonnewald",
      "Sebastian Maneth",
      "Are Olsen",
      "Boris P. Koch"
    ],
    "published": "2025-04-25",
    "abstract": "Defining ocean regions and water masses helps to understand marine processes and can serve downstream tasks such as defining marine protected areas. However, such definitions often result from subjective decisions potentially producing misleading, unreproducible outcomes. Here, the aim was to objectively define regions of the North Atlantic through systematic comparison of clustering methods within the Native Emergent Manifold Interrogation (NEMI) framework (Sonnewald, 2023). About 300 million measured salinity, temperature, and oxygen, nitrate, phosphate and silicate concentration values served as input for various clustering methods (k-Means, agglomerative Ward, and Density-Based Spatial Clustering of Applications with Noise (DBSCAN)). Uniform Manifold Approximation and Projection (UMAP) emphasised (dis-)similarities in the data while reducing dimensionality. Based on systematic validation of clustering methods and their hyperparameters using internal, external and relative validation techniques, results showed that UMAP-DBSCAN best represented the data. Strikingly, internal validation metrics proved systematically unreliable for comparing clustering methods. To address stochastic variability, 100 UMAP-DBSCAN clustering runs were conducted and aggregated following NEMI, yielding a final set of 321 clusters. Reproducibility was evaluated via ensemble overlap ($88.81\\pm1.8\\%$) and mean grid cell-wise uncertainty ($15.49\\pm20\\%$). Case studies of the Mediterranean Sea, deep Atlantic waters and Labrador Sea showed strong agreement with common water mass definitions. This study revealed a more detailed regionalisation compared to previous concepts such as the Longhurst provinces through systematic clustering method comparison. The applied method is objective, efficient and reproducible and will support future research on biogeochemical differences and changes in oceanic regions.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Improving Significant Wave Height Prediction Using Chronos Models",
    "url": "http://arxiv.org/abs/2504.16834v2",
    "authors": [
      "Yilin Zhai",
      "Hongyuan Shi",
      "Chao Zhan",
      "Qing Wang",
      "Zaijin You",
      "Nan Wang"
    ],
    "published": "2025-04-23",
    "abstract": "Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Recognition",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Fourier analysis of the physics of transfer learning for data-driven subgrid-scale models of ocean turbulence",
    "url": "http://arxiv.org/abs/2504.15487v1",
    "authors": [
      "Moein Darman",
      "Pedram Hassanzadeh",
      "Laure Zanna",
      "Ashesh Chattopadhyay"
    ],
    "published": "2025-04-21",
    "abstract": "Transfer learning (TL) is a powerful tool for enhancing the performance of neural networks (NNs) in applications such as weather and climate prediction and turbulence modeling. TL enables models to generalize to out-of-distribution data with minimal training data from the new system. In this study, we employ a 9-layer convolutional NN to predict the subgrid forcing in a two-layer ocean quasi-geostrophic system and examine which metrics best describe its performance and generalizability to unseen dynamical regimes. Fourier analysis of the NN kernels reveals that they learn low-pass, Gabor, and high-pass filters, regardless of whether the training data are isotropic or anisotropic. By analyzing the activation spectra, we identify why NNs fail to generalize without TL and how TL can overcome these limitations: the learned weights and biases from one dataset underestimate the out-of-distribution sample spectra as they pass through the network, leading to an underestimation of output spectra. By re-training only one layer with data from the target system, this underestimation is corrected, enabling the NN to produce predictions that match the target spectra. These findings are broadly applicable to data-driven parameterization of dynamical systems.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "How to systematically develop an effective AI-based bias correction model?",
    "url": "http://arxiv.org/abs/2504.15322v1",
    "authors": [
      "Xiao Zhou",
      "Yuze Sun",
      "Jie Wu",
      "Xiaomeng Huang"
    ],
    "published": "2025-04-21",
    "abstract": "This study introduces ReSA-ConvLSTM, an artificial intelligence (AI) framework for systematic bias correction in numerical weather prediction (NWP). We propose three innovations by integrating dynamic climatological normalization, ConvLSTM with temporal causality constraints, and residual self-attention mechanisms. The model establishes a physics-aware nonlinear mapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years (1981-2021) of global atmospheric data, the framework reduces systematic biases in 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure (SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to operational ECMWF outputs. The lightweight architecture (10.6M parameters) enables efficient generalization to multiple variables and downstream applications, reducing retraining time by 85% for cross-variable correction while improving ocean model skill through bias-corrected boundary conditions. The ablation experiments demonstrate that our innovations significantly improve the model's correction performance, suggesting that incorporating variable characteristics into the model helps enhance forecasting skills.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions",
    "url": "http://arxiv.org/abs/2504.13476v1",
    "authors": [
      "Jiadong Lou",
      "Bingqing Liu",
      "Yuanheng Xiong",
      "Xiaodong Zhang",
      "Xu Yuan"
    ],
    "published": "2025-04-18",
    "abstract": "Phytoplankton absorb and scatter light in unique ways, subtly altering the color of water, changes that are often minor for human eyes to detect but can be captured by sensitive ocean color instruments onboard satellites from space. Hyperspectral sensors, paired with advanced algorithms, are expected to significantly enhance the characterization of phytoplankton community composition, especially in coastal waters where ocean color remote sensing applications have historically encountered significant challenges. This study presents novel machine learning-based solutions for NASA's hyperspectral missions, including EMIT and PACE, tackling high-fidelity retrievals of phytoplankton absorption coefficient and chlorophyll a from their hyperspectral remote sensing reflectance. Given that a single Rrs spectrum may correspond to varied combinations of inherent optical properties and associated concentrations, the Variational Autoencoder (VAE) is used as a backbone in this study to handle such multi-distribution prediction problems. We first time tailor the VAE model with innovative designs to achieve hyperspectral retrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex estuarine-coastal waters. Validation with extensive experimental observation demonstrates superior performance of the VAE models with high precision and low bias. The in-depth analysis of VAE's advanced model structures and learning designs highlights the improvement and advantages of VAE-based solutions over the mixture density network (MDN) approach, particularly on high-dimensional data, such as PACE. Our study provides strong evidence that current EMIT and PACE hyperspectral data as well as the upcoming Surface Biology Geology mission will open new pathways toward a better understanding of phytoplankton community dynamics in aquatic ecosystems when integrated with AI technologies.",
    "categories": [
      "ocean",
      "fish_plankton"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Trend Filtered Mixture of Experts for Automated Gating of High-Frequency Flow Cytometry Data",
    "url": "http://arxiv.org/abs/2504.12287v1",
    "authors": [
      "Sangwon Hyun",
      "Tim Coleman",
      "Francois Ribalet",
      "Jacob Bien"
    ],
    "published": "2025-04-16",
    "abstract": "Ocean microbes are critical to both ocean ecosystems and the global climate. Flow cytometry, which measures cell optical properties in fluid samples, is routinely used in oceanographic research. Despite decades of accumulated data, identifying key microbial populations (a process known as ``gating'') remains a significant analytical challenge. To address this, we focus on gating multidimensional, high-frequency flow cytometry data collected {\\it continuously} on board oceanographic research vessels, capturing time- and space-wise variations in the dynamic ocean. Our paper proposes a novel mixture-of-experts model in which both the gating function and the experts are given by trend filtering. The model leverages two key assumptions: (1) Each snapshot of flow cytometry data is a mixture of multivariate Gaussians and (2) the parameters of these Gaussians vary smoothly over time. Our method uses regularization and a constraint to ensure smoothness and that cluster means match biologically distinct microbe types. We demonstrate, using flow cytometry data from the North Pacific Ocean, that our proposed model accurately matches human-annotated gating and corrects significant errors.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Stochastic generative methods for stable and accurate closure modeling of chaotic dynamical systems",
    "url": "http://arxiv.org/abs/2504.09750v1",
    "authors": [
      "Emily Williams",
      "David Darmofal"
    ],
    "published": "2025-04-13",
    "abstract": "Traditional deterministic subgrid-scale (SGS) models are often dissipative and unstable, especially in regions of chaotic and turbulent flow. Ongoing work in climate science and ocean modeling motivates the use of stochastic SGS models for chaotic dynamics. Further, developing stochastic generative models of underlying dynamics is a rapidly expanding field. In this work, we aim to incorporate stochastic integration toward closure modeling for chaotic dynamical systems. Further, we want to explore the potential stabilizing effect that stochastic models could have on linearized chaotic systems. We propose parametric and generative approaches for closure modeling using stochastic differential equations (SDEs). We derive and implement a quadratic diffusion model based on the fluctuations, demonstrating increased accuracy from bridging theoretical models with generative approaches. Results are demonstrated on the Lorenz-63 dynamical system.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Low latency global carbon budget reveals a continuous decline of the land carbon sink during the 2023/24 El Nino event",
    "url": "http://arxiv.org/abs/2504.09189v1",
    "authors": [
      "Piyu Ke",
      "Philippe Ciais",
      "Yitong Yao",
      "Stephen Sitch",
      "Wei Li",
      "Yidi Xu",
      "Xiaomeng Du",
      "Xiaofan Gui",
      "Ana Bastos",
      "Sonke Zaehle",
      "Ben Poulter",
      "Thomas Colligan",
      "Auke M. van der Woude",
      "Wouter Peters",
      "Zhu Liu",
      "Zhe Jin",
      "Xiangjun Tian",
      "Yilong Wang",
      "Junjie Liu",
      "Sudhanshu Pandey",
      "Chris O'Dell",
      "Jiang Bian",
      "Chuanlong Zhou",
      "John Miller",
      "Xin Lan",
      "Michael O'Sullivan",
      "Pierre Friedlingstein",
      "Guido R. van der Werf",
      "Glen P. Peters",
      "Shilong Piao",
      "Frederic Chevallier"
    ],
    "published": "2025-04-12",
    "abstract": "The high growth rate of atmospheric CO2 in 2023 was found to be caused by a severe reduction of the global net land carbon sink. Here we update the global CO2 budget from January 1st to July 1st 2024, during which El Ni\u00f1o drought conditions continued to prevail in the Tropics but ceased by March 2024. We used three dynamic global vegetation models (DGVMs), machine learning emulators of ocean models, three atmospheric inversions driven by observations from the second Orbiting Carbon Observatory (OCO-2) satellite, and near-real-time fossil CO2 emissions estimates. In a one-year period from July 2023 to July 2024 covering the El Ni\u00f1o 2023/24 event, we found a record-high CO2 growth rate of 3.66~$\\pm$~0.09 ppm~yr$^{-1}$ ($\\pm$~1 standard deviation) since 1979. Yet, the CO2 growth rate anomaly obtained after removing the long term trend is 1.1 ppm~yr$^{-1}$, which is marginally smaller than the July--July growth rate anomalies of the two major previous El Ni\u00f1o events in 1997/98 and 2015/16. The atmospheric CO2 growth rate anomaly was primarily driven by a 2.24 GtC~yr$^{-1}$ reduction in the net land sink including 0.3 GtC~yr$^{-1}$ of fire emissions, partly offset by a 0.38 GtC~yr$^{-1}$ increase in the ocean sink relative to the 2015--2022 July--July mean. The tropics accounted for 97.5\\% of the land CO2 flux anomaly, led by the Amazon (50.6\\%), central Africa (34\\%), and Southeast Asia (8.2\\%), with extra-tropical sources in South Africa and southern Brazil during April--July 2024. Our three DGVMs suggest greater tropical CO2 losses in 2023/2024 than during the two previous large El Ni\u00f1o in 1997/98 and 2015/16, whereas inversions indicate losses more comparable to 2015/16. Overall, this update of the low latency budget highlights the impact of recent El Ni\u00f1o droughts in explaining the high CO2 growth rate until July 2024.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "KunPeng: A Global Ocean Environmental Model",
    "url": "http://arxiv.org/abs/2504.04766v1",
    "authors": [
      "Yi Zhao",
      "Jiaqi Li",
      "Haitao Xia",
      "Tianjiao Zhang",
      "Zerong Zeng",
      "Tianyu Ren",
      "Yucheng Zhang",
      "Chao Zhu",
      "Shengtong Xu",
      "Hongchun Yuan"
    ],
    "published": "2025-04-07",
    "abstract": "Inspired by the similarity of the atmosphere-ocean physical coupling mechanism, this study innovatively migrates meteorological large-model techniques to the ocean domain, constructing the KunPeng global ocean environmental prediction model. Aimed at the discontinuous characteristics of marine space, we propose a terrain-adaptive mask constraint mechanism to mitigate effectively training divergence caused by abrupt gradients at land-sea boundaries. To fully integrate far-, medium-, and close-range marine features, a longitude-cyclic deformable convolution network (LC-DCN) is employed to enhance the dynamic receptive field, achieving refined modeling of multi-scale oceanic characteristics. A Deformable Convolution-enhanced Multi-Step Prediction module (DC-MTP) is employed to strengthen temporal dependency feature extraction capabilities. Experimental results demonstrate that this model achieves an average ACC of 0.80 in 15-day global predictions at 0.25$^\\circ$ resolution, outperforming comparative models by 0.01-0.08. The average mean squared error (MSE) is 0.41 (representing a 5%-31% reduction) and the average mean absolute error (MAE) is 0.44 (0.6%-21% reduction) compared to other models. Significant improvements are particularly observed in sea surface parameter prediction, deep-sea region characterization, and current velocity field forecasting. Through a horizontal comparison of the applicability of operators at different scales in the marine domain, this study reveals that local operators significantly outperform global operators under slow-varying oceanic processes, demonstrating the effectiveness of dynamic feature pyramid representations in predicting marine physical parameters.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "ConfEviSurrogate: A Conformalized Evidential Surrogate Model for Uncertainty Quantification",
    "url": "http://arxiv.org/abs/2504.02919v1",
    "authors": [
      "Yuhan Duan",
      "Xin Zhao",
      "Neng Shi",
      "Han-Wei Shen"
    ],
    "published": "2025-04-03",
    "abstract": "Surrogate models, crucial for approximating complex simulation data across sciences, inherently carry uncertainties that range from simulation noise to model prediction errors. Without rigorous uncertainty quantification, predictions become unreliable and hence hinder analysis. While methods like Monte Carlo dropout and ensemble models exist, they are often costly, fail to isolate uncertainty types, and lack guaranteed coverage in prediction intervals. To address this, we introduce ConfEviSurrogate, a novel Conformalized Evidential Surrogate Model that can efficiently learn high-order evidential distributions, directly predict simulation outcomes, separate uncertainty sources, and provide prediction intervals. A conformal prediction-based calibration step further enhances interval reliability to ensure coverage and improve efficiency. Our ConfEviSurrogate demonstrates accurate predictions and robust uncertainty estimates in diverse simulations, including cosmology, ocean dynamics, and fluid dynamics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Generalizable Implicit Neural Representations via Parameterized Latent Dynamics for Baroclinic Ocean Forecasting",
    "url": "http://arxiv.org/abs/2503.21588v1",
    "authors": [
      "Guang Zhao",
      "Xihaier Luo",
      "Seungjun Lee",
      "Yihui Ren",
      "Shinjae Yoo",
      "Luke Van Roekel",
      "Balu Nadiga",
      "Sri Hari Krishna Narayanan",
      "Yixuan Sun",
      "Wei Xu"
    ],
    "published": "2025-03-27",
    "abstract": "Mesoscale ocean dynamics play a critical role in climate systems, governing heat transport, hurricane genesis, and drought patterns. However, simulating these processes at high resolution remains computationally prohibitive due to their nonlinear, multiscale nature and vast spatiotemporal domains. Implicit neural representations (INRs) reduce the computational costs as resolution-independent surrogates but fail in many-query scenarios (inverse modeling) requiring rapid evaluations across diverse parameters. We present PINROD, a novel framework combining dynamics-aware implicit neural representations with parameterized neural ordinary differential equations to address these limitations. By integrating parametric dependencies into latent dynamics, our method efficiently captures nonlinear oceanic behavior across varying boundary conditions and physical parameters. Experiments on ocean mesoscale activity data show superior accuracy over existing baselines and improved computational efficiency compared to standard numerical simulations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Regularization of ML models for Earth systems by using longer model timesteps",
    "url": "http://arxiv.org/abs/2503.18023v1",
    "authors": [
      "Raghul Parthipan",
      "Mohit Anand",
      "Hannah M Christensen",
      "Frederic Vitart",
      "Damon J Wischik",
      "Jakob Zscheischler"
    ],
    "published": "2025-03-23",
    "abstract": "Regularization is a technique to improve generalization of machine learning (ML) models. A common form of regularization in the ML literature is to train on data where similar inputs map to different outputs. This improves generalization by preventing ML models from becoming overconfident in their predictions. This paper shows how using longer timesteps when modelling chaotic Earth systems naturally leads to more of this regularization. We show this in two domains. We explain how using longer model timesteps can improve results and demonstrate that increased regularization is one of the causes. We explain why longer model timesteps lead to improved regularization in these systems and present a procedure to pick the model timestep. We also carry out a benchmarking exercise on ORAS5 ocean reanalysis data to show that a longer model timestep (28 days) than is typically used gives realistic simulations. We suggest that there will be many opportunities to use this type of regularization in Earth system problems because the Earth system is chaotic and the regularization is so easy to implement.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Seismotectonics and Slip Behavior of a Submarine Plate Boundary Fault from Seismicity Repeaters and Tomography using a high-resolution earthquake catalog from machine learning",
    "url": "http://arxiv.org/abs/2503.17148v1",
    "authors": [
      "D. Lange",
      "Y. Ren",
      "I. Grevemeyer"
    ],
    "published": "2025-03-21",
    "abstract": "The Blanco transform fault system (BTFS) is highly segmented and represents an evolving transform plate boundary in the Northeast Pacific Ocean. Its seismic behavior was captured with a dense network of 54 ocean-bottom-seismometers operated for one year. We created a high-resolution earthquake catalog based on different machine learning onset pickers, resulting in a high-resolution seismicity catalog with 12.708 events outlining the current deformation and stress release along a major transform fault. Seismicity reveals lateral changes of seismic behavior, indicating seismic and aseismic fault patches or segments, complex along-strike and off-axis deformation, step-overs, and internal faulting within pull-apart basins. Seismicity along simple linear fault strands is localized within 2 km of the seafloor expression of the fault. Repeaters indicate an average of 21 cm slip, exceeding the geological slip rate by ~4 times. Based on the repeater behavior, we suggest that the (overall aseismic) slip is spatially very heterogeneous, consisting of many small seismic patches, each one releasing its seismic slip every 4 years. Along the BTFS, the coupling of the fault is variable and varies between fully locked and fully creeping. Local earthquake tomography shows elevated vp/vs values exceeding 2, suggesting significant serpentinization from seawater entering the transform faults, the oceanic crust and mantle. The study shows how to use modern machine learning pickers on OBS data to provide essential insights into the physics of faulting along major plate boundary faults in time and space, including the partitioning of slip seismic and aseismic faulting with high resolution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Informative Path Planning to Explore and Map Unknown Planetary Surfaces with Gaussian Processes",
    "url": "http://arxiv.org/abs/2503.16613v1",
    "authors": [
      "Ashten Akemoto",
      "Frances Zhu"
    ],
    "published": "2025-03-20",
    "abstract": "Many environments, such as unvisited planetary surfaces and oceanic regions, remain unexplored due to a lack of prior knowledge. Autonomous vehicles must sample upon arrival, process data, and either transmit findings to a teleoperator or decide where to explore next. Teleoperation is suboptimal, as human intuition lacks mathematical guarantees for optimality. This study evaluates an informative path planning algorithm for mapping a scalar variable distribution while minimizing travel distance and ensuring model convergence. We compare traditional open loop coverage methods (e.g., Boustrophedon, Spiral) with information-theoretic approaches using Gaussian processes, which update models iteratively with confidence metrics. The algorithm's performance is tested on three surfaces, a parabola, Townsend function, and lunar crater hydration map, to assess noise, convexity, and function behavior. Results demonstrate that information-driven methods significantly outperform naive exploration in reducing model error and travel distance while improving convergence potential.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Data-Driven Probabilistic Air-Sea Flux Parameterization",
    "url": "http://arxiv.org/abs/2503.03990v2",
    "authors": [
      "Jiarong Wu",
      "Pavel Perezhogin",
      "David John Gagne",
      "Brandon Reichl",
      "Aneesh C. Subramanian",
      "Elizabeth Thompson",
      "Laure Zanna"
    ],
    "published": "2025-03-06",
    "abstract": "Accurately quantifying air-sea fluxes is important for understanding air-sea interactions and improving coupled weather and climate systems. This study introduces a probabilistic framework to represent the highly variable nature of air-sea fluxes, which is missing in deterministic bulk algorithms. Assuming Gaussian distributions conditioned on the input variables, we use artificial neural networks and eddy-covariance measurement data to estimate the mean and variance by minimizing negative log-likelihood loss. The trained neural networks provide alternative mean flux estimates to existing bulk algorithms, and quantify the uncertainty around the mean estimates. Stochastic parameterization of air-sea turbulent fluxes can be constructed by sampling from the predicted distributions. Tests in a single-column forced upper-ocean model suggest that changes in flux algorithms influence sea surface temperature and mixed layer depth seasonally. The ensemble spread in stochastic runs is most pronounced during spring restratification.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "An ocean front detection and tracking algorithm",
    "url": "http://arxiv.org/abs/2502.15250v4",
    "authors": [
      "Yishuo Wang",
      "Feng Zhou",
      "Qicheng Meng",
      "Muping Zhou",
      "Zhijun Hu",
      "Chengqing Zhang",
      "Tianhao Zhao"
    ],
    "published": "2025-02-21",
    "abstract": "Existing ocean front detection methods--including histogram-based variance analysis, Lyapunov exponent, gradient thresholding, and machine learning--suffer from critical limitations: discontinuous outputs, over-detection, reliance on single-threshold decisions, and lack of open-source implementations. To address these challenges, this paper proposes the Bayesian Front Detection and Tracking framework with Metric Space Analysis (BFDT-MSA). The framework introduces three innovations: (1) a Bayesian decision mechanism that integrates gradient priors and field operators to eliminate manual threshold sensitivity; (2) morphological refinement algorithms for merging fragmented fronts, deleting spurious rings, and thinning frontal zones to pixel-level accuracy; and (3) a novel metric space definition for temporal front tracking, enabling systematic analysis of front evolution. Validated on global SST data (2022--2024), BFDT-MSA reduces over-detection by $73\\%$ compared to histogram-based methods while achieving superior intensity ($0.16^\\circ$C/km), continuity, and spatiotemporal coherence. The open-source release bridges a critical gap in reproducible oceanographic research.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "A Study on Monthly Marine Heatwave Forecasts in New Zealand: An Investigation of Imbalanced Regression Loss Functions with Neural Network Models",
    "url": "http://arxiv.org/abs/2502.13495v1",
    "authors": [
      "Ding Ning",
      "Varvara Vetrova",
      "S\u00e9bastien Delaux",
      "Rachael Tappenden",
      "Karin R. Bryan",
      "Yun Sing Koh"
    ],
    "published": "2025-02-19",
    "abstract": "Marine heatwaves (MHWs) are extreme ocean-temperature events with significant impacts on marine ecosystems and related industries. Accurate forecasts (one to six months ahead) of MHWs would aid in mitigating these impacts. However, forecasting MHWs presents a challenging imbalanced regression task due to the rarity of extreme temperature anomalies in comparison to more frequent moderate conditions. In this study, we examine monthly MHW forecasts for 12 locations around New Zealand. We use a fully-connected neural network and compare standard and specialized regression loss functions, including the mean squared error (MSE), the mean absolute error (MAE), the Huber, the weighted MSE, the focal-R, the balanced MSE, and a proposed scaling-weighted MSE. Results show that (i) short lead times (one month) are considerably more predictable than three- and six-month leads, (ii) models trained with the standard MSE or MAE losses excel at forecasting average conditions but struggle to capture extremes, and (iii) specialized loss functions such as the balanced MSE and our scaling-weighted MSE substantially improve forecasting of MHW and suspected MHW events. These findings underscore the importance of tailored loss functions for imbalanced regression, particularly in forecasting rare but impactful events such as MHWs.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Uncertainty-permitting machine learning reveals sources of dynamic sea level predictability across daily-to-seasonal timescales",
    "url": "http://arxiv.org/abs/2502.11293v2",
    "authors": [
      "Andrew Brettin",
      "Laure Zanna",
      "Elizabeth A. Barnes"
    ],
    "published": "2025-02-16",
    "abstract": "Reliable dynamic sea level forecasts are hindered by numerous sources of uncertainty on daily-to-seasonal timescales (1-180 days) due to atmospheric boundary conditions and internal ocean variability. Studies have demonstrated that certain initial states can extend predictability horizons; thus, identifying these initial conditions may help improve forecast skill. Here, we identify sources of dynamic sea level predictability on daily-to-seasonal timescales using neural networks trained on CESM2 large ensemble data to forecast dynamic sea level. The forecasts yield not only a point estimate for sea level but also a standard deviation to quantify forecast uncertainty based on the initial conditions. Forecasted uncertainties can be leveraged to identify state-dependent sources of predictability at most locations and forecast leads. Network forecasts, particularly in the low-latitude Indo-Pacific, exhibit skillful deterministic predictions and skillfully forecast exceedance probabilities relative to local linear baselines. For networks trained at Guam and in the western Indian Ocean, the transfer of sources of predictability from local sources to remote sources is presented by the deteriorating utility of initial condition information for predicting exceedance events. Propagating Rossby waves are identified as a potential source of predictability for dynamic sea level at Guam. In the Indian Ocean, persistence of thermosteric sea level anomalies from the Indian Ocean Dipole may be a source of predictability on subseasonal timescales, but El Ni\u00f1o drives predictability on seasonal timescales. This work shows how uncertainty-quantifying machine learning can help identify changes in sources of state-dependent predictability over a range of forecast leads.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Coordinated control of multiple autonomous surface vehicles: challenges and advances -- a systematic review",
    "url": "http://arxiv.org/abs/2502.10080v1",
    "authors": [
      "Manuel Gantiva Osorioa",
      "Carmelina Ierardia",
      "Isabel Jurado Floresa",
      "Mario Pereira Mart\u00edna",
      "Pablo Mill\u00e1n Gata"
    ],
    "published": "2025-02-14",
    "abstract": "The increasing use and implementation of Autonomous Surface Vessels (ASVs) for various activities in maritime environments is expected to drive a rise in developments and research on their control. Particularly, the coordination of multiple ASVs presents novel challenges and opportunities, requiring interdisciplinary research efforts at the intersection of robotics, control theory, communication systems, and marine sciences. The wide variety of missions or objectives for which these vessels can be collectively used allows for the application and combination of different control techniques. This includes the exploration of machine learning to consider aspects previously deemed infeasible. This review provides a comprehensive exploration of coordinated ASV control while addressing critical gaps left by previous reviews. Unlike previous works, we adopt a systematic approach to ensure integrity and minimize bias in article selection. We delve into the complex world of sub-actuated ASVs with a focus on customized control strategies and the integration of machine learning techniques for increased autonomy. By synthesizing recent advances and identifying emerging trends, we offer insights that drive this field forward, providing both a comprehensive overview of state-of-the-art techniques and guidance for future research efforts.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "CO2 Hydration at the Air-Water Interface: A Surface-Mediated 'In and Out' Mechanism",
    "url": "http://arxiv.org/abs/2502.08348v2",
    "authors": [
      "Samuel G. H. Brookes",
      "Venkat Kapil",
      "Angelos Michaelides",
      "Christoph Schran"
    ],
    "published": "2025-02-12",
    "abstract": "An understanding of the CO$_2$ + H$_2$O hydration reaction is crucial for modeling the effects of ocean acidification, for enabling novel carbon storage solutions, and as a model process in the geosciences. While the mechanism of this reaction has been investigated extensively in the condensed phase, its mechanism at the air-water interface remains elusive, leaving uncertain the contribution that surface-adsorbed CO$_2$ makes to the overall acidification reaction. In this study, we employ machine-learned potentials trained to various levels of theory to provide a molecular-level understanding of CO$_2$ hydration at the air-water interface. We show that reaction at the interface follows a surface-mediated `In and Out' mechanism: CO$_2$ diffuses into the aqueous surface layer, reacts to form carbonic acid, and is subsequently expelled from solution. We show that this surface layer provides a bulk-like solvation environment, engendering similar modes of reactivity and near-identical free energy profiles for the bulk and interfacial processes. Our study unveils a new, unconventional reaction mechanism that underscores the dynamic nature of the molecular reaction site at the air-water interface. The similarity between bulk and interfacial profiles shows that CO$_2$ hydration is equally as feasible under these two solvation environments and that acidification rates are likely enhanced by this additional surface contribution.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A physics-based data-driven model for CO$_2$ gas diffusion electrodes to drive automated laboratories",
    "url": "http://arxiv.org/abs/2502.06323v1",
    "authors": [
      "Ivan Grega",
      "F\u00e9lix Therrien",
      "Abhishek Soni",
      "Karry Ocean",
      "Kevan Dettelbach",
      "Ribwar Ahmadi",
      "Mehrdad Mokhtari",
      "Curtis P. Berlinguette",
      "Yoshua Bengio"
    ],
    "published": "2025-02-10",
    "abstract": "The electrochemical reduction of atmospheric CO$_2$ into high-energy molecules with renewable energy is a promising avenue for energy storage that can take advantage of existing infrastructure especially in areas where sustainable alternatives to fossil fuels do not exist. Automated laboratories are currently being developed and used to optimize the composition and operating conditions of gas diffusion electrodes (GDEs), the device in which this reaction takes place. Improving the efficiency of GDEs is crucial for this technology to become viable. Here we present a modeling framework to efficiently explore the high-dimensional parameter space of GDE designs in an active learning context. At the core of the framework is an uncertainty-aware physics model calibrated with experimental data. The model has the flexibility to capture various input parameter spaces and any carbon products which can be modeled with Tafel kinetics. It is interpretable, and a Gaussian process layer can capture deviations of real data from the function space of the physical model itself. We deploy the model in a simulated active learning setup with real electrochemical data gathered by the AdaCarbon automated laboratory and show that it can be used to efficiently traverse the multi-dimensional parameter space.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment",
    "url": "http://arxiv.org/abs/2502.05409v1",
    "authors": [
      "Maneesha Wickramasuriya",
      "Beomyeol Yu",
      "Taeyoung Lee",
      "Murray Snyder"
    ],
    "published": "2025-02-08",
    "abstract": "This paper proposes a vision-in-the-loop simulation environment for deep monocular pose estimation of a UAV operating in an ocean environment. Recently, a deep neural network with a transformer architecture has been successfully trained to estimate the pose of a UAV relative to the flight deck of a research vessel, overcoming several limitations of GPS-based approaches. However, validating the deep pose estimation scheme in an actual ocean environment poses significant challenges due to the limited availability of research vessels and the associated operational costs. To address these issues, we present a photo-realistic 3D virtual environment leveraging recent advancements in Gaussian splatting, a novel technique that represents 3D scenes by modeling image pixels as Gaussian distributions in 3D space, creating a lightweight and high-quality visual model from multiple viewpoints. This approach enables the creation of a virtual environment integrating multiple real-world images collected in situ. The resulting simulation enables the indoor testing of flight maneuvers while verifying all aspects of flight software, hardware, and the deep monocular pose estimation scheme. This approach provides a cost-effective solution for testing and validating the autonomous flight of shipboard UAVs, specifically focusing on vision-based control and estimation algorithms.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Transformer",
      "Deep Neural Network"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Capturing Extreme Events in Turbulence using an Extreme Variational Autoencoder (xVAE)",
    "url": "http://arxiv.org/abs/2502.04685v1",
    "authors": [
      "Likun Zhang",
      "Kiran Bhaganagar",
      "Christopher K. Wikle"
    ],
    "published": "2025-02-07",
    "abstract": "Turbulent flow fields are characterized by extreme events that are statistically intermittent and carry a significant amount of energy and physical importance. To emulate these flows, we introduce the extreme variational Autoencoder (xVAE), which embeds a max-infinitely divisible process with heavy-tailed distributions into a standard VAE framework, enabling accurate modeling of extreme events. xVAEs are neural network models that reduce system dimensionality by learning non-linear latent representations of data. We demonstrate the effectiveness of xVAE in large-eddy simulation data of wildland fire plumes, where intense heat release and complex plume-atmosphere interactions generate extreme turbulence. Comparisons with the commonly used Proper Orthogonal Decomposition (POD) modes show that xVAE is more robust in capturing extreme values and provides a powerful uncertainty quantification framework using variational Bayes. Additionally, xVAE enables analysis of the so-called copulas of fields to assess risks associated with rare events while rigorously accounting for uncertainty, such as simultaneous exceedances of high thresholds across multiple locations. The proposed approach provides a new direction for studying realistic turbulent flows, such as high-speed aerodynamics, space propulsion, and atmospheric and oceanic systems that are characterized by extreme events.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Wake-Informed 3D Path Planning for Autonomous Underwater Vehicles Using A* and Neural Network Approximations",
    "url": "http://arxiv.org/abs/2502.01918v2",
    "authors": [
      "Zachary Cooper-Baldock",
      "Stephen Turnock",
      "Karl Sammut"
    ],
    "published": "2025-02-04",
    "abstract": "Autonomous Underwater Vehicles (AUVs) encounter significant energy, control and navigation challenges in complex underwater environments, particularly during close-proximity operations, such as launch and recovery (LAR), where fluid interactions and wake effects present additional navigational and energy challenges. Traditional path planning methods fail to incorporate these detailed wake structures, resulting in increased energy consumption, reduced control stability, and heightened safety risks. This paper presents a novel wake-informed, 3D path planning approach that fully integrates localized wake effects and global currents into the planning algorithm. Two variants of the A* algorithm - a current-informed planner and a wake-informed planner - are created to assess its validity and two neural network models are then trained to approximate these planners for real-time applications. Both the A* planners and NN models are evaluated using important metrics such as energy expenditure, path length, and encounters with high-velocity and turbulent regions. The results demonstrate a wake-informed A* planner consistently achieves the lowest energy expenditure and minimizes encounters with high-velocity regions, reducing energy consumption by up to 11.3%. The neural network models are observed to offer computational speedup of 6 orders of magnitude, but exhibit 4.51 - 19.79% higher energy expenditures and 9.81 - 24.38% less optimal paths. These findings underscore the importance of incorporating detailed wake structures into traditional path planning algorithms and the benefits of neural network approximations to enhance energy efficiency and operational safety for AUVs in complex 3D domains.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Statistical Learning Approach to Mediterranean Cyclones",
    "url": "http://arxiv.org/abs/2501.15694v1",
    "authors": [
      "L. Roveri",
      "L. Fery",
      "L. Cavicchia",
      "F. Grotto"
    ],
    "published": "2025-01-26",
    "abstract": "Mediterranean cyclones are extreme meteorological events of which much less is known compared to their tropical, oceanic counterparts. The raising interest in such phenomena is due to their impact on a region increasingly more affected by climate change, but a precise characterization remains a non trivial task. In this work we showcase how a Bayesian algorithm (Latent Dirichlet Allocation) can classify Mediterranean cyclones relying on wind velocity data, leading to a drastic dimensional reduction that allows the use of supervised statistical learning techniques for detecting and tracking new cyclones.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Physics-informed neural networks for phase-resolved data assimilation and prediction of nonlinear ocean waves",
    "url": "http://arxiv.org/abs/2501.08430v1",
    "authors": [
      "Svenja Ehlers",
      "Norbert Hoffmann",
      "Tianning Tang",
      "Adrian H. Callaghan",
      "Rui Cao",
      "Enrique M. Padilla",
      "Yuxin Fang",
      "Merten Stender"
    ],
    "published": "2025-01-14",
    "abstract": "The assimilation and prediction of phase-resolved surface gravity waves are critical challenges in ocean science and engineering. Potential flow theory (PFT) has been widely employed to develop wave models and numerical techniques for wave prediction. However, traditional wave prediction methods are often limited. For example, most simplified wave models have a limited ability to capture strong wave nonlinearity, while fully nonlinear PFT solvers often fail to meet the speed requirements of engineering applications. This computational inefficiency also hinders the development of effective data assimilation techniques, which are required to reconstruct spatial wave information from sparse measurements to initialize the wave prediction. To address these challenges, we propose a novel solver method that leverages physics-informed neural networks (PINNs) that parameterize PFT solutions as neural networks. This provides a computationally inexpensive way to assimilate and predict wave data. The proposed PINN framework is validated through comparisons with analytical linear PFT solutions and experimental data collected in a laboratory wave flume. The results demonstrate that our approach accurately captures and predicts irregular, nonlinear, and dispersive wave surface dynamics. Moreover, the PINN can infer the fully nonlinear velocity potential throughout the entire fluid volume solely from surface elevation measurements, enabling the calculation of fluid velocities that are difficult to measure experimentally.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Predicting the dynamics of a gas pocket during breaking wave impacts using machine learning",
    "url": "http://arxiv.org/abs/2501.03641v2",
    "authors": [
      "Rodrigo Ezeta",
      "Bulent D\u00fcz"
    ],
    "published": "2025-01-07",
    "abstract": "We investigate the feasibility and accuracy of a machine learning model to predict the dynamics of a gas pocket that is formed when a breaking wave impacts on a solid wall. The proposed ML model is based on the convolutional long short-term memory structure and is trained with experimental data. In particular, it takes as input two high-speed camera snapshots before impact and produces as output six scalars that describe the dynamics of the gas pocket. The experiments are performed in a wave flume, where we use solitons -- in combination with a bathymetry profile -- to generate wave breaking close to a solid wall which is instrumented with dynamic pressure sensors. By varying the water depth $h_\\ell$ and the parameter $\u03b1= A/h_\\ell$, where $A$ is the soliton wave amplitude, we are able to generate a family of unique breaking waves with different gas pocket sizes and wave kinematics. In this so-called phase space of wave generation ($h_\\ell$, $\u03b1$), we perform experiments on 67 different wave states that form our dataset. Experimentally, we find that the frequency of oscillation of the gas pocket can be attributed to the initial volume of gas plus a geometric correction and that the maximum and minimum pressures are qualitatively well captured by the one-dimensional Bagnold model. In terms of the ML model, we compare its performance to the experimental data and find that the model quantitatively reproduces the trends found in the experiments -- in particular for the maximum and minimum pressure in the gas pocket and the frequency of oscillation.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "When the whole is greater than the sum of its parts: Scaling black-box inference to large data settings through divide-and-conquer",
    "url": "http://arxiv.org/abs/2412.20323v1",
    "authors": [
      "Emily C. Hector",
      "Amanda Lenzi"
    ],
    "published": "2024-12-29",
    "abstract": "Black-box methods such as deep neural networks are exceptionally fast at obtaining point estimates of model parameters due to their amortisation of the loss function computation, but are currently restricted to settings for which simulating training data is inexpensive. When simulating data is computationally expensive, both the training and uncertainty quantification, which typically relies on a parametric bootstrap, become intractable. We propose a black-box divide-and-conquer estimation and inference framework when data simulation is computationally expensive that trains a black-box estimation method on a partition of the multivariate data domain, estimates and bootstraps on the partitioned data, and combines estimates and inferences across data partitions. Through the divide step, only small training data need be simulated, substantially accelerating the training. Further, the estimation and bootstrapping can be conducted in parallel across multiple computing nodes to further speed up the procedure. Finally, the conquer step accounts for any dependence between data partitions through a statistically and computationally efficient weighted average. We illustrate the implementation of our framework in high-dimensional spatial settings with Gaussian and max-stable processes. Applications to modeling extremal temperature data from both a climate model and observations from the National Oceanic and Atmospheric Administration highlight the feasibility of estimation and inference of max-stable process parameters with tens of thousands of locations.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Deep Neural Network"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "State-of-the-Art Underwater Vehicles and Technologies Enabling Smart Ocean: Survey and Classifications",
    "url": "http://arxiv.org/abs/2412.18667v1",
    "authors": [
      "Jiajie Xu",
      "Xabier Irigoien",
      "Mohamed-Slim Alouini"
    ],
    "published": "2024-12-24",
    "abstract": "The exploration and sustainable use of marine environments have become increasingly critical as oceans cover over 70% of surface of Earth. This paper provides a comprehensive survey and classification of state-of-the-art underwater vehicles (UVs) and supporting technologies essential for enabling a smart ocean. We categorize UVs into several types, including remotely operated vehicles (ROVs), autonomous underwater vehicles (AUVs), hybrid underwater vehicles (HUVs), unmanned surface vehicles (USVs), and underwater bionic vehicles (UBVs). These technologies are fundamental in a wide range of applications, such as environmental monitoring, deep-sea exploration, defense, and underwater infrastructure inspection. Additionally, the paper explores advancements in underwater communication technologies, namely acoustic, optical, and hybrid systems, as well as key support facilities, including submerged buoys, underwater docking stations, and wearable underwater localization systems. By classifying the vehicles and analyzing their technological capabilities and limitations, this work aims to guide future developments in underwater exploration and monitoring, addressing challenges such as energy efficiency, communication limitations, and environmental adaptability. The paper concludes by discussing the integration of artificial intelligence and machine learning in enhancing the autonomy and operational efficiency of these systems, paving the way for the realization of a fully interconnected and sustainable Smart Ocean.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Iterative Encoding-Decoding VAEs Anomaly Detection in NOAA's DART Time Series: A Machine Learning Approach for Enhancing Data Integrity for NASA's GRACE-FO Verification and Validation",
    "url": "http://arxiv.org/abs/2412.16375v1",
    "authors": [
      "Kevin Lee"
    ],
    "published": "2024-12-20",
    "abstract": "NOAA's Deep-ocean Assessment and Reporting of Tsunamis (DART) data are critical for NASA-JPL's tsunami detection, real-time operations, and oceanographic research. However, these time-series data often contain spikes, steps, and drifts that degrade data quality and obscure essential oceanographic features. To address these anomalies, the work introduces an Iterative Encoding-Decoding Variational Autoencoders (Iterative Encoding-Decoding VAEs) model to improve the quality of DART time series. Unlike traditional filtering and thresholding methods that risk distorting inherent signal characteristics, Iterative Encoding-Decoding VAEs progressively remove anomalies while preserving the data's latent structure. A hybrid thresholding approach further retains genuine oceanographic features near boundaries. Applied to complex DART datasets, this approach yields reconstructions that better maintain key oceanic properties compared to classical statistical techniques, offering improved robustness against spike removal and subtle step changes. The resulting high-quality data supports critical verification and validation efforts for the GRACE-FO mission at NASA-JPL, where accurate surface measurements are essential to modeling Earth's gravitational field and global water dynamics. Ultimately, this data processing method enhances tsunami detection and underpins future climate modeling with improved interpretability and reliability.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Discovering Boundary Equations for Wave Breaking using Machine Learning",
    "url": "http://arxiv.org/abs/2412.12348v1",
    "authors": [
      "Tianning Tang",
      "Yuntian Chen",
      "Rui Cao",
      "Wouter Mostert",
      "Paul H. Taylor",
      "Mark L. McAllister",
      "Bing Tai",
      "Yuxiang Ma",
      "Adrian H. Callaghan",
      "Thomas A. A. Adcock"
    ],
    "published": "2024-12-16",
    "abstract": "Many supervised machine learning methods have revolutionised the empirical modelling of complex systems. These empirical models, however, are usually \"black boxes\" and provide only limited physical explanations about the underlying systems. Instead, so-called \"knowledge discovery\" methods can be used to explore the governing equations that describe observed phenomena. This paper focuses on how we can use such methods to explore underlying physics and also model a commonly observed yet not fully understood phenomenon - the breaking of ocean waves. In our work, we use symbolic regression to explore the equation that describes wave-breaking evolution from a dataset of in silico waves generated using expensive numerical methods. Our work discovers a new boundary equation that provides a reduced-order description of how the surface elevation (i.e., the water-air interface) evolves forward in time, including the instances when the wave breaks - a problem that has defied traditional approaches. Compared to the existing empirical models, the unique equation-based nature of our model allows further mathematical interpretation, which provides an opportunity to explore the fundamentals of breaking waves. Further expert-AI collaborative research reveals the physical meaning of each term of the discovered equation, which suggests a new characteristic of breaking waves in deep water - a decoupling between the water-air interface and the fluid velocities. This novel reduced-order model also hints at computationally efficient ways to simulate breaking waves for engineering applications.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Global Estimation of Subsurface Eddy Kinetic Energy of Mesoscale Eddies Using a Multiple-input Residual Neural Network",
    "url": "http://arxiv.org/abs/2412.10656v1",
    "authors": [
      "Chenyue Xie",
      "An-Kang Gao",
      "Xiyun Lu"
    ],
    "published": "2024-12-14",
    "abstract": "Oceanic eddy kinetic energy (EKE) is a key quantity for measuring the intensity of mesoscale eddies and for parameterizing eddy effects in ocean climate models. Three decades of satellite altimetry observations allow a global assessment of sea surface information. However, the subsurface EKE with spatial filter has not been systematically studied due to the sparseness of subsurface observational data. The subsurface EKE can be inferred both theoretically and numerically from sea surface observations but is limited by the issue of decreasing correlation with sea surface variables as depth increases. In this work, inspired by the Taylor-series expansion of subsurface EKE, a multiple-input neural network approach is proposed to reconstruct the subsurface monthly mean EKE from sea surface variables and subsurface climatological variables (e.g., horizontal filtered velocity gradients). Four neural networks are trained on a high-resolution global ocean reanalysis dataset, namely, surface-input fully connected neural network model (FCNN), surface-input Residual neural network model (ResNet), multiple-input fully connected neural network model (MI-FCNN), and multiple-input residual neural network model (MI-ResNet). The proposed MI-FCNN and MI-ResNet models integrate the surface input variables and the vertical profiles of subsurface variables. The MI-ResNet model outperforms the FCNN, ResNet, and MI-FCNN models, and traditional physics-based models in both regional and global reconstruction of subsurface EKE in the upper 2000 m. In addition, the MI-ResNet model performs well for both regional and global observational data based on transfer learning. These findings reveal the potential of the MI-ResNet model for efficient and accurate reconstruction of subsurface oceanic variables.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "ResNet"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "AI-powered Digital Twin of the Ocean: Reliable Uncertainty Quantification for Real-time Wave Height Prediction with Deep Ensemble",
    "url": "http://arxiv.org/abs/2412.05475v2",
    "authors": [
      "Dongeon Lee",
      "Sunwoong Yang",
      "Jae-Won Oh",
      "Su-Gil Cho",
      "Sanghyuk Kim",
      "Namwoo Kang"
    ],
    "published": "2024-12-07",
    "abstract": "Environmental pollution and fossil fuel depletion have prompted the need for renewable energy-based power generation. However, its stability is often challenged by low energy density and non-stationary conditions. Wave energy converters (WECs), in particular, need reliable real-time wave height prediction to address these issues caused by irregular wave patterns, which can lead to the inefficient and unstable operation of WECs. In this study, we propose an AI-powered reliable real-time wave height prediction model that integrates long short-term memory (LSTM) networks for temporal prediction with deep ensemble (DE) for robust uncertainty quantification (UQ), ensuring high accuracy and reliability. To further enhance the reliability, uncertainty calibration is applied, which has proven to significantly improve the quality of the quantified uncertainty. Using real operational data from an oscillating water column-wave energy converter (OWC-WEC) system in Jeju, South Korea, the model achieves notable accuracy (R2 > 0.9), while increasing uncertainty quality by over 50% through simple calibration technique. Furthermore, a comprehensive parametric study is conducted to explore the effects of key model hyperparameters, offering valuable guidelines for diverse operational scenarios, characterized by differences in wavelength, amplitude, and period. These results demonstrate the model's capability to deliver reliable predictions, facilitating digital twin of the ocean.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GLONET: Mercator's end-to-end neural Global Ocean forecasting system",
    "url": "http://arxiv.org/abs/2412.05454v3",
    "authors": [
      "Anass El Aouni",
      "Quentin Gaudel",
      "Charly Regnier",
      "Simon Van Gennip",
      "Olivier Le Galloudec",
      "Marie Drevillon",
      "Yann Drillet",
      "Jean-Michel Lellouche"
    ],
    "published": "2024-12-06",
    "abstract": "Accurate ocean forecasting is crucial in different areas ranging from science to decision making. Recent advancements in data-driven models have shown significant promise, particularly in weather forecasting community, but yet no data-driven approaches have matched the accuracy and the scalability of traditional global ocean forecasting systems that rely on physics-driven numerical models and can be very computationally expensive, depending on their spatial resolution or complexity. Here, we introduce GLONET, a global ocean neural network-based forecasting system, developed by Mercator Ocean International. GLONET is trained on the global Mercator Ocean physical reanalysis GLORYS12 to integrate physics-based principles through neural operators and networks, which dynamically capture local-global interactions within a unified, scalable framework, ensuring high small-scale accuracy and efficient dynamics. GLONET's performance is assessed and benchmarked against two other forecasting systems: the global Mercator Ocean analysis and forecasting 1/12 high-resolution physical system GLO12 and a recent neural-based system also trained from GLORYS12. A series of comprehensive validation metrics is proposed, specifically tailored for neural network-based ocean forecasting systems, which extend beyond traditional point-wise error assessments that can introduce bias towards neural networks optimized primarily to minimize such metrics. The preliminary evaluation of GLONET shows promising results, for temperature, sea surface height, salinity and ocean currents. GLONET's experimental daily forecast are accessible through the European Digital Twin Ocean platform EDITO.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "ACE2-SOM: Coupling an ML atmospheric emulator to a slab ocean and learning the sensitivity of climate to changed CO$_2$",
    "url": "http://arxiv.org/abs/2412.04418v2",
    "authors": [
      "Spencer K. Clark",
      "Oliver Watt-Meyer",
      "Anna Kwa",
      "Jeremy McGibbon",
      "Brian Henn",
      "W. Andre Perkins",
      "Elynn Wu",
      "Lucas M. Harris",
      "Christopher S. Bretherton"
    ],
    "published": "2024-12-05",
    "abstract": "While autoregressive machine-learning-based emulators have been trained to produce stable and accurate rollouts in the climate of the present-day and recent past, none so far have been trained to emulate the sensitivity of climate to substantial changes in CO$_2$ or other greenhouse gases. As an initial step we couple the Ai2 Climate Emulator version 2 to a slab ocean model (hereafter ACE2-SOM) and train it on output from a collection of equilibrium-climate physics-based reference simulations with varying levels of CO$_2$. We test it in equilibrium and non-equilibrium climate scenarios with CO$_2$ concentrations seen and unseen in training.\n  ACE2-SOM performs well in equilibrium-climate inference with both in-sample and out-of-sample CO$_2$ concentrations, accurately reproducing the emergent time-mean spatial patterns of surface temperature and precipitation change with CO$_2$ doubling, tripling, or quadrupling. In addition, the vertical profile of atmospheric warming and change in extreme precipitation rates up to the 99.9999th percentile closely agree with the reference model. Non-equilibrium-climate inference is more challenging. With CO$_2$ increasing gradually at a rate of 2% year$^{-1}$, ACE2-SOM can accurately emulate the global annual mean trends of surface and lower-to-middle atmosphere fields but produces unphysical jumps in stratospheric fields. With an abrupt quadrupling of CO$_2$, ML-controlled fields transition unrealistically quickly to the 4xCO$_2$ regime. In doing so they violate global energy conservation and exhibit unphysical sensitivities of and surface and top of atmosphere radiative fluxes to instantaneous changes in CO$_2$. Future emulator development needed to address these issues should improve its generalizability to diverse climate change scenarios.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Samudra: An AI Global Ocean Emulator for Climate",
    "url": "http://arxiv.org/abs/2412.03795v4",
    "authors": [
      "Surya Dheeshjith",
      "Adam Subel",
      "Alistair Adcroft",
      "Julius Busecke",
      "Carlos Fernandez-Granda",
      "Shubham Gupta",
      "Laure Zanna"
    ],
    "published": "2024-12-05",
    "abstract": "AI emulators for forecasting have emerged as powerful tools that can outperform conventional numerical predictions. The next frontier is to build emulators for long climate simulations with skill across a range of spatiotemporal scales, a particularly important goal for the ocean. Our work builds a skillful global emulator of the ocean component of a state-of-the-art climate model. We emulate key ocean variables, sea surface height, horizontal velocities, temperature, and salinity, across their full depth. We use a modified ConvNeXt UNet architecture trained on multi-depth levels of ocean data. We show that the ocean emulator - Samudra - which exhibits no drift relative to the truth, can reproduce the depth structure of ocean variables and their interannual variability. Samudra is stable for centuries and 150 times faster than the original ocean model. Samudra struggles to capture the correct magnitude of the forcing trends and simultaneously remain stable, requiring further work.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Mapping The Layers of The Ocean Floor With a Convolutional Neural Network",
    "url": "http://arxiv.org/abs/2412.05329v1",
    "authors": [
      "Guilherme G. D. Fernandes",
      "Vitor S. P. P. Oliveira",
      "Jo\u00e3o P. I. Astolfo"
    ],
    "published": "2024-12-04",
    "abstract": "The mapping of ocean floor layers is a current challenge for the oil industry. Existing solution methods involve mapping through seismic methods and wave inversion, which are complex and computationally expensive. The introduction of artificial neural networks, specifically UNet, to predict velocity models based on seismic shots reflected from the ocean floor shows promise for optimising this process. In this study, two neural network architectures are validated for velocity model inversion and compared in terms of stability metrics such as loss function and similarity coefficient, as well as the differences between predicted and actual models. Indeed, neural networks prove promising as a solution to this challenge, achieving S\u00f8rensen-Dice coefficient values above 70%.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "FathomGPT: A Natural Language Interface for Interactively Exploring Ocean Science Data",
    "url": "http://arxiv.org/abs/2412.02784v1",
    "authors": [
      "Nabin Khanal",
      "Chun Meng Yu",
      "Jui-Cheng Chiu",
      "Anav Chaudhary",
      "Ziyue Zhang",
      "Kakani Katija",
      "Angus G. Forbes"
    ],
    "published": "2024-12-03",
    "abstract": "We introduce FathomGPT, an open source system for the interactive investigation of ocean science data via a natural language interface. FathomGPT was developed in close collaboration with marine scientists to enable researchers to explore and analyze the FathomNet image database. FathomGPT provides a custom information retrieval pipeline that leverages OpenAI's large language models to enable: the creation of complex queries to retrieve images, taxonomic information, and scientific measurements; mapping common names and morphological features to scientific names; generating interactive charts on demand; and searching by image or specified patterns within an image. In designing FathomGPT, particular emphasis was placed on enhancing the user's experience by facilitating free-form exploration and optimizing response times. We present an architectural overview and implementation details of FathomGPT, along with a series of ablation studies that demonstrate the effectiveness of our approach to name resolution, fine tuning, and prompt modification. We also present usage scenarios of interactive data exploration sessions and document feedback from ocean scientists and machine learning experts.",
    "categories": [
      "ocean"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Composing Open-domain Vision with RAG for Ocean Monitoring and Conservation",
    "url": "http://arxiv.org/abs/2412.02262v1",
    "authors": [
      "Sepand Dyanatkar",
      "Angran Li",
      "Alexander Dungate"
    ],
    "published": "2024-12-03",
    "abstract": "Climate change's destruction of marine biodiversity is threatening communities and economies around the world which rely on healthy oceans for their livelihoods. The challenge of applying computer vision to niche, real-world domains such as ocean conservation lies in the dynamic and diverse environments where traditional top-down learning struggle with long-tailed distributions, generalization, and domain transfer. Scalable species identification for ocean monitoring is particularly difficult due to the need to adapt models to new environments and identify rare or unseen species. To overcome these limitations, we propose leveraging bottom-up, open-domain learning frameworks as a resilient, scalable solution for image and video analysis in marine applications. Our preliminary demonstration uses pretrained vision-language models (VLMs) combined with retrieval-augmented generation (RAG) as grounding, leaving the door open for numerous architectural, training and engineering optimizations. We validate this approach through a preliminary application in classifying fish from video onboard fishing vessels, demonstrating impressive emergent retrieval and prediction capabilities without domain-specific training or knowledge of the task itself.",
    "categories": [
      "ocean",
      "ship_trajectories"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Current effects on wind generated waves near an Ocean Eddy Dipole",
    "url": "http://arxiv.org/abs/2511.12711v1",
    "authors": [
      "Nelson Violante-Carvalho",
      "Thiago de Paula",
      "Leandro Calado",
      "Felipe Marques dos Santos",
      "Luiz Mariano Carvalho",
      "Andre Luiz Cordeiro dos Santos",
      "Wilton Z. Arruda",
      "Leandro Farina"
    ],
    "published": "2025-11-16",
    "abstract": "Ocean eddy dipoles are among the most common mesoscale features and may be ubiquitous across the global oceans. However, wave-current interactions in their proximity have not been extensively studied. Here we examine the impact of surface currents on the wave field near an ocean eddy dipole. Using the WW3 wave model, we conducted idealized numerical simulations to assess the influence of different configurations on the spatial variability of Significant Wave Height ($H_s$). Additionally, a two-month hindcast of a strong dipole event in the Southwestern Atlantic Ocean was performed using three distinct surface current products: SSalto/Duacs, HYCOM NCODA and GlobCurrent. Among these, HYCOM, which incorporates ageostrophic effects, provided a more detailed representation of oceanic energy compared to GlobCurrent and SSalto/Duacs, which primarily reflect geostrophic components. The hindcast assessment employed denoised altimeter-derived $H_s$ data, with a spatial resolution of approximately 6~km. The greatest increase in wave energy occurs in the region between the peak values of positive and negative vorticity, where the opposing surface currents reach their maximum intensity. Therefore, dipoles act as converging lenses for surface waves, channeling their refraction towards the central jet. Despite its poorer spatial and temporal resolutions, SSalto-Duacs surface current data provides more reliable $H_s$ fields, in the study region where geostrophic dynamics are expected to be significant or even dominant.\n  HYCOM captures a broader range of dynamical processes, essential for accurately representing the total energy, though discrepancies with SSalto/Duacs data may arise from assimilation inaccuracies and model limitations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Uncoupled high-latitude wave models in COAMPS",
    "url": "http://arxiv.org/abs/2508.16755v1",
    "authors": [
      "W. E. Rogers",
      "T. J. Campbell",
      "J. Yu",
      "R. A. Allard"
    ],
    "published": "2025-08-22",
    "abstract": "This report describes six demonstration cases of two numerical ocean wave models in high latitude regions where waves interact with sea ice. The two wave models are SWAN (Simulating WAves Nearshore) and WW3 (WAVEWATCH III), run in uncoupled mode within the Navy's coupled regional modeling system, COAMPS(R) (Coupled Ocean Atmosphere Mesoscale Prediction System). The COAMPS software handles a large majority of the tasks associated with the setup, running, and post-processing of the wave models. All six cases are cycling runs with 12-hour increments, each providing a continuous hindcast of four to 26 days duration. SWAN is applied in a Bering Strait case and two Gulf of Bothnia cases. WW3 is applied in a Sea of Okhotsk case and two Barents Sea cases. Verification is performed by visual inspection of model output fields, and by comparing model runs with alternative settings. In the standard configuration, forcing comes from archived global model output, including information on surface wind vectors, sea ice concentration and thickness, and surface current vectors, and the wave model uses a new empirical formula for dissipation of wave energy by sea ice that is dependent on ice thickness. The Barents Sea cases are compared to spectral wave data from satellite (SWIM instrument on CFOSAT) and from motion sensors deployed on the ice by the Norwegian Meteorological Institute. Experiments are performed with non-standard settings, 1) disabling the dissipation by sea ice, 2) using an older formula for dissipation by sea ice which does not depend on ice thickness, 3) using higher resolution wind and sea ice concentration forcing fields, and 4) omitting surface currents. The impact of these settings on model skill is quantified by comparison to the observations. The skill is also compared to that from a global (thus, lower resolution) ocean wave model.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "A validated coupled three-dimensional hydrodynamic and spectral wind-wave model for the western north Atlantic Ocean",
    "url": "http://arxiv.org/abs/2505.18803v1",
    "authors": [
      "Maria Venolia",
      "Reza Marsooli",
      "Jaime R. Calzada"
    ],
    "published": "2025-05-24",
    "abstract": "Wind-wave and ocean current interactions affect critical coastal and oceanic processes, yet modeling these interactions presents significant challenges. The western North Atlantic Ocean provides an ideal test environment for coupled hydrodynamics and wind wave models, thanks to its energetic surface currents such as the Gulf Stream. This study evaluates a high-resolution coupled SCHISM WWM III model, utilizing NOAA's 'STOFS-3D-Atlantic' computational mesh, while incorporating three-dimensional baroclinic dynamics to account for density stratification effects. We evaluate the model's calculated water level and tidal predictions against NOAA tide gauge measurements during December 2016. The coupled model demonstrates robust skills in reproducing tidal constituents, non-tidal components, and total water level predictions along the U.S. East and Gulf of Mexico Coasts. In addition, we systematically evaluate three wave physics parameterizations (Ardhuin, Makin and Stam, and Cycle Three) in the spectral wave model to quantify their effects on the modeled wave characteristics. This validated modeling framework enhances our ability to understand and predict complex coastal and oceanic processes, offering significant applications for coastal management, maritime operations, and climate adaptation planning throughout the western North Atlantic region.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "ORCAst: Operational High-Resolution Current Forecasts",
    "url": "http://arxiv.org/abs/2501.12054v1",
    "authors": [
      "Pierre Garcia",
      "In\u00e8s Larroche",
      "Am\u00e9lie Pesnec",
      "Hannah Bull",
      "Th\u00e9o Archambault",
      "Evangelos Moschos",
      "Alexandre Stegner",
      "Anastase Charantonis",
      "Dominique B\u00e9r\u00e9ziat"
    ],
    "published": "2025-01-21",
    "abstract": "We present ORCAst, a multi-stage, multi-arm network for Operational high-Resolution Current forecAsts over one week. Producing real-time nowcasts and forecasts of ocean surface currents is a challenging problem due to indirect or incomplete information from satellite remote sensing data. Entirely trained on real satellite data and in situ measurements from drifters, our model learns to forecast global ocean surface currents using various sources of ground truth observations in a multi-stage learning procedure. Our multi-arm encoder-decoder model architecture allows us to first predict sea surface height and geostrophic currents from larger quantities of nadir and SWOT altimetry data, before learning to predict ocean surface currents from much more sparse in situ measurements from drifters. Training our model on specific regions improves performance. Our model achieves stronger nowcast and forecast performance in predicting ocean surface currents than various state-of-the-art methods.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Nowcast"
    ],
    "is_recent": false
  },
  {
    "title": "An analysis on OpenMetBuoy-v2021 drifter in-situ data and Lagrangian trajectory simulations in the Agulhas Current System",
    "url": "http://arxiv.org/abs/2409.20096v1",
    "authors": [
      "Bente Moerman",
      "\u00d8yvind Breivik",
      "Lars R. Hole",
      "Gaute Hope",
      "Johnny A. Johannessen",
      "Jean Rabault"
    ],
    "published": "2024-09-30",
    "abstract": "In order to perform a sensitivity analysis of Lagrangian trajectory models, Lagrangian trajectory simulations have been compared to six OpenMetBuoy-v2021 drifter trajectories in the Agulhas Current System (Jan-Mar 2023). Three different Lagrangian trajectory simulations have been assessed: (1) two offline Lagrangian tracking tools, OpenDrift and Parcels, (2) three Eulerian ocean surface current products, HYCOM, Mercator and Globcurrent, and (3) the addition of wind and/or wave forcing parameterizations. The latter has also been evaluated by strong ocean current, high wind speed and Stokes drift regimes.\n  Firstly, using the same time stepping scheme and linear interpolation methods, the different Lagrangian simulators OpenDrift and Parcels, performed identically. Secondly, the Globcurrent product showed the highest mean skill of the three ocean current products, although it underestimated the speed for strong ocean currents due to its spatial resolution. The HYCOM and Mercator model simulations showed, respectively, 40\\% and 15\\% lower skill than the Globcurrent simulations. Finally, the addition of the Stokes drift and a wind drift factor (WDF), improved the Lagrangian simulation performance in skill and speed, especially in high wind (>10 m/s) and/or Stokes drift regimes (>0.15 m/s). The optimal WDF for the OpenMetBuoy-v2021 is found to be ~1.8\\% and ~2.3\\% for simulations including and excluding Stokes drift forcing respectively. To further improve the incorporation of Stokes drift and direct wind drag on the trajectory simulations, a more physically based solution is advised as there are still numerous wind and wave related processes that remain unresolved, like wave-current interactions and vertical shear.\n  To statistically strengthen the conclusions from this research, incorporating additional observed drifter trajectories would be highly favourable.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "High Frequency Radar Observing System Simulation Experiment in the Western Mediterranean Sea: a Lagrangian assessment approach",
    "url": "http://arxiv.org/abs/2406.03579v2",
    "authors": [
      "Jaime Hernandez Lasheras",
      "Alejandro Orfila",
      "Alex Santana",
      "Ismael Hernandez Carrasco",
      "Baptiste Mourre"
    ],
    "published": "2024-06-05",
    "abstract": "The impact of the expansion of a high-frequency radar (HFR) system in a dynamic coastal area (the Ibiza Channel in the Western Mediterranean Sea) is evaluated through an Observing System Simulation Experiment (OSSE). The installation of two new antennas in the Iberian Peninsula would complement the existing ones in the islands of Ibiza and Formentera, providing surface currents observations of the full channel. Two different configurations of the same model, validated to give realistic simulations, are used: i) a Nature Run (NR) which is considered as the real ocean state and that is used to generate pseudo-observations, and ii) a Control Run (CR) in which the pseudo-observations are assimilated. The OSSE is first validated by comparison against a previous Observing System Experiment (OSE). The impact of the new antennas for forecasting surface currents is evaluated in two different periods with different levels of agreement between NR and CR. The HFR expansion is found to contribute to significantly correct the circulation patterns in the Channel, leading to surface merdional velocity error reductions up to 19%. The effects on the transport in the area are also analyzed from a Lagrangian perspective, showing that DA can help to better represent the Lagrangian Coherent Structures present in the NR and constrain the ocean dynamics.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Synthetic RAW data generator for ESA HARMONY mission",
    "url": "http://arxiv.org/abs/2405.09938v1",
    "authors": [
      "Goulven Monnier",
      "Benjamin Camus",
      "Yann-Herv\u00e9 Hellouvry",
      "Pierre Dubois",
      "Erik de Witte"
    ],
    "published": "2024-05-16",
    "abstract": "In this paper, we introduce HEEPS/MARE, the end-to-end simulator developed for the SAR oceanographic products of ESA Earth Explorer 10 mission, Harmony, expected to launch in Decembre 2029. Harmony is primarily dedicated to the observation of small-scale motion and deformation fields of the Earth surface (oceans, glaciers and ice sheets, solid Earth), thanks to passive SAR/ATI receivers carried by two companion satellites for Sentinel-1. The paper focuses on the raw data generator designed to efficiently simulate large, heterogeneous, moving oceanic areas and produce the acquired SAR/ATI bistatic IQ signals. The heterogeneous sea-surface model, bistatic scattering model, multi-GPU implementation and achieved performance are emphasized. Finally, sample results are presented, to illustrate the ability of Harmony to map wind and surface current vectors at kilometric scale.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Scattering of surface waves by ocean currents: the U2H map",
    "url": "http://arxiv.org/abs/2402.05652v3",
    "authors": [
      "Han Wang",
      "Ana B. Villas B\u00f4as",
      "Jacques Vanneste",
      "William R. Young"
    ],
    "published": "2024-02-08",
    "abstract": "Ocean turbulence at meso- and submesocales affects the propagation of surface waves through refraction and scattering, inducing spatial modulations in significant wave height (SWH). We develop a theoretical framework that relates these modulations to the current that induces them. We exploit the asymptotic smallness of the ratio of typical current speed to wave group speed to derive a linear map -- the U2H map -- between surface current velocity and SWH anomaly. The U2H map is a convolution, non-local in space, expressible as a product in Fourier space by a factor independent of the magnitude of the wavenumber vector. Analytic expressions of the U2H map show how the SWH responds differently to the vortical and divergent parts of the current, and how the anisotropy of the wave spectrum is key to large current-induced SWH anomalies. We implement the U2H map numerically and test its predictions against WAVEWATCH III numerical simulations for both idealised and realistic current configurations.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
    "url": "http://arxiv.org/abs/2406.02770v1",
    "authors": [
      "Kathrin Donandt",
      "Karim B\u00f6ttger",
      "Dirk S\u00f6ffker"
    ],
    "published": "2024-06-04",
    "abstract": "Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Architecture for Trajectory-Based Fishing Ship Classification with AIS Data",
    "url": "http://arxiv.org/abs/2501.02038v1",
    "authors": [
      "David S\u00e1nchez Pedroche",
      "Daniel Amigo",
      "Jes\u00fas Garc\u00eda",
      "Jose M. Molina"
    ],
    "published": "2025-01-03",
    "abstract": "This paper proposes a data preparation process for managing real-world kinematic data and detecting fishing vessels. The solution is a binary classification that classifies ship trajectories into either fishing or non-fishing ships. The data used are characterized by the typical problems found in classic data mining applications using real-world data, such as noise and inconsistencies. The two classes are also clearly unbalanced in the data, a problem which is addressed using algorithms that resample the instances. For classification, a series of features are extracted from spatiotemporal data that represent the trajectories of the ships, available from sequences of Automatic Identification System (AIS) reports. These features are proposed for the modelling of ship behavior but, because they do not contain context-related information, the classification can be applied in other scenarios. Experimentation shows that the proposed data preparation process is useful for the presented classification problem. In addition, positive results are obtained using minimal information.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)",
    "url": "http://arxiv.org/abs/2405.09596v2",
    "authors": [
      "Nicolas Drapier",
      "Aladine Chetouani",
      "Aur\u00e9lien Chateigner"
    ],
    "published": "2024-05-15",
    "abstract": "The prediction of ship trajectories is a growing field of study in artificial intelligence. Traditional methods rely on the use of LSTM, GRU networks, and even Transformer architectures for the prediction of spatio-temporal series. This study proposes a viable alternative for predicting these trajectories using only GNSS positions. It considers this spatio-temporal problem as a natural language processing problem. The latitude/longitude coordinates of AIS messages are transformed into cell identifiers using the H3 index. Thanks to the pseudo-octal representation, it becomes easier for language models to learn the spatial hierarchy of the H3 index. The method is compared with a classical Kalman filter, widely used in the maritime domain, and introduces the Fr\u00e9chet distance as the main evaluation metric. We show that it is possible to predict ship trajectories quite precisely up to 8 hours ahead with 30 minutes of context, using solely GNSS positions, without relying on any additional information such as speed, course, or external conditions - unlike many traditional methods. We demonstrate that this alternative works well enough to predict trajectories worldwide.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "Transformer",
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Maritime Vessel Tracking",
    "url": "http://arxiv.org/abs/2512.11707v1",
    "authors": [
      "John Mahlon Scott",
      "Hsin-Hsiung Huang"
    ],
    "published": "2025-12-12",
    "abstract": "The Automatic Identification System (AIS) provides time stamped vessel positions and kinematic reports that enable maritime authorities to monitor traffic. We consider the problem of relabeling AIS trajectories when vessel identifiers are missing, focusing on a challenging nationwide setting in which tracks are heavily downsampled and span diverse operating environments across continental U.S. waters. We propose a hybrid pipeline that first applies a physics-based screening step to project active track endpoints forward in time and select a small set of plausible ancestors for each new observation. A supervised neural classifier then chooses among these candidates, or initiates a new track, using engineered space time and kinematic consistency features. On held out data, this approach improves posit accuracy relative to unsupervised baselines, demonstrating that combining simple motion models with learned disambiguation can scale vessel relabeling to heterogeneous, high volume AIS streams.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence",
    "url": "http://arxiv.org/abs/2512.09670v1",
    "authors": [
      "Gil Weissman",
      "Amir Ivry",
      "Israel Cohen"
    ],
    "published": "2025-12-10",
    "abstract": "The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept",
    "url": "http://arxiv.org/abs/2510.23994v1",
    "authors": [
      "Geoffery Agorku",
      "Sarah Hernandez",
      "Hayley Hames",
      "Cade Wagner"
    ],
    "published": "2025-10-28",
    "abstract": "Accurate, real-time estimation of barge quantity on inland waterways remains a critical challenge due to the non-self-propelled nature of barges and the limitations of existing monitoring systems. This study introduces a novel method to use Automatic Identification System (AIS) vessel tracking data to predict the number of barges in tow using Machine Learning (ML). To train and test the model, barge instances were manually annotated from satellite scenes across the Lower Mississippi River. Labeled images were matched to AIS vessel tracks using a spatiotemporal matching procedure. A comprehensive set of 30 AIS-derived features capturing vessel geometry, dynamic movement, and trajectory patterns were created and evaluated using Recursive Feature Elimination (RFE) to identify the most predictive variables. Six regression models, including ensemble, kernel-based, and generalized linear approaches, were trained and evaluated. The Poisson Regressor model yielded the best performance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of the 30 features. The feature importance analysis revealed that metrics capturing vessel maneuverability such as course entropy, speed variability and trip length were most predictive of barge count. The proposed approach provides a scalable, readily implementable method for enhancing Maritime Domain Awareness (MDA), with strong potential applications in lock scheduling, port management, and freight planning. Future work will expand the proof of concept presented here to explore model transferability to other inland rivers with differing operational and environmental conditions.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models",
    "url": "http://arxiv.org/abs/2509.13047v1",
    "authors": [
      "Nolan Platt",
      "Pragyansmita Nayak"
    ],
    "published": "2025-09-16",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventing over- fitting and ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves 75% accuracy on maritime tasks, while being substantially cheaper than using a larger model for inference. We show that smaller, cheaper models - when fine tuned properly - can provide similar accuracy compared to larger models that are prohibitively expensive. Our work contributes to the growing field of synthetic dataset generation for specialized AI applications and presents a highly reproducible framework for domains where manual annotation is infeasible. Beyond expand- ing research in the growing field of specialized small language models, our approach has immediate applications in maritime safety, security operations, and vessel traffic management systems in various industries.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction",
    "url": "http://arxiv.org/abs/2503.21834v1",
    "authors": [
      "Haomin Yu",
      "Tianyi Li",
      "Kristian Torp",
      "Christian S. Jensen"
    ],
    "published": "2025-03-27",
    "abstract": "Accurate vessel trajectory prediction facilitates improved navigational safety, routing, and environmental protection. However, existing prediction methods are challenged by the irregular sampling time intervals of the vessel tracking data from the global AIS system and the complexity of vessel movement. These aspects render model learning and generalization difficult. To address these challenges and improve vessel trajectory prediction, we propose the multi-modal knowledge-enhanced framework (MAKER) for vessel trajectory prediction. To contend better with the irregular sampling time intervals, MAKER features a Large language model-guided Knowledge Transfer (LKT) module that leverages pre-trained language models to transfer trajectory-specific contextual knowledge effectively. To enhance the ability to learn complex trajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning (KSL) module. This module employs kinematic knowledge to progressively integrate complex patterns during training, allowing for adaptive learning and enhanced generalization. Experimental results on two vessel trajectory datasets show that MAKER can improve the prediction accuracy of state-of-the-art methods by 12.08%-17.86%.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Predicting Barge Presence and Quantity on Inland Waterways using Vessel Tracking Data: A Machine Learning Approach",
    "url": "http://arxiv.org/abs/2501.00615v2",
    "authors": [
      "Geoffery Agorku",
      "Sarah Hernandez",
      "Maria Falquez",
      "Subhadipto Poddar",
      "Shihao Pang"
    ],
    "published": "2024-12-31",
    "abstract": "This study presents a machine learning approach to predict the number of barges transported by vessels on inland waterways using tracking data from the Automatic Identification System (AIS). While AIS tracks the location of tug and tow vessels, it does not monitor the presence or number of barges transported by those vessels. Understanding the number and types of barges conveyed along river segments, between ports, and at ports is crucial for estimating the quantities of freight transported on the nation's waterways. This insight is also valuable for waterway management and infrastructure operations impacting areas such as targeted dredging operations, and data-driven resource allocation. Labeled sample data was generated using observations from traffic cameras located along key river segments and matched to AIS data records. A sample of 164 vessels representing up to 42 barge convoys per vessel was used for model development. The methodology involved first predicting barge presence and then predicting barge quantity. Features derived from the AIS data included speed measures, vessel characteristics, turning measures, and interaction terms. For predicting barge presence, the AdaBoost model achieved an F1 score of 0.932. For predicting barge quantity, the Random Forest combined with an AdaBoost ensemble model achieved an F1 score of 0.886. Bayesian optimization was used for hyperparameter tuning. By advancing predictive modeling for inland waterways, this study offers valuable insights for transportation planners and organizations, which require detailed knowledge of traffic volumes, including the flow of commodities, their destinations, and the tonnage moving in and out of ports.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "DisBeaNet: A Deep Neural Network to augment Unmanned Surface Vessels for maritime situational awareness",
    "url": "http://arxiv.org/abs/2405.06149v2",
    "authors": [
      "Srikanth Vemula",
      "Eulises Franco",
      "Michael Frye"
    ],
    "published": "2024-05-10",
    "abstract": "Intelligent detection and tracking of the vessels on the sea play a significant role in conducting traffic avoidance in unmanned surface vessels(USV). Current traffic avoidance software relies mainly on Automated Identification System (AIS) and radar to track other vessels to avoid collisions and acts as a typical perception system to detect targets. However, in a contested environment, emitting radar energy also presents the vulnerability to detection by adversaries. Deactivating these Radiofrequency transmitting sources will increase the threat of detection and degrade the USV's ability to monitor shipping traffic in the vicinity. Therefore, an intelligent visual perception system based on an onboard camera with passive sensing capabilities that aims to assist USV in addressing this problem is presented in this paper. This paper will present a novel low-cost vision perception system for detecting and tracking vessels in the maritime environment. This novel low-cost vision perception system is introduced using the deep learning framework. A neural network, DisBeaNet, can detect vessels, track, and estimate the vessel's distance and bearing from the monocular camera. The outputs obtained from this neural network are used to determine the latitude and longitude of the identified vessel.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Maritime object classification with SAR imagery using quantum kernel methods",
    "url": "http://arxiv.org/abs/2512.11367v1",
    "authors": [
      "John Tanner",
      "Nicholas Davies",
      "Pascal Elahi",
      "Casey R. Myers",
      "Du Huynh",
      "Wei Liu",
      "Mark Reynolds",
      "Jingbo Wang"
    ],
    "published": "2025-12-12",
    "abstract": "Illegal, unreported, and unregulated (IUU) fishing causes global economic losses of \\$10-25 billion annually and undermines marine sustainability and governance. Synthetic Aperture Radar (SAR) provides reliable maritime surveillance under all weather and lighting conditions, but classifying small maritime objects in SAR imagery remains challenging. We investigate quantum machine learning for this task, focusing on Quantum Kernel Methods (QKMs) applied to real and complex SAR chips extracted from the SARFish dataset. We tackle two binary classification problems, the first for distinguishing vessels from non-vessels, and the second for distinguishing fishing vessels from other types of vessels. We compare QKMs applied to real and complex SAR chips against classical Laplacian, RBF, and linear kernels applied to real SAR chips. Using noiseless numerical simulations of the quantum kernels, we find that QKMs are capable of obtaining equal or better performance than the classical kernel on these tasks in the best case, but do not demonstrate a clear advantage for the complex SAR data. This work presents the first application of QKMs to maritime classification in SAR imagery and offers insight into the potential and current limitations of quantum-enhanced learning for maritime surveillance.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent and Independent Data under Preferential Sampling",
    "url": "http://arxiv.org/abs/2509.09336v1",
    "authors": [
      "Daniela Silva",
      "Raquel Menezes",
      "Gon\u00e7alo Ara\u00fajo",
      "Ana Machado",
      "Renato Rosa",
      "Ana Moreno",
      "Alexandra Silva",
      "Susana Garrido"
    ],
    "published": "2025-09-11",
    "abstract": "Sustainable management of marine ecosystems is vital for maintaining healthy fishery resources, and benefits from advanced scientific tools to accurately assess species distribution patterns. In fisheries science, two primary data sources are used: fishery-independent data (FID), collected through systematic surveys, and fishery-dependent data (FDD), obtained from commercial fishing activities. While these sources provide complementary information, their distinct sampling schemes - systematic for FID and preferential for FDD - pose significant integration challenges. This study introduces a novel spatio-temporal model that integrates FID and FDD, addressing challenges associated with zero-inflation and preferential sampling (PS) common in ecological data. The model employs a six-layer structure to differentiate between presence-absence and biomass observations, offering a robust framework for ecological studies affected by PS biases. Simulation results demonstrate the model's accuracy in parameter estimation across diverse PS scenarios and its ability to detect preferential signals. Application to the study of the distribution patterns of the European sardine populations along the southern Portuguese continental shelf illustrates the model's effectiveness in integrating diverse data sources and incorporating environmental and vessel-specific covariates. The model reveals spatio-temporal variability in sardine presence and biomass, providing actionable insights for fisheries management. Beyond ecology, this framework offers broad applicability to data integration challenges in other disciplines.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks",
    "url": "http://arxiv.org/abs/2509.18109v1",
    "authors": [
      "Jonatan Katz Nielsen"
    ],
    "published": "2025-09-09",
    "abstract": "Accurate recognition of vessel types from Automatic Identification System (AIS) tracks is essential for safety oversight and combating illegal, unreported, and unregulated (IUU) activity. This paper presents a strait-scale, machine-learning pipeline that classifies moving vessels using only AIS data. We analyze eight days of historical AIS from the Danish Maritime Authority covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After forward/backward filling voyage records, removing kinematic and geospatial outliers, and segmenting per-MMSI tracks while excluding stationary periods ($\\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g., SOG statistics), temporal, geospatial (Haversine distances, spans), and ship-shape attributes computed from AIS A/B/C/D reference points (length, width, aspect ratio, bridge-position ratio). To avoid leakage, we perform grouped train/test splits by MMSI and use stratified 5-fold cross-validation. Across five classes (cargo, tanker, passenger, high-speed craft, fishing; N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall 92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the bridge-position ratio and maximum SOG as the most discriminative signals; principal errors occur between cargo and tanker, reflecting similar transit behavior. We demonstrate operational value by backfilling missing ship types on unseen data and discuss improvements such as DBSCAN based trip segmentation and gradient-boosted ensembles to handle frequent-stop ferries and further lift performance. The results show that lightweight features over AIS trajectories enable real-time vessel type classification in straits.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Classification",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Integrating Activity Predictions in Knowledge Graphs",
    "url": "http://arxiv.org/abs/2507.19733v3",
    "authors": [
      "Forrest Hare",
      "Alec Sculley",
      "Cameron Stockton"
    ],
    "published": "2025-07-26",
    "abstract": "We argue that ontology-structured knowledge graphs can play a crucial role in generating predictions about future events. By leveraging the semantic framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies (CCO), we demonstrate how data such as the movements of a fishing vessel can be organized in and retrieved from a knowledge graph. These query results are then used to create Markov chain models, allowing us to predict future states based on the vessel's history. To fully support this process, we introduce the term `spatiotemporal instant' to complete the necessary structural semantics. Additionally, we critique the prevailing ontological model of probability, according to which probabilities are about the future. We propose an alternative view, where at least some probabilities are treated as being about actual process profiles, which better captures the dynamics of real-world phenomena. Finally, we demonstrate how our Markov chain-based probability calculations can be seamlessly integrated back into the knowledge graph, enabling further analysis and decision-making.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Modeling Maritime Transportation Behavior Using AIS Trajectories and Markovian Processes in the Gulf of St. Lawrence",
    "url": "http://arxiv.org/abs/2506.00025v3",
    "authors": [
      "Gabriel Spadon",
      "Ruixin Song",
      "Vaishnav Vaidheeswaran",
      "Md Mahbub Alam",
      "Floris Goerlandt",
      "Ronald Pelot"
    ],
    "published": "2025-05-22",
    "abstract": "Maritime transportation is central to the global economy, and analyzing its large-scale behavioral data is critical for operational planning, environmental stewardship, and governance. This work presents a spatio-temporal analytical framework based on discrete-time Markov chains to model vessel movement patterns in the Gulf of St. Lawrence, with particular emphasis on disruptions induced by the COVID-19 pandemic. We discretize the maritime domain into hexagonal cells and construct mobility signatures for distinct vessel types using cell transition frequencies and dwell times. These features are used to build origin-destination matrices and spatial transition probability models that characterize maritime dynamics across multiple temporal resolutions. Focusing on commercial, fishing, and passenger vessels, we analyze the temporal evolution of mobility behaviors during the pandemic, highlighting significant yet transient disruptions to recurring transport patterns. The methodology we contribute to this paper allows for an extensive behavioral analytics key for transportation planning. Accordingly, our findings reveal vessel-specific mobility signatures that persist across spatially disjoint regions, suggesting behaviors invariant to time. In contrast, we observe temporal deviations among passenger and fishing vessels during the pandemic, reflecting the influence of social isolation measures and operational constraints on non-essential maritime transport in this region.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Spatio-temporal characterisation of underwater noise through semantic trajectories",
    "url": "http://arxiv.org/abs/2501.11131v1",
    "authors": [
      "Giulia Rovinelli",
      "Davide Rocchesso",
      "Marta Simeoni",
      "Esteban Zim\u00e1nyi",
      "Alessandra Raffaet\u00e0"
    ],
    "published": "2025-01-19",
    "abstract": "Underwater noise pollution from human activities, particularly shipping, has been recognised as a serious threat to marine life. The sound generated by vessels can have various adverse effects on fish and aquatic ecosystems in general. In this setting, the estimation and analysis of the underwater noise produced by vessels is an important challenge for the preservation of the marine environment. In this paper we propose a model for the spatio-temporal characterisation of the underwater noise generated by vessels. The approach is based on the reconstruction of the vessels' trajectories from Automatic Identification System (AIS) data and on their deployment in a spatio-temporal database. Trajectories are enriched with semantic information like the acoustic characteristics of the vessels' engines or the activity performed by the vessels. We define a model for underwater noise propagation and use the trajectories' information to infer how noise propagates in the area of interest. We develop our approach for the case study of the fishery activities in the Northern Adriatic sea, an area of the Mediterranean sea which is well known to be highly exploited. We implement our approach using MobilityDB, an open source geospatial trajectory data management and analysis platform, which offers spatio-temporal operators and indexes improving the efficiency of our system. We use this platform to conduct various analyses of the underwater noise generated in the Northern Adriatic Sea, aiming at estimating the impact of fishing activities on underwater noise pollution and at demonstrating the flexibility and expressiveness of our approach.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Identifying companies and financial actors exposed to marine tipping points",
    "url": "http://arxiv.org/abs/2411.10307v2",
    "authors": [
      "Juan C. Rocha",
      "Jean-Baptiste Jouffray",
      "Frida Bengtsson",
      "Bianca-Ioana Voicu",
      "Paula A. S\u00e1nchez",
      "Victor Galaz"
    ],
    "published": "2024-11-15",
    "abstract": "Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Here we used satellite data to track fishing vessels operating in areas prone to marine regime shifts, as identified by their loss of resilience and vulnerability to marine heatwaves, and uncovered their corporate beneficial owners and shareholders. Despite some data gaps, we identified key countries, companies, and shareholders exposed to tipping risk. We also outline the potential challenges and opportunities that these actors may face if marine ecosystems shift to less productive states.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Vessel Re-identification and Activity Detection in Thermal Domain for Maritime Surveillance",
    "url": "http://arxiv.org/abs/2406.08294v1",
    "authors": [
      "Yasod Ginige",
      "Ransika Gunasekara",
      "Darsha Hewavitharana",
      "Manjula Ariyarathne",
      "Ranga Rodrigo",
      "Peshala Jayasekara"
    ],
    "published": "2024-06-12",
    "abstract": "Maritime surveillance is vital to mitigate illegal activities such as drug smuggling, illegal fishing, and human trafficking. Vision-based maritime surveillance is challenging mainly due to visibility issues at night, which results in failures in re-identifying vessels and detecting suspicious activities. In this paper, we introduce a thermal, vision-based approach for maritime surveillance with object tracking, vessel re-identification, and suspicious activity detection capabilities. For vessel re-identification, we propose a novel viewpoint-independent algorithm which compares features of the sides of the vessel separately (separate side-spaces) leveraging shape information in the absence of color features. We propose techniques to adapt tracking and activity detection algorithms for the thermal domain and train them using a thermal dataset we created. This dataset will be the first publicly available benchmark dataset for thermal maritime surveillance. Our system is capable of re-identifying vessels with an 81.8% Top1 score and identifying suspicious activities with a 72.4\\% frame mAP score; a new benchmark for each task in the thermal domain.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General Optimization Problems",
    "url": "http://arxiv.org/abs/2405.02850v7",
    "authors": [
      "Jiawen Li",
      "Anwar PP Abdul Majeed",
      "Pascal Lefevre"
    ],
    "published": "2024-05-05",
    "abstract": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a quantum-inspired metaheuristic designed to address general optimization problems. The HEO mimics the effects between quantum such as tunneling, entanglement. After the introduction to the HEO mechansims, the study presents a comprehensive evaluation of HEO's performance against extensively-used optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary analysis encompasses 14 benchmark functions with dimension 30, demonstrating HEO's effectiveness and adaptability in navigating general optimization problems. The test of HEO in Pressure Vessel Design and Tubular Column Design also infers its feasibility and potential in real-time applications. Further validation of HEO in Osmancik-97 and Cammeo Rice Classification achieves a higher accuracy record.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "FAD-SAR: A Novel Fishing Activity Detection System via Synthetic Aperture Radar Images Based on Deep Learning Method",
    "url": "http://arxiv.org/abs/2404.18245v2",
    "authors": [
      "Yanbing Bai",
      "Siao Li",
      "Rui-Yang Ju",
      "Zihao Yang",
      "Jinze Yu",
      "Jen-Shiun Chiang"
    ],
    "published": "2024-04-28",
    "abstract": "Illegal, unreported, and unregulated (IUU) fishing activities seriously affect various aspects of human life. However, traditional methods for detecting and monitoring IUU fishing activities at sea have limitations. Although synthetic aperture radar (SAR) can complement existing vessel detection systems, extracting useful information from SAR images using traditional methods remains a challenge, especially in IUU fishing. This paper proposes a deep learning based fishing activity detection system, which is implemented on the xView3 dataset using six classical object detection models: SSD, RetinaNet, FSAF, FCOS, Faster R-CNN, and Cascade R-CNN. In addition, this work employs different enhancement techniques to improve the performance of the Faster R-CNN model. The experimental results demonstrate that training the Faster R-CNN model using the Online Hard Example Mining (OHEM) strategy increases the Avg-F1 value from 0.212 to 0.216.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Shipping traffic through the Arctic Ocean: spatial distribution, temporal evolution and its dependence on the sea ice extent",
    "url": "http://arxiv.org/abs/2403.01856v1",
    "authors": [
      "Jorge P. Rodr\u00edguez",
      "Konstantin Klemm",
      "Carlos M. Duarte",
      "V\u00edctor M. Egu\u00edluz"
    ],
    "published": "2024-03-04",
    "abstract": "The reduction in sea ice cover with Arctic warming facilitates the transit of ships through routes that are remarkably shorter than the traditional shipping routes. Automatic Identification System (AIS), ideally designed to avoid vessel collisions, transmits on vessel navigation information (currently 27 types of messages) such as name, position or speed, is a powerful data source to monitor the progress of Arctic shipping as the ice cover decreases. Based on the analysis of an online platform collecting shipping AIS data, we quantified the spatial distribution of shipping through the Arctic Ocean, its intensity and the temporal evolution, in relation to the area released by the sea ice area. Shipping through the Arctic Ocean is distributed spatially following a heavy-tailed distribution, implying heavy traffic through a limited Arctic area, with an exponent that depends on the vessel category. Fishing is the category with the largest spatial spread, with the width of shipping routes correlated with the proximal sea ice area. The time evolution of these routes is characterized by increasing extended periods of shipping activity through the year. AIS data offers valuable information on the activity of the international fleet worldwide. In the context of the new international agreements, it is a valuable source to monitor shipping, fishing and the potential impact in marine life among other aspects. Here we have focused on the Arctic shipping in recent years, which is rapidly growing, particularly around the Northeastern and Northwest Passage coastal routes, providing an opportunity for the design of shorter shipping routes and reduced greenhouse gas emissions from transport of goods, but at a risk of impacts on the Arctic ecosystem.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Prediction of the Economic Behavior of Fishery Biotechnology Companies Based on Machine Learning-Based Deep Metacellular Automata",
    "url": "http://arxiv.org/abs/2402.13509v2",
    "authors": [
      "Liguo Chen",
      "Hongyang Hua",
      "Xinyue Luo",
      "Guoli Xu",
      "Xu Yan"
    ],
    "published": "2024-02-21",
    "abstract": "Ocean warming significantly affects the fishing industry, with species like Scottish herring and mackerel migrating northwards. Our research, a fusion of artificial intelligence, data science, and operations research, addresses this crisis. Using Long Short Term Memory networks, we forecast sea surface temperatures (SST) and model fish migratory patterns with Enhanced Cellular Automata. A corrective factor within our model adjusts for human impact on SST, guiding diverse mitigation scenarios. We apply operational research to strategize responses, including the modernization of fishing vessels as a less costly alternative to relocation. Our data-driven approach, suggesting fleet modernization, strategic relocation, and product diversification, offers an effective approach to mitigating the threats to the ocean warming phenomenon.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Experimental investigations of underwater and airborne noises produced by a large hovercraft in Ural River estuary",
    "url": "http://arxiv.org/abs/2401.14204v1",
    "authors": [
      "A. I. Vedenev",
      "O. Yu. Kochetov",
      "A. A. Lunkov",
      "A. S. Shurup",
      "S. S. Kassymbekova"
    ],
    "published": "2024-01-25",
    "abstract": "Simultaneous measurements of underwater and airborne noises produced by Griffon Hoverwork BHT130 hovercraft were carried out in environmentally sensitive area - wildlife preserve in the area of the Ural River estuary near the Caspian Sea shelf. Measurements were organized to assess the possible negative impact of noise from hovercraft on fish and birds in wildlife preserve. The particle velocity of underwater noise was estimated by using a gradient-type vector receiver. That was a distinctive aspect of the underwater noise studies since the majority of fish perceives the sound in terms of vibration of particles, and only a few as the pressure. Using synchronous recording of underwater and airborne noises, the mutual correlation of these data was investigated. The obtained correlation levels between underwater and airborne noises produced by hovercraft can be used for simplified estimation of the upper boundary of underwater noise level by measuring levels of airborne noise. The measured and estimated maximal levels of underwater noises of hovercraft are considerably lower than noises from conventional vessels with underwater engines, that makes hovercraft attractive alternative for use in locations with high underwater noise requirements, such as Ural River estuary and Caspian Sea shelf.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case",
    "url": "http://arxiv.org/abs/2401.02278v1",
    "authors": [
      "Febrian Kurniawan",
      "Gandeva Bayu Satrya",
      "Firuz Kamalov"
    ],
    "published": "2024-01-04",
    "abstract": "The enormous demand for seafood products has led to exploitation of marine resources and near-extinction of some species. In particular, overfishing is one the main issues in sustainable marine development. In alignment with the protection of marine resources and sustainable fishing, this study proposes to advance fish classification techniques that support identifying protected fish species using state-of-the-art machine learning. We use a custom modification of the MobileNet model to design a lightweight classifier called M-MobileNet that is capable of running on limited hardware. As part of the study, we compiled a labeled dataset of 37,462 images of fish found in the waters of the Indonesian archipelago. The proposed model is trained on the dataset to classify images of the captured fish into their species and give recommendations on whether they are consumable or not. Our modified MobileNet model uses only 50\\% of the top layer parameters with about 42% GTX 860M utility and achieves up to 97% accuracy in fish classification and determining its consumability. Given the limited computing capacity available on many fishing vessels, the proposed model provides a practical solution to on-site fish classification. In addition, synchronized implementation of the proposed model on multiple vessels can supply valuable information about the movement and location of different species of fish.",
    "categories": [
      "ship_trajectories"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Chlorophyll-a Mapping and Prediction in the Mar Menor Lagoon Using C2RCC-Processed Sentinel 2 Imagery",
    "url": "http://arxiv.org/abs/2510.09736v1",
    "authors": [
      "Antonio Mart\u00ednez-Ibarra",
      "Aurora Gonz\u00e1lez-Vidal",
      "Adri\u00e1n C\u00e1novas-Rodr\u00edguez",
      "Antonio F. Skarmeta"
    ],
    "published": "2025-10-10",
    "abstract": "The Mar Menor, Europe's largest coastal lagoon, located in Spain, has undergone severe eutrophication crises. Monitoring chlorophyll-a (Chl-a) is essential to anticipate harmful algal blooms and guide mitigation. Traditional in situ measurements are spatially and temporally limited. Satellite-based approaches provide a more comprehensive view, enabling scalable, long-term, and transferable monitoring. This study aims to overcome limitations of chlorophyll monitoring, often restricted to surface estimates or limited temporal coverage, by developing a reliable methodology to predict and map Chl-a across the water column of the Mar Menor. The work integrates Sentinel 2 imagery with buoy-based ground truth to create models capable of high-resolution, depth-specific monitoring, enhancing early-warning capabilities for eutrophication. Nearly a decade of Sentinel 2 images was atmospherically corrected using C2RCC processors. Buoy data were aggregated by depth (0-1 m, 1-2 m, 2-3 m, 3-4 m). Multiple ML and DL algorithms-including RF, XGBoost, CatBoost, Multilater Perceptron Networks, and ensembles-were trained and validated using cross-validation. Systematic band-combination experiments and spatial aggregation strategies were tested to optimize prediction. Results show depth-dependent performance. At the surface, C2X-Complex with XGBoost and ensemble models achieved R2 = 0.89; at 1-2 m, CatBoost and ensemble models reached R2 = 0.87; at 2-3 m, TOA reflectances with KNN performed best (R2 = 0.81); while at 3-4 m, RF achieved R2 = 0.66. Generated maps successfully reproduced known eutrophication events (e.g., 2016 crisis, 2025 surge), confirming robustness. The study delivers an end-to-end, validated methodology for depth-specific Chl-amapping. Its integration of multispectral band combinations, buoy calibration, and ML/DL modeling offers a transferable framework for other turbid coastal systems.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Branched Broomrape Detection in Tomato Farms Using Satellite Imagery and Time-Series Analysis",
    "url": "http://arxiv.org/abs/2509.10804v1",
    "authors": [
      "Mohammadreza Narimani",
      "Alireza Pourreza",
      "Ali Moghimi",
      "Parastoo Farajpoor",
      "Hamid Jafarbiglu",
      "Mohsen Mesgaran"
    ],
    "published": "2025-09-13",
    "abstract": "Branched broomrape (Phelipanche ramosa (L.) Pomel) is a chlorophyll-deficient parasitic plant that threatens tomato production by extracting nutrients from the host, with reported yield losses up to 80 percent. Its mostly subterranean life cycle and prolific seed production (more than 200,000 seeds per plant, viable for up to 20 years) make early detection essential. We present an end-to-end pipeline that uses Sentinel-2 imagery and time-series analysis to identify broomrape-infested tomato fields in California. Regions of interest were defined from farmer-reported infestations, and images with less than 10 percent cloud cover were retained. We processed 12 spectral bands and sun-sensor geometry, computed 20 vegetation indices (e.g., NDVI, NDMI), and derived five plant traits (Leaf Area Index, Leaf Chlorophyll Content, Canopy Chlorophyll Content, Fraction of Absorbed Photosynthetically Active Radiation, and Fractional Vegetation Cover) using a neural network calibrated with ground-truth and synthetic data. Trends in Canopy Chlorophyll Content delineated transplanting-to-harvest periods, and phenology was aligned using growing degree days. Vegetation pixels were segmented and used to train a Long Short-Term Memory (LSTM) network on 18,874 pixels across 48 growing-degree-day time points. The model achieved 88 percent training accuracy and 87 percent test accuracy, with precision 0.86, recall 0.92, and F1 0.89. Permutation feature importance ranked NDMI, Canopy Chlorophyll Content, FAPAR, and a chlorophyll red-edge index as most informative, consistent with the physiological effects of infestation. Results show the promise of satellite-driven time-series modeling for scalable detection of parasitic stress in tomato farms.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Generalization performance of neural mapping schemes for the space-time interpolation of satellite-derived ocean colour datasets",
    "url": "http://arxiv.org/abs/2503.11588v1",
    "authors": [
      "Thi Thuy Nga Nguyen",
      "Cl\u00e9ment Dorffer",
      "Fr\u00e9d\u00e9ric Jourdin",
      "Ronan Fablet"
    ],
    "published": "2025-03-14",
    "abstract": "Neural mapping schemes have become appealing approaches to deliver gap-free satellite-derived products for sea surface tracers. The generalization performance of these learning-based approaches naturally arises as a key challenge. This is particularly true for satellite-derived ocean colour products given the variety of bio-optical variables of interest, as well as the diversity of processes and scales involved. Considering region-specific and parameter-specific neural mapping schemes will result in substantial training costs. This study addresses generalization performance of neural mapping schemes to deliver gap-free satellite-derived ocean colour products. We develop a comprehensive experimental framework using real multi-sensor ocean colour datasets for two regions (the Mediterranean Sea and the North Sea) and a representative set of bio-optical parameters (Chlorophyll-a concentration, suspended particulate matter concentration, particulate backscattering coefficient). We consider several neural mapping schemes, and we report excellent generalization performance across regions and bio-optical parameters without any fine-tuning using appropriate dataset-specific normalization procedures. We discuss further how these results provide new insights towards the large-scale deployment of neural schemes for the processing of satellite-derived ocean colour datasets beyond case-study-specific demonstrations.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Bayesian hierarchical framework for fusion of remote sensing data: An example with solar-induced fluorescence",
    "url": "http://arxiv.org/abs/2503.03901v1",
    "authors": [
      "Manju Johny",
      "Jonathan Hobbs",
      "Vineet Yadav",
      "Margaret Johnson",
      "Nicholas Parazoo",
      "Hai Nguyen",
      "Amy Braverman"
    ],
    "published": "2025-03-05",
    "abstract": "Solar-induced chlorophyll fluorescence (SIF) has emerged as an effective indicator of vegetation productivity and plant health. The global quantification of SIF and its associated uncertainties yields many important capabilities, including improving carbon flux estimation, improving the identification of carbon sources and sinks, monitoring a variety of ecosystems, and evaluating carbon sequestration efforts. Long-term, regional-to-global scale monitoring is now feasible with the availability of SIF estimates from multiple Earth-observing satellites. These efforts can be aided by a rigorous accounting of the sources of uncertainty present in satellite SIF data products. In this paper, we introduce a Bayesian Hierarchical Model (BHM) for the estimation of SIF and associated uncertainties from Orbiting Carbon Observatory-2 (OCO-2) satellite observations at one-degree resolution with global coverage. The hierarchical structure of our modeling framework allows for convenient model specification, quantification of various sources of variation, and the incorporation of seasonal SIF information through Fourier terms in the regression model. The modeling framework leverages the predictable seasonality of SIF in most temperate land areas. The resulting data product complements existing atmospheric carbon dioxide estimates at the same spatio-temporal resolution.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Improving Water Quality Time-Series Prediction in Hong Kong using Sentinel-2 MSI Data and Google Earth Engine Cloud Computing",
    "url": "http://arxiv.org/abs/2408.14010v2",
    "authors": [
      "Rohin Sood",
      "Kevin Zhu"
    ],
    "published": "2024-08-26",
    "abstract": "Effective water quality monitoring in coastal regions is crucial due to the progressive deterioration caused by pollution and human activities. To address this, this study develops time-series models to predict chlorophyll-a (Chl-a), suspended solids (SS), and turbidity using Sentinel-2 satellite data and Google Earth Engine (GEE) in the coastal regions of Hong Kong. Leveraging Long Short-Term Memory (LSTM) Recurrent Neural Networks, the study incorporates extensive temporal datasets to enhance prediction accuracy. The models utilize spectral data from Sentinel-2, focusing on optically active components, and demonstrate that selected variables closely align with the spectral characteristics of Chl-a and SS. The results indicate improved predictive performance over previous methods, highlighting the potential for remote sensing technology in continuous and comprehensive water quality assessment.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "LSTM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Advancing Applications of Satellite Photogrammetry: Novel Approaches for Built-up Area Modeling and Natural Environment Monitoring using Stereo/Multi-view Satellite Image-derived 3D Data",
    "url": "http://arxiv.org/abs/2404.12487v1",
    "authors": [
      "Shengxi Gui"
    ],
    "published": "2024-04-18",
    "abstract": "With the development of remote sensing technology in recent decades, spaceborne sensors with sub-meter and meter spatial resolution (Worldview and PlanetScope) have achieved a considerable image quality to generate 3D geospatial data via a stereo matching pipeline. These achievements have significantly increased the data accessibility in 3D, necessitating adapting these 3D geospatial data to analyze human and natural environments. This dissertation explores several novel approaches based on stereo and multi-view satellite image-derived 3D geospatial data, to deal with remote sensing application issues for built-up area modeling and natural environment monitoring, including building model 3D reconstruction, glacier dynamics tracking, and lake algae monitoring. Specifically, the dissertation introduces four parts of novel approaches that deal with the spatial and temporal challenges with satellite-derived 3D data. The first study advances LoD-2 building modeling from satellite-derived Orthophoto and DSMs with a novel approach employing a model-driven workflow that generates building rectangular 3D geometry models. Secondly, we further enhanced our building reconstruction framework for dense urban areas and non-rectangular purposes, we implemented deep learning for unit-level segmentation and introduced a gradient-based circle reconstruction for circular buildings to develop a polygon composition technique for advanced building LoD2 reconstruction. Our third study utilizes high-spatiotemporal resolution PlanetScope satellite imagery for glacier tracking at 3D level in mid-latitude regions. Finally, we proposed a term as \"Algal Behavior Function\" to refine the quantification of chlorophyll-a concentrations from satellite imagery in water quality monitoring, addressing algae fluctuations and timing discrepancies between satellite observations and field measurements, thus enhancing the precision of underwater algae volume estimates. Overall, this dissertation demonstrates the extensive potential of satellite photogrammetry applications in addressing urban and environmental challenges. It further showcases innovative analytical methodologies that enhance the applicability of adapting stereo and multi-view very high-resolution satellite-derived 3D data. (See full abstract in the document)",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Segmentation",
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Seasonality of primary productivity affects coastal species more than its magnitude",
    "url": "http://arxiv.org/abs/2401.11289v1",
    "authors": [
      "Carlota Muniz",
      "Christopher McQuaid",
      "Nicolas Weidberg"
    ],
    "published": "2024-01-20",
    "abstract": "While the importance of extreme conditions is recognised, patterns in species abundances are often interpreted through average environmental conditions within their distributional range. For marine species with pelagic larvae, temperature and phytoplankton concentration are key variables. Along the south coast of South Africa, conspicuous spatial patterns in recruitment rates and the abundances of different mussel species exist, with focal areas characterized by large populations. We studied 15 years of sea surface temperature (SST) and chlorophyll-a (chl-a) satellite data, using spectral analyses to partition their temporal variability over ecologically relevant time periods, including seasonal (101 to 365 days) and intra-seasonal cycles (20 to 100 days). Adult cover and mussel recruitment were measured at 10 sites along the south coast and regression models showed that about 70 percent of the variability in recruitment and adult cover was explained by seasonal variability in chl-a, while mean annual chl-a and SST only explained 30 percent of the recruitment, with no significant effect for adult cover. SST and chl-a at two upwelling centres showed less predictable seasonal cycles during the second half of the study period with a significant cooling trend during austral autumn, coinciding with one of the mussel reproductive peaks. This likely reflects recent changes in the Agulhas Current, the world largest western boundary current, which affects coastal ecosystems by driving upwelling.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Fine scale depth regulation of invertebrate larvae around coastal fronts",
    "url": "http://arxiv.org/abs/2401.10303v1",
    "authors": [
      "Nicolas Weidberg",
      "Wayne Goschen",
      "Jennifer M. Jackson",
      "Paula Pattrick",
      "Christopher D. McQuaid",
      "Francesca Porri"
    ],
    "published": "2024-01-18",
    "abstract": "Vertical migrations of zooplankters have been widely described, but their active movements through shallow, highly dynamic water columns within the inner shelf may be more complex and difficult to characterize. In this study, invertebrate larvae, currents, and hydrographic variables were sampled at different depths during and after the presence of fronts on three different cruises off the southern coast of South Africa. Internal wave dynamics were observed in the hydrographic data set but also through satellite imagery, although strong surface convergent currents were absent and thermal stratification was weak. During the first two cruises, fronts were more conspicuous and they preceded strong onshore currents at depth which developed with the rising tide. Vertical distributions of larvae changed accordingly, with higher abundances at these deep layers once the front disappeared. The third cruise was carried out during slack tides, the front was not conspicuous, deep strong onshore currents did not occur afterward and larval distributions did not change consistently through time. Overall, the vertical distributions of many larval taxa matched the vertical profiles of shoreward currents and multivariate analyses revealed that these flows structured the larval community, which was neither influenced by temperature nor chlorophyll. Thus, the ability to regulate active vertical positioning may enhance shoreward advection and determine nearshore larval distributions.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Spatio-temporal Shared-Field Modeling of Beluga and Bowhead Whale Sightings Using a Joint Marked Log-Gaussian Cox Process",
    "url": "http://arxiv.org/abs/2512.06450v1",
    "authors": [
      "Mauli Pant",
      "Linda Fernandez",
      "Indranil Sahoo"
    ],
    "published": "2025-12-06",
    "abstract": "We analyze a decade of aerial survey whale sighting data (2010-2019) to model the spatio-temporal distributions and group sizes of beluga (Delphinapterus leucas) and bowhead (Balaena mysticetus) whales in the United States Arctic. To jointly model these species, we develop a multi-species Log-Gaussian Cox Process (LGCP) in which species specific intensity surfaces are linked through a shared latent spatial Gaussian field. This structure allows the model to capture broad spatial patterns common to both species while still accommodating species level responses to environmental covariates and seasonal variation. The latent field is represented using the Stochastic Partial Differential Equation (SPDE) approach with an anisotropic Matern covariance, implemented on an ocean constrained triangulated mesh so that spatial dependence aligns with marine geography. Whale group size is incorporated through a marked point process extension with species specific negative binomial marks, allowing occurrence and group sizes to be jointly analyzed within a unified framework. Inference is carried out using the Integrated Nested Laplace Approximation (INLA), enabling efficient model fitting over a decade of survey effort. The results highlight persistent multi-species hotspots and distinct environmental associations for each species, demonstrating the value of shared field LGCPs for joint species distribution modeling in data sparse and heterogeneous survey settings.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "The Future Orchid Diversity of Great Britain and Ireland using an SDM Approach",
    "url": "http://arxiv.org/abs/2511.01122v1",
    "authors": [
      "Sofoklis Mouratidis",
      "Konstantinos Kougioumoutzis",
      "Martha Charitonidou",
      "John M. Halley"
    ],
    "published": "2025-11-03",
    "abstract": "In this paper we use Species Distribution Models (SDMs) to forecast the future diversity and distribution of orchids in Great Britain and Ireland under scenarios of climate and land-use change. The study analyzes occurrence data for native orchid taxa in the BSBI database at a fine spatial resolution (1 km^2, monads) and incorporates multiple environmental variables including climate, land use, topography, and soil. These SDMs project significant losses in orchid species richness by 2050 and 2070, especially under severe climate and land-use scenarios, with declines expected across most species and regions, including Ireland where historical data previously indicated gains. The models reveal vulnerable species likely to face extinction by 2070, emphasizing the impact of both climate warming and habitat modifications. This approach differs from previous trend-based analyses by integrating future projections, high-resolution spatial data, and dynamic land-use scenarios, thereby providing higher-resolution estimates of orchid range contractions and diversity losses. While current observed orchid trends show some regional increases, particularly in Ireland, the SDM forecasts indicate substantial future risks. The study also discusses uncertainties due to niche truncation from geographic data limits and highlights the need for broader-scale modeling for more robust predictions. Overall, the paper anticipates conservation challenges for orchid biodiversity in response to ongoing environmental changes.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models",
    "url": "http://arxiv.org/abs/2510.19749v2",
    "authors": [
      "Catherine Villeneuve",
      "Benjamin Akera",
      "M\u00e9lisande Teng",
      "David Rolnick"
    ],
    "published": "2025-10-22",
    "abstract": "Species distribution models (SDMs), which aim to predict species occurrence based on environmental variables, are widely used to monitor and respond to biodiversity change. Recent deep learning advances for SDMs have been shown to perform well on complex and heterogeneous datasets, but their effectiveness remains limited by spatial biases in the data. In this paper, we revisit deep SDMs from a Bayesian perspective and introduce BATIS, a novel and practical framework wherein prior predictions are updated iteratively using limited observational data. Models must appropriately capture both aleatoric and epistemic uncertainty to effectively combine fine-grained local insights with broader ecological patterns. We benchmark an extensive set of uncertainty quantification approaches on a novel dataset including citizen science observations from the eBird platform. Our empirical study shows how Bayesian deep learning approaches can greatly improve the reliability of SDMs in data-scarce locations, which can contribute to ecological understanding and conservation efforts.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation",
    "url": "http://arxiv.org/abs/2510.19305v1",
    "authors": [
      "Chirag Padubidri",
      "Pranesh Velmurugan",
      "Andreas Lanitis",
      "Andreas Kamilaris"
    ],
    "published": "2025-10-22",
    "abstract": "Monitoring species distribution is vital for conservation efforts, enabling the assessment of environmental impacts and the development of effective preservation strategies. Traditional data collection methods, including citizen science, offer valuable insights but remain limited in coverage and completeness. Species Distribution Modelling (SDM) helps address these gaps by using occurrence data and environmental variables to predict species presence across large regions. In this study, we enhance SDM accuracy for frogs (Anura) by applying deep learning and data imputation techniques using data from the \"EY - 2022 Biodiversity Challenge.\" Our experiments show that data balancing significantly improved model performance, reducing the Mean Absolute Error (MAE) from 189 to 29 in frog counting tasks. Feature selection identified key environmental factors influencing occurrence, optimizing inputs while maintaining predictive accuracy. The multimodal ensemble model, integrating land cover, NDVI, and other environmental inputs, outperformed individual models and showed robust generalization across unseen regions. The fusion of image and tabular data improved both frog counting and habitat classification, achieving 84.9% accuracy with an AUC of 0.90. This study highlights the potential of multimodal learning and data preprocessing techniques such as balancing and imputation to improve predictive ecological modeling when data are sparse or incomplete, contributing to more precise and scalable biodiversity monitoring.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "A Single Index Approach to Integrated Species Distribution Modeling for Fisheries Abundance Data",
    "url": "http://arxiv.org/abs/2509.15379v1",
    "authors": [
      "Quan Vu",
      "Francis K. C. Hui",
      "A. H. Welsh",
      "Samuel Muller",
      "Eva Cantoni",
      "Christopher R. Haak"
    ],
    "published": "2025-09-18",
    "abstract": "In fisheries ecology, species abundance data are often collected by multiple surveys, each with unique characteristics. This article focuses on Atlantic sea scallop abundance data along the northeast coast of the United States, collected from two bottom trawl surveys which cover a larger spatial domain but have low catch efficiency, and a dredge survey which is more efficient but limited to domains where the species are believed to be present. To model such data, integrated species distribution models (ISDMs) have been proposed to incorporate information from multiple surveys, by including common environmental effects along with correlated survey-specific spatial fields. However, while flexible, these ISDMs can be susceptible to overfitting, which can complicate interpretability of the shared environmental effects and potentially lead to poor predictive performance. To overcome these drawbacks, we introduce a novel single index ISDM, built from a single index (with spatial random effects) that represents a latent measure of the true species distribution, and survey-specific catch efficiency functions which map the single index to the survey-specific expected catch. Our results show that the single index ISDM offers more meaningful interpretations of the environmental effects and survey catch efficiency differences, while potentially achieving better predictive performance than existing ISDMs.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Modelling species distributions using remote sensing predictors: Comparing Dynamic Habitat Index and LULC",
    "url": "http://arxiv.org/abs/2509.14862v1",
    "authors": [
      "Ma\u00efri Souza Oliveira",
      "Cl\u00e9mentine Pr\u00e9au",
      "Samuel Alleaume",
      "Maxime Lenormand",
      "Sandra Luque"
    ],
    "published": "2025-09-18",
    "abstract": "This study compares the predictive capacity of the Dynamic Habitat Index (DHI) - a remote sensing (RS)-based measure of habitat productivity and variability - against traditional land-use/land-cover (LULC) metrics in species distribution modelling (SDM) applications. RS and LULC-based SDMs were built using distribution data for eleven bird, amphibian, and mammal species in \u00cele-de-France. Predictor variables were derived from Sentinel-2 RS data and LULC classifications, with the latter incorporating Euclidean distance to habitat types. Ensemble SDMs were built using nine algorithms and evaluated with the Continuous Boyce Index (CBI) and a calibrated AUC. Habitat suitability scores and their binary transformations were assessed using niche overlap indices (Schoener, Warren, and Spearman rank correlation coefficient). Both RS and LULC approaches exhibited similar predictive accuracy overall. After binarisation however, the resulting niche maps diverged significantly. While LULC-based models exhibited spatial constraints (habitat suitability decreased as distance from recorded occurrences increased), RS-based models, which used continuous data, were not affected by geographic bias or distance effects. These results underscore the need to account for spatial biases in LULC-based SDMs. The DHI may offer a more spatially neutral alternative, making it a promising predictor for modelling species niches at regional scales.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Investigating Different Geo Priors for Image Classification",
    "url": "http://arxiv.org/abs/2508.15946v1",
    "authors": [
      "Angela Zhu",
      "Christian Lange",
      "Max Hamilton"
    ],
    "published": "2025-08-21",
    "abstract": "Species distribution models encode spatial patterns of species occurrence making them effective priors for vision-based species classification when location information is available. In this study, we evaluate various SINR (Spatial Implicit Neural Representations) models as a geographical prior for visual classification of species from iNaturalist observations. We explore the impact of different model configurations and adjust how we handle predictions for species not included in Geo Prior training. Our analysis reveals factors that contribute to the effectiveness of these models as Geo Priors, factors that may differ from making accurate range maps.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations",
    "url": "http://arxiv.org/abs/2508.06704v1",
    "authors": [
      "Hager Radi Abdelwahed",
      "M\u00e9lisande Teng",
      "Robin Zbinden",
      "Laura Pollock",
      "Hugo Larochelle",
      "Devis Tuia",
      "David Rolnick"
    ],
    "published": "2025-08-08",
    "abstract": "Species distribution models (SDMs) are widely used to predict species' geographic distributions, serving as critical tools for ecological research and conservation planning. Typically, SDMs relate species occurrences to environmental variables representing abiotic factors, such as temperature, precipitation, and soil properties. However, species distributions are also strongly influenced by biotic interactions with other species, which are often overlooked. While some methods partially address this limitation by incorporating biotic interactions, they often assume symmetrical pairwise relationships between species and require consistent co-occurrence data. In practice, species observations are sparse, and the availability of information about the presence or absence of other species varies significantly across locations. To address these challenges, we propose CISO, a deep learning-based method for species distribution modeling Conditioned on Incomplete Species Observations. CISO enables predictions to be conditioned on a flexible number of species observations alongside environmental variables, accommodating the variability and incompleteness of available biotic data. We demonstrate our approach using three datasets representing different species groups: sPlotOpen for plants, SatBird for birds, and a new dataset, SatButterfly, for butterflies. Our results show that including partial biotic information improves predictive performance on spatially separate test sets. When conditioned on a subset of species within the same dataset, CISO outperforms alternative methods in predicting the distribution of the remaining species. Furthermore, we show that combining observations from multiple datasets can improve performance. CISO is a promising ecological tool, capable of incorporating incomplete biotic information and identifying potential interactions between species from disparate taxa.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Uncovering symmetric and asymmetric species associations from community and environmental data",
    "url": "http://arxiv.org/abs/2507.09317v1",
    "authors": [
      "Sara Si-Moussi",
      "Esther Galbrun",
      "Mickael Hedde",
      "Giovanni Poggiato",
      "Matthias Rohr",
      "Wilfried Thuiller"
    ],
    "published": "2025-07-12",
    "abstract": "There is no much doubt that biotic interactions shape community assembly and ultimately the spatial co-variations between species. There is a hope that the signal of these biotic interactions can be observed and retrieved by investigating the spatial associations between species while accounting for the direct effects of the environment. By definition, biotic interactions can be both symmetric and asymmetric. Yet, most models that attempt to retrieve species associations from co-occurrence or co-abundance data internally assume symmetric relationships between species. Here, we propose and validate a machine-learning framework able to retrieve bidirectional associations by analyzing species community and environmental data.\n  Our framework (1) models pairwise species associations as directed influences from a source to a target species, parameterized with two species-specific latent embeddings: the effect of the source species on the community, and the response of the target species to the community; and (2) jointly fits these associations within a multi-species conditional generative model with different modes of interactions between environmental drivers and biotic associations. Using both simulated and empirical data, we demonstrate the ability of our framework to recover known asymmetric and symmetric associations and highlight the properties of the learned association networks. By comparing our approach to other existing models such as joint species distribution models and probabilistic graphical models, we show its superior capacity at retrieving symmetric and asymmetric interactions. The framework is intuitive, modular and broadly applicable across various taxonomic groups.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Digging deeper: deep joint species distribution modeling reveals environmental drivers of Earthworm Communities",
    "url": "http://arxiv.org/abs/2506.13568v1",
    "authors": [
      "Sara Si-moussi",
      "Wilfried Thuiller",
      "Esther Galbrun",
      "Thibaud Deca\u00ebns",
      "Sylvain G\u00e9rard",
      "Daniel F. March\u00e1n",
      "Claire Marsden",
      "Yvan Capowiez",
      "Micka\u00ebl Hedde"
    ],
    "published": "2025-06-16",
    "abstract": "Earthworms are key drivers of soil function, influencing organic matter turnover, nutrient cycling, and soil structure. Understanding the environmental controls on their distribution is essential for predicting the impacts of land use and climate change on soil ecosystems. While local studies have identified abiotic drivers of earthworm communities, broad-scale spatial patterns remain underexplored.\n  We developed a multi-species, multi-task deep learning model to jointly predict the distribution of 77 earthworm species across metropolitan France, using historical (1960-1970) and contemporary (1990-2020) records. The model integrates climate, soil, and land cover variables to estimate habitat suitability. We applied SHapley Additive exPlanations (SHAP) to identify key environmental drivers and used species clustering to reveal ecological response groups.\n  The joint model achieved high predictive performance (TSS >= 0.7) and improved predictions for rare species compared to traditional species distribution models. Shared feature extraction across species allowed for more robust identification of common and contrasting environmental responses. Precipitation variability, temperature seasonality, and land cover emerged as dominant predictors of earthworm distribution. Species clustering revealed distinct ecological strategies tied to climatic and land use gradients.\n  Our study advances both the methodological and ecological understanding of soil biodiversity. We demonstrate the utility of interpretable deep learning approaches for large-scale soil fauna modeling and provide new insights into earthworm habitat specialization. These findings support improved soil biodiversity monitoring and conservation planning in the face of global environmental change.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "ROC Curves for Spatial Point Patterns and Presence-Absence Data",
    "url": "http://arxiv.org/abs/2506.03414v1",
    "authors": [
      "Adrian Baddeley",
      "Ege Rubak",
      "Suman Rakshit",
      "Gopalan Nair"
    ],
    "published": "2025-06-03",
    "abstract": "Receiver Operating Characteristic (ROC) curves have recently been used to evaluate the performance of models for spatial presence-absence or presence-only data. Applications include species distribution modelling and mineral prospectivity analysis. We clarify the interpretation of the ROC curve in this context. Contrary to statements in the literature, ROC does not measure goodness-of-fit of a spatial model, and its interpretation as a measure of predictive ability is weak; it is a measure of ranking ability, insensitive to the precise form of the model. To gain insight we draw connections between ROC and existing statistical techniques for spatial point pattern data. The area under the ROC curve (AUC) is related to hypothesis tests of the null hypothesis that the explanatory variables have no effect. The shape of the ROC curve has a diagnostic interpretation. This suggests several new techniques, which extend the scope of application of ROC curves for spatial data, to support variable selection and model selection, analysis of segregation between different types of points, adjustment for a baseline, and analysis of spatial case-control data. The new techniques are illustrated with several real example datasets. Open source R code implementing the techniques is available in the development version of our package spatstat [Baddeley and Turner, 2005, Baddeley et al., 2015] and will be included in the next public release.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Modelling benthic animals in space and time using Bayesian Point Process with cross validation: the case of Holoturians",
    "url": "http://arxiv.org/abs/2506.01763v2",
    "authors": [
      "Daniele Poggio",
      "Gian Mario Sangiovanni",
      "Gianluca Mastrantonio",
      "Giovanna Jona Lasinio",
      "Edoardo Casoli",
      "Stefano Moro",
      "Daniele Ventura"
    ],
    "published": "2025-06-02",
    "abstract": "Understanding the spatial distribution of Holothurians is an essential task for ecosystem monitoring and sustainable management, particularly in the Mediterranean habitats. However, species distribution modeling is often complicated by the presence-only nature of the data and heterogeneous sampling designs. This study develops a spatio-temporal framework based on Log-Gaussian Cox Processes to analyze Holothurians' positions collected across nine survey campaigns conducted from 2022 to 2024 near Giglio Island, Italy. The surveys combined high-resolution photogrammetry with diver-based visual censuses, leading to varying detection probabilities across habitats, especially within Posidonia oceanica meadows. We adopt a model with a shared spatial Gaussian process component to accommodate this complexity, accounting for habitat structure, environmental covariates, and temporal variability. Model estimation is performed using Integrated Nested Laplace Approximation. We evaluate the predictive performances of alternative model specifications through a novel k-fold cross-validation strategy for point processes, using the Continuous Ranked Probability Score. Our approach provides a flexible and computationally efficient framework for integrating heterogeneous presence-only data in marine ecology and comparing the predictive ability of alternative models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "GeoThinneR: An R Package for Efficient Spatial Thinning of Species Occurrences and Point Data",
    "url": "http://arxiv.org/abs/2505.07867v1",
    "authors": [
      "J. Mestre-Tom\u00e1s"
    ],
    "published": "2025-05-09",
    "abstract": "In this paper we present GeoThinneR, an R package for efficient and flexible spatial thinning of species occurrence data. Spatial thinning is a widely used preprocessing step in species distribution modeling (SDM) that can help reduce sampling bias, but existing R implementations rely on brute-force algorithms that scale poorly with large datasets. GeoThinneR implements multiple thinning approaches, including ensuring a minimum distance between points, subsampling points on a grid, and filtering based on decimal precision. To handle large datasets, it introduces two optimized algorithms based on local kd-trees and adaptive neighbor estimation, which greatly reduce memory usage and execution time. Additional functionalities such as group-wise thinning and point prioritization are included to facilitate its use in SDM workflows. We here provide performance benchmarks using both simulated and real-world data to demonstrate substantial performance improvements over existing tools.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GLOSSA: a user-friendly R Shiny application for Bayesian machine learning analysis of marine species distribution",
    "url": "http://arxiv.org/abs/2505.05862v1",
    "authors": [
      "J. Mestre-Tom\u00e1s",
      "A. Fuster-Alonso",
      "J. M. Bellido",
      "M. Coll"
    ],
    "published": "2025-05-09",
    "abstract": "Species distribution models (SDMs) are one of the most common statistical methods to assess species occupancy and geographic distribution patterns. With the increasing complexity of ecological data, many methodological approaches have been developed, often accessible through command-line interfaces or graphical user interfaces (GUIs). However, few species distribution modeling tools are designed to be well-documented, user-friendly, flexible, and reproducible.\n  Here we introduce GLOSSA, an open-source R package and Shiny app designed for species distribution modeling using species occurrence and environmental data. GLOSSA's user-friendly interface guides users through steps including data uploading, processing, model fitting, spatial and temporal projections, and interactive visualization of results. The app also calculates variable importance, generates response curves with environmental variables, and performs cross-validation. At its core, GLOSSA modeling approach is based on Bayesian Additive Regression Trees (BART), an innovative machine learning method.\n  We present the functionality and versatility of GLOSSA through three case studies, addressing a range of ecological scenarios at regional and global scales. Along with comprehensive documentation, examples, and tutorials, these case studies illustrate how an intuitive graphical interface can make species distribution modeling accessible to a broad audience.\n  GLOSSA stands out as an easy-to-use tool for species distribution modeling, providing an intuitive interface, detailed documentation, flexible modeling, and interactive result exploration and export options. Additionally, its outputs can be used directly to inform marine ecosystem models (MEMs), enhancing its utility in ecological research and applications.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Mapping biodiversity at very-high resolution in Europe",
    "url": "http://arxiv.org/abs/2504.05231v1",
    "authors": [
      "C\u00e9sar Leblanc",
      "Lukas Picek",
      "Benjamin Deneu",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "R\u00e9mi Palard",
      "Alexis Joly"
    ],
    "published": "2025-04-07",
    "abstract": "This paper describes a cascading multimodal pipeline for high-resolution biodiversity mapping across Europe, integrating species distribution modeling, biodiversity indicators, and habitat classification. The proposed pipeline first predicts species compositions using a deep-SDM, a multimodal model trained on remote sensing, climate time series, and species occurrence data at 50x50m resolution. These predictions are then used to generate biodiversity indicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM designed for species-to-habitat mapping. With this approach, continental-scale species distribution maps, biodiversity indicator maps, and habitat maps are produced, providing fine-grained ecological insights. Unlike traditional methods, this framework enables joint modeling of interspecies dependencies, bias-aware training with heterogeneous presence-absence data, and large-scale inference from multi-source remote sensing inputs.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "LLM",
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Climplicit: Climatic Implicit Embeddings for Global Ecological Tasks",
    "url": "http://arxiv.org/abs/2504.05089v2",
    "authors": [
      "Johannes Dollinger",
      "Damien Robert",
      "Elena Plekhanova",
      "Lukas Drees",
      "Jan Dirk Wegner"
    ],
    "published": "2025-04-07",
    "abstract": "Deep learning on climatic data holds potential for macroecological applications. However, its adoption remains limited among scientists outside the deep learning community due to storage, compute, and technical expertise barriers. To address this, we introduce Climplicit, a spatio-temporal geolocation encoder pretrained to generate implicit climatic representations anywhere on Earth. By bypassing the need to download raw climatic rasters and train feature extractors, our model uses x3500 less disk space and significantly reduces computational needs for downstream tasks. We evaluate our Climplicit embeddings on biomes classification, species distribution modeling, and plant trait regression. We find that single-layer probing our Climplicit embeddings consistently performs better or on par with training a model from scratch on downstream tasks and overall better than alternative geolocation encoding models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Bayesian Deep Latent Class Regression",
    "url": "http://arxiv.org/abs/2503.17531v2",
    "authors": [
      "Yuren Zhou",
      "Yuqi Gu",
      "David B. Dunson"
    ],
    "published": "2025-03-21",
    "abstract": "High-dimensional categorical data arise in diverse scientific domains and are often accompanied by covariates. Latent class regression models are routinely used in such settings, reducing dimensionality by assuming conditional independence of the categorical variables given a single latent class that depends on covariates through a logistic regression model. However, such methods become unreliable as the dimensionality increases. To address this, we propose a flexible family of deep latent class models. Our model satisfies key theoretical properties, including identifiability and posterior consistency, and we establish a Bayes oracle clustering property that ensures robustness against the curse of dimensionality. We develop efficient posterior computation methods, validate them through simulation studies, and apply our model to joint species distribution modeling in ecology.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling",
    "url": "http://arxiv.org/abs/2503.13057v2",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Gencer Sumbul",
      "Chiara Vanalli",
      "Benjamin Kellenberger",
      "Devis Tuia"
    ],
    "published": "2025-03-17",
    "abstract": "Species Distribution Models (SDMs) play a vital role in biodiversity research, conservation planning, and ecological niche modeling by predicting species distributions based on environmental conditions. The selection of predictors is crucial, strongly impacting both model accuracy and how well the predictions reflect ecological patterns. To ensure meaningful insights, input variables must be carefully chosen to match the study objectives and the ecological requirements of the target species. However, existing SDMs, including both traditional and deep learning-based approaches, often lack key capabilities for variable selection: (i) flexibility to choose relevant predictors at inference without retraining; (ii) robustness to handle missing predictor values without compromising accuracy; and (iii) explainability to interpret and accurately quantify each predictor's contribution. To overcome these limitations, we introduce MaskSDM, a novel deep learning-based SDM that enables flexible predictor selection by employing a masked training strategy. This approach allows the model to make predictions with arbitrary subsets of input variables while remaining robust to missing data. It also provides a clearer understanding of how adding or removing a given predictor affects model performance and predictions. Additionally, MaskSDM leverages Shapley values for precise predictor contribution assessments, improving upon traditional approximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling the distributions of 12,738 plant species. Our results show that MaskSDM outperforms imputation-based methods and approximates models trained on specific subsets of variables. These findings underscore MaskSDM's potential to increase the applicability and adoption of SDMs, laying the groundwork for developing foundation models in SDMs that can be readily applied to diverse ecological applications.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Heterogeneous graph neural networks for species distribution modeling",
    "url": "http://arxiv.org/abs/2503.11900v3",
    "authors": [
      "Lauren Harrell",
      "Christine Kaeser-Chen",
      "Burcu Karagol Ayan",
      "Keith Anderson",
      "Michelangelo Conserva",
      "Elise Kleeman",
      "Maxim Neumann",
      "Matt Overlan",
      "Melissa Chapman",
      "Drew Purves"
    ],
    "published": "2025-03-14",
    "abstract": "Species distribution models (SDMs) are necessary for measuring and predicting occurrences and habitat suitability of species and their relationship with environmental factors. We introduce a novel presence-only SDM with graph neural networks (GNN). In our model, species and locations are treated as two distinct node sets, and the learning task is predicting detection records as the edges that connect locations to species. Using GNN for SDM allows us to model fine-grained interactions between species and the environment. We evaluate the potential of this methodology on the six-region dataset compiled by National Center for Ecological Analysis and Synthesis (NCEAS) for benchmarking SDMs. For each of the regions, the heterogeneous GNN model is comparable to or outperforms previously-benchmarked single-species SDMs as well as a feed-forward neural network baseline model.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "GNN",
      "SDM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling",
    "url": "http://arxiv.org/abs/2502.03480v1",
    "authors": [
      "Diana Koldasbayeva",
      "Alexey Zaytsev"
    ],
    "published": "2025-01-27",
    "abstract": "Species Distribution Models (SDMs) often suffer from spatial autocorrelation (SAC), leading to biased performance estimates. We tested cross-validation (CV) strategies - random splits, spatial blocking with varied distances, environmental (ENV) clustering, and a novel spatio-temporal method - under two proposed training schemes: LAST FOLD, widely used in spatial CV at the cost of data loss, and RETRAIN, which maximizes data usage but risks reintroducing SAC. LAST FOLD consistently yielded lower errors and stronger correlations. Spatial blocking at an optimal distance (SP 422) and ENV performed best, achieving Spearman and Pearson correlations of 0.485 and 0.548, respectively, although ENV may be unsuitable for long-term forecasts involving major environmental shifts. A spatio-temporal approach yielded modest benefits in our moderately variable dataset, but may excel with stronger temporal changes. These findings highlight the need to align CV approaches with the spatial and temporal structure of SDM data, ensuring rigorous validation and reliable predictive outcomes.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Applying the maximum entropy principle to neural networks enhances multi-species distribution models",
    "url": "http://arxiv.org/abs/2412.19217v3",
    "authors": [
      "Maxime Ryckewaert",
      "Diego Marcos",
      "Christophe Botella",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2024-12-26",
    "abstract": "The rapid expansion of citizen science initiatives has led to a significant growth of biodiversity databases, and particularly presence-only (PO) observations. PO data are invaluable for understanding species distributions and their dynamics, but their use in a Species Distribution Model (SDM) is curtailed by sampling biases and the lack of information on absences. Poisson point processes are widely used for SDMs, with Maxent being one of the most popular methods. Maxent maximises the entropy of a probability distribution across sites as a function of predefined transformations of variables, called features. In contrast, neural networks and deep learning have emerged as a promising technique for automatic feature extraction from complex input variables. Arbitrarily complex transformations of input variables can be learned from the data efficiently through backpropagation and stochastic gradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses neural networks to automatically learn shared features among species, using the maximum entropy principle. To do so, it employs a normalised Poisson loss where for each species, presence probabilities across sites are modelled by a neural network. We evaluate DeepMaxent on a benchmark dataset known for its spatial sampling biases, using PO data for calibration and presence-absence (PA) data for validation across six regions with different biological groups and covariates. Our results indicate that DeepMaxent performs better than Maxent and other leading SDMs across all regions and taxonomic groups. The method performs particularly well in regions of uneven sampling, demonstrating substantial potential to increase SDM performances. In particular, our approach yields more accurate predictions than traditional single-species models, which opens up new possibilities for methodological enhancement.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MiTREE: Multi-input Transformer Ecoregion Encoder for Species Distribution Modelling",
    "url": "http://arxiv.org/abs/2412.18995v1",
    "authors": [
      "Theresa Chen",
      "Yao-Yi Chiang"
    ],
    "published": "2024-12-25",
    "abstract": "Climate change poses an extreme threat to biodiversity, making it imperative to efficiently model the geographical range of different species. The availability of large-scale remote sensing images and environmental data has facilitated the use of machine learning in Species Distribution Models (SDMs), which aim to predict the presence of a species at any given location. Traditional SDMs, reliant on expert observation, are labor-intensive, but advancements in remote sensing and citizen science data have facilitated machine learning approaches to SDM development. However, these models often struggle with leveraging spatial relationships between different inputs -- for instance, learning how climate data should inform the data present in satellite imagery -- without upsampling or distorting the original inputs. Additionally, location information and ecological characteristics at a location play a crucial role in predicting species distribution models, but these aspects have not yet been incorporated into state-of-the-art approaches. In this work, we introduce MiTREE: a multi-input Vision-Transformer-based model with an ecoregion encoder. MiTREE computes spatial cross-modal relationships without upsampling as well as integrates location and ecological context. We evaluate our model on the SatBird Summer and Winter datasets, the goal of which is to predict bird species encounter rates, and we find that our approach improves upon state-of-the-art baselines.",
    "categories": [
      "geo_reasoning",
      "fish_plankton"
    ],
    "architectures": [
      "Transformer",
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Spatial Clustering of Citizen Science Data Improves Downstream Species Distribution Models",
    "url": "http://arxiv.org/abs/2412.15559v3",
    "authors": [
      "Nahian Ahmed",
      "Mark Roth",
      "Tyler A. Hallman",
      "W. Douglas Robinson",
      "Rebecca A. Hutchinson"
    ],
    "published": "2024-12-20",
    "abstract": "Citizen science biodiversity data present great opportunities for ecology and conservation across vast spatial and temporal scales. However, the opportunistic nature of these data lacks the sampling structure required by modeling methodologies that address a pervasive challenge in ecological data collection: imperfect detection, i.e., the likelihood of under-observing species on field surveys. Occupancy modeling is an example of an approach that accounts for imperfect detection by explicitly modeling the observation process separately from the biological process of habitat selection. This produces species distribution models that speak to the pattern of the species on a landscape after accounting for imperfect detection in the data, rather than the pattern of species observations corrupted by errors. To achieve this benefit, occupancy models require multiple surveys of a site across which the site's status (i.e., occupied or not) is assumed constant. Since citizen science data are not collected under the required repeated-visit protocol, observations may be grouped into sites post hoc. Existing approaches for constructing sites discard some observations and/or consider only geographic distance and not environmental similarity. In this study, we compare ten approaches for site construction in terms of their impact on downstream species distribution models for 31 bird species in Oregon, using observations recorded in the eBird database. We find that occupancy models built on sites constructed by spatial clustering algorithms perform better than existing alternatives.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Joint species distribution modeling of abundance data through latent variable barcodes",
    "url": "http://arxiv.org/abs/2412.08793v2",
    "authors": [
      "Braden Scherting",
      "Otso Ovaskainen",
      "David B. Dunson"
    ],
    "published": "2024-12-11",
    "abstract": "Accelerating global biodiversity loss has highlighted the role of complex relationships and shared patterns among species in determining their responses to environmental changes. The structure of an ecological community, represented by patterns of dependence among constituent species, signals its robustness more than individual species distributions. We focus on obtaining community-level insights based on underlying patterns in abundances of bird species in Finland. We propose \\texttt{barcode}, a modeling framework to infer latent binary and continuous features of samples and species, expanding the class of concurrent ordinations. This approach introduces covariates and spatial autocorrelation hierarchically to facilitate ecological interpretations of the learned features. By analyzing 132 bird species counts, we infer the dominant environmental drivers of the community, species clusters and regions of common profile. Three of the learned drivers correspond to distinct climactic regions with different dominant forest types. Three further drivers are spatially heterogeneous and signal urban, agricultural, and wetland areas, respectively.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "FathomVerse: A community science dataset for ocean animal discovery",
    "url": "http://arxiv.org/abs/2412.01701v1",
    "authors": [
      "Genevieve Patterson",
      "Joost Daniels",
      "Benjamin Woodward",
      "Kevin Barnard",
      "Giovanna Sainz",
      "Lonny Lundsten",
      "Kakani Katija"
    ],
    "published": "2024-12-02",
    "abstract": "Can computer vision help us explore the ocean? The ultimate challenge for computer vision is to recognize any visual phenomena, more than only the objects and animals humans encounter in their terrestrial lives. Previous datasets have explored everyday objects and fine-grained categories humans see frequently. We present the FathomVerse v0 detection dataset to push the limits of our field by exploring animals that rarely come in contact with people in the deep sea. These animals present a novel vision challenge.\n  The FathomVerse v0 dataset consists of 3843 images with 8092 bounding boxes from 12 distinct morphological groups recorded at two locations on the deep seafloor that are new to computer vision. It features visually perplexing scenarios such as an octopus intertwined with a sea star, and confounding categories like vampire squids and sea spiders. This dataset can push forward research on topics like fine-grained transfer learning, novel category discovery, species distribution modeling, and carbon cycle analysis, all of which are important to the care and husbandry of our planet.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Joint Spatiotemporal Modeling of Zooplankton and Whale Abundance in a Dynamic Marine Environment",
    "url": "http://arxiv.org/abs/2411.06001v1",
    "authors": [
      "Bokgyeong Kang",
      "Erin M. Schliep",
      "Alan E. Gelfand",
      "Christopher W. Clark",
      "Christine A. Hudak",
      "Charles A. Mayo",
      "Ryan Schosberg",
      "Tina M. Yack",
      "Robert S. Schick"
    ],
    "published": "2024-11-08",
    "abstract": "North Atlantic right whales are an endangered species; their entire population numbers approximately 372 individuals, and they are subject to major anthropogenic threats. They feed on zooplankton species whose distribution shifts in a dynamic and warming oceanic environment. Because right whales in turn follow their shifting food resource, it is necessary to jointly study the distribution of whales and their prey. The innovative joint species distribution modeling (JSDM) contribution here is different from anything in the large JDSM literature, reflecting the processes and data we have to work with. Specifically, our JSDM supplies a geostatistical model for expected amount of zooplankton collected at a site. We require a point pattern model for the intensity of right whale abundance. The two process models are joined through a latent conditional-marginal specification. Further, each species has two data sources to inform their respective distributions and these sources require novel data fusion. What emerges is a complex multi-level model. Through simulation we demonstrate the ability of our joint specification to identify model unknowns and learn better about the species distributions than modeling them individually. We then apply our modeling to real data from Cape Cod Bay, Massachusetts in the U.S.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Multi-Scale and Multimodal Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2411.04016v1",
    "authors": [
      "Nina van Tiel",
      "Robin Zbinden",
      "Emanuele Dalsasso",
      "Benjamin Kellenberger",
      "Lo\u00efc Pellissier",
      "Devis Tuia"
    ],
    "published": "2024-11-06",
    "abstract": "Species distribution models (SDMs) aim to predict the distribution of species by relating occurrence data with environmental variables. Recent applications of deep learning to SDMs have enabled new avenues, specifically the inclusion of spatial data (environmental rasters, satellite images) as model predictors, allowing the model to consider the spatial context around each species' observations. However, the appropriate spatial extent of the images is not straightforward to determine and may affect the performance of the model, as scale is recognized as an important factor in SDMs. We develop a modular structure for SDMs that allows us to test the effect of scale in both single- and multi-scale settings. Furthermore, our model enables different scales to be considered for different modalities, using a late fusion approach. Results on the GeoLifeCLEF 2023 benchmark indicate that considering multimodal data and learning multi-scale representations leads to more accurate models.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Hybrid Spatial Representations for Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2410.10937v2",
    "authors": [
      "Shiran Yuan",
      "Hao Zhao"
    ],
    "published": "2024-10-14",
    "abstract": "We address an important problem in ecology called Species Distribution Modeling (SDM), whose goal is to predict whether a species exists at a certain position on Earth. In particular, we tackle a challenging version of this task, where we learn from presence-only data in a community-sourced dataset, model a large number of species simultaneously, and do not use any additional environmental information. Previous work has used neural implicit representations to construct models that achieve promising results. However, implicit representations often generate predictions of limited spatial precision. We attribute this limitation to their inherently global formulation and inability to effectively capture local feature variations. This issue is especially pronounced with presence-only data and a large number of species. To address this, we propose a hybrid embedding scheme that combines both implicit and explicit embeddings. Specifically, the explicit embedding is implemented with a multiresolution hashgrid, enabling our models to better capture local information. Experiments demonstrate that our results exceed other works by a large margin on various standard benchmarks, and that the hybrid representation is better than both purely implicit and explicit ones. Qualitative visualizations and comprehensive ablation studies reveal that our hybrid representation successfully addresses the two main challenges. Our code is open-sourced at https://github.com/Shiran-Yuan/HSR-SDM.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MALPOLON: A Framework for Deep Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2409.18102v1",
    "authors": [
      "Theo Larcher",
      "Lukas Picek",
      "Benjamin Deneu",
      "Titouan Lorieul",
      "Maximilien Servajean",
      "Alexis Joly"
    ],
    "published": "2024-09-26",
    "abstract": "This paper describes a deep-SDM framework, MALPOLON. Written in Python and built upon the PyTorch library, this framework aims to facilitate training and inferences of deep species distribution models (deep-SDM) and sharing for users with only general Python language skills (e.g., modeling ecologists) who are interested in testing deep learning approaches to build new SDMs. More advanced users can also benefit from the framework's modularity to run more specific experiments by overriding existing classes while taking advantage of press-button examples to train neural networks on multiple classification tasks using custom or provided raw and pre-processed datasets. The framework is open-sourced on GitHub and PyPi along with extensive documentation and examples of use in various scenarios. MALPOLON offers straightforward installation, YAML-based configuration, parallel computing, multi-GPU utilization, baseline and foundational models for benchmarking, and extensive tutorials/documentation, aiming to enhance accessibility and performance scalability for ecologists and researchers.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Integrating systematic surveys with historical data to model the distribution of Ornithodoros turicata americanus, a vector of epidemiological concern in North America",
    "url": "http://arxiv.org/abs/2409.12761v1",
    "authors": [
      "Sebastian Botero-Canola",
      "Carson Torhorst",
      "Nicholas Canino",
      "Lorenza Beati",
      "Kathleen C. O Hara",
      "Angela M. James",
      "Samantha M. Wisely"
    ],
    "published": "2024-09-19",
    "abstract": "Globally, vector-borne diseases are increasing in distribution and frequency, affecting humans, domestic animals and livestock, and wildlife. Science-based management and prevention of these diseases requires a sound understanding of the distribution and environmental requirements of the vectors and hosts involved in disease transmission. Integrated Species Distribution Models (ISDM) account for diverse data types through hierarchical modeling and represent a significant advancement in species distribution modeling that have not yet been leveraged in disease ecology. We used this approach, as implemented in the recently developed R package RISDM, to assess the distribution of the soft tick subspecies Ornithodoros turicata americanus. We created an ISDM for O. t. americanus, using systematically collected field data and historical records of this tick species in the southeastern US, to predict its distribution and assess potential correlations with environmental variables. Given the novelty of this method, we compared the results to a conventional Maxent SDM and validated the results through data partitioning using true skills statistics (TSS), sensitivity, and area under the ROC curve (AUC) metrics. We found that a combination of climatic variables describing seasonality and temperature extremes, along with the amount of sand in the soil, determined the predicted intensity of occurrence of this tick species. When projected in geographic space, this distribution model predicted 62% of Florida as suitable habitat for this tick species. The ISDM presented a higher TSS and AUC than the Maxent conventional model, while sensitivity was similar between both models. Our case example shows the utility of ISDMs in disease ecology studies and highlights the broad range of geographic suitability for this important disease vector.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Trophic Cascades and Habitat Suitability in Udanti Sitnadi Tiger Reserve: Impacts of Prey Depletion and Climate Change on Predator Prey Dynamics",
    "url": "http://arxiv.org/abs/2409.00193v1",
    "authors": [
      "Krishnendu Basak",
      "Chiranjib Chaudhuri",
      "M Suraj",
      "Moiz Ahmed"
    ],
    "published": "2024-08-30",
    "abstract": "This study investigates the trophic cascades and habitat suitability in Udanti Sitnadi Tiger Reserve (USTR), highlighting the roles of apex predators, subordinate predators, and prey species in maintaining ecosystem balance. Using the Trophic Species Distribution Model (SDM), we explored prey-predator interactions and habitat suitability, revealing that tigers, due to prey depletion, increasingly rely on cattle, while leopards adapt by preying on smaller species. The study emphasizes the need for prey augmentation and habitat restoration to support apex predators. Additionally, climate change projections for 2021-2040 and 2081-2100 under CMIP6 scenarios SSP245 and SSP585 indicate significant regional habitat shifts, necessitating adaptive management strategies. Kuladighat is projected to face habitat contraction, while Sitanadi may experience habitat expansion. Effective conservation efforts such as habitat restoration, prey augmentation and predator recovery are the most important steps needed to maintain the purpose of a Tiger reserve and conservation potential of Udanti-Sonabeda Tiger Conservation Unit (TCU). To achieve these dynamics, focusing on community participation, anti-poaching measures, and scientific recommendations are the most crucial components to focus on. This comprehensive analysis underscores the critical role of targeted conservation activities in prey-depleted landscapes to ensure the long-term survival of tigers and the overall health of forest ecosystems, enhancing biodiversity and mitigating human-wildlife conflicts in USTR.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Dogs on forest trails; Understanding ecology of Striped Hyena and wild Canids in the presence of free-ranging dogs in Udanti-Sitanadi Tiger Reserve, Central India using Joint Distribution and Deep Neural Networks",
    "url": "http://arxiv.org/abs/2409.00185v1",
    "authors": [
      "Chiranjib Chaudhuri",
      "Krishnendu Basak",
      "M Suraj",
      "Moiz Ahmed",
      "Amit Kumar"
    ],
    "published": "2024-08-30",
    "abstract": "This study uses Joint Species Distribution Models (JSDMs) and Deep Neural Networks (DNNs) to explore how wild carnivores and free-ranging dogs interact in the Udanti-Sitanadi Tiger Reserve (USTR) in Central India. The research focuses on key species like the Striped Hyena, Grey Wolf, Golden Jackal, and Indian Fox, revealing significant overlaps in habitat with free-ranging dogs, especially in densely populated areas like the Sitanadi region of the tiger reserve. These overlaps pose serious risks to wildlife through competition for resources, predation, and the spread of diseases. The study shows that the Striped Hyena prefers gentle slopes and forested areas, while the Grey Wolf tends to avoid cropland and thrives in regions with higher rainfall that supports a stable prey base. The Golden Jackal, more adaptable than the others, favors west-facing slopes and stable temperatures, whereas the Indian Fox is mainly found in the less disturbed, mountainous Kuladighat region. Additionally, the study highlights the potential impacts of climate change, predicting that the Grey Wolf could face habitat extinction under more severe scenarios. These findings underscore the urgent need for conservation strategies tailored to address both dog wild carnivore interactions and the growing challenges posed by climate change, focusing on protecting the critical habitats of vulnerable species like the Striped Hyena and Grey Wolf.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "Deep Neural Network",
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Generating Binary Species Range Maps",
    "url": "http://arxiv.org/abs/2408.15956v1",
    "authors": [
      "Filip Dorm",
      "Christian Lange",
      "Scott Loarie",
      "Oisin Mac Aodha"
    ],
    "published": "2024-08-28",
    "abstract": "Accurately predicting the geographic ranges of species is crucial for assisting conservation efforts. Traditionally, range maps were manually created by experts. However, species distribution models (SDMs) and, more recently, deep learning-based variants offer a potential automated alternative. Deep learning-based SDMs generate a continuous probability representing the predicted presence of a species at a given location, which must be binarized by setting per-species thresholds to obtain binary range maps. However, selecting appropriate per-species thresholds to binarize these predictions is non-trivial as different species can require distinct thresholds. In this work, we evaluate different approaches for automatically identifying the best thresholds for binarizing range maps using presence-only data. This includes approaches that require the generation of additional pseudo-absence data, along with ones that only require presence data. We also propose an extension of an existing presence-only technique that is more robust to outliers. We perform a detailed evaluation of different thresholding techniques on the tasks of binary range estimation and large-scale fine-grained visual classification, and we demonstrate improved performance over existing pseudo-absence free approaches using our method.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GeoPlant: Spatial Plant Species Prediction Dataset",
    "url": "http://arxiv.org/abs/2408.13928v2",
    "authors": [
      "Lukas Picek",
      "Christophe Botella",
      "Maximilien Servajean",
      "C\u00e9sar Leblanc",
      "R\u00e9mi Palard",
      "Th\u00e9o Larcher",
      "Benjamin Deneu",
      "Diego Marcos",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "published": "2024-08-25",
    "abstract": "The difficulty of monitoring biodiversity at fine scales and over large areas limits ecological knowledge and conservation efforts. To fill this gap, Species Distribution Models (SDMs) predict species across space from spatially explicit features. Yet, they face the challenge of integrating the rich but heterogeneous data made available over the past decade, notably millions of opportunistic species observations and standardized surveys, as well as multimodal remote sensing data. In light of that, we have designed and developed a new European-scale dataset for SDMs at high spatial resolution (10--50m), including more than 10k species (i.e., most of the European flora). The dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive Presence-Absence survey records, all accompanied by diverse environmental rasters (e.g., elevation, human footprint, and soil) traditionally used in SDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10 m resolution, a 20-year time series of climatic variables, and satellite time series from the Landsat program. In addition to the data, we provide an openly accessible SDM benchmark (hosted on Kaggle), which has already attracted an active community and a set of strong baselines for single predictor/modality and multimodal approaches. All resources, e.g., the dataset, pre-trained models, and baseline methods (in the form of notebooks), are available on Kaggle, allowing one to start with our dataset literally with two mouse clicks.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Fast fitting of phylogenetic mixed-effects models",
    "url": "http://arxiv.org/abs/2408.05333v2",
    "authors": [
      "Bert van der Veen",
      "Robert Brian O'Hara"
    ],
    "published": "2024-08-09",
    "abstract": "Mixed-effects models are among the most commonly used statistical methods for the exploration of multispecies data. In recent years, also Joint Species Distribution Models and Generalized Linear Latent Variale Models have gained in popularity when the goal is to incorporate residual covariation between species that cannot be explained due to measured environmental covariates. Few software implementations of such models exist that can additionally incorporate phylogenetic information, and those that exist tend to utilize Markov chain Monte Carlo methods for estimation, so that model fitting takes a long time. In this article we develop new methods for quickly and flexibly fitting phylogenetic mixed-effects models, potentially incorporating residual covariation between species using latent variables, with the possibility to estimate the strength of phylogenetic structuring in species responses per environmental covariate, and while incorporating correlation between different covariate effects. By combining Variational approximations, a sparse approximation to the phylogenetic precision matrix, and parallel computation, phylogenetic mixed-effects models can be fitted much more quickly than the current state-of-the-art. Two simulation studies demonstrate that the proposed combination of approximations is fast and enjoys high accuracy. We explore sensitivity of the approximation to the ordering of species with a real world dataset of wood-decaying fungi.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Implication of modelling choices on connectivity estimation: A comparative analysis",
    "url": "http://arxiv.org/abs/2407.09564v1",
    "authors": [
      "Marie Soret",
      "Sylvain Moulherat",
      "Maxime Lenormand",
      "Sandra Luque"
    ],
    "published": "2024-07-05",
    "abstract": "We focus on connectivity methods used to understand and predict how landscapes and habitats facilitate or impede the movement and dispersal of species. Our objective is to compare the implication of methodological choices at three stages of the modelling framework: landscape characterisation, connectivity estimation, and connectivity assessment. What are the convergences and divergences of different modelling approaches? What are the implications of their combined results for landscape planning? We implemented two landscape characterisation approaches: expert opinion and species distribution model (SDM); four connectivity estimation models: Euclidean distance, least-cost paths (LCP), circuit theory, and stochastic movement simulation (SMS); and two connectivity indices: flux and area-weighted flux (dPCflux). We compared outcomes such as movement maps and habitat prioritisation for a rural landscape in southwestern France. Landscape characterisation is the main factor influencing connectivity assessment. The movement maps reflect the models' assumptions: LCP produced narrow beams reflecting the optimal pathways; whereas circuit theory and SMS produced wider estimation reflecting movement stochasticity, with SMS integrating behavioural drivers. The indices highlighted different aspects: dPCflux the surface of suitable habitats and flux their proximity. We recommend focusing on landscape characterisation before engaging further in the modelling framework. We emphasise the importance of stochasticity and behavioural drivers in connectivity, which can be reflected using circuit theory, SMS or other stochastic individual-based models. We stress the importance of using multiple indices to capture the multi-factorial aspect of connectivity.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning",
    "url": "http://arxiv.org/abs/2406.15658v3",
    "authors": [
      "Nemin Wu",
      "Qian Cao",
      "Zhangyu Wang",
      "Zeping Liu",
      "Yanlin Qi",
      "Jielu Zhang",
      "Joshua Ni",
      "Xiaobai Yao",
      "Hongxu Ma",
      "Lan Mu",
      "Stefano Ermon",
      "Tanuja Ganu",
      "Akshay Nambi",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "published": "2024-06-21",
    "abstract": "Spatial representation learning (SRL) aims at learning general-purpose neural network representations from various types of spatial data (e.g., points, polylines, polygons, networks, images, etc.) in their native formats. Learning good spatial representations is a fundamental problem for various downstream applications such as species distribution modeling, weather forecasting, trajectory generation, geographic question answering, etc. Even though SRL has become the foundation of almost all geospatial artificial intelligence (GeoAI) research, we have not yet seen significant efforts to develop an extensive deep learning framework and benchmark to support SRL model development and evaluation. To fill this gap, we propose TorchSpatial, a learning framework and benchmark for location (point) encoding, which is one of the most fundamental data types of spatial representation learning. TorchSpatial contains three key components: 1) a unified location encoding framework that consolidates 15 commonly recognized location encoders, ensuring scalability and reproducibility of the implementations; 2) the LocBench benchmark tasks encompassing 7 geo-aware image classification and 10 geo-aware image regression datasets; 3) a comprehensive suite of evaluation metrics to quantify geo-aware model's overall performance as well as their geographic bias, with a novel Geo-Bias Score metric. Finally, we provide a detailed analysis and insights into the model performance and geographic bias of different location encoders. We believe TorchSpatial will foster future advancement of spatial representation learning and spatial fairness in GeoAI research. The TorchSpatial model framework and LocBench benchmark are available at https://github.com/seai-lab/TorchSpatial, and the Geo-Bias Score evaluation framework is available at https://github.com/seai-lab/PyGBS.",
    "categories": [
      "geo_reasoning",
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Cumulant-based approximation for fast and efficient prediction for species distribution",
    "url": "http://arxiv.org/abs/2405.14456v1",
    "authors": [
      "Osamu Komori",
      "Yusuke Saigusa",
      "Shinto Eguchi",
      "Yasuhiro Kubota"
    ],
    "published": "2024-05-23",
    "abstract": "Species distribution modeling plays an important role in estimating the habitat suitability of species using environmental variables. For this purpose, Maxent and the Poisson point process are popular and powerful methods extensively employed across various ecological and biological sciences. However, the computational speed becomes prohibitively slow when using huge background datasets, which is often the case with fine-resolution data or global-scale estimations. To address this problem, we propose a computationally efficient species distribution model using a cumulant-based approximation (CBA) applied to the loss function of $\u03b3$-divergence. Additionally, we introduce a sequential estimating algorithm with an $L_1$ penalty to select important environmental variables closely associated with species distribution. The regularized geometric-mean method, derived from the CBA, demonstrates high computational efficiency and estimation accuracy. Moreover, by applying CBA to Maxent, we establish that Maxent and Fisher linear discriminant analysis are equivalent under a normality assumption. This equivalence leads to an highly efficient computational method for estimating species distribution. The effectiveness of our proposed methods is illustrated through simulation studies and by analyzing data on 226 species from the National Centre for Ecological Analysis and Synthesis and 709 Japanese vascular plant species. The computational efficiency of the proposed methods is significantly improved compared to Maxent, while maintaining comparable estimation accuracy. A R package {\\tt CBA} is also prepared to provide all programming codes used in simulation studies and real data analysis.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Comparison of Joint Species Distribution Models for Percent Cover Data",
    "url": "http://arxiv.org/abs/2403.11562v1",
    "authors": [
      "Pekka Korhonen",
      "Francis K. C. Hui",
      "Jenni Niku",
      "Sara Taskinen",
      "Bert van der Veen"
    ],
    "published": "2024-03-18",
    "abstract": "1. Joint species distribution models (JSDMs) have gained considerable traction among ecologists over the past decade, due to their capacity to answer a wide range of questions at both the species- and the community-level. The family of generalized linear latent variable models in particular has proven popular for building JSDMs, being able to handle many response types including presence-absence data, biomass, overdispersed and/or zero-inflated counts.\n  2. We extend latent variable models to handle percent cover data, with vegetation, sessile invertebrate, and macroalgal cover data representing the prime examples of such data arising in community ecology.\n  3. Sparsity is a commonly encountered challenge with percent cover data. Responses are typically recorded as percentages covered per plot, though some species may be completely absent or present, i.e., have 0% or 100% cover respectively, rendering the use of beta distribution inadequate.\n  4. We propose two JSDMs suitable for percent cover data, namely a hurdle beta model and an ordered beta model. We compare the two proposed approaches to a beta distribution for shifted responses, transformed presence-absence data, and an ordinal model for percent cover classes. Results demonstrate the hurdle beta JSDM was generally the most accurate at retrieving the latent variables and predicting ecological percent cover data.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Imbalance-aware Presence-only Loss Function for Species Distribution Modeling",
    "url": "http://arxiv.org/abs/2403.07472v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Marc Ru\u00dfwurm",
      "Devis Tuia"
    ],
    "published": "2024-03-12",
    "abstract": "In the face of significant biodiversity decline, species distribution models (SDMs) are essential for understanding the impact of climate change on species habitats by connecting environmental conditions to species occurrences. Traditionally limited by a scarcity of species observations, these models have significantly improved in performance through the integration of larger datasets provided by citizen science initiatives. However, they still suffer from the strong class imbalance between species within these datasets, often resulting in the penalization of rare species--those most critical for conservation efforts. To tackle this issue, this study assesses the effectiveness of training deep learning models using a balanced presence-only loss function on large citizen science-based datasets. We demonstrate that this imbalance-aware loss function outperforms traditional loss functions across various datasets and tasks, particularly in accurately modeling rare species with limited observations.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Infinite joint species distribution models",
    "url": "http://arxiv.org/abs/2402.13384v3",
    "authors": [
      "Federica Stolf",
      "David B. Dunson"
    ],
    "published": "2024-02-20",
    "abstract": "Joint species distribution models are popular in ecology for modeling covariate effects on species occurrence, while characterizing cross-species dependence. Data consist of multivariate binary indicators of the occurrences of different species in each sample, along with sample-specific covariates. A key problem is that current models implicitly assume that the list of species under consideration is predefined and finite, while for highly diverse groups of organisms, it is impossible to anticipate which species will be observed in a study and discovery of unknown species is common. This article proposes a new modeling paradigm for statistical ecology, which generalizes traditional multivariate probit models to accommodate large numbers of rare species and new species discovery. We discuss theoretical properties of the proposed modeling paradigm and implement efficient algorithms for posterior computation. Simulation studies and applications to fungal biodiversity data provide compelling support for the new modeling class.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Novel community data in ecology -- properties and prospects",
    "url": "http://arxiv.org/abs/2401.10860v1",
    "authors": [
      "Florian Hartig",
      "Nerea Abrego",
      "Alex Bush",
      "Jonathan M. Chase",
      "Gurutzeta Guillera-Arroita",
      "Mathew A. Leibold",
      "Otso Ovaskainen",
      "Lo\u00efc Pellissier",
      "Maximilian Pichler",
      "Giovanni Poggiato",
      "Laura Pollock",
      "Sara Si-Moussi",
      "Wilfried Thuiller",
      "Duarte S. Viana",
      "David I. Warton",
      "Damaris Zurell",
      "Douglas W. Yu"
    ],
    "published": "2024-01-19",
    "abstract": "New technologies for acquiring biological information such as eDNA, acoustic or optical sensors, make it possible to generate spatial community observations at unprecedented scales. The potential of these novel community data to standardize community observations at high spatial, temporal, and taxonomic resolution and at large spatial scale ('many rows and many columns') has been widely discussed, but so far, there has been little integration of these data with ecological models and theory. Here, we review these developments and highlight emerging solutions, focusing on statistical methods for analyzing novel community data, in particular joint species distribution models; the new ecological questions that can be answered with these data; and the potential implications of these developments for policy and conservation.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A sequential Monte Carlo algorithm for data assimilation problems in ecology",
    "url": "http://arxiv.org/abs/2401.06515v1",
    "authors": [
      "Kwaku Peprah Adjei",
      "Rob Cooke",
      "Nick Isaac",
      "Robert B. O'Hara"
    ],
    "published": "2024-01-12",
    "abstract": "1. Temporal trends in species distributions are necessary for monitoring changes in biodiversity, which aids policymakers and conservationists in making informed decisions. Dynamic species distribution models are often fitted to ecological time series data using Markov Chain Monte Carlo algorithms to produce these temporal trends. However, the fitted models can be time-consuming to produce and run, making it inefficient to refit them as new observations become available.\n  2. We propose an algorithm that updates model parameters and the latent state distribution (e.g. true occupancy) using the saved information from a previously fitted model. This algorithm capitalises on the strength of importance sampling to generate new posterior samples of interest by updating the model output. The algorithm was validated with simulation studies on linear Gaussian state space models and occupancy models, and we applied the framework to Crested Tits in Switzerland and Yellow Meadow Ants in the UK.\n  3. We found that models updated with the proposed algorithm captured the true model parameters and latent state values as good as the models refitted to the expanded dataset. Moreover, the updated models were much faster to run and preserved the trajectory of the derived quantities.\n  4. The proposed approach serves as an alternative to conventional methods for updating state-space models (SSMs), and it is most beneficial when the fitted SSMs have a long run time. Overall, we provide a Monte Carlo algorithm to efficiently update complex models, a key issue in developing biodiversity models and indicators.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts",
    "url": "http://arxiv.org/abs/2401.05470v1",
    "authors": [
      "Joaquim Estopinan",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "Fran\u00e7ois Munoz",
      "Alexis Joly"
    ],
    "published": "2024-01-10",
    "abstract": "The post-2020 global biodiversity framework needs ambitious, research-based targets. Estimating the accelerated extinction risk due to climate change is critical. The International Union for Conservation of Nature (IUCN) measures the extinction risk of species. Automatic methods have been developed to provide information on the IUCN status of under-assessed taxa. However, these compensatory methods are based on current species characteristics, mainly geographical, which precludes their use in future projections. Here, we evaluate a novel method for classifying the IUCN status of species benefiting from the generalisation power of species distribution models based on deep learning. Our method matches state-of-the-art classification performance while relying on flexible SDM-based features that capture species' environmental preferences. Cross-validation yields average accuracies of 0.61 for status classification and 0.78 for binary classification. Climate change will reshape future species distributions. Under the species-environment equilibrium hypothesis, SDM projections approximate plausible future outcomes. Two extremes of species dispersal capacity are considered: unlimited or null. The projected species distributions are translated into features feeding our IUCN classification method. Finally, trends in threatened species are analysed over time and i) by continent and as a function of average ii) latitude or iii) altitude. The proportion of threatened species is increasing globally, with critical rates in Africa, Asia and South America. Furthermore, the proportion of threatened species is predicted to peak around the two Tropics, at the Equator, in the lowlands and at altitudes of 800-1,500 m.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale",
    "url": "http://arxiv.org/abs/2401.04691v1",
    "authors": [
      "Joaquim Estopinan",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly",
      "Fran\u00e7ois Munoz"
    ],
    "published": "2024-01-09",
    "abstract": "Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution. We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island. Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. The highest level of threat is found at Madagascar and the neighbouring islands. In Sumatra, we found good correspondence of protected areas with our indicators, but supplementing current IUCN assessments with status predictions results in alarming levels of species threat across the island. Recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale. As an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning",
    "url": "http://arxiv.org/abs/2401.02989v1",
    "authors": [
      "Robin Zbinden",
      "Nina van Tiel",
      "Benjamin Kellenberger",
      "Lloyd Hughes",
      "Devis Tuia"
    ],
    "published": "2024-01-03",
    "abstract": "Species distribution modeling is a highly versatile tool for understanding the intricate relationship between environmental conditions and species occurrences. However, the available data often lacks information on confirmed species absence and is limited to opportunistically sampled, presence-only observations. To overcome this limitation, a common approach is to employ pseudo-absences, which are specific geographic locations designated as negative samples. While pseudo-absences are well-established for single-species distribution models, their application in the context of multi-species neural networks remains underexplored. Notably, the significant class imbalance between species presences and pseudo-absences is often left unaddressed. Moreover, the existence of different types of pseudo-absences (e.g., random and target-group background points) adds complexity to the selection process. Determining the optimal combination of pseudo-absences types is difficult and depends on the characteristics of the data, particularly considering that certain types of pseudo-absences can be used to mitigate geographic biases. In this paper, we demonstrate that these challenges can be effectively tackled by integrating pseudo-absences in the training of multi-species neural networks through modifications to the loss function. This adjustment involves assigning different weights to the distinct terms of the loss function, thereby addressing both the class imbalance and the choice of pseudo-absence types. Additionally, we propose a strategy to set these loss weights using spatial block cross-validation with presence-only data. We evaluate our approach using a benchmark dataset containing independent presence-absence data from six different regions and report improved results when compared to competing approaches.",
    "categories": [
      "fish_plankton"
    ],
    "architectures": [
      "SDM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Robust Geospatial Coordination of Multi-Agent Communications Networks Under Attrition",
    "url": "http://arxiv.org/abs/2512.02079v1",
    "authors": [
      "Jonathan S. Kent",
      "Eliana Stefani",
      "Brian K. Plancher"
    ],
    "published": "2025-11-30",
    "abstract": "Fast, efficient, robust communication during wildfire and other emergency responses is critical. One way to achieve this is by coordinating swarms of autonomous aerial vehicles carrying communications equipment to form an ad-hoc network connecting emergency response personnel to both each other and central command. However, operating in such extreme environments may lead to individual networking agents being damaged or rendered inoperable, which could bring down the network and interrupt communications.\n  To overcome this challenge and enable multi-agent UAV networking in difficult environments, this paper introduces and formalizes the problem of Robust Task Networking Under Attrition (RTNUA), which extends connectivity maintenance in multi-robot systems to explicitly address proactive redundancy and attrition recovery. We introduce Physics-Informed Robust Employment of Multi-Agent Networks ($\u03a6$IREMAN), a topological algorithm leveraging physics-inspired potential fields to solve this problem. Through simulation across 25 problem configurations, $\u03a6$IREMAN consistently outperforms the DCCRS baseline, and on large-scale problems with up to 100 tasks and 500 drones, maintains $>99.9\\%$ task uptime despite substantial attrition, demonstrating both effectiveness and scalability.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "PINN"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism",
    "url": "http://arxiv.org/abs/2511.17198v1",
    "authors": [
      "Kaiyu Li",
      "Jiayu Wang",
      "Zhi Wang",
      "Hui Qiao",
      "Weizhan Zhang",
      "Deyu Meng",
      "Xiangyong Cao"
    ],
    "published": "2025-11-21",
    "abstract": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
    "url": "http://arxiv.org/abs/2511.04464v1",
    "authors": [
      "Carnot Braun",
      "Rafael O. Jarczewski",
      "Gabriel U. Talasso",
      "Leandro A. Villas",
      "Allan M. de Souza"
    ],
    "published": "2025-11-06",
    "abstract": "Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL",
    "url": "http://arxiv.org/abs/2510.25997v1",
    "authors": [
      "Manu Redd",
      "Tao Zhe",
      "Dongjie Wang"
    ],
    "published": "2025-10-29",
    "abstract": "Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing access to structured data, allowing users to query databases without learning SQL. Yet existing systems struggle with realistic spatio-temporal queries, where success requires aligning vague user phrasing with schema-specific categories, handling temporal reasoning, and choosing appropriate outputs. We present an agentic pipeline that extends a naive text-to-SQL baseline (llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The agent can plan, decompose, and adapt queries through schema inspection, SQL generation, execution, and visualization tools. We evaluate on 35 natural-language queries over the NYC and Tokyo check-in dataset, covering spatial, temporal, and multi-dataset reasoning. The agent achieves substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and enhances usability through maps, plots, and structured natural-language summaries. Crucially, our design enables more natural human-database interaction, supporting users who lack SQL expertise, detailed schema knowledge, or prompting skill. We conclude that agentic orchestration, rather than stronger SQL generators alone, is a promising foundation for interactive geospatial assistants.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response",
    "url": "http://arxiv.org/abs/2510.12061v1",
    "authors": [
      "Yiheng Chen",
      "Lingyao Li",
      "Zihui Ma",
      "Qikai Hu",
      "Yilun Zhu",
      "Min Deng",
      "Runlong Yu"
    ],
    "published": "2025-10-14",
    "abstract": "Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany",
    "url": "http://arxiv.org/abs/2510.10640v1",
    "authors": [
      "Piyush Pant",
      "Marcellius William Suntoro",
      "Ayesha Siddiqua",
      "Muhammad Shehryaar Sharif",
      "Daniyal Ahmed"
    ],
    "published": "2025-10-12",
    "abstract": "This paper presents EA-GeoAI, an integrated framework for demand forecasting and equitable hospital planning in Germany through 2030. We combine district-level demographic shifts, aging population density, and infrastructure balances into a unified Equity Index. An interpretable Agentic AI optimizer then allocates beds and identifies new facility sites to minimize unmet need under budget and travel-time constraints. This approach bridges GeoAI, long-term forecasting, and equity measurement to deliver actionable recommendations for policymakers.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Context-Aware Visual Prompting: Automating Geospatial Web Dashboards with Large Language Models and Agent Self-Validation for Decision Support",
    "url": "http://arxiv.org/abs/2511.20656v1",
    "authors": [
      "Haowen Xu",
      "Jose Tupayachi",
      "Xiao-Ying Yu"
    ],
    "published": "2025-10-10",
    "abstract": "The development of web-based geospatial dashboards for risk analysis and decision support is often challenged by the difficulty in visualization of big, multi-dimensional environmental data, implementation complexity, and limited automation. We introduce a generative AI framework that harnesses Large Language Models (LLMs) to automate the creation of interactive geospatial dashboards from user-defined inputs including UI wireframes, requirements, and data sources. By incorporating a structured knowledge graph, the workflow embeds domain knowledge into the generation process and enable accurate and context-aware code completions. A key component of our approach is the Context-Aware Visual Prompting (CAVP) mechanism, which extracts encodes and interface semantics from visual layouts to guide LLM driven generation of codes. The new framework also integrates a self-validation mechanism that uses an agent-based LLM and Pass@k evaluation alongside semantic metrics to assure output reliability. Dashboard snippets are paired with data visualization codebases and ontological representations, enabling a pipeline that produces scalable React-based completions using the MVVM architectural pattern. Our results demonstrate improved performance over baseline approaches and expanded functionality over third party platforms, while incorporating multi-page, fully functional interfaces. We successfully developed a framework to implement LLMs, demonstrated the pipeline for automated code generation, deployment, and performed chain-of-thought AI agents in self-validation. This integrative approach is guided by structured knowledge and visual prompts, providing an innovative geospatial solution in enhancing risk analysis and decision making.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models",
    "url": "http://arxiv.org/abs/2509.21593v1",
    "authors": [
      "Peng Luo",
      "Xiayin Lou",
      "Yu Zheng",
      "Zhuo Zheng",
      "Stefano Ermon"
    ],
    "published": "2025-09-25",
    "abstract": "Geospatial modeling provides critical solutions for pressing global challenges such as sustainability and climate change. Existing large language model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at evolving generic code but lack the domain knowledge and multi-step reasoning required for complex geospatial problems. We introduce GeoEvolve, a multi-agent LLM framework that couples evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms. GeoEvolve operates in two nested loops: an inner loop leverages a code evolver to generate and mutate candidate solutions, while an outer agentic controller evaluates global elites and queries a GeoKnowRAG module -- a structured geospatial knowledge base that injects theoretical priors from geography. This knowledge-guided evolution steers the search toward theoretically meaningful and computationally efficient algorithms. We evaluate GeoEvolve on two fundamental and classical tasks: spatial interpolation (kriging) and spatial uncertainty quantification (geospatial conformal prediction). Across these benchmarks, GeoEvolve automatically improves and discovers new algorithms, incorporating geospatial theory on top of classical models. It reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution. These results demonstrate that GeoEvolve provides a scalable path toward automated, knowledge-driven geospatial modeling, opening new opportunities for trustworthy and efficient AI-for-Science discovery.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
    "url": "http://arxiv.org/abs/2509.19002v2",
    "authors": [
      "Hao Wang",
      "Eiki Murata",
      "Lingfang Zhang",
      "Ayako Sato",
      "So Fukuda",
      "Ziqi Yin",
      "Wentao Hu",
      "Keisuke Nakao",
      "Yusuke Nakamura",
      "Sebastian Zwirner",
      "Yi-Chia Chen",
      "Hiroyuki Otomo",
      "Hiroki Ouchi",
      "Daisuke Kawahara"
    ],
    "published": "2025-09-23",
    "abstract": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents",
    "url": "http://arxiv.org/abs/2509.18633v2",
    "authors": [
      "Yara Mohajerani"
    ],
    "published": "2025-09-23",
    "abstract": "Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems. We present a novel geospatial agent-based model that integrates climate hazard data with evolutionary learning for economic agents. Our framework combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviours that allow firms to evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation. We demonstrate the framework using riverine flood projections under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to converge with baseline (no hazard) production levels after decades of disruption due to climate stress. Our results reveal systemic risks where even agents that are not directly exposed to floods face impacts through supply chain disruptions, with the end-of-century average price of goods 5.6% higher under RCP8.5 compared to the baseline in our illustrative economic network. This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2509.13164v2",
    "authors": [
      "Jiawei Wang",
      "Haowei Sun",
      "Xintao Yan",
      "Shuo Feng",
      "Jun Gao",
      "Henry X. Liu"
    ],
    "published": "2025-09-16",
    "abstract": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires extensive and diverse data, particularly safety-critical events. Existing data are mostly generated from simulators with a significant sim-to-real gap or collected from on-road testing that is costly and unsafe. This paper presents TeraSim-World, an automated pipeline that synthesizes realistic and geographically diverse safety-critical data for E2E autonomous driving at anywhere in the world. Starting from an arbitrary location, TeraSim-World retrieves real-world maps and traffic demand from geospatial data sources. Then, it simulates agent behaviors from naturalistic driving datasets, and orchestrates diverse adversities to create corner cases. Informed by street views of the same location, it achieves photorealistic, geographically grounded sensor rendering via the frontier video generation model Cosmos-Drive. By bridging agent and sensor simulations, TeraSim-World provides a scalable and critical data synthesis framework for training and evaluation of E2E autonomous driving systems. Codes and videos are available at https://wjiawei.com/terasim-world-web/ .",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation",
    "url": "http://arxiv.org/abs/2509.08863v3",
    "authors": [
      "Qianqian Luo",
      "Qingming Lin",
      "Liuchang Xu",
      "Sensen Wu",
      "Ruichen Mao",
      "Chao Wang",
      "Hailin Feng",
      "Bo Huang",
      "Zhenhong Du"
    ],
    "published": "2025-09-10",
    "abstract": "Large Language Models (LLMs) have demonstrated substantial progress in task automation and natural language understanding. However, without domain expertise in geographic information science (GIS), they continue to encounter limitations including reduced accuracy and unstable performance when processing complex tasks. To address these challenges, we propose GeoJSON Agents-a novel multi-agent LLM architecture specifically designed for geospatial analysis. This framework transforms natural language instructions into structured GeoJSON operations through two LLM enhancement techniques: Function Calling and Code Generation. The architecture integrates three core components: task parsing, agent collaboration, and result integration. The Planner agent systematically decomposes user-defined tasks into executable subtasks, while Worker agents perform spatial data processing and analysis either by invoking predefined function APIs or by generating and executing Python-based analytical code. The system produces reusable, standards-compliant GeoJSON outputs through iterative refinement. To evaluate both approaches, we constructed a benchmark comprising 70 tasks spanning basic, intermediate, and advanced complexity levels, conducting experiments with OpenAI's GPT-4o as the core model. Results indicate that the Code Generation-based agent achieved 97.14% accuracy, while the Function Calling-based agent attained 85.71%-both significantly outperforming the best-performing general-purpose model (48.57%). Comparative analysis reveals Code Generation offers superior flexibility for complex, open-ended tasks, whereas Function Calling provides enhanced execution stability for structured operations. This study represents the first systematic integration of GeoJSON data with a multi-agent LLM framework and provides empirical evidence comparing two mainstream enhancement methodologies in geospatial context.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration",
    "url": "http://arxiv.org/abs/2509.05933v2",
    "authors": [
      "Md Hasebul Hasan",
      "Mahir Labib Dihan",
      "Tanzima Hashem",
      "Mohammed Eunus Ali",
      "Md Rizwan Parvez"
    ],
    "published": "2025-09-07",
    "abstract": "Agentic AI has significantly extended the capabilities of large language models (LLMs) by enabling complex reasoning and tool use. However, most existing frameworks are tailored to domains such as mathematics, coding, or web automation, and fall short on geospatial tasks that require spatial reasoning, multi-hop planning, and real-time map interaction. To address these challenges, we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with customized toolsets and agentic scaffolds for map-integrated geospatial reasoning. Unlike existing flat agent-based approaches that treat tools uniformly-often overwhelming the LLM when handling similar but subtly different geospatial APIs-MapAgent decouples planning from execution. A high-level planner decomposes complex queries into subgoals, which are routed to specialized modules. For tool-heavy modules-such as map-based services-we then design a dedicated map-tool agent that efficiently orchestrates related APIs adaptively in parallel to effectively fetch geospatial data relevant for the query, while simpler modules (e.g., solution generation or answer extraction) operate without additional agent overhead. This hierarchical design reduces cognitive load, improves tool selection accuracy, and enables precise coordination across similar APIs. We evaluate MapAgent on four diverse geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented and agentic baselines. We open-source our framwork at https://github.com/Hasebul/MapAgent.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries",
    "url": "http://arxiv.org/abs/2508.15752v1",
    "authors": [
      "Jon E. Froehlich",
      "Jared Hwang",
      "Zeyu Wang",
      "John S. O'Meara",
      "Xia Su",
      "William Huang",
      "Yang Zhang",
      "Alex Fiannaca",
      "Philip Nelson",
      "Shaun Kane"
    ],
    "published": "2025-08-21",
    "abstract": "Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement",
    "url": "http://arxiv.org/abs/2508.04080v1",
    "authors": [
      "Jinfan Tang",
      "Kunming Wu",
      "Ruifeng Gongxie",
      "Yuya He",
      "Yuankai Wu"
    ],
    "published": "2025-08-06",
    "abstract": "Recent studies have extended the application of large language models (LLMs) to geographic problems, revealing surprising geospatial competence even without explicit spatial supervision. However, LLMs still face challenges in spatial consistency, multi-hop reasoning, and geographic bias. To address these issues, we propose GeoSR, a self-refining agentic reasoning framework that embeds core geographic principles -- most notably Tobler's First Law of Geography -- into an iterative prediction loop. In GeoSR, the reasoning process is decomposed into three collaborating agents: (1) a variable-selection agent that selects relevant covariates from the same location; (2) a point-selection agent that chooses reference predictions at nearby locations generated by the LLM in previous rounds; and (3) a refine agent that coordinates the iterative refinement process by evaluating prediction quality and triggering further rounds when necessary. This agentic loop progressively improves prediction quality by leveraging both spatial dependencies and inter-variable relationships. We validate GeoSR on tasks ranging from physical-world property estimation to socioeconomic prediction. Experimental results show consistent improvements over standard prompting strategies, demonstrating that incorporating geostatistical priors and spatially structured reasoning into LLMs leads to more accurate and equitable geospatial predictions. The code of GeoSR is available at https://github.com/JinfanTang/GeoSR.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GeoFlow: Agentic Workflow Automation for Geospatial Tasks",
    "url": "http://arxiv.org/abs/2508.04719v1",
    "authors": [
      "Amulya Bhattaram",
      "Justin Chung",
      "Stanley Chung",
      "Ranit Gupta",
      "Janani Ramamoorthy",
      "Kartikeya Gullapalli",
      "Diana Marculescu",
      "Dimitrios Stamoulis"
    ],
    "published": "2025-08-05",
    "abstract": "We present GeoFlow, a method that automatically generates agentic workflows for geospatial tasks. Unlike prior work that focuses on reasoning decomposition and leaves API selection implicit, our method provides each agent with detailed tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow increases agentic success by 6.8% and reduces token usage by up to fourfold across major LLM families compared to state-of-the-art approaches.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis",
    "url": "http://arxiv.org/abs/2508.05666v1",
    "authors": [
      "Alejandro Godinez"
    ],
    "published": "2025-08-01",
    "abstract": "We present HySemRAG, a framework that combines Extract, Transform, Load (ETL) pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale literature synthesis and identify methodological research gaps. The system addresses limitations in existing RAG architectures through a multi-layered approach: hybrid retrieval combining semantic search, keyword filtering, and knowledge graph traversal; an agentic self-correction framework with iterative quality assurance; and post-hoc citation verification ensuring complete traceability. Our implementation processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis using modified Docling architecture, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. The system creates dual data products - a Neo4j knowledge graph enabling complex relationship queries and Qdrant vector collections supporting semantic search - serving as foundational infrastructure for verifiable information synthesis. Evaluation across 643 observations from 60 testing sessions demonstrates structured field extraction achieving 35.1% higher semantic similarity scores (0.655 $\\pm$ 0.178) compared to PDF chunking approaches (0.485 $\\pm$ 0.204, p < 0.000001). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy in validated responses. Applied to geospatial epidemiology literature on ozone exposure and cardiovascular disease, the system identifies methodological trends and research gaps, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Towards Urban Planing AI Agent in the Age of Agentic AI",
    "url": "http://arxiv.org/abs/2507.14730v4",
    "authors": [
      "Rui Liu",
      "Tao Zhe",
      "Zhong-Ren Peng",
      "Necati Catbas",
      "Xinyue Ye",
      "Dongjie Wang",
      "Yanjie Fu"
    ],
    "published": "2025-07-19",
    "abstract": "Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. Existing studies conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints and reshape automated urban design. We further identify critical gaps of existing generative urban planning studies: 1) the generative structure has to be predefined with strong assumption: all of adversarial generator-discriminator, forward and inverse diffusion structures, hierarchical zone-POI generative structure are predefined by humans; 2) ignore the power of domain expert developed tools: domain urban planners have developed various tools in the urban planning process guided by urban theory, while existing pure neural networks based generation ignore the power of the tools developed by urban planner practitioners. To address these limitations, we outline a future research direction agentic urban AI planner, calling for a new synthesis of agentic AI and participatory urbanism.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Start from the End: A Framework for Computational Policy Exploration to Inform Effective and Geospatially Consistent Interventions applied to COVID-19 in St. Louis",
    "url": "http://arxiv.org/abs/2507.10870v2",
    "authors": [
      "David O'Gara",
      "Matt Kasman",
      "Matthew D. Haslam",
      "Ross A. Hammond"
    ],
    "published": "2025-07-15",
    "abstract": "Mathematical models are a powerful tool to study infectious disease dynamics and intervention strategies against them in social systems. However, due to their detailed implementation and steep computational requirements, practitioners and stakeholders are typically only able to explore a small subset of all possible intervention scenarios, a severe limitation when preparing for disease outbreaks. In this work, we propose a parameter exploration framework utilizing emulator models to make uncertainty-aware predictions of high-dimensional parameter spaces and identify large numbers of feasible response strategies. We apply our framework to a case study of a large-scale agent-based disease model of the COVID-19 ``Omicron wave'' in St. Louis, Missouri that took place from December 2021 to February 2022. We identify large numbers of response strategies that would have been estimated to have reduced disease spread by a substantial amount. We also identify policy interventions that would have been able to reduce the geospatial variation in disease spread, which has additional implications for designing thoughtful response strategies.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "\"Hi AirStar, Guide Me to the Badminton Court.\"",
    "url": "http://arxiv.org/abs/2507.04430v1",
    "authors": [
      "Ziqin Wang",
      "Jinyu Chen",
      "Xiangyi Zheng",
      "Qinan Liao",
      "Linjiang Huang",
      "Si Liu"
    ],
    "published": "2025-07-06",
    "abstract": "Unmanned Aerial Vehicles, operating in environments with relatively few obstacles, offer high maneuverability and full three-dimensional mobility. This allows them to rapidly approach objects and perform a wide range of tasks often challenging for ground robots, making them ideal for exploration, inspection, aerial imaging, and everyday assistance. In this paper, we introduce AirStar, a UAV-centric embodied platform that turns a UAV into an intelligent aerial assistant: a large language model acts as the cognitive core for environmental understanding, contextual reasoning, and task planning. AirStar accepts natural interaction through voice commands and gestures, removing the need for a remote controller and significantly broadening its user base. It combines geospatial knowledge-driven long-distance navigation with contextual reasoning for fine-grained short-range control, resulting in an efficient and accurate vision-and-language navigation (VLN) capability.Furthermore, the system also offers built-in capabilities such as cross-modal question answering, intelligent filming, and target tracking. With a highly extensible framework, it supports seamless integration of new functionalities, paving the way toward a general-purpose, instruction-driven intelligent UAV agent. The supplementary PPT is available at \\href{https://buaa-colalab.github.io/airstar.github.io}{https://buaa-colalab.github.io/airstar.github.io}.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Tracking"
    ],
    "is_recent": false
  },
  {
    "title": "Generative AI as a Pillar for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning",
    "url": "http://arxiv.org/abs/2506.02485v2",
    "authors": [
      "Haowen Xu",
      "Sisi Zlatanova",
      "Ruiyu Liang",
      "Ismet Canbulat"
    ],
    "published": "2025-06-03",
    "abstract": "Wildfires increasingly threaten human life, ecosystems, and infrastructure, with events like the 2025 Palisades and Eaton fires in Los Angeles County underscoring the urgent need for more advanced prediction frameworks. Existing physics-based and deep learning models struggle to capture dynamic wildfire spread across both 2D and 3D domains, especially when incorporating real-time, multimodal geospatial data. This paper explores how generative Artificial Intelligence (AI) models-such as GANs, VAEs, and Transformers-can serve as transformative tools for wildfire prediction and simulation. These models offer superior capabilities in managing uncertainty, integrating multimodal inputs, and generating realistic, scalable wildfire scenarios. We introduce a new paradigm that leverages large language models (LLMs) for literature synthesis, classification, and knowledge extraction, conducting a systematic review of recent studies applying generative AI to fire prediction and monitoring. We highlight how generative approaches uniquely address challenges faced by traditional simulation and deep learning methods. Finally, we outline five key future directions for generative AI in wildfire management, including unified multimodal modeling of 2D and 3D dynamics, agentic AI systems and chatbots for decision intelligence, and real-time scenario generation on mobile devices, along with a discussion of critical challenges. Our findings advocate for a paradigm shift toward multimodal generative frameworks to support proactive, data-informed wildfire response.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer",
      "GAN",
      "Autoencoder",
      "LLM"
    ],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings",
    "url": "http://arxiv.org/abs/2506.00634v1",
    "authors": [
      "Adam Visokay",
      "Ruth Bagley",
      "Ian Kennedy",
      "Chris Hess",
      "Kyle Crowder",
      "Rob Voigt",
      "Denis Peskoff"
    ],
    "published": "2025-05-31",
    "abstract": "Rental listings offer a unique window into how urban space is socially constructed through language. We analyze Chicago Craigslist rental advertisements from 2018 to 2024 to examine how listing agents characterize neighborhoods, identifying mismatches between institutional boundaries and neighborhood claims. Through manual and large language model annotation, we classify unstructured listings from Craigslist according to their neighborhood. Geospatial analysis reveals three distinct patterns: properties with conflicting neighborhood designations due to competing spatial definitions, border properties with valid claims to adjacent neighborhoods, and ``reputation laundering\" where listings claim association with distant, desirable neighborhoods. Through topic modeling, we identify patterns that correlate with spatial positioning: listings further from neighborhood centers emphasize different amenities than centrally-located units. Our findings demonstrate that natural language processing techniques can reveal how definitions of urban spaces are contested in ways that traditional methods overlook.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "MapStory: Prototyping Editable Map Animations with LLM Agents",
    "url": "http://arxiv.org/abs/2505.21966v2",
    "authors": [
      "Aditya Gunturu",
      "Ben Pearman",
      "Keiichi Ihara",
      "Morteza Faraji",
      "Bryan Wang",
      "Rubaiat Habib Kazi",
      "Ryo Suzuki"
    ],
    "published": "2025-05-28",
    "abstract": "We introduce MapStory, an LLM-powered animation prototyping tool that generates editable map animation sequences directly from natural language text by leveraging a dual-agent LLM architecture. Given a user written script, MapStory automatically produces a scene breakdown, which decomposes the text into key map animation primitives such as camera movements, visual highlights, and animated elements. Our system includes a researcher agent that accurately queries geospatial information by leveraging an LLM with web search, enabling automatic extraction of relevant regions, paths, and coordinates while allowing users to edit and query for changes or additional information to refine the results. Additionally, users can fine-tune parameters of these primitive blocks through an interactive timeline editor. We detail the system's design and architecture, informed by formative interviews with professional animators and by an analysis of 200 existing map animation videos. Our evaluation, which includes expert interviews (N=5) and a usability study (N=12), demonstrates that MapStory enables users to create map animations with ease, facilitates faster iteration, encourages creative exploration, and lowers barriers to creating map-centric stories.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning",
    "url": "http://arxiv.org/abs/2505.21863v3",
    "authors": [
      "Shikhhar Siingh",
      "Abhinav Rawat",
      "Chitta Baral",
      "Vivek Gupta"
    ],
    "published": "2025-05-28",
    "abstract": "Publicly significant images from events hold valuable contextual information, crucial for journalism and education. However, existing methods often struggle to extract this relevance accurately. To address this, we introduce GETReason (Geospatial Event Temporal Reasoning), a framework that moves beyond surface-level image descriptions to infer deeper contextual meaning. We propose that extracting global event, temporal, and geospatial information enhances understanding of an image's significance. Additionally, we introduce GREAT (Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric for evaluating reasoning-based image understanding. Our layered multi-agent approach, assessed using a reasoning-weighted metric, demonstrates that meaningful insights can be inferred, effectively linking images to their broader event context.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "AI-in-the-Loop Planning for Transportation Electrification: Case Studies from Austin, Texas",
    "url": "http://arxiv.org/abs/2504.21185v2",
    "authors": [
      "Seung Jun Choi"
    ],
    "published": "2025-04-29",
    "abstract": "This study explores the integration of AI in transportation electrification planning in Austin, TX, focusing on the use of Geospatial AI (GeoAI), Generative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site selection, localized GenAI models support meta-level estimations, and LLMs enable scenario simulations. These AI applications require human oversight. GeoAI outputs must be evaluated with land use data, GenAI models are not always accurate, and LLMs are prone to hallucinations. To ensure accountable planning, human planners must work alongside AI agents. Establishing a community feedback loop is essential to audit automated decisions. Planners should place Community Experience (CX) at the center of Urban Planning AI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation",
    "url": "http://arxiv.org/abs/2504.09587v3",
    "authors": [
      "Haotian Xu",
      "Yue Hu",
      "Chen Gao",
      "Zhengqiu Zhu",
      "Yong Zhao",
      "Yong Li",
      "Quanjun Yin"
    ],
    "published": "2025-04-13",
    "abstract": "Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Geo-OLM: Enabling Sustainable Earth Observation Studies with Cost-Efficient Open Language Models & State-Driven Workflows",
    "url": "http://arxiv.org/abs/2504.04319v1",
    "authors": [
      "Dimitrios Stamoulis",
      "Diana Marculescu"
    ],
    "published": "2025-04-06",
    "abstract": "Geospatial Copilots hold immense potential for automating Earth observation (EO) and climate monitoring workflows, yet their reliance on large-scale models such as GPT-4o introduces a paradox: tools intended for sustainability studies often incur unsustainable costs. Using agentic AI frameworks in geospatial applications can amass thousands of dollars in API charges or requires expensive, power-intensive GPUs for deployment, creating barriers for researchers, policymakers, and NGOs. Unfortunately, when geospatial Copilots are deployed with open language models (OLMs), performance often degrades due to their dependence on GPT-optimized logic. In this paper, we present Geo-OLM, a tool-augmented geospatial agent that leverages the novel paradigm of state-driven LLM reasoning to decouple task progression from tool calling. By alleviating the workflow reasoning burden, our approach enables low-resource OLMs to complete geospatial tasks more effectively. When downsizing to small models below 7B parameters, Geo-OLM outperforms the strongest prior geospatial baselines by 32.8% in successful query completion rates. Our method performs comparably to proprietary models achieving results within 10% of GPT-4o, while reducing inference costs by two orders of magnitude from \\$500-\\$1000 to under \\$10. We present an in-depth analysis with geospatial downstream benchmarks, providing key insights to help practitioners effectively deploy OLMs for EO applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GIScience in the Era of Artificial Intelligence: A Research Agenda Towards Autonomous GIS",
    "url": "http://arxiv.org/abs/2503.23633v5",
    "authors": [
      "Zhenlong Li",
      "Huan Ning",
      "Song Gao",
      "Krzysztof Janowicz",
      "Wenwen Li",
      "Samantha T. Arundel",
      "Chaowei Yang",
      "Budhendra Bhaduri",
      "Shaowen Wang",
      "A-Xing Zhu",
      "Mark Gahegan",
      "Shashi Shekhar",
      "Xinyue Ye",
      "Grant McKenzie",
      "Guido Cervone",
      "Michael E. Hodgson"
    ],
    "published": "2025-03-31",
    "abstract": "The advent of generative AI exemplified by large language models (LLMs) opens new ways to represent and compute geographic information and transcends the process of geographic knowledge production, driving geographic information systems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core, autonomous GIS can independently generate and execute geoprocessing workflows to perform spatial analysis. In this vision paper, we further elaborate on the concept of autonomous GIS and present a conceptual framework that defines its five autonomous goals, five autonomous levels, five core functions, and three operational scales. We demonstrate how autonomous GIS could perform geospatial data retrieval, spatial analysis, and map making with four proof-of-concept GIS agents. We conclude by identifying critical challenges and future research directions, including fine-tuning and self-growing decision-cores, autonomous modeling, and examining the societal and practical implications of autonomous GIS. By establishing the groundwork for a paradigm shift in GIScience, this paper envisions a future where GIS moves beyond traditional workflows to autonomously reason, derive, innovate, and advance geospatial solutions to pressing global challenges. Meanwhile, as we design and deploy increasingly intelligent geospatial systems, we carry a responsibility to ensure they are developed in socially responsible ways, serve the public good, and support the continued value of human geographic insight in an AI-augmented future.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks",
    "url": "http://arxiv.org/abs/2503.18129v2",
    "authors": [
      "Varvara Krechetova",
      "Denis Kochedykov"
    ],
    "published": "2025-03-23",
    "abstract": "This paper establishes a benchmark for evaluating tool-calling capabilities of large language models (LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet 3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o, GPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23 geospatial functions. Our benchmark comprises tasks in four categories of increasing complexity, with both solvable and intentionally unsolvable tasks to test rejection accuracy. We develop a LLM-as-Judge evaluation framework to compare agent solutions against reference solutions. Results show o4-mini and Claude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1, GPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the last two are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due its preference to provide any solution rather than reject a task, proved to be less accurate. We observe significant differences in token usage, with Anthropic models consuming more tokens than competitors. Common errors include misunderstanding geometrical relationships, relying on outdated knowledge, and inefficient data manipulation. The resulting benchmark set, evaluation framework, and data generation pipeline are released as open-source resources (available at https://github.com/Solirinai/GeoBenchX), providing one more standardized method for the ongoing evaluation of LLMs for GeoAI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Towards a Barrier-free GeoQA Portal: Natural Language Interaction with Geospatial Data Using Multi-Agent LLMs and Semantic Search",
    "url": "http://arxiv.org/abs/2503.14251v1",
    "authors": [
      "Yu Feng",
      "Puzhen Zhang",
      "Guohui Xiao",
      "Linfang Ding",
      "Liqiu Meng"
    ],
    "published": "2025-03-18",
    "abstract": "A Barrier-Free GeoQA Portal: Enhancing Geospatial Data Accessibility with a Multi-Agent LLM Framework\n  Geoportals are vital for accessing and analyzing geospatial data, promoting open spatial data sharing and online geo-information management. Designed with GIS-like interaction and layered visualization, they often challenge non-expert users with complex functionalities and overlapping layers that obscure spatial relationships. We propose a GeoQA Portal using a multi-agent Large Language Model framework for seamless natural language interaction with geospatial data. Complex queries are broken into subtasks handled by specialized agents, retrieving relevant geographic data efficiently. Task plans are shown to users, boosting transparency. The portal supports default and custom data inputs for flexibility. Semantic search via word vector similarity aids data retrieval despite imperfect terms. Case studies, evaluations, and user tests confirm its effectiveness for non-experts, bridging GIS complexity and public access, and offering an intuitive solution for future geoportals.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Social Cyber Geographical Worldwide Inventory of Bots",
    "url": "http://arxiv.org/abs/2501.18839v1",
    "authors": [
      "Lynnette Hui Xian Ng",
      "Kathleen M. Carley"
    ],
    "published": "2025-01-31",
    "abstract": "Social Cyber Geography is the space in the digital cyber realm that is produced through social relations. Communication in the social media ecosystem happens not only because of human interactions, but is also fueled by algorithmically controlled bot agents. Most studies have not looked at the social cyber geography of bots because they focus on bot activity within a single country. Since creating a bot uses universal programming technology, bots, how prevalent are these bots throughout the world? To quantify bot activity worldwide, we perform a multilingual and geospatial analysis on a large dataset of social data collected from X during the Coronavirus pandemic in 2021. This pandemic affected most of the world, and thus is a common topic of discussion. Our dataset consists of ~100 mil posts generated by ~31mil users. Most bot studies focus only on English-speaking countries, because most bot detection algorithms are built for the English language. However, only 47\\% of the bots write in the English language. To accommodate multiple languages in our bot detection algorithm, we built Multilingual BotBuster, a multi-language bot detection algorithm to identify the bots in this diverse dataset. We also create a Geographical Location Identifier to swiftly identify the countries a user affiliates with in his description. Our results show that bots can appear to move from one country to another, but the language they write in remains relatively constant. Bots distribute narratives on distinct topics related to their self-declared country affiliation. Finally, despite the diverse distribution of bot locations around the world, the proportion of bots per country is about 20%. Our work stresses the importance of a united analysis of the cyber and physical realms, where we combine both spheres to inventorize the language and location of social media bots and understand communication strategies.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Multi-Agent Geospatial Copilots for Remote Sensing Workflows",
    "url": "http://arxiv.org/abs/2501.16254v1",
    "authors": [
      "Chaehong Lee",
      "Varatheepan Paramanayakam",
      "Andreas Karatzas",
      "Yanan Jian",
      "Michael Fore",
      "Heming Liao",
      "Fuxun Yu",
      "Ruopu Li",
      "Iraklis Anagnostopoulos",
      "Dimitrios Stamoulis"
    ],
    "published": "2025-01-27",
    "abstract": "We present GeoLLM-Squad, a geospatial Copilot that introduces the novel multi-agent paradigm to remote sensing (RS) workflows. Unlike existing single-agent approaches that rely on monolithic large language models (LLM), GeoLLM-Squad separates agentic orchestration from geospatial task-solving, by delegating RS tasks to specialized sub-agents. Built on the open-source AutoGen and GeoLLM-Engine frameworks, our work enables the modular integration of diverse applications, spanning urban monitoring, forestry protection, climate analysis, and agriculture studies. Our results demonstrate that while single-agent systems struggle to scale with increasing RS task complexity, GeoLLM-Squad maintains robust performance, achieving a 17% improvement in agentic correctness over state-of-the-art baselines. Our findings highlight the potential of multi-agent AI in advancing RS workflows.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Probabilistic Mission Design in Neuro-Symbolic Systems",
    "url": "http://arxiv.org/abs/2501.01439v1",
    "authors": [
      "Simon Kohaut",
      "Benedict Flade",
      "Daniel Ochs",
      "Devendra Singh Dhami",
      "Julian Eggert",
      "Kristian Kersting"
    ],
    "published": "2024-12-25",
    "abstract": "Advanced Air Mobility (AAM) is a growing field that demands accurate modeling of legal concepts and restrictions in navigating intelligent vehicles. In addition, any implementation of AAM needs to face the challenges posed by inherently dynamic and uncertain human-inhabited spaces robustly. Nevertheless, the employment of Unmanned Aircraft Systems (UAS) beyond visual line of sight (BVLOS) is an endearing task that promises to enhance significantly today's logistics and emergency response capabilities. To tackle these challenges, we present a probabilistic and neuro-symbolic architecture to encode legal frameworks and expert knowledge over uncertain spatial relations and noisy perception in an interpretable and adaptable fashion. More specifically, we demonstrate Probabilistic Mission Design (ProMis), a system architecture that links geospatial and sensory data with declarative, Hybrid Probabilistic Logic Programs (HPLP) to reason over the agent's state space and its legality. As a result, ProMis generates Probabilistic Mission Landscapes (PML), which quantify the agent's belief that a set of mission conditions is satisfied across its navigation space. Extending prior work on ProMis' reasoning capabilities and computational characteristics, we show its integration with potent machine learning models such as Large Language Models (LLM) and Transformer-based vision models. Hence, our experiments underpin the application of ProMis with multi-modal input data and how our method applies to many important AAM scenarios.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer",
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis",
    "url": "http://arxiv.org/abs/2411.03205v4",
    "authors": [
      "Temitope Akinboyewa",
      "Zhenlong Li",
      "Huan Ning",
      "M. Naser Lessani"
    ],
    "published": "2024-11-05",
    "abstract": "Recent advancements in Generative AI offer promising capabilities for spatial analysis. Despite their potential, the integration of generative AI with established GIS platforms remains underexplored. In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example. Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters. The implementation of this framework resulted in the development of a \"GIS Copilot\" that allows GIS users to interact with QGIS using natural language commands for spatial analysis. The GIS Copilot was evaluated with over 100 spatial analysis tasks with three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps. The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks. This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise. While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "An LLM Agent for Automatic Geospatial Data Analysis",
    "url": "http://arxiv.org/abs/2410.18792v2",
    "authors": [
      "Yuxing Chen",
      "Weijie Wang",
      "Sylvain Lobry",
      "Camille Kurtz"
    ],
    "published": "2024-10-24",
    "abstract": "Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors. Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries. To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively. GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing. In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks. This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization. By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks. Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls. Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion. In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated Shapefile Processing",
    "url": "http://arxiv.org/abs/2410.12376v2",
    "authors": [
      "Qingming Lin",
      "Rui Hu",
      "Huaxia Li",
      "Sensen Wu",
      "Yadong Li",
      "Kai Fang",
      "Hailin Feng",
      "Zhenhong Du",
      "Liuchang Xu"
    ],
    "published": "2024-10-16",
    "abstract": "Vector data is one of the two core data structures in geographic information science (GIS), essential for accurately storing and representing geospatial information. Shapefile, the most widely used vector data format, has become the industry standard supported by all major geographic information systems. However, processing this data typically requires specialized GIS knowledge and skills, creating a barrier for researchers from other fields and impeding interdisciplinary research in spatial data analysis. Moreover, while large language models (LLMs) have made significant advancements in natural language processing and task automation, they still face challenges in handling the complex spatial and topological relationships inherent in GIS vector data. To address these challenges, we propose ShapefileGPT, an innovative framework powered by LLMs, specifically designed to automate Shapefile tasks. ShapefileGPT utilizes a multi-agent architecture, in which the planner agent is responsible for task decomposition and supervision, while the worker agent executes the tasks. We developed a specialized function library for handling Shapefiles and provided comprehensive API documentation, enabling the worker agent to operate Shapefiles efficiently through function calling. For evaluation, we developed a benchmark dataset based on authoritative textbooks, encompassing tasks in categories such as geometric operations and spatial queries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the GPT series models. In comparison to traditional LLMs, ShapefileGPT effectively handles complex vector data analysis tasks, overcoming the limitations of traditional LLMs in spatial analysis. This breakthrough opens new pathways for advancing automation and intelligence in the GIS field, with significant potential in interdisciplinary data analysis and application contexts.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Spatiotemporal Modeling and Forecasting at Scale with Dynamic Generalized Linear Models",
    "url": "http://arxiv.org/abs/2410.07161v1",
    "authors": [
      "Pranay Pherwani",
      "Nicholas Hass",
      "Anna K. Yanchenko"
    ],
    "published": "2024-10-09",
    "abstract": "Spatiotemporal data consisting of timestamps, GPS coordinates, and IDs occurs in many settings. Modeling approaches for this type of data must address challenges in terms of sensor noise, uneven sampling rates, and non-persistent IDs. In this work, we characterize and forecast human mobility at scale with dynamic generalized linear models (DGLMs). We represent mobility data as occupancy counts of spatial cells over time and use DGLMs to model the occupancy counts for each spatial cell in an area of interest. DGLMs are flexible to varying numbers of occupancy counts across spatial cells, are dynamic, and easily incorporate daily and weekly seasonality in the aggregate-level behavior. Our overall approach is robust to various types of noise and scales linearly in the number of spatial cells, time bins, and agents. Our results show that DGLMs provide accurate occupancy count forecasts over a variety of spatial resolutions and forecast horizons. We also present scaling results for spatiotemporal data consisting of hundreds of millions of observations. Our approach is flexible to support several downstream applications, including characterizing human mobility, forecasting occupancy counts, and anomaly detection for aggregate-level behaviors.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast",
      "Anomaly Detection"
    ],
    "is_recent": false
  },
  {
    "title": "AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction",
    "url": "http://arxiv.org/abs/2408.13986v2",
    "authors": [
      "Jie Feng",
      "Yuwei Du",
      "Jie Zhao",
      "Yong Li"
    ],
    "published": "2024-08-26",
    "abstract": "Next location prediction plays a crucial role in various real-world applications. Recently, due to the limitation of existing deep learning methods, attempts have been made to apply large language models (LLMs) to zero-shot next location prediction task. However, they directly generate the final output using LLMs without systematic design, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this paper, we introduce AgentMove, a systematic agentic prediction framework to achieve generalized next location prediction. In AgentMove, we first decompose the mobility prediction task and design specific modules to complete them, including spatial-temporal memory for individual mobility pattern mining, world knowledge generator for modeling the effects of urban structure and collective knowledge extractor for capturing the shared patterns among population. Finally, we combine the results of three modules and conduct a reasoning step to generate the final predictions. Extensive experiments utilizing mobility data from two distinct sources reveal that AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of 12 metrics and it shows robust predictions with various LLMs as base and also less geographical bias across cities. Our codes are available via https://github.com/tsinghua-fib-lab/AgentMove.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "An Autonomous GIS Agent Framework for Geospatial Data Retrieval",
    "url": "http://arxiv.org/abs/2407.21024v2",
    "authors": [
      "Huan Ning",
      "Zhenlong Li",
      "Temitope Akinboyewa",
      "M. Naser Lessani"
    ],
    "published": "2024-07-13",
    "abstract": "Powered by the emerging large language models (LLMs), autonomous geographic information systems (GIS) agents have the potential to accomplish spatial analyses and cartographic tasks. However, a research gap exists to support fully autonomous GIS agents: how to enable agents to discover and download the necessary data for geospatial analyses. This study proposes an autonomous GIS agent framework capable of retrieving required geospatial data by generating, executing, and debugging programs. The framework utilizes the LLM as the decision-maker, selects the appropriate data source (s) from a pre-defined source list, and fetches the data from the chosen source. Each data source has a handbook that records the metadata and technical details for data retrieval. The proposed framework is designed in a plug-and-play style to ensure flexibility and extensibility. Human users or autonomous data scrawlers can add new data sources by adding new handbooks. We developed a prototype agent based on the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a Python program. Experiment results demonstrate its capability of retrieving data from various sources including OpenStreetMap, administrative boundaries and demographic data from the US Census Bureau, satellite basemaps from ESRI World Imagery, global digital elevation model (DEM) from OpenTopography.org, weather data from a commercial provider, the COVID-19 cases from the NYTimes GitHub. Our study is among the first attempts to develop an autonomous geospatial data retrieval agent.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Geospatial Trajectory Generation via Efficient Abduction: Deployment for Independent Testing",
    "url": "http://arxiv.org/abs/2407.06447v2",
    "authors": [
      "Divyagna Bavikadi",
      "Dyuman Aditya",
      "Devendra Parkar",
      "Paulo Shakarian",
      "Graham Mueller",
      "Chad Parvis",
      "Gerardo I. Simari"
    ],
    "published": "2024-07-08",
    "abstract": "The ability to generate artificial human movement patterns while meeting location and time constraints is an important problem in the security community, particularly as it enables the study of the analog problem of detecting such patterns while maintaining privacy.  We frame this problem as an instance of abduction guided by a novel parsimony function represented as an aggregate truth value over an annotated logic program.  This approach has the added benefit of affording explainability to an analyst user.  By showing that any subset of such a program can provide a lower bound on this parsimony requirement, we are able to abduce movement trajectories efficiently through an informed (i.e., A*) search.  We describe how our implementation was enhanced with the application of multiple techniques in order to be scaled and integrated with a cloud-based software stack that included bottom-up rule learning, geolocated knowledge graph retrieval/management, and interfaces with government systems for independently conducted government-run tests for which we provide results.  We also report on our own experiments showing that we not only provide exact results but also scale to very large scenarios and provide realistic agent trajectories that can go undetected by machine learning anomaly detectors.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "IQLS: Framework for leveraging Metadata to enable Large Language Model based queries to complex, versatile Data",
    "url": "http://arxiv.org/abs/2405.15792v1",
    "authors": [
      "Sami Azirar",
      "Hossam A. Gabbar",
      "Chaouki Regoui"
    ],
    "published": "2024-05-04",
    "abstract": "As the amount and complexity of data grows, retrieving it has become a more difficult task that requires greater knowledge and resources. This is especially true for the logistics industry, where new technologies for data collection provide tremendous amounts of interconnected real-time data. The Intelligent Query and Learning System (IQLS) simplifies the process by allowing natural language use to simplify data retrieval . It maps structured data into a framework based on the available metadata and available data models. This framework creates an environment for an agent powered by a Large Language Model. The agent utilizes the hierarchical nature of the data to filter iteratively by making multiple small context-aware decisions instead of one-shot data retrieval. After the Data filtering, the IQLS enables the agent to fulfill tasks given by the user query through interfaces. These interfaces range from multimodal transportation information retrieval to route planning under multiple constraints. The latter lets the agent define a dynamic object, which is determined based on the query parameters. This object represents a driver capable of navigating a road network. The road network is depicted as a graph with attributes based on the data. Using a modified version of the Dijkstra algorithm, the optimal route under the given constraints can be determined. Throughout the entire process, the user maintains the ability to interact and guide the system. The IQLS is showcased in a case study on the Canadian logistics sector, allowing geospatial, visual, tabular and text data to be easily queried semantically in natural language.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Evaluating Tool-Augmented Agents in Remote Sensing Platforms",
    "url": "http://arxiv.org/abs/2405.00709v1",
    "authors": [
      "Simranjit Singh",
      "Michael Fore",
      "Dimitrios Stamoulis"
    ],
    "published": "2024-04-23",
    "abstract": "Tool-augmented Large Language Models (LLMs) have shown impressive capabilities in remote sensing (RS) applications. However, existing benchmarks assume question-answering input templates over predefined image-text data pairs. These standalone instructions neglect the intricacies of realistic user-grounded tasks. Consider a geospatial analyst: they zoom in a map area, they draw a region over which to collect satellite imagery, and they succinctly ask \"Detect all objects here\". Where is `here`, if it is not explicitly hardcoded in the image-text template, but instead is implied by the system state, e.g., the live map positioning? To bridge this gap, we present GeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual, and click-based actions on a real UI platform. Through in-depth evaluation of state-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights towards stronger agents for RS applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots",
    "url": "http://arxiv.org/abs/2404.15500v1",
    "authors": [
      "Simranjit Singh",
      "Michael Fore",
      "Dimitrios Stamoulis"
    ],
    "published": "2024-04-23",
    "abstract": "Geospatial Copilots unlock unprecedented potential for performing Earth Observation (EO) applications through natural language instructions. However, existing agents rely on overly simplified single tasks and template-based prompts, creating a disconnect with real-world scenarios. In this work, we present GeoLLM-Engine, an environment for tool-augmented agents with intricate tasks routinely executed by analysts on remote sensing platforms. We enrich our environment with geospatial API tools, dynamic maps/UIs, and external multimodal knowledge bases to properly gauge an agent's proficiency in interpreting realistic high-level natural language commands and its functional correctness in task completions. By alleviating overheads typically associated with human-in-the-loop benchmark curation, we harness our massively parallel engine across 100 GPT-4-Turbo nodes, scaling to over half a million diverse multi-tool tasks and across 1.1 million satellite images. By moving beyond traditional single-task image-caption paradigms, we investigate state-of-the-art agents and prompting techniques against long-horizon prompts.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions",
    "url": "http://arxiv.org/abs/2402.16364v2",
    "authors": [
      "Tzuf Paz-Argaman",
      "Sayali Kulkarni",
      "John Palowitch",
      "Jason Baldridge",
      "Reut Tsarfaty"
    ],
    "published": "2024-02-26",
    "abstract": "When communicating routes in natural language, the concept of acquired spatial knowledge is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are typically non-sequential, contain allocentric relations, with multiple spatial relations and implicit actions, without any explicit verification. This paper introduces the Rendezvous (RVS) task and dataset, which includes 10,404 examples of English geospatial instructions for reaching a target location using map-knowledge. Our analysis reveals that RVS exhibits a richer use of spatial allocentric relations, and requires resolving more spatial relations simultaneously compared to previous text-based navigation benchmarks.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction",
    "url": "http://arxiv.org/abs/2402.06861v2",
    "authors": [
      "Yansong Ning",
      "Hao Liu"
    ],
    "published": "2024-02-10",
    "abstract": "Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments",
    "url": "http://arxiv.org/abs/2401.04290v1",
    "authors": [
      "Sean Kulinski",
      "Nicholas R. Waytowich",
      "James Z. Hare",
      "David I. Inouye"
    ],
    "published": "2024-01-09",
    "abstract": "Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast",
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery",
    "url": "http://arxiv.org/abs/2512.07276v1",
    "authors": [
      "Mai Tsujimoto",
      "Junjue Wang",
      "Weihao Xuan",
      "Naoto Yokoya"
    ],
    "published": "2025-12-08",
    "abstract": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding",
    "url": "http://arxiv.org/abs/2512.02715v1",
    "authors": [
      "Peirong Zhang",
      "Yidan Zhang",
      "Luxiao Xu",
      "Jinliang Lin",
      "Zonghao Guo",
      "Fengxiang Wang",
      "Xue Yang",
      "Kaiwen Wei",
      "Lei Wang"
    ],
    "published": "2025-12-02",
    "abstract": "Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes",
    "url": "http://arxiv.org/abs/2511.23332v1",
    "authors": [
      "Shuo Ni",
      "Di Wang",
      "He Chen",
      "Haonan Guo",
      "Ning Zhang",
      "Jing Zhang"
    ],
    "published": "2025-11-28",
    "abstract": "Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "GeoZero: Incentivizing Reasoning from Scratch on Geospatial Scenes",
    "url": "http://arxiv.org/abs/2511.22645v1",
    "authors": [
      "Di Wang",
      "Shunyu Liu",
      "Wentao Jiang",
      "Fengxiang Wang",
      "Yi Liu",
      "Xiaolei Qin",
      "Zhiming Luo",
      "Chaoyang Zhou",
      "Haonan Guo",
      "Jing Zhang",
      "Bo Du",
      "Dacheng Tao",
      "Liangpei Zhang"
    ],
    "published": "2025-11-27",
    "abstract": "Multimodal large language models (MLLMs) have undergone rapid development in advancing geospatial scene understanding. Recent studies have sought to enhance the reasoning capabilities of remote sensing MLLMs, typically through cold-start training with elaborately curated chain-of-thought (CoT) data. However, this approach not only incurs substantial annotation costs but also introduces human biases that may limit the diversity of model reasoning. To address these challenges, we propose GeoZero, a framework that enables MLLMs to perform geospatial reasoning without any predefined CoT supervision. Specifically, we construct two datasets, GeoZero-Instruct and GeoZero-Hard. GeoZero-Instruct allows the model to acquire preliminary geospatial knowledge through supervised fine-tuning, while GeoZero-Hard stimulates deep reasoning during the subsequent reinforcement learning stage. Furthermore, we introduce Answer-Anchored Group Relative Policy Optimization (A$^2$GRPO), where the reasoning process is regularized by the model's own answers, encouraging diverse yet accurate thinking. Extensive experiments on multiple remote sensing vision-language benchmarks demonstrate that GeoZero not only surpasses existing state-of-the-art methods but also fosters universal emergent reasoning capabilities across diverse geospatial tasks. Code,data,and models will be publicly available at https://github.com/MiliLab/GeoZero.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Towards Unified Vision Language Models for Forest Ecological Analysis in Earth Observation",
    "url": "http://arxiv.org/abs/2511.16853v1",
    "authors": [
      "Xizhe Xue",
      "Xiao Xiang Zhu"
    ],
    "published": "2025-11-20",
    "abstract": "Recent progress in vision language models (VLMs) has enabled remarkable perception and reasoning capabilities, yet their potential for scientific regression in Earth Observation (EO) remains largely unexplored. Existing EO datasets mainly emphasize semantic understanding tasks such as captioning or classification, lacking benchmarks that align multimodal perception with measurable biophysical variables. To fill this gap, we present REO-Instruct, the first unified benchmark designed for both descriptive and regression tasks in EO. REO-Instruct establishes a cognitively interpretable logic chain in forest ecological scenario (human activity,land-cover classification, ecological patch counting, above-ground biomass (AGB) regression), bridging qualitative understanding and quantitative prediction. The dataset integrates co-registered Sentinel-2 and ALOS-2 imagery with structured textual annotations generated and validated through a hybrid human AI pipeline. Comprehensive evaluation protocols and baseline results across generic VLMs reveal that current models struggle with numeric reasoning, highlighting an essential challenge for scientific VLMs. REO-Instruct offers a standardized foundation for developing and assessing next-generation geospatial models capable of both description and scientific inference. The project page are publicly available at \\href{https://github.com/zhu-xlab/REO-Instruct}{REO-Instruct}.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Classification",
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness",
    "url": "http://arxiv.org/abs/2511.13712v1",
    "authors": [
      "Kiana Vu",
      "\u0130smet Sel\u00e7uk \u00d6zer",
      "Phung Lai",
      "Zheng Wu",
      "Thilanka Munasinghe",
      "Jennifer Wei"
    ],
    "published": "2025-11-17",
    "abstract": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial Chain of Thought Reasoning for Enhanced Visual Question Answering on Satellite Imagery",
    "url": "http://arxiv.org/abs/2511.11198v1",
    "authors": [
      "Shambhavi Shanker",
      "Manikandan Padmanaban",
      "Jagabondhu Hazra"
    ],
    "published": "2025-11-14",
    "abstract": "Geospatial chain of thought (CoT) reasoning is essential for advancing Visual Question Answering (VQA) on satellite imagery, particularly in climate related applications such as disaster monitoring, infrastructure risk assessment, urban resilience planning, and policy support. Existing VQA models enable scalable interpretation of remote sensing data but often lack the structured reasoning required for complex geospatial queries. We propose a VQA framework that integrates CoT reasoning with Direct Preference Optimization (DPO) to improve interpretability, robustness, and accuracy. By generating intermediate rationales, the model better handles tasks involving detection, classification, spatial relations, and comparative analysis, which are critical for reliable decision support in high stakes climate domains. Experiments show that CoT supervision improves accuracy by 34.9\\% over direct baselines, while DPO yields additional gains in accuracy and reasoning quality. The resulting system advances VQA for multispectral Earth observation by enabling richer geospatial reasoning and more effective climate use cases.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning",
    "url": "http://arxiv.org/abs/2511.06316v2",
    "authors": [
      "MD Thamed Bin Zaman Chowdhury",
      "Moazzem Hossain"
    ],
    "published": "2025-11-09",
    "abstract": "Reliable geospatial information on road accidents is vital for safety analysis and infrastructure planning, yet most low- and middle-income countries continue to face a critical shortage of accurate, location-specific crash data. Existing text-based geocoding tools perform poorly in multilingual and unstructured news environments, where incomplete place descriptions and mixed language (e.g. Bangla-English) scripts obscure spatial context. To address these limitations, this study introduces ALIGN (Accident Location Inference through Geo-Spatial Neural Reasoning), a vision-language framework that emulates human spatial reasoning to infer accident location coordinates directly from available textual and map-based cues. ALIGN integrates large language and vision-language model mechanisms within a multi-stage pipeline that performs optical character recognition, linguistic reasoning, and map-level verification through grid-based spatial scanning. The framework systematically evaluates each predicted location against contextual and visual evidence, ensuring interpretable, fine-grained geolocation outcomes without requiring model retraining. Applied to Bangla-language news data source, ALIGN demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district- and sub-district-level crash sites. Beyond its technical contribution, the framework establishes a high accuracy foundation for automated crash mapping in data-scarce regions, supporting evidence-driven road-safety policymaking and the broader integration of multimodal artificial intelligence in transportation analytics.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation",
    "url": "http://arxiv.org/abs/2510.18751v2",
    "authors": [
      "Patterson Hsieh",
      "Jerry Yeh",
      "Mao-Chi He",
      "Wen-Han Hsieh",
      "Elvis Hsieh"
    ],
    "published": "2025-10-21",
    "abstract": "Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Coordinates from Context: Using LLMs to Ground Complex Location References",
    "url": "http://arxiv.org/abs/2510.08741v1",
    "authors": [
      "Tessa Masis",
      "Brendan O'Connor"
    ],
    "published": "2025-10-09",
    "abstract": "Geocoding is the task of linking a location reference to an actual geographic location and is essential for many downstream analyses of unstructured text. In this paper, we explore the challenging setting of geocoding compositional location references. Building on recent work demonstrating LLMs' abilities to reason over geospatial data, we evaluate LLMs' geospatial knowledge versus reasoning skills relevant to our task. Based on these insights, we propose an LLM-based strategy for geocoding compositional location references. We show that our approach improves performance for the task and that a relatively small fine-tuned LLM can achieve comparable performance with much larger off-the-shelf models.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective",
    "url": "http://arxiv.org/abs/2510.01639v1",
    "authors": [
      "Thinh Hung Truong",
      "Jey Han Lau",
      "Jianzhong Qi"
    ],
    "published": "2025-10-02",
    "abstract": "We explore the geospatial reasoning capabilities of Large Language Models (LLMs), specifically, whether LLMs can read road network maps and perform navigation. We frame trajectory recovery as a proxy task, which requires models to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with over 4,000 real-world trajectories across diverse regions and transportation modes. Using road network as context, our prompting framework enables LLMs to generate valid paths without accessing any external navigation tools. Experiments show that LLMs outperform off-the-shelf baselines and specialized trajectory recovery models, with strong zero-shot generalization. Fine-grained analysis shows that LLMs have strong comprehension of the road network and coordinate systems, but also pose systematic biases with respect to regions and transportation modes. Finally, we demonstrate how LLMs can enhance navigation experiences by reasoning over maps in flexible ways to incorporate user preferences.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning",
    "url": "http://arxiv.org/abs/2510.00072v1",
    "authors": [
      "Chenhui Xu",
      "Fuxun Yu",
      "Michael J. Bianco",
      "Jacob Kovarskiy",
      "Raphael Tang",
      "Qi Zhang",
      "Zirui Xu",
      "Will LeVine",
      "Brandon Dubbs",
      "Heming Liao",
      "Cassandra Burgess",
      "Suvam Bag",
      "Jay Patravali",
      "Rupanjali Kukal",
      "Mikael Figueroa",
      "Rishi Madhok",
      "Nikolaos Karianakis",
      "Jinjun Xiong"
    ],
    "published": "2025-09-29",
    "abstract": "We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm\" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at https://huggingface.co/miniHui/Geo-R1.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Forecast",
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models",
    "url": "http://arxiv.org/abs/2509.22221v1",
    "authors": [
      "Jiaqi Liu",
      "Lang Sun",
      "Ronghao Fu",
      "Bo Yang"
    ],
    "published": "2025-09-26",
    "abstract": "Vision-Language Models (VLMs) in remote sensing often fail at complex analytical tasks, a limitation stemming from their end-to-end training paradigm that bypasses crucial reasoning steps and leads to unverifiable outputs. To address this limitation, we introduce the Perceptually-Grounded Geospatial Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as a verifiable, multi-step process. We instill this analytical process through a two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale dataset of structured Geo-CoT rationales. This strategy first employs supervised fine-tuning (SFT) to instill the foundational cognitive architecture, then leverages Group Reward Policy Optimization (GRPO) to refine the model's reasoning policy towards factual correctness. The resulting model, RSThinker, outputs both a final answer and its justifying, verifiable analytical trace. This capability yields dominant performance, significantly outperforming state-of-the-art models across a comprehensive range of tasks. The public release of our Geo-CoT380k dataset and RSThinker model upon publication serves as a concrete pathway from opaque perception towards structured, verifiable reasoning for Earth Observation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2509.21976v2",
    "authors": [
      "Zilun Zhang",
      "Zian Guan",
      "Tiancheng Zhao",
      "Haozhan Shen",
      "Tianyu Li",
      "Yuxiang Cai",
      "Zhonggen Su",
      "Zhaojun Liu",
      "Jianwei Yin",
      "Xiang Li"
    ],
    "published": "2025-09-26",
    "abstract": "Referring expression understanding in remote sensing poses unique challenges, as it requires reasoning over complex object-context relationships. While supervised fine-tuning (SFT) on multimodal large language models achieves strong performance with massive labeled datasets, they struggle in data-scarce scenarios, leading to poor generalization. To address this limitation, we propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm for few-shot geospatial referring. Geo-R1 enforces the model to first generate explicit, interpretable reasoning chains that decompose referring expressions, and then leverage these rationales to localize target objects. This \"reason first, then act\" process enables the model to make more effective use of limited annotations, enhances generalization, and provides interpretability. We validate Geo-R1 on three carefully designed few-shot geospatial referring benchmarks, where our model consistently and substantially outperforms SFT baselines. It also demonstrates strong cross-dataset generalization, highlighting its robustness. Code and data will be released at: https://github.com/Geo-R1/geo-r1.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery",
    "url": "http://arxiv.org/abs/2509.20628v1",
    "authors": [
      "Yiming Xiao",
      "Archit Gupta",
      "Miguel Esparza",
      "Yu-Hsuan Ho",
      "Antonia Sebastian",
      "Hannah Weas",
      "Rose Houck",
      "Ali Mostafavi"
    ],
    "published": "2025-09-25",
    "abstract": "Building-level occupancy after disasters is vital for triage, inspections, utility re-energization, and equitable resource allocation. Overhead imagery provides rapid coverage but often misses facade and access cues that determine habitability, while street-view imagery captures those details but is sparse and difficult to align with parcels. We present FacadeTrack, a street-level, language-guided framework that links panoramic video to parcels, rectifies views to facades, and elicits interpretable attributes (for example, entry blockage, temporary coverings, localized debris) that drive two decision strategies: a transparent one-stage rule and a two-stage design that separates perception from conservative reasoning. Evaluated across two post-Hurricane Helene surveys, the two-stage approach achieves a precision of 0.927, a recall of 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a precision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond accuracy, intermediate attributes and spatial diagnostics reveal where and why residual errors occur, enabling targeted quality control. The pipeline provides auditable, scalable occupancy assessments suitable for integration into geospatial and emergency-management workflows.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "RoadMind: Towards a Geospatial AI Expert for Disaster Response",
    "url": "http://arxiv.org/abs/2509.19354v1",
    "authors": [
      "Ahmed El Fekih Zguir",
      "Ferda Ofli",
      "Muhammad Imran"
    ],
    "published": "2025-09-18",
    "abstract": "Large Language Models (LLMs) have shown impressive performance across a range of natural language tasks, but remain limited in their ability to reason about geospatial data, particularly road networks, distances, and directions. This gap poses challenges in disaster scenarios, where spatial understanding is critical for tasks such as evacuation planning and resource allocation. In this work, we present RoadMind, a self-supervised framework that enhances the geospatial reasoning capabilities of LLMs using structured data from OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data for a given city and converts it into multiple supervision formats tailored to key spatial tasks. We pretrain and fine-tune LLMs on these representations using QLoRA adapters and 4-bit quantized models. We evaluate our approach on three disaster-prone cities with varying global representation, Los Angeles, Christchurch, and Manila, across tasks such as road segment identification, nearest road retrieval, and distance/direction estimation. Our results show that models trained via RoadMind significantly outperform strong baselines, including state-of-the-art LLMs equipped with advanced prompt engineering. This demonstrates the potential of structured geospatial data to enhance language models with robust spatial reasoning, enabling more effective offline AI systems for disaster response.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation",
    "url": "http://arxiv.org/abs/2509.05881v1",
    "authors": [
      "Qianheng Zhang",
      "Song Gao",
      "Chen Wei",
      "Yibo Zhao",
      "Ying Nie",
      "Ziru Chen",
      "Shijie Chen",
      "Yu Su",
      "Huan Sun"
    ],
    "published": "2025-09-07",
    "abstract": "Recent advances in large language models (LLMs) have fueled growing interest in automating geospatial analysis and GIS workflows, yet their actual capabilities remain uncertain. In this work, we call for rigorous evaluation of LLMs on well-defined geoprocessing tasks before making claims about full GIS automation. To this end, we present GeoAnalystBench, a benchmark of 50 Python-based tasks derived from real-world geospatial problems and carefully validated by GIS experts. Each task is paired with a minimum deliverable product, and evaluation covers workflow validity, structural alignment, semantic similarity, and code quality (CodeBLEU). Using this benchmark, we assess both proprietary and open source models. Results reveal a clear gap: proprietary models such as ChatGPT-4o-mini achieve high validity 95% and stronger code alignment (CodeBLEU 0.39), while smaller open source models like DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5% validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as spatial relationship detection or optimal site selection, remain the most challenging across all models. These findings demonstrate both the promise and limitations of current LLMs in GIS automation and provide a reproducible framework to advance GeoAI research with human-in-the-loop support.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "GRASP: Geospatial pixel Reasoning viA Structured Policy learning",
    "url": "http://arxiv.org/abs/2508.17102v2",
    "authors": [
      "Chengjie Jiang",
      "Yunqi Zhou",
      "Jiafeng Yan",
      "Jing Li",
      "Jiayang Li",
      "Yue Zhou",
      "Hongjie He",
      "Jonathan Li"
    ],
    "published": "2025-08-23",
    "abstract": "Geospatial pixel reasoning aims to generate segmentation masks in remote sensing imagery directly from natural-language instructions. Most existing approaches follow a paradigm that fine-tunes multimodal large language models under supervision with dense pixel-level masks as ground truth. While effective within the training data distribution, this design suffers from two main drawbacks: (1) the high cost of large-scale dense mask annotation, and (2) the limited generalization capability of supervised fine-tuning in out-of-domain scenarios. To address these issues, we propose GRASP, a structured policy-learning framework that integrates a multimodal large language model with a pretrained segmentation model in a cascaded manner. To enhance generalization, we introduce PRIME, a training paradigm that replaces supervised fine-tuning with reinforcement learning to better align reasoning and grounding behaviors with task objectives. To reduce annotation costs, we design BoP-Rewards, which substitutes dense mask labels with bounding box and positive points. It further verifies outputs through two complementary signals: format, which constrains the reasoning and grounding structure to remain syntactically parsable, and accuracy, which evaluates the quality of predicted boxes and points. For evaluation, we train our method and all baselines on EarthReason and GeoPixInstruct, constructing an in-domain benchmark by merging their test sets. We further release GRASP-1k, a fully out-of-domain benchmark with reasoning-intensive queries, reasoning traces, and fine-grained masks. Experimental results demonstrate state-of-the-art (SOTA) in-domain performance and up to 54\\% improvement in out-of-domain scenarios, confirming that reinforcement learning with cost-aware rewards provides a robust and scalable paradigm for geospatial pixel reasoning. All code and datasets will be released publicly.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Segmentation",
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges",
    "url": "http://arxiv.org/abs/2508.06832v1",
    "authors": [
      "Haifeng Li",
      "Wang Guo",
      "Haiyang Wu",
      "Mengwei Wu",
      "Jipeng Zhang",
      "Qing Zhu",
      "Yu Liu",
      "Xin Huang",
      "Chao Tao"
    ],
    "published": "2025-08-09",
    "abstract": "The mainstream paradigm of remote sensing image interpretation has long been dominated by vision-centered models, which rely on visual features for semantic understanding. However, these models face inherent limitations in handling multi-modal reasoning, semantic abstraction, and interactive decision-making. While recent advances have introduced Large Language Models (LLMs) into remote sensing workflows, existing studies primarily focus on downstream applications, lacking a unified theoretical framework that explains the cognitive role of language. This review advocates a paradigm shift from vision-centered to language-centered remote sensing interpretation. Drawing inspiration from the Global Workspace Theory (GWT) of human cognition, We propose a language-centered framework for remote sensing interpretation that treats LLMs as the cognitive central hub integrating perceptual, task, knowledge and action spaces to enable unified understanding, reasoning, and decision-making. We first explore the potential of LLMs as the central cognitive component in remote sensing interpretation, and then summarize core technical challenges, including unified multimodal representation, knowledge association, and reasoning and decision-making. Furthermore, we construct a global workspace-driven interpretation mechanism and review how language-centered solutions address each challenge. Finally, we outline future research directions from four perspectives: adaptive alignment of multimodal data, task understanding under dynamic knowledge constraints, trustworthy reasoning, and autonomous interaction. This work aims to provide a conceptual foundation for the next generation of remote sensing interpretation systems and establish a roadmap toward cognition-driven intelligent geospatial analysis.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence",
    "url": "http://arxiv.org/abs/2508.04852v2",
    "authors": [
      "Chenhui Qiang",
      "Zhaoyang Wei",
      "Xumeng Han",
      "Zipeng Wang",
      "Siyao Li",
      "Xiangyuan Lan",
      "Jianbin Jiao",
      "Zhenjun Han"
    ],
    "published": "2025-08-06",
    "abstract": "With the rapid development of MLLMs, evaluating their visual capabilities has become increasingly crucial. Current benchmarks primarily fall into two main types: basic perception benchmarks, which focus on local details but lack deep reasoning (e.g., \"what is in the image?\"), and mainstream reasoning benchmarks, which concentrate on prominent image elements but may fail to assess subtle clues requiring intricate analysis. However, profound visual understanding and complex reasoning depend more on interpreting subtle, inconspicuous local details than on perceiving salient, macro-level objects. These details, though occupying minimal image area, often contain richer, more critical information for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel framework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues, often occupying on average just 0.25% of the image area; 2) integrate these clues with world knowledge for complex reasoning. Comprising 374 carefully designed questions across Geospatial, Temporal, Situational, Intent, System State, and Symbolic reasoning, each question in VER-Bench is accompanied by structured evidence: visual clues and question-related reasoning derived from them. VER-Bench reveals current models' limitations in extracting subtle visual evidence and constructing evidence-based arguments, highlighting the need to enhance models's capabilities in fine-grained visual evidence extraction, integration, and reasoning for genuine visual understanding and human-like analysis. Dataset and additional materials are available https://github.com/verbta/ACMMM-25-Materials.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models",
    "url": "http://arxiv.org/abs/2508.01608v1",
    "authors": [
      "Lingyao Li",
      "Runlong Yu",
      "Qikai Hu",
      "Bowei Li",
      "Min Deng",
      "Yang Zhou",
      "Xiaowei Jia"
    ],
    "published": "2025-08-03",
    "abstract": "Image geolocalization, the task of identifying the geographic location depicted in an image, is important for applications in crisis response, digital forensics, and location-based intelligence. While recent advances in large language models (LLMs) offer new opportunities for visual reasoning, their ability to perform image geolocalization remains underexplored. In this study, we introduce a benchmark called IMAGEO-Bench that systematically evaluates accuracy, distance error, geospatial bias, and reasoning process. Our benchmark includes three diverse datasets covering global street scenes, points of interest (POIs) in the United States, and a private collection of unseen images. Through experiments on 10 state-of-the-art LLMs, including both open- and closed-source models, we reveal clear performance disparities, with closed-source models generally showing stronger reasoning. Importantly, we uncover geospatial biases as LLMs tend to perform better in high-resource regions (e.g., North America, Western Europe, and California) while exhibiting degraded performance in underrepresented areas. Regression diagnostics demonstrate that successful geolocalization is primarily dependent on recognizing urban settings, outdoor environments, street-level imagery, and identifiable landmarks. Overall, IMAGEO-Bench provides a rigorous lens into the spatial reasoning capabilities of LLMs and offers implications for building geolocation-aware AI systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning",
    "url": "http://arxiv.org/abs/2507.19586v1",
    "authors": [
      "Shengyuan Wang",
      "Jie Feng",
      "Tianhui Liu",
      "Dan Pei",
      "Yong Li"
    ],
    "published": "2025-07-25",
    "abstract": "Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations of geospatial information) that compromise their reliability. While the phenomenon of general knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark. Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow",
    "url": "http://arxiv.org/abs/2507.19280v2",
    "authors": [
      "Liang Yao",
      "Fan Liu",
      "Hongbo Lu",
      "Chuanyi Zhang",
      "Rui Min",
      "Shengxiang Xu",
      "Shimin Di",
      "Pai Peng"
    ],
    "published": "2025-07-25",
    "abstract": "Remote sensing imagery presents vast, inherently unstructured spatial data, necessitating sophisticated reasoning to interpret complex user intents and contextual relationships beyond simple recognition tasks. In this paper, we aim to construct an Earth observation workflow to handle complex queries by reasoning about spatial context and user intent. As a reasoning workflow, it should autonomously explore and construct its own inference paths, rather than being confined to predefined ground-truth sequences. Ideally, its architecture ought to be unified yet generalized, possessing capabilities to perform diverse reasoning tasks through one model without requiring additional fine-tuning. Existing remote sensing approaches rely on supervised fine-tuning paradigms and task-specific heads, limiting both autonomous reasoning and unified generalization. To this end, we propose RemoteReasoner, a unified workflow for geospatial reasoning. The design of RemoteReasoner integrates a multi-modal large language model (MLLM) for interpreting user instructions and localizing targets, together with task transformation strategies that enable multi-granularity tasks, including object-, region-, and pixel-level. In contrast to existing methods, our framework is trained with reinforcement learning (RL) to endow the MLLM sufficient reasoning autonomy. At the inference stage, our transformation strategies enable diverse task output formats without requiring task-specific decoders or further fine-tuning. Experiments demonstrated that RemoteReasoner achieves state-of-the-art (SOTA) performance across multi-granularity reasoning tasks. Furthermore, it retains the MLLM's inherent generalization capability, demonstrating robust performance on unseen tasks and out-of-distribution categories.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Recognition",
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine",
    "url": "http://arxiv.org/abs/2506.10365v1",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Huayi Wu",
      "Haoyue Jiao",
      "Ziqi Liu",
      "Lutong Xie",
      "Chang Liu",
      "Jianyuan Liang",
      "Yaxian Qing",
      "Xiaopu Zhang",
      "Dehua Peng",
      "Zhipeng Gui",
      "Xuefeng Guan"
    ],
    "published": "2025-06-12",
    "abstract": "Geospatial code generation is becoming a key frontier in integrating artificial intelligence with geo-scientific analysis, yet standardised automated evaluation tools for this task remain absent. This study presents AutoGEEval++, an enhanced framework building on AutoGEEval, and the first automated assessment system for large language models (LLMs) generating geospatial code on Google Earth Engine (GEE). It supports diverse data modalities and varying task complexities. Built on the GEE Python API, AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test cases across 26 data types and three task categories: unit, combo, and theme tests. It includes a submission programme and a judge module to realise an end-to-end automated evaluation pipeline from code generation to execution-based validation. The framework adopts multi-dimensional metrics-accuracy, resource usage, run-time efficiency, and error types-balancing hallucination control and efficiency, and enabling boundary testing and error pattern analysis. Using AutoGEEval++, we evaluate 24 state-of-the-art LLMs (as of June 2025), including general-purpose, reasoning-enhanced, code-centric, and geoscience-specific models. Results reveal clear performance, stability, and error differences across task types, model designs, and deployment settings, confirming AutoGEEval++'s practical value and scalability in vertical-domain code generation. This work establishes the first standardised evaluation protocol and foundational benchmark for GEE-based LLM code generation, providing a unified basis for performance comparison and a methodological framework for systematic, domain-specific code evaluation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data",
    "url": "http://arxiv.org/abs/2505.17116v1",
    "authors": [
      "Akash Dhruv",
      "Yangxinyu Xie",
      "Jordan Branham",
      "Tanwi Mallick"
    ],
    "published": "2025-05-21",
    "abstract": "This paper presents a comparative study of large language models (LLMs) in interpreting grid-structured geospatial data. We evaluate the performance of a base model through structured prompting and contrast it with a fine-tuned variant trained on a dataset of user-assistant interactions. Our results highlight the strengths and limitations of zero-shot prompting and demonstrate the benefits of fine-tuning for structured geospatial and temporal reasoning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models",
    "url": "http://arxiv.org/abs/2505.12900v1",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Huayi Wu",
      "Jianyuan Liang",
      "Haoyue Jiao",
      "Yaxian Qing",
      "Xiaopu Zhang",
      "Xu Li",
      "Zhipeng Gui",
      "Xuefeng Guan",
      "Longgang Xiang"
    ],
    "published": "2025-05-19",
    "abstract": "Geospatial code generation is emerging as a key direction in the integration of artificial intelligence and geoscientific analysis. However, there remains a lack of standardized tools for automatic evaluation in this domain. To address this gap, we propose AutoGEEval, the first multimodal, unit-level automated evaluation framework for geospatial code generation tasks on the Google Earth Engine (GEE) platform powered by large language models (LLMs). Built upon the GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench) comprising 1325 test cases that span 26 GEE data types. The framework integrates both question generation and answer verification components to enable an end-to-end automated evaluation pipeline-from function invocation to execution validation. AutoGEEval supports multidimensional quantitative analysis of model outputs in terms of accuracy, resource consumption, execution efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including general-purpose, reasoning-augmented, code-centric, and geoscience-specialized models-revealing their performance characteristics and potential optimization pathways in GEE code generation. This work provides a unified protocol and foundational resource for the development and assessment of geospatial code generation models, advancing the frontier of automated natural language to domain-specific code translation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark",
    "url": "http://arxiv.org/abs/2505.12254v1",
    "authors": [
      "Yiwei Ou",
      "Xiaobin Ren",
      "Ronggui Sun",
      "Guansong Gao",
      "Ziyi Jiang",
      "Kaiqi Zhao",
      "Manfredo Manfredini"
    ],
    "published": "2025-05-18",
    "abstract": "Existing visual place recognition (VPR) datasets predominantly rely on vehicle-mounted imagery, lack multimodal diversity and underrepresent dense, mixed-use street-level spaces, especially in non-Western urban contexts. To address these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for street-level place recognition in complex, pedestrian-only environments. The dataset comprises 78,575 annotated images and 2,512 video clips captured across 207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in Chengdu, China. Each image is labeled with precise GPS coordinates, timestamp, and textual metadata, and covers varied lighting conditions, viewpoints, and timeframes. MMS-VPR follows a systematic and replicable data collection protocol with minimal device requirements, lowering the barrier for scalable dataset creation. Importantly, the dataset forms an inherent spatial graph with 125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place recognition. We further define two application-specific subsets -- Dataset_Edges and Dataset_Points -- to support fine-grained and graph-based evaluation tasks. Extensive benchmarks using conventional VPR models, graph neural networks, and multimodal baselines show substantial improvements when leveraging multimodal and structural cues. MMS-VPR facilitates future research at the intersection of computer vision, geospatial understanding, and multimodal reasoning. The dataset is publicly available at https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "CLIP",
      "GNN"
    ],
    "applications": [
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era",
    "url": "http://arxiv.org/abs/2505.09651v1",
    "authors": [
      "Xixuan Hao",
      "Yutian Jiang",
      "Xingchen Zou",
      "Jiabo Liu",
      "Yifang Yin",
      "Yuxuan Liang"
    ],
    "published": "2025-05-13",
    "abstract": "Location Intelligence (LI), the science of transforming location-centric geospatial data into actionable knowledge, has become a cornerstone of modern spatial decision-making. The rapid evolution of Geospatial Representation Learning is fundamentally reshaping LI development through two successive technological revolutions: the deep learning breakthrough and the emerging large language model (LLM) paradigm. While deep neural networks (DNNs) have demonstrated remarkable success in automated feature extraction from structured geospatial data (e.g., satellite imagery, GPS trajectories), the recent integration of LLMs introduces transformative capabilities for cross-modal geospatial reasoning and unstructured geo-textual data processing. This survey presents a comprehensive review of geospatial representation learning across both technological eras, organizing them into a structured taxonomy based on the complete pipeline comprising: (1) data perspective, (2) methodological perspective and (3) application perspective. We also highlight current advancements, discuss existing limitations, and propose potential future research directions in the LLM era. This work offers a thorough exploration of the field and providing a roadmap for further innovation in LI. The summary of the up-to-date paper list can be found in https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo continuous updates.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Deep Neural Network",
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence",
    "url": "http://arxiv.org/abs/2503.16326v1",
    "authors": [
      "Long Yuan",
      "Fengran Mo",
      "Kaiyu Huang",
      "Wenjie Wang",
      "Wangyuxuan Zhai",
      "Xiaoyu Zhu",
      "You Li",
      "Jinan Xu",
      "Jian-Yun Nie"
    ],
    "published": "2025-03-20",
    "abstract": "The rapid advancement of multimodal large language models (LLMs) has opened new frontiers in artificial intelligence, enabling the integration of diverse large-scale data types such as text, images, and spatial information. In this paper, we explore the potential of multimodal LLMs (MLLM) for geospatial artificial intelligence (GeoAI), a field that leverages spatial data to address challenges in domains including Geospatial Semantics, Health Geography, Urban Geography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo) tailored to geospatial applications, capable of processing and analyzing heterogeneous data sources, including satellite imagery, geospatial metadata, and textual descriptions. By combining the strengths of natural language understanding and spatial reasoning, our model enhances the ability of instruction following and the accuracy of GeoAI systems. Results demonstrate that our model outperforms task-specific models and existing LLMs on diverse geospatial tasks, effectively addressing the multimodality nature while achieving competitive results on the zero-shot geospatial tasks. Our code will be released after publication.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning",
    "url": "http://arxiv.org/abs/2503.13517v2",
    "authors": [
      "Hao Cui",
      "Zahra Shamsi",
      "Gowoon Cheon",
      "Xuejian Ma",
      "Shutong Li",
      "Maria Tikhanovskaya",
      "Peter Norgaard",
      "Nayantara Mudur",
      "Martyna Plomecka",
      "Paul Raccuglia",
      "Yasaman Bahri",
      "Victor V. Albert",
      "Pranesh Srinivasan",
      "Haining Pan",
      "Philippe Faist",
      "Brian Rohr",
      "Ekin Dogus Cubuk",
      "Muratahan Aykol",
      "Amil Merchant",
      "Michael J. Statt",
      "Dan Morris",
      "Drew Purves",
      "Elise Kleeman",
      "Ruth Alcantara",
      "Matthew Abraham",
      "Muqthar Mohammad",
      "Ean Phing VanLee",
      "Chenfei Jiang",
      "Elizabeth Dorfman",
      "Eun-Ah Kim",
      "Michael P Brenner",
      "Viren Jain",
      "Sameera Ponda",
      "Subhashini Venugopalan"
    ],
    "published": "2025-03-14",
    "abstract": "Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Geospatial Reasoning Questions",
    "url": "http://arxiv.org/abs/2502.18470v5",
    "authors": [
      "Dazhou Yu",
      "Riyang Bao",
      "Ruiyu Ning",
      "Jinghong Peng",
      "Gengchen Mai",
      "Liang Zhao"
    ],
    "published": "2025-02-04",
    "abstract": "Answering real-world geospatial questions--such as finding restaurants along a travel route or amenities near a landmark--requires reasoning over both geographic relationships and semantic user intent. However, existing large language models (LLMs) lack spatial computing capabilities and access to up-to-date, ubiquitous real-world geospatial data, while traditional geospatial systems fall short in interpreting natural language. To bridge this gap, we introduce Spatial-RAG, a Retrieval-Augmented Generation (RAG) framework designed for geospatial question answering. Spatial-RAG integrates structured spatial databases with LLMs via a hybrid spatial retriever that combines sparse spatial filtering and dense semantic matching. It formulates the answering process as a multi-objective optimization over spatial and semantic relevance, identifying Pareto-optimal candidates and dynamically selecting the best response based on user intent. Experiments across multiple tourism and map-based QA datasets show that Spatial-RAG significantly improves accuracy, precision, and ranking performance over strong baselines.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Neurosymbolic AI for Travel Demand Prediction: Integrating Decision Tree Rules into Neural Networks",
    "url": "http://arxiv.org/abs/2502.01680v1",
    "authors": [
      "Kamal Acharya",
      "Mehul Lad",
      "Liang Sun",
      "Houbing Song"
    ],
    "published": "2025-02-02",
    "abstract": "Travel demand prediction is crucial for optimizing transportation planning, resource allocation, and infrastructure development, ensuring efficient mobility and economic sustainability. This study introduces a Neurosymbolic Artificial Intelligence (Neurosymbolic AI) framework that integrates decision tree (DT)-based symbolic rules with neural networks (NNs) to predict travel demand, leveraging the interpretability of symbolic reasoning and the predictive power of neural learning. The framework utilizes data from diverse sources, including geospatial, economic, and mobility datasets, to build a comprehensive feature set. DTs are employed to extract interpretable if-then rules that capture key patterns, which are then incorporated as additional features into a NN to enhance its predictive capabilities. Experimental results show that the combined dataset, enriched with symbolic rules, consistently outperforms standalone datasets across multiple evaluation metrics, including Mean Absolute Error (MAE), \\(R^2\\), and Common Part of Commuters (CPC). Rules selected at finer variance thresholds (e.g., 0.0001) demonstrate superior effectiveness in capturing nuanced relationships, reducing prediction errors, and aligning with observed commuter patterns. By merging symbolic and neural learning paradigms, this Neurosymbolic approach achieves both interpretability and accuracy.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets",
    "url": "http://arxiv.org/abs/2412.21015v2",
    "authors": [
      "Mahir Labib Dihan",
      "Mohammed Eunus Ali",
      "Md Rizwan Parvez"
    ],
    "published": "2024-12-30",
    "abstract": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, an extensible open-source framework that streamlines the creation of reproducible, traceable map-based QA datasets. MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/bVv7-NYRsTw.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "StaR Maps: Unveiling Uncertainty in Geospatial Relations",
    "url": "http://arxiv.org/abs/2412.18356v1",
    "authors": [
      "Simon Kohaut",
      "Benedict Flade",
      "Julian Eggert",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "published": "2024-12-24",
    "abstract": "The growing complexity of intelligent transportation systems and their applications in public spaces has increased the demand for expressive and versatile knowledge representation. While various mapping efforts have achieved widespread coverage, including detailed annotation of features with semantic labels, it is essential to understand their inherent uncertainties, which are commonly underrepresented by the respective geographic information systems. Hence, it is critical to develop a representation that combines a statistical, probabilistic perspective with the relational nature of geospatial data. Further, such a representation should facilitate an honest view of the data's accuracy and provide an environment for high-level reasoning to obtain novel insights from task-dependent queries. Our work addresses this gap in two ways. First, we present Statistical Relational Maps (StaR Maps) as a representation of uncertain, semantic map data. Second, we demonstrate efficient computation of StaR Maps to scale the approach to wide urban spaces. Through experiments on real-world, crowd-sourced data, we underpin the application and utility of StaR Maps in terms of representing uncertain knowledge and reasoning for complex geospatial information.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Decoding Urban Industrial Complexity: Enhancing Knowledge-Driven Insights via IndustryScopeGPT",
    "url": "http://arxiv.org/abs/2411.15758v1",
    "authors": [
      "Siqi Wang",
      "Chao Liang",
      "Yunfan Gao",
      "Yang Liu",
      "Jing Li",
      "Haofen Wang"
    ],
    "published": "2024-11-24",
    "abstract": "Industrial parks are critical to urban economic growth. Yet, their development often encounters challenges stemming from imbalances between industrial requirements and urban services, underscoring the need for strategic planning and operations. This paper introduces IndustryScopeKG, a pioneering large-scale multi-modal, multi-level industrial park knowledge graph, which integrates diverse urban data including street views, corporate, socio-economic, and geospatial information, capturing the complex relationships and semantics within industrial parks. Alongside this, we present the IndustryScopeGPT framework, which leverages Large Language Models (LLMs) with Monte Carlo Tree Search to enhance tool-augmented reasoning and decision-making in Industrial Park Planning and Operation (IPPO). Our work significantly improves site recommendation and functional planning, demonstrating the potential of combining LLMs with structured datasets to advance industrial park management. This approach sets a new benchmark for intelligent IPPO research and lays a robust foundation for advancing urban industrial development. The dataset and related code are available at https://github.com/Tongji-KGLLM/IndustryScope.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model",
    "url": "http://arxiv.org/abs/2411.14476v1",
    "authors": [
      "Zongrong Li",
      "Junhao Xu",
      "Siqin Wang",
      "Yifan Wu",
      "Haiyang Li"
    ],
    "published": "2024-11-19",
    "abstract": "Geospatial predictions are crucial for diverse fields such as disaster management, urban planning, and public health. Traditional machine learning methods often face limitations when handling unstructured or multi-modal data like street view imagery. To address these challenges, we propose StreetViewLLM, a novel framework that integrates a large language model with the chain-of-thought reasoning and multimodal data sources. By combining street view imagery with geographic coordinates and textual data, StreetViewLLM improves the precision and granularity of geospatial predictions. Using retrieval-augmented generation techniques, our approach enhances geographic information extraction, enabling a detailed analysis of urban environments. The model has been applied to seven global cities, including Hong Kong, Tokyo, Singapore, Los Angeles, New York, London, and Paris, demonstrating superior performance in predicting urban indicators, including population density, accessibility to healthcare, normalized difference vegetation index, building height, and impervious surface. The results show that StreetViewLLM consistently outperforms baseline models, offering improved predictive accuracy and deeper insights into the built environment. This research opens new opportunities for integrating the large language model into urban analytics, decision-making in urban planning, infrastructure management, and environmental monitoring.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Can Large Language Models Generate Geospatial Code?",
    "url": "http://arxiv.org/abs/2410.09738v2",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Jianyuan Liang",
      "Anqi Zhao",
      "Zhipeng Gui",
      "Rui Li",
      "Huayi Wu"
    ],
    "published": "2024-10-13",
    "abstract": "With the growing demand for spatiotemporal data processing and geospatial modeling, automating geospatial code generation has become essential for productivity. Large language models (LLMs) show promise in code generation but face challenges like domain-specific knowledge gaps and \"coding hallucinations.\" This paper introduces GeoCode-Eval (GCE), a framework for assessing LLMs' ability to generate geospatial code across three dimensions: \"Cognition and Memory,\" \"Comprehension and Interpretation,\" and \"Innovation and Creation,\" distributed across eight capability levels. We developed a benchmark dataset, GeoCode-Bench, consisting of 5,000 multiple-choice, 1,500 fill-in-the-blank, 1,500 true/false questions, and 1,000 subjective tasks covering code summarization, generation, completion, and correction. Using GeoCode-Bench, we evaluated three commercial closed-source LLMs, four open-source general-purpose LLMs, and 14 specialized code generation models. We also conducted experiments on few-shot and zero-shot learning, Chain of Thought reasoning, and multi-round majority voting to measure their impact on geospatial code generation. Additionally, we fine-tuned the Code LLaMA-7B model using Google Earth Engine-related JavaScript, creating GEECode-GPT, and evaluated it on subjective tasks. Results show that constructing pre-training and instruction datasets significantly improves code generation, offering insights for optimizing LLMs in specific domains.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Evaluation of Code LLMs on Geospatial Code Generation",
    "url": "http://arxiv.org/abs/2410.04617v2",
    "authors": [
      "Piotr Gramacki",
      "Bruno Martins",
      "Piotr Szyma\u0144ski"
    ],
    "published": "2024-10-06",
    "abstract": "Software development support tools have been studied for a long time, with recent approaches using Large Language Models (LLMs) for code generation. These models can generate Python code for data science and machine learning applications. LLMs are helpful for software engineers because they increase productivity in daily work. An LLM can also serve as a \"mentor\" for inexperienced software developers, and be a viable learning support. High-quality code generation with LLMs can also be beneficial in geospatial data science. However, this domain poses different challenges, and code generation LLMs are typically not evaluated on geospatial tasks. Here, we show how we constructed an evaluation benchmark for code generation models, based on a selection of geospatial tasks. We categorised geospatial tasks based on their complexity and required tools. Then, we created a dataset with tasks that test model capabilities in spatial reasoning, spatial data processing, and geospatial tools usage. The dataset consists of specific coding problems that were manually created for high quality. For every problem, we proposed a set of test scenarios that make it possible to automatically check the generated code for correctness. In addition, we tested a selection of existing code generation LLMs for code generation in the geospatial domain. We share our dataset and reproducible evaluation code on a public GitHub repository, arguing that this can serve as an evaluation benchmark for new LLMs in the future. Our dataset will hopefully contribute to the development new models capable of solving geospatial coding tasks with high accuracy. These models will enable the creation of coding assistants tailored for geospatial applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Into the Unknown: Generating Geospatial Descriptions for New Environments",
    "url": "http://arxiv.org/abs/2406.19967v1",
    "authors": [
      "Tzuf Paz-Argaman",
      "John Palowitch",
      "Sayali Kulkarni",
      "Reut Tsarfaty",
      "Jason Baldridge"
    ],
    "published": "2024-06-28",
    "abstract": "Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships (independent of the observer's viewpoint) using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data. Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (`shop north of school') generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models",
    "url": "http://arxiv.org/abs/2406.13948v2",
    "authors": [
      "Jie Feng",
      "Tianhui Liu",
      "Yuwei Du",
      "Siqi Guo",
      "Yuming Lin",
      "Yong Li"
    ],
    "published": "2024-06-20",
    "abstract": "Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \\textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \\textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \\textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \\textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \\textit{CityInstruction} by \\textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \\textit{CityEval}.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "STAR: A First-Ever Dataset and A Large-Scale Benchmark for Scene Graph Generation in Large-Size Satellite Imagery",
    "url": "http://arxiv.org/abs/2406.09410v3",
    "authors": [
      "Yansheng Li",
      "Linlin Wang",
      "Tingzhu Wang",
      "Xue Yang",
      "Junwei Luo",
      "Qi Wang",
      "Youming Deng",
      "Wenbin Wang",
      "Xian Sun",
      "Haifeng Li",
      "Bo Dang",
      "Yongjun Zhang",
      "Yi Yu",
      "Junchi Yan"
    ],
    "published": "2024-06-13",
    "abstract": "Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting understanding of geospatial scenarios from perception to cognition. In SAI, objects exhibit great variations in scales and aspect ratios, and there exist rich relationships between objects (even between spatially disjoint objects), which makes it attractive to holistically conduct SGG in large-size very-high-resolution (VHR) SAI. However, there lack such SGG datasets. Due to the complexity of large-size SAI, mining triplets <subject, relationship, object> heavily relies on long-range contextual reasoning. Consequently, SGG models designed for small-size natural imagery are not directly applicable to large-size SAI. This paper constructs a large-scale dataset for SGG in large-size VHR SAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, named STAR (Scene graph generaTion in lArge-size satellite imageRy), encompassing over 210K objects and over 400K triplets. To realize SGG in large-size SAI, we propose a context-aware cascade cognition (CAC) framework to understand SAI regarding object detection (OBD), pair pruning and relationship prediction for SGG. We also release a SAI-oriented SGG toolkit with about 30 OBD and 10 SGG methods which need further adaptation by our devised modules on our challenging STAR dataset. The dataset and toolkit are available at: https://linlin-dev.github.io/project/STAR.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Quantifying Geospatial in the Common Crawl Corpus",
    "url": "http://arxiv.org/abs/2406.04952v2",
    "authors": [
      "Ilya Ilyankou",
      "Meihui Wang",
      "Stefano Cavazzi",
      "James Haworth"
    ],
    "published": "2024-06-07",
    "abstract": "Large language models (LLMs) exhibit emerging geospatial capabilities, stemming from their pre-training on vast unlabelled text datasets that are often derived from the Common Crawl (CC) corpus. However, the geospatial content within CC remains largely unexplored, impacting our understanding of LLMs' spatial reasoning. This paper investigates the prevalence of geospatial data in recent Common Crawl releases using Gemini 1.5, a powerful language model. By analyzing a sample of documents and manually revising the results, we estimate that 18.7% of web documents in CC contain geospatial information such as coordinates and addresses. We find little difference in prevalence between Enlgish- and non-English-language documents. Our findings provide quantitative insights into the nature and extent of geospatial data in CC, and lay the groundwork for future studies of geospatial biases of LLMs.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Geospatial Knowledge Graphs",
    "url": "http://arxiv.org/abs/2405.07664v1",
    "authors": [
      "Rui Zhu"
    ],
    "published": "2024-05-13",
    "abstract": "Geospatial knowledge graphs have emerged as a novel paradigm for representing and reasoning over geospatial information. In this framework, entities such as places, people, events, and observations are depicted as nodes, while their relationships are represented as edges. This graph-based data format lays the foundation for creating a \"FAIR\" (Findable, Accessible, Interoperable, and Reusable) environment, facilitating the management and analysis of geographic information. This entry first introduces key concepts in knowledge graphs along with their associated standardization and tools. It then delves into the application of knowledge graphs in geography and environmental sciences, emphasizing their role in bridging symbolic and subsymbolic GeoAI to address cross-disciplinary geospatial challenges. At the end, new research directions related to geospatial knowledge graphs are outlined.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GlobalBuildingMap -- Unveiling the Mystery of Global Buildings",
    "url": "http://arxiv.org/abs/2404.13911v2",
    "authors": [
      "Xiao Xiang Zhu",
      "Qingyu Li",
      "Yilei Shi",
      "Yuanyuan Wang",
      "Adam Stewart",
      "Jonathan Prexl"
    ],
    "published": "2024-04-22",
    "abstract": "Understanding how buildings are distributed globally is crucial to revealing the human footprint on our home planet. This built environment affects local climate, land surface albedo, resource distribution, and many other key factors that influence well-being and human health. Despite this, quantitative and comprehensive data on the distribution and properties of buildings worldwide is lacking. To this end, by using a big data analytics approach and nearly 800,000 satellite images, we generated the highest resolution and highest accuracy building map ever created: the GlobalBuildingMap (GBM). A joint analysis of building maps and solar potentials indicates that rooftop solar energy can supply the global energy consumption need at a reasonable cost. Specifically, if solar panels were placed on the roofs of all buildings, they could supply 1.1-3.3 times -- depending on the efficiency of the solar device -- the global energy consumption in 2020, which is the year with the highest consumption on record. We also identified a clear geospatial correlation between building areas and key socioeconomic variables, which indicates our global building map can serve as an important input to modeling global socioeconomic needs and drivers.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery",
    "url": "http://arxiv.org/abs/2401.06762v1",
    "authors": [
      "Caleb Robinson",
      "Isaac Corley",
      "Anthony Ortiz",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres",
      "Peyman Najafirad"
    ],
    "published": "2024-01-12",
    "abstract": "Fully understanding a complex high-resolution satellite or aerial imagery scene often requires spatial reasoning over a broad relevant context. The human object recognition system is able to understand object in a scene over a long-range relevant context. For example, if a human observes an aerial scene that shows sections of road broken up by tree canopy, then they will be unlikely to conclude that the road has actually been broken up into disjoint pieces by trees and instead think that the canopy of nearby trees is occluding the road. However, there is limited research being conducted to understand long-range context understanding of modern machine learning models. In this work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial Context (RSC), for evaluating the spatial long-range context understanding of geospatial machine learning models and show how commonly used semantic segmentation models can fail at this task. For example, we show that a U-Net trained to segment roads from background in aerial imagery achieves an 84% recall on unoccluded roads, but just 63.5% recall on roads covered by tree canopy despite being trained to model both the same way. We further analyze how the performance of models changes as the relevant context for a decision (unoccluded roads in our case) varies in distance. We release the code to reproduce our experiments and dataset of imagery and masks to encourage future research in this direction -- https://github.com/isaaccorley/ChesapeakeRSC.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Segmentation",
      "Recognition"
    ],
    "is_recent": false
  },
  {
    "title": "AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice",
    "url": "http://arxiv.org/abs/2512.10114v1",
    "authors": [
      "Mesafint Fanuel",
      "Mahmoud Nabil Mahmoud",
      "Crystal Cook Marshal",
      "Vishal Lakhotia",
      "Biswanath Dari",
      "Kaushik Roy",
      "Shaohu Zhang"
    ],
    "published": "2025-12-10",
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries",
    "url": "http://arxiv.org/abs/2509.25264v2",
    "authors": [
      "Shuyang Hou",
      "Haoyue Jiao",
      "Ziqi Liu",
      "Lutong Xie",
      "Guanyu Chen",
      "Shaowen Wu",
      "Xuefeng Guan",
      "Huayi Wu"
    ],
    "published": "2025-09-28",
    "abstract": "Large language models (LLMs) have shown strong performance in natural language to SQL (NL2SQL) tasks within general databases. However, extending to GeoSQL introduces additional complexity from spatial data types, function invocation, and coordinate systems, which greatly increases generation and execution difficulty. Existing benchmarks mainly target general SQL, and a systematic evaluation framework for GeoSQL is still lacking. To fill this gap, we present GeoSQL-Eval, the first end-to-end automated evaluation framework for PostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing LLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task categories-conceptual understanding, syntax-level SQL generation, and schema retrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic databases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model, covering four cognitive dimensions, five capability levels, and twenty task types to establish a comprehensive process from knowledge acquisition and syntax generation to semantic alignment, execution accuracy, and robustness. We evaluate 24 representative models across six categories and apply the entropy weight method with statistical analyses to uncover performance differences, common error patterns, and resource usage. Finally, we release a public GeoSQL-Eval leaderboard platform for continuous testing and global comparison. This work extends the NL2GeoSQL paradigm and provides a standardized, interpretable, and extensible framework for evaluating LLMs in spatial database contexts, offering valuable references for geospatial information science and related applications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data",
    "url": "http://arxiv.org/abs/2509.17544v2",
    "authors": [
      "Juan Ca\u00f1ada",
      "Ra\u00fal Alonso",
      "Julio Molleda",
      "Fidel D\u00edez"
    ],
    "published": "2025-09-22",
    "abstract": "The increasing availability of open Earth Observation (EO) and agricultural datasets holds great potential for supporting sustainable land management. However, their high technical entry barrier limits accessibility for non-expert users. This study presents an open-source conversational assistant that integrates multimodal retrieval and large language models (LLMs) to enable natural language interaction with heterogeneous agricultural and geospatial data. The proposed architecture combines orthophotos, Sentinel-2 vegetation indices, and user-provided documents through retrieval-augmented generation (RAG), allowing the system to flexibly determine whether to rely on multimodal evidence, textual knowledge, or both in formulating an answer. To assess response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional quantitative evaluation framework. Preliminary results show that the system is capable of generating clear, relevant, and context-aware responses to agricultural queries, while remaining reproducible and scalable across geographic regions. The primary contributions of this work include an architecture for fusing multimodal EO and textual knowledge sources, a demonstration of lowering the barrier to access specialized agricultural information through natural language interaction, and an open and reproducible design.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route",
    "url": "http://arxiv.org/abs/2509.18173v1",
    "authors": [
      "Hongyi Luo",
      "Qing Cheng",
      "Daniel Matos",
      "Hari Krishna Gadi",
      "Yanfeng Zhang",
      "Lu Liu",
      "Yongliang Wang",
      "Niclas Zeller",
      "Daniel Cremers",
      "Liqiu Meng"
    ],
    "published": "2025-09-17",
    "abstract": "Humans can interpret geospatial information through natural language, while the geospatial cognition capabilities of Large Language Models (LLMs) remain underexplored. Prior research in this domain has been constrained by non-quantifiable metrics, limited evaluation datasets and unclear research hierarchies. Therefore, we propose a large-scale benchmark and conduct a comprehensive evaluation of the geospatial route cognition of LLMs. We create a large-scale evaluation dataset comprised of 36000 routes from 12 metropolises worldwide. Then, we introduce PathBuilder, a novel tool for converting natural language instructions into navigation routes, and vice versa, bridging the gap between geospatial information and natural language. Finally, we propose a new evaluation framework and metrics to rigorously assess 11 state-of-the-art (SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs exhibit limitation to reverse routes: most reverse routes neither return to the starting point nor are similar to the optimal route. Additionally, LLMs face challenges such as low robustness in route generation and high confidence for their incorrect answers. Code\\ \\&\\ Data available here: \\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "An AI system to help scientists write expert-level empirical software",
    "url": "http://arxiv.org/abs/2509.06503v1",
    "authors": [
      "Eser Ayg\u00fcn",
      "Anastasiya Belyaeva",
      "Gheorghe Comanici",
      "Marc Coram",
      "Hao Cui",
      "Jake Garrison",
      "Renee Johnston Anton Kast",
      "Cory Y. McLean",
      "Peter Norgaard",
      "Zahra Shamsi",
      "David Smalling",
      "James Thompson",
      "Subhashini Venugopalan",
      "Brian P. Williams",
      "Chujun He",
      "Sarah Martinson",
      "Martyna Plomecka",
      "Lai Wei",
      "Yuchen Zhou",
      "Qian-Ze Zhu",
      "Matthew Abraham",
      "Erica Brand",
      "Anna Bulanova",
      "Jeffrey A. Cardille",
      "Chris Co",
      "Scott Ellsworth",
      "Grace Joseph",
      "Malcolm Kane",
      "Ryan Krueger",
      "Johan Kartiwa",
      "Dan Liebling",
      "Jan-Matthis Lueckmann",
      "Paul Raccuglia",
      "Xuefei",
      "Wang",
      "Katherine Chou",
      "James Manyika",
      "Yossi Matias",
      "John C. Platt",
      "Lizzie Dorfman",
      "Shibl Mourad",
      "Michael P. Brenner"
    ],
    "published": "2025-09-08",
    "abstract": "The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system that creates expert-level scientific software whose goal is to maximize a quality metric. The system uses a Large Language Model (LLM) and Tree Search (TS) to systematically improve the quality metric and intelligently navigate the large space of possible solutions. The system achieves expert-level results when it explores and integrates complex research ideas from external sources. The effectiveness of tree search is demonstrated across a wide range of benchmarks. In bioinformatics, it discovered 40 novel methods for single-cell data analysis that outperformed the top human-developed methods on a public leaderboard. In epidemiology, it generated 14 models that outperformed the CDC ensemble and all other individual models for forecasting COVID-19 hospitalizations. Our method also produced state-of-the-art software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting and numerical solution of integrals. By devising and implementing novel solutions to diverse tasks, the system represents a significant step towards accelerating scientific progress.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial Question Answering on Historical Maps Using Spatio-Temporal Knowledge Graphs and Large Language Models",
    "url": "http://arxiv.org/abs/2508.21491v1",
    "authors": [
      "Ziyi Liu",
      "Sidi Wu",
      "Lorenz Hurni"
    ],
    "published": "2025-08-29",
    "abstract": "Recent advances have enabled the extraction of vectorized features from digital historical maps. To fully leverage this information, however, the extracted features must be organized in a structured and meaningful way that supports efficient access and use. One promising approach is question answering (QA), which allows users -- especially those unfamiliar with database query languages -- to retrieve knowledge in a natural and intuitive manner. In this project, we developed a GeoQA system by integrating a spatio-temporal knowledge graph (KG) constructed from historical map data with large language models (LLMs). Specifically, we have defined the ontology to guide the construction of the spatio-temporal KG and investigated workflows of two different types of GeoQA: factual and descriptive. Additional data sources, such as historical map images and internet search results, are incorporated into our framework to provide extra context for descriptive GeoQA. Evaluation results demonstrate that the system can generate answers with a high delivery rate and a high semantic accuracy. To make the framework accessible, we further developed a web application that supports interactive querying and visualization.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "jXBW: Fast Substructure Search for Large-Scale JSONL Datasets with LLM Applications",
    "url": "http://arxiv.org/abs/2508.12536v2",
    "authors": [
      "Yasuo Tabei"
    ],
    "published": "2025-08-18",
    "abstract": "JSON Lines (JSONL) is widely used for managing large collections of semi-structured data, ranging from large language model (LLM) prompts to chemical compound records and geospatial datasets. A key operation is substructure search, which identifies all JSON objects containing a query pattern. This task underpins applications such as drug discovery (querying compounds for functional groups), prompt engineering (extracting prompts with schema fragments), and geospatial analytics (finding entities with nested attributes). However, existing methods are inefficient: traversal requires exhaustive tree matching, succinct JSON representations save space but do not accelerate search, and XML-based approaches incur conversion overhead and semantic mismatches. We present jXBW, a compressed index for efficient substructure search over JSONL. jXBW introduces three innovations: (i) a merged tree representation that consolidates repeated structures, (ii) a succinct tree index based on the eXtended Burrows--Wheeler Transform (XBW), and (iii) a three-phase algorithm for substructure search. These enable query-dependent complexity, where cost depends on query characteristics rather than dataset size, while retaining succinct space. This resolves a key bottleneck in retrieval-augmented generation (RAG) systems requiring structure-aware retrieval. Experiments on seven real datasets, including PubChem (1M compounds) and OSM geospatial data (6.6M objects), achieve up to 4,700$\\times$ speedup over tree-based methods and over $6\\times 10^6$ speedup relative to XML-based approaches. jXBW makes JSONL substructure search practical for the first time, opening opportunities for large-scale LLM-based analytics.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)",
    "url": "http://arxiv.org/abs/2508.04846v1",
    "authors": [
      "Mahdi Nazari Ashani",
      "Ali Asghar Alesheikh",
      "Saba Kazemi",
      "Kimya Kheirkhah",
      "Yasin Mohammadi",
      "Fatemeh Rezaie",
      "Amir Mahdi Manafi",
      "Hedieh Zarkesh"
    ],
    "published": "2025-08-06",
    "abstract": "Autonomous web-based geographical information systems (AWebGIS) aim to perform geospatial operations from natural language input, providing intuitive, intelligent, and hands-free interaction. However, most current solutions rely on cloud-based large language models (LLMs), which require continuous internet access and raise users' privacy and scalability issues due to centralized server processing. This study compares three approaches to enabling AWebGIS: (1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2) a semi-automated offline method using classical machine learning classifiers such as support vector machine and random forest; and (3) a fully autonomous offline (client-side) method based on a fine-tuned small language model (SLM), specifically T5-small model, executed in the client's web browser. The third approach, which leverages SLMs, achieved the highest accuracy among all methods, with an exact matching accuracy of 0.93, Levenshtein similarity of 0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L scores of 0.98. Crucially, this client-side computation strategy reduces the load on backend servers by offloading processing to the user's device, eliminating the need for server-based inference. These results highlight the feasibility of browser-executable models for AWebGIS solutions.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis",
    "url": "http://arxiv.org/abs/2508.01341v3",
    "authors": [
      "Markus B. Pettersson",
      "Connor T. Jerzak",
      "Adel Daoud"
    ],
    "published": "2025-08-02",
    "abstract": "Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a \"one map, many trials\" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GeoJSEval: An Automated Evaluation Framework for Large Language Models on JavaScript-Based Geospatial Computation and Visualization Code Generation",
    "url": "http://arxiv.org/abs/2507.20553v1",
    "authors": [
      "Guanyu Chen",
      "Haoyue Jiao",
      "Shuyang Hou",
      "Ziqi Liu",
      "Lutong Xie",
      "Shaowen Wu",
      "Huayi Wu",
      "Xuefeng Guan",
      "Zhipeng Gui"
    ],
    "published": "2025-07-28",
    "abstract": "With the widespread adoption of large language models (LLMs) in code generation tasks, geospatial code generation has emerged as a critical frontier in the integration of artificial intelligence and geoscientific analysis. This trend underscores the urgent need for systematic evaluation methodologies to assess LLMs generation capabilities in geospatial contexts. In particular, geospatial computation and visualization tasks in JavaScript environments rely heavily on orchestrating diverse frontend libraries and ecosystems, placing elevated demands on a model's semantic understanding and code synthesis abilities. To address this challenge, we propose GeoJSEval--the first multimodal, function-level automatic evaluation framework for LLMs in JavaScript-based geospatial code generation. GeoJSEval comprises three core components: a standardized test suite (GeoJSEval-Bench), a code submission engine, and an evaluation module. It includes 432 function-level tasks and 2,071 structured test cases spanning five widely used JavaScript geospatial libraries and 25 mainstream geospatial data types. GeoJSEval enables multidimensional quantitative evaluation across metrics such as accuracy, output stability, execution efficiency, resource consumption, and error type distribution, and integrates boundary testing mechanisms to enhance robustness and coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs using GeoJSEval, revealing significant performance disparities and bottlenecks in spatial semantic understanding, code reliability, and function invocation accuracy. GeoJSEval provides a foundational methodology, evaluation resource, and practical toolkit for the standardized assessment and optimization of geospatial code generation models, with strong extensibility and applicability in real-world scenarios.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic Estimation using LLM",
    "url": "http://arxiv.org/abs/2507.13323v1",
    "authors": [
      "Kyeongjin Ahn",
      "Sungwon Han",
      "Seungeon Lee",
      "Donghyun Ahn",
      "Hyoshin Kim",
      "Jungwon Kim",
      "Jihee Kim",
      "Sangyoon Park",
      "Meeyoung Cha"
    ],
    "published": "2025-07-17",
    "abstract": "Socio-economic indicators like regional GDP, population, and education levels, are crucial to shaping policy decisions and fostering sustainable development. This research introduces GeoReg a regression model that integrates diverse data sources, including satellite imagery and web-based geospatial information, to estimate these indicators even for data-scarce regions such as developing countries. Our approach leverages the prior knowledge of large language model (LLM) to address the scarcity of labeled data, with the LLM functioning as a data engineer by extracting informative features to enable effective estimation in few-shot settings. Specifically, our model obtains contextual relationships between data features and the target indicator, categorizing their correlations as positive, negative, mixed, or irrelevant. These features are then fed into the linear estimator with tailored weight constraints for each category. To capture nonlinear patterns, the model also identifies meaningful feature interactions and integrates them, along with nonlinear transformations. Experiments across three countries at different stages of development demonstrate that our model outperforms baselines in estimating socio-economic indicators, even for low-income countries with limited data availability.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Regression"
    ],
    "is_recent": false
  },
  {
    "title": "SPOT: Bridging Natural Language and Geospatial Search for Investigative Journalists",
    "url": "http://arxiv.org/abs/2506.13188v1",
    "authors": [
      "Lynn Khellaf",
      "Ipek Baris Schlicht",
      "Tilman Mirass",
      "Julia Bayer",
      "Tilman Wagner",
      "Ruben Bouwmeester"
    ],
    "published": "2025-06-16",
    "abstract": "OpenStreetMap (OSM) is a vital resource for investigative journalists doing geolocation verification. However, existing tools to query OSM data such as Overpass Turbo require familiarity with complex query languages, creating barriers for non-technical users. We present SPOT, an open source natural language interface that makes OSM's rich, tag-based geographic data more accessible through intuitive scene descriptions. SPOT interprets user inputs as structured representations of geospatial object configurations using fine-tuned Large Language Models (LLMs), with results being displayed in an interactive map interface. While more general geospatial search tasks are conceivable, SPOT is specifically designed for use in investigative journalism, addressing real-world challenges such as hallucinations in model output, inconsistencies in OSM tagging, and the noisy nature of user input. It combines a novel synthetic data pipeline with a semantic bundling system to enable robust, accurate query generation. To our knowledge, SPOT is the first system to achieve reliable natural language access to OSM data at this level of accuracy. By lowering the technical barrier to geolocation verification, SPOT contributes a practical tool to the broader efforts to support fact-checking and combat disinformation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant",
    "url": "http://arxiv.org/abs/2506.11781v1",
    "authors": [
      "Gaspard Merten",
      "Gilles Dejaegere",
      "Mahmoud Sakr"
    ],
    "published": "2025-06-13",
    "abstract": "Geospatial data analysis plays a crucial role in tackling intricate societal challenges such as urban planning and climate modeling. However, employing tools like GeoPandas, a prominent Python library for geospatial data manipulation, necessitates expertise in complex domain-specific syntax and workflows. GeoPandas-AI addresses this gap by integrating LLMs directly into the GeoPandas workflow, transforming the GeoDataFrame class into an intelligent, stateful class for both data analysis and geospatial code development. This paper formalizes the design of such a smart class and provides an open-source implementation of GeoPandas-AI in PyPI package manager. Through its innovative combination of conversational interfaces and stateful exploitation of LLMs for code generation and data analysis, GeoPandas-AI introduces a new paradigm for code-copilots and instantiates it for geospatial development.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment",
    "url": "http://arxiv.org/abs/2506.06355v1",
    "authors": [
      "Lingyao Li",
      "Dawei Li",
      "Zhenhui Ou",
      "Xiaoran Xu",
      "Jingxiao Liu",
      "Zihui Ma",
      "Runlong Yu",
      "Min Deng"
    ],
    "published": "2025-06-02",
    "abstract": "Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes. Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios. This study examines multiple LLMs to proactively estimate perceived earthquake impacts. Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales. Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real reports at the zip code level. Techniques such as RAG and ICL can improve simulation performance, while visual inputs notably enhance accuracy compared to structured numerical data alone. These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features",
    "url": "http://arxiv.org/abs/2506.00203v1",
    "authors": [
      "Omid Reza Abbasi",
      "Franz Welscher",
      "Georg Weinberger",
      "Johannes Scholz"
    ],
    "published": "2025-05-30",
    "abstract": "As large language models (LLMs) continue to evolve, questions about their trustworthiness in delivering factual information have become increasingly important. This concern also applies to their ability to accurately represent the geographic world. With recent advancements in this field, it is relevant to consider whether and to what extent LLMs' representations of the geographical world can be trusted. This study evaluates the performance of GPT-4o and Gemini 2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and reverse geocoding. In the geocoding task, both models exhibited systematic and random errors in estimating the coordinates of St. Anne's Column in Innsbruck, Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash demonstrating more precision but a significant systematic offset. For elevation estimation, both models tended to underestimate elevations across Austria, though they captured overall topographical trends, and Gemini 2.0 Flash performed better in eastern regions. The reverse geocoding task, which involved identifying Austrian federal states from coordinates, revealed that Gemini 2.0 Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating better consistency across regions. Despite these findings, neither model achieved an accurate reconstruction of Austria's federal states, highlighting persistent misclassifications. The study concludes that while LLMs can approximate geographic information, their accuracy and reliability are inconsistent, underscoring the need for fine-tuning with geographical information to enhance their utility in GIScience and Geoinformatics.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models",
    "url": "http://arxiv.org/abs/2505.24340v1",
    "authors": [
      "Gilles Quentin Hacheme",
      "Girmaw Abebe Tadesse",
      "Caleb Robinson",
      "Akram Zaytar",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ],
    "published": "2025-05-30",
    "abstract": "Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that claim zero-shot classification capabilities for satellite imagery nonetheless rely on task-specific pretraining and adaptation to reach competitive performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot classification framework: a vision Large Language Model (vLLM) generates rich, human-readable image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). This modular, and interpretable pipeline enables flexible image classification for a large range of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced, and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class classification tasks (UC Merced, RESISC45), we implemented a recursive LLM-driven clustering to form meta-classes at successive depths, followed by hierarchical classification-first resolving coarse groups, then finer distinctions-to deliver competitive zero-shot performance. GVL is open-sourced at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in real-world geospatial workflows.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM",
      "CLIP"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "The Role of Open-Source LLMs in Shaping the Future of GeoAI",
    "url": "http://arxiv.org/abs/2504.17833v2",
    "authors": [
      "Xiao Huang",
      "Zhengzhong Tu",
      "Xinyue Ye",
      "Michael Goodchild"
    ],
    "published": "2025-04-24",
    "abstract": "Large Language Models (LLMs) are transforming geospatial artificial intelligence (GeoAI), offering new capabilities in data processing, spatial analysis, and decision support. This paper examines the open-source paradigm's critical role in this transformation. While proprietary LLMs offer accessibility, they often limit the customization, interoperability, and transparency vital for specialized geospatial tasks. Conversely, open-source alternatives significantly advance Geographic Information Science (GIScience) by fostering greater adaptability, reproducibility, and community-driven innovation. Open frameworks empower researchers to tailor solutions, integrate cutting-edge methodologies (e.g., reinforcement learning, advanced spatial indexing), and align with FAIR (Findable, Accessible, Interoperable, and Reusable) principles. However, the growing reliance on any LLM necessitates careful consideration of security vulnerabilities, ethical risks, and robust governance for AI-generated geospatial outputs. This paper argues that GIScience advances best not through a single model type, but by cultivating a diverse, interoperable ecosystem combining open-source foundations for innovation, custom geospatial models, and interdisciplinary collaboration. By critically evaluating the opportunities and challenges of open-source LLMs within the broader GeoAI landscape, this work contributes to a thorough discourse on leveraging LLMs to effectively advance spatial research, policy, and decision-making in an equitable, sustainable, and scientifically rigorous manner.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Reinforcement Learning"
    ],
    "is_recent": false
  },
  {
    "title": "Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction",
    "url": "http://arxiv.org/abs/2503.20981v1",
    "authors": [
      "Xiaoran Xu",
      "Zhaoqian Xue",
      "Chi Zhang",
      "Jhonatan Medri",
      "Junjie Xiong",
      "Jiayan Zhou",
      "Jin Jin",
      "Yongfeng Zhang",
      "Siyuan Ma",
      "Lingyao Li"
    ],
    "published": "2025-03-26",
    "abstract": "Investigating the public experience of urgent care facilities is essential for promoting community healthcare development. Traditional survey methods often fall short due to limited scope, time, and spatial coverage. Crowdsourcing through online reviews or social media offers a valuable approach to gaining such insights. With recent advancements in large language models (LLMs), extracting nuanced perceptions from reviews has become feasible. This study collects Google Maps reviews across the DMV and Florida areas and conducts prompt engineering with the GPT model to analyze the aspect-based sentiment of urgent care. We first analyze the geospatial patterns of various aspects, including interpersonal factors, operational efficiency, technical quality, finances, and facilities. Next, we determine Census Block Group(CBG)-level characteristics underpinning differences in public perception, including population density, median income, GINI Index, rent-to-income ratio, household below poverty rate, no insurance rate, and unemployment rate. Our results show that interpersonal factors and operational efficiency emerge as the strongest determinants of patient satisfaction in urgent care, while technical quality, finances, and facilities show no significant independent effects when adjusted for in multivariate models. Among socioeconomic and demographic factors, only population density demonstrates a significant but modest association with patient ratings, while the remaining factors exhibit no significant correlations. Overall, this study highlights the potential of crowdsourcing to uncover the key factors that matter to residents and provide valuable insights for stakeholders to improve public satisfaction with urgent care.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite Images by Poverty Level: Advancing Tools for Social Science Research",
    "url": "http://arxiv.org/abs/2501.14546v1",
    "authors": [
      "Hamid Sarmadi",
      "Ola Hall",
      "Thorsteinn R\u00f6gnvaldsson",
      "Mattias Ohlsson"
    ],
    "published": "2025-01-24",
    "abstract": "This paper investigates the novel application of Large Language Models (LLMs) with vision capabilities to analyze satellite imagery for village-level poverty prediction. Although LLMs were originally designed for natural language understanding, their adaptability to multimodal tasks, including geospatial analysis, has opened new frontiers in data-driven research. By leveraging advancements in vision-enabled LLMs, we assess their ability to provide interpretable, scalable, and reliable insights into human poverty from satellite images. Using a pairwise comparison approach, we demonstrate that ChatGPT can rank satellite images based on poverty levels with accuracy comparable to domain experts. These findings highlight both the promise and the limitations of LLMs in socioeconomic research, providing a foundation for their integration into poverty assessment workflows. This study contributes to the ongoing exploration of unconventional data sources for welfare analysis and opens pathways for cost-effective, large-scale poverty monitoring.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Augmenting a Large Language Model with a Combination of Text and Visual Data for Conversational Visualization of Global Geospatial Data",
    "url": "http://arxiv.org/abs/2501.09521v1",
    "authors": [
      "Omar Mena",
      "Alexandre Kouyoumdjian",
      "Lonni Besan\u00e7on",
      "Michael Gleicher",
      "Ivan Viola",
      "Anders Ynnerman"
    ],
    "published": "2025-01-16",
    "abstract": "We present a method for augmenting a Large Language Model (LLM) with a combination of text and visual data to enable accurate question answering in visualization of scientific data, making conversational visualization possible. LLMs struggle with tasks like visual data interaction, as they lack contextual visual information. We address this problem by merging a text description of a visualization and dataset with snapshots of the visualization. We extract their essential features into a structured text file, highly compact, yet descriptive enough to appropriately augment the LLM with contextual information, without any fine-tuning. This approach can be applied to any visualization that is already finally rendered, as long as it is associated with some textual description.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Comparative Performance of Advanced NLP Models and LLMs in Multilingual Geo-Entity Detection",
    "url": "http://arxiv.org/abs/2412.20414v1",
    "authors": [
      "Kalin Kopanov"
    ],
    "published": "2024-12-29",
    "abstract": "The integration of advanced Natural Language Processing (NLP) methodologies and Large Language Models (LLMs) has significantly enhanced the extraction and analysis of geospatial data from multilingual texts, impacting sectors such as national and international security. This paper presents a comprehensive evaluation of leading NLP models -- SpaCy, XLM-RoBERTa, mLUKE, GeoLM -- and LLMs, specifically OpenAI's GPT 3.5 and GPT 4, within the context of multilingual geo-entity detection. Utilizing datasets from Telegram channels in English, Russian, and Arabic, we examine the performance of these models through metrics such as accuracy, precision, recall, and F1 scores, to assess their effectiveness in accurately identifying geospatial references. The analysis exposes each model's distinct advantages and challenges, underscoring the complexities involved in achieving precise geo-entity identification across varied linguistic landscapes. The conclusions drawn from this experiment aim to direct the enhancement and creation of more advanced and inclusive NLP tools, thus advancing the field of geospatial analysis and its application to global security.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on the Google Earth Engine Platform Powered by Large Language Models",
    "url": "http://arxiv.org/abs/2412.05587v2",
    "authors": [
      "Shuyang Hou",
      "Jianyuan Liang",
      "Anqi Zhao",
      "Huayi Wu"
    ],
    "published": "2024-12-07",
    "abstract": "As the scale and complexity of spatiotemporal data continue to grow rapidly, the use of geospatial modeling on the Google Earth Engine (GEE) platform presents dual challenges: improving the coding efficiency of domain experts and enhancing the coding capabilities of interdisciplinary users. To address these challenges and improve the performance of large language models (LLMs) in geospatial code generation tasks, we propose a framework for building a geospatial operator knowledge base tailored to the GEE JavaScript API. This framework consists of an operator syntax knowledge table, an operator relationship frequency table, an operator frequent pattern knowledge table, and an operator relationship chain knowledge table. By leveraging Abstract Syntax Tree (AST) techniques and frequent itemset mining, we systematically extract operator knowledge from 185,236 real GEE scripts and syntax documentation, forming a structured knowledge base. Experimental results demonstrate that the framework achieves over 90% accuracy, recall, and F1 score in operator knowledge extraction. When integrated with the Retrieval-Augmented Generation (RAG) strategy for LLM-based geospatial code generation tasks, the knowledge base improves performance by 20-30%. Ablation studies further quantify the necessity of each knowledge table in the knowledge base construction. This work provides robust support for the advancement and application of geospatial code modeling techniques, offering an innovative approach to constructing domain-specific knowledge bases that enhance the code generation capabilities of LLMs, and fostering the deeper integration of generative AI technologies within the field of geoinformatics.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Chain-of-Programming (CoP) : Empowering Large Language Models for Geospatial Code Generation",
    "url": "http://arxiv.org/abs/2411.10753v1",
    "authors": [
      "Shuyang Hou",
      "Haoyue Jiao",
      "Zhangxiao Shen",
      "Jianyuan Liang",
      "Anqi Zhao",
      "Xiaopu Zhang",
      "Jianxun Wang",
      "Huayi Wu"
    ],
    "published": "2024-11-16",
    "abstract": "With the rapid growth of interdisciplinary demands for geospatial modeling and the rise of large language models (LLMs), geospatial code generation technology has seen significant advancements. However, existing LLMs often face challenges in the geospatial code generation process due to incomplete or unclear user requirements and insufficient knowledge of specific platform syntax rules, leading to the generation of non-executable code, a phenomenon known as \"code hallucination.\" To address this issue, this paper proposes a Chain of Programming (CoP) framework, which decomposes the code generation process into five steps: requirement analysis, algorithm design, code implementation, code debugging, and code annotation. The framework incorporates a shared information pool, knowledge base retrieval, and user feedback mechanisms, forming an end-to-end code generation flow from requirements to code without the need for model fine-tuning. Based on a geospatial problem classification framework and evaluation benchmarks, the CoP strategy significantly improves the logical clarity, syntactical correctness, and executability of the generated code, with improvements ranging from 3.0% to 48.8%. Comparative and ablation experiments further validate the superiority of the CoP strategy over other optimization approaches and confirm the rationality and necessity of its key components. Through case studies on building data visualization and fire data analysis, this paper demonstrates the application and effectiveness of CoP in various geospatial scenarios. The CoP framework offers a systematic, step-by-step approach to LLM-based geospatial code generation tasks, significantly enhancing code generation performance in geospatial tasks and providing valuable insights for code generation in other vertical domains.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base for Geospatial Code Generation Tasks Using Large Language Models",
    "url": "http://arxiv.org/abs/2410.20975v1",
    "authors": [
      "Shuyang Hou",
      "Anqi Zhao",
      "Jianyuan Liang",
      "Zhangxiao Shen",
      "Huayi Wu"
    ],
    "published": "2024-10-28",
    "abstract": "The rise of spatiotemporal data and the need for efficient geospatial modeling have spurred interest in automating these tasks with large language models (LLMs). However, general LLMs often generate errors in geospatial code due to a lack of domain-specific knowledge on functions and operators. To address this, a retrieval-augmented generation (RAG) approach, utilizing an external knowledge base of geospatial functions and operators, is proposed. This study introduces a framework to construct such a knowledge base, leveraging geospatial script semantics. The framework includes: Function Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination Statistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like Chain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and align geospatial functions. An example knowledge base, Geo-FuB, built from 154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics show a high accuracy, reaching 88.89% overall, with structural and semantic accuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize geospatial code generation through the RAG and fine-tuning paradigms is highlighted.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks",
    "url": "http://arxiv.org/abs/2410.17031v2",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Anqi Zhao",
      "Jianyuan Liang",
      "Zhipeng Gui",
      "Xuefeng Guan",
      "Rui Li",
      "Huayi Wu"
    ],
    "published": "2024-10-22",
    "abstract": "The increasing demand for spatiotemporal data and modeling tasks in geosciences has made geospatial code generation technology a critical factor in enhancing productivity. Although large language models (LLMs) have demonstrated potential in code generation tasks, they often encounter issues such as refusal to code or hallucination in geospatial code generation due to a lack of domain-specific knowledge and code corpora. To address these challenges, this paper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along with the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and LoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first LLM focused on geospatial code generation, fine-tuned from Code Llama-7B. Furthermore, we establish a comprehensive geospatial code evaluation framework, incorporating option matching, expert validation, and prompt engineering scoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the GeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms other models in multiple-choice accuracy by 9.1% to 32.1%, in code summarization ability by 1.7% to 25.4%, and in code generation capability by 1.2% to 25.1%. This paper provides a solution and empirical validation for enhancing LLMs' performance in geospatial code generation, extends the boundaries of domain-specific model applications, and offers valuable insights into unlocking their potential in geospatial code generation.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Beyond Words: Evaluating Large Language Models in Transportation Planning",
    "url": "http://arxiv.org/abs/2409.14516v1",
    "authors": [
      "Shaowei Ying",
      "Zhenlong Li",
      "Manzhu Yu"
    ],
    "published": "2024-09-22",
    "abstract": "The resurgence and rapid advancement of Generative Artificial Intelligence (GenAI) in 2023 has catalyzed transformative shifts across numerous industry sectors, including urban transportation and logistics. This study investigates the evaluation of Large Language Models (LLMs), specifically GPT-4 and Phi-3-mini, to enhance transportation planning. The study assesses the performance and spatial comprehension of these models through a transportation-informed evaluation framework that includes general geospatial skills, general transportation domain skills, and real-world transportation problem-solving. Utilizing a mixed-methods approach, the research encompasses an evaluation of the LLMs' general Geographic Information System (GIS) skills, general transportation domain knowledge as well as abilities to support human decision-making in the real-world transportation planning scenarios of congestion pricing. Results indicate that GPT-4 demonstrates superior accuracy and reliability across various GIS and transportation-specific tasks compared to Phi-3-mini, highlighting its potential as a robust tool for transportation planners. Nonetheless, Phi-3-mini exhibits competence in specific analytical scenarios, suggesting its utility in resource-constrained environments. The findings underscore the transformative potential of GenAI technologies in urban transportation planning. Future work could explore the application of newer LLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a broader set of real-world transportation planning and operations challenges, to deepen the integration of advanced AI models in transportation management practices.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning",
    "url": "http://arxiv.org/abs/2408.12116v2",
    "authors": [
      "Junlin He",
      "Tong Nie",
      "Wei Ma"
    ],
    "published": "2024-08-22",
    "abstract": "In the geospatial domain, universal representation models are significantly less prevalent than their extensive use in natural language processing and computer vision. This discrepancy arises primarily from the high costs associated with the input of existing representation models, which often require street views and mobility data. To address this, we develop a novel, training-free method that leverages large language models (LLMs) and auxiliary map data from OpenStreetMap to derive geolocation representations (LLMGeovec). LLMGeovec can represent the geographic semantics of city, country, and global scales, which acts as a generic enhancer for spatio-temporal learning. Specifically, by direct feature concatenation, we introduce a simple yet effective paradigm for enhancing multiple spatio-temporal tasks including geographic prediction (GP), long-term time series forecasting (LTSF), and graph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly integrate into a wide spectrum of spatio-temporal learning models, providing immediate enhancements. Experimental results demonstrate that LLMGeovec achieves global coverage and significantly boosts the performance of leading GP, LTSF, and GSTF models. Our codes are available at \\url{https://github.com/Umaruchain/LLMGeovec}.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks",
    "url": "http://arxiv.org/abs/2406.13945v3",
    "authors": [
      "Jie Feng",
      "Jun Zhang",
      "Tianhui Liu",
      "Xin Zhang",
      "Tianjian Ouyang",
      "Junbo Yan",
      "Yuwei Du",
      "Siqi Guo",
      "Yong Li"
    ],
    "published": "2024-06-20",
    "abstract": "As large language models (LLMs) continue to advance and gain widespread use, establishing systematic and reliable evaluation methodologies for LLMs and vision-language models (VLMs) has become essential to ensure their real-world effectiveness and reliability. There have been some early explorations about the usability of LLMs for limited urban tasks, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for urban research lies in the diversity of urban data, the complexity of application scenarios and the highly dynamic nature of the urban environment. In this paper, we design \\textit{CityBench}, an interactive simulator based evaluation platform, as the first systematic benchmark for evaluating the capabilities of LLMs for diverse tasks in urban research. First, we build \\textit{CityData} to integrate the diverse urban data and \\textit{CitySimu} to simulate fine-grained urban dynamics. Based on \\textit{CityData} and \\textit{CitySimu}, we design 8 representative urban tasks in 2 categories of perception-understanding and decision-making as the \\textit{CityBench}. With extensive results from 30 well-known LLMs and VLMs in 13 cities around the world, we find that advanced LLMs and VLMs can achieve competitive performance in diverse urban tasks requiring commonsense and semantic understanding abilities, e.g., understanding the human dynamics and semantic inference of urban images. Meanwhile, they fail to solve the challenging urban tasks requiring professional knowledge and high-level numerical abilities, e.g., geospatial prediction and traffic control task.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial Big Data: Survey and Challenges",
    "url": "http://arxiv.org/abs/2404.18428v1",
    "authors": [
      "Jiayang Wu",
      "Wensheng Gan",
      "Han-Chieh Chao",
      "Philip S. Yu"
    ],
    "published": "2024-04-29",
    "abstract": "In recent years, geospatial big data (GBD) has obtained attention across various disciplines, categorized into big earth observation data and big human behavior data. Identifying geospatial patterns from GBD has been a vital research focus in the fields of urban management and environmental sustainability. This paper reviews the evolution of GBD mining and its integration with advanced artificial intelligence (AI) techniques. GBD consists of data generated by satellites, sensors, mobile devices, and geographical information systems, and we categorize geospatial data based on different perspectives. We outline the process of GBD mining and demonstrate how it can be incorporated into a unified framework. Additionally, we explore new technologies like large language models (LLM), the Metaverse, and knowledge graphs, and how they could make GBD even more useful. We also share examples of GBD helping with city management and protecting the environment. Finally, we discuss the real challenges that come up when working with GBD, such as issues with data retrieval and security. Our goal is to give readers a clear view of where GBD mining stands today and where it might go next.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "LAMP: A Language Model on the Map",
    "url": "http://arxiv.org/abs/2403.09059v2",
    "authors": [
      "Pasquale Balsebre",
      "Weiming Huang",
      "Gao Cong"
    ],
    "published": "2024-03-14",
    "abstract": "Large Language Models (LLMs) are poised to play an increasingly important role in our lives, providing assistance across a wide array of tasks. In the geospatial domain, LLMs have demonstrated the ability to answer generic questions, such as identifying a country's capital; nonetheless, their utility is hindered when it comes to answering fine-grained questions about specific places, such as grocery stores or restaurants, which constitute essential aspects of people's everyday lives. This is mainly because the places in our cities haven't been systematically fed into LLMs, so as to understand and memorize them. This study introduces a novel framework for fine-tuning a pre-trained model on city-specific data, to enable it to provide accurate recommendations, while minimizing hallucinations. We share our model, LAMP, and the data used to train it. We conduct experiments to analyze its ability to correctly retrieving spatial objects, and compare it to well-known open- and closed- source language models, such as GPT-4. Finally, we explore its emerging capabilities through a case study on day planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
    "url": "http://arxiv.org/abs/2411.19325v2",
    "authors": [
      "Muhammad Sohail Danish",
      "Muhammad Akhtar Munir",
      "Syed Roshaan Ali Shah",
      "Kartik Kuckreja",
      "Fahad Shahbaz Khan",
      "Paolo Fraccaro",
      "Alexandre Lacoste",
      "Salman Khan"
    ],
    "published": "2024-11-28",
    "abstract": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management. Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery. To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales. We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark is publicly available at https://github.com/The-AI-Alliance/GEO-Bench-VLM .",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Segmentation",
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Zero-shot Building Age Classification from Facade Image Using GPT-4",
    "url": "http://arxiv.org/abs/2404.09921v1",
    "authors": [
      "Zichao Zeng",
      "June Moh Goo",
      "Xinglei Wang",
      "Bin Chi",
      "Meihui Wang",
      "Jan Boehm"
    ],
    "published": "2024-04-15",
    "abstract": "A building's age of construction is crucial for supporting many geospatial applications. Much current research focuses on estimating building age from facade images using deep learning. However, building an accurate deep learning model requires a considerable amount of labelled training data, and the trained models often have geographical constraints. Recently, large pre-trained vision language models (VLMs) such as GPT-4 Vision, which demonstrate significant generalisation capabilities, have emerged as potential training-free tools for dealing with specific vision tasks, but their applicability and reliability for building information remain unexplored. In this study, a zero-shot building age classifier for facade images is developed using prompts that include logical instructions. Taking London as a test case, we introduce a new dataset, FI-London, comprising facade images and building age epochs. Although the training-free classifier achieved a modest accuracy of 39.69%, the mean absolute error of 0.85 decades indicates that the model can predict building age epochs successfully albeit with a small bias. The ensuing discussion reveals that the classifier struggles to predict the age of very old buildings and is challenged by fine-grained predictions within 2 decades. Overall, the classifier utilising GPT-4 Vision is capable of predicting the rough age epoch of a building from a single facade image without any training.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LVM"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Trust in foundation models and GenAI: A geographic perspective",
    "url": "http://arxiv.org/abs/2510.17942v1",
    "authors": [
      "Grant McKenzie",
      "Krzysztof Janowicz",
      "Carsten Kessler"
    ],
    "published": "2025-10-20",
    "abstract": "Large-scale pre-trained machine learning models have reshaped our understanding of artificial intelligence across numerous domains, including our own field of geography. As with any new technology, trust has taken on an important role in this discussion. In this chapter, we examine the multifaceted concept of trust in foundation models, particularly within a geographic context. As reliance on these models increases and they become relied upon for critical decision-making, trust, while essential, has become a fractured concept. Here we categorize trust into three types: epistemic trust in the training data, operational trust in the model's functionality, and interpersonal trust in the model developers. Each type of trust brings with it unique implications for geographic applications. Topics such as cultural context, data heterogeneity, and spatial relationships are fundamental to the spatial sciences and play an important role in developing trust. The chapter continues with a discussion of the challenges posed by different forms of biases, the importance of transparency and explainability, and ethical responsibilities in model development. Finally, the novel perspective of geographic information scientists is emphasized with a call for further transparency, bias mitigation, and regionally-informed policies. Simply put, this chapter aims to provide a conceptual starting point for researchers, practitioners, and policy-makers to better understand trust in (generative) GeoAI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety",
    "url": "http://arxiv.org/abs/2510.15434v1",
    "authors": [
      "Huan Chen",
      "Ting Han",
      "Siyu Chen",
      "Zhihao Guo",
      "Yiping Chen",
      "Meiliu Wu"
    ],
    "published": "2025-10-17",
    "abstract": "Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two fundamental challenges persist: (1) how to construct street-level indicators that capture accident-related features, and (2) how to quantify their causal impacts across different accident types. To address these challenges, we propose Semantic4Safety, a framework that applies zero-shot semantic segmentation to SVIs to derive 11 interpretable streetscape indicators, and integrates road type as contextual information to analyze approximately 30,000 accident records in Austin. Specifically, we train an eXtreme Gradient Boosting (XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP) to interpret both global and local feature contributions, and then apply Generalized Propensity Score (GPS) weighting and Average Treatment Effect (ATE) estimation to control confounding and quantify causal effects. Results uncover heterogeneous, accident-type-specific causal patterns: features capturing scene complexity, exposure, and roadway geometry dominate predictive power; larger drivable area and emergency space reduce risk, whereas excessive visual openness can increase it. By bridging predictive modeling with causal inference, Semantic4Safety supports targeted interventions and high-risk corridor diagnosis, offering a scalable, data-informed tool for urban road safety planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ],
    "is_recent": false
  },
  {
    "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)",
    "url": "http://arxiv.org/abs/2510.14661v1",
    "authors": [
      "Weikang Yu",
      "Vincent Nwazelibe",
      "Xianping Ma",
      "Xiaokang Zhang",
      "Richard Gloaguen",
      "Xiao Xiang Zhu",
      "Pedram Ghamisi"
    ],
    "published": "2025-10-16",
    "abstract": "Mining activities are essential for industrial and economic development, but remain a leading source of environmental degradation, contributing to deforestation, soil erosion, and water contamination. Sustainable resource management and environmental governance require consistent, long-term monitoring of mining-induced land surface changes, yet existing datasets are often limited in temporal depth or geographic scope. To address this gap, we present EuroMineNet, the first comprehensive multitemporal benchmark for mining footprint mapping and monitoring based on Sentinel-2 multispectral imagery. Spanning 133 mining sites across the European Union, EuroMineNet provides annual observations and expert-verified annotations from 2015 to 2024, enabling GeoAI-based models to analyze environmental dynamics at a continental scale. It supports two sustainability-driven tasks: (1) multitemporal mining footprint mapping for consistent annual land-use delineation, evaluated with a novel Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change detection to capture both gradual and abrupt surface transformations. Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI methods effectively identify long-term environmental changes, challenges remain in detecting short-term dynamics critical for timely mitigation. By advancing temporally consistent and explainable mining monitoring, EuroMineNet contributes to sustainable land-use management, environmental resilience, and the broader goal of applying GeoAI for social and environmental good. We release the codes and datasets by aligning with FAIR and the open science paradigm at https://github.com/EricYu97/EuroMineNet.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Deep Learning to Identify the Spatio-Temporal Cascading Effects of Train Delays in a High-Density Network",
    "url": "http://arxiv.org/abs/2510.09350v1",
    "authors": [
      "Vu Duc Anh Nguyen",
      "Ziyue Li"
    ],
    "published": "2025-10-10",
    "abstract": "The operational efficiency of railway networks, a cornerstone of modern economies, is persistently undermined by the cascading effects of train delays. Accurately forecasting this delay propagation is a critical challenge for real-time traffic management. While recent research has leveraged Graph Neural Networks (GNNs) to model the network structure of railways, a significant gap remains in developing frameworks that provide multi-step autoregressive forecasts at a network-wide scale, while simultaneously offering the live, interpretable explanations needed for decision support. This paper addresses this gap by developing and evaluating a novel XGeoAI framework for live, explainable, multi-step train delay forecasting. The core of this work is a two-stage, autoregressive Graph Attention Network (GAT) model, trained on a real-world dataset covering over 40% of the Dutch railway network. The model represents the system as a spatio-temporal graph of operational events (arrivals and departures) and is enriched with granular features, including platform and station congestion. To test its viability for live deployment, the model is rigorously evaluated using a sequential, k-step-ahead forecasting protocol that simulates real-world conditions where prediction errors can compound. The results demonstrate that while the proposed GATv2 model is challenged on pure error metrics (MAE) by a simpler Persistence baseline, it achieves consistently higher precision in classifying delay events -- a crucial advantage for a reliable decision support tool.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "OBSR: Open Benchmark for Spatial Representations",
    "url": "http://arxiv.org/abs/2510.05879v2",
    "authors": [
      "Julia Moska",
      "Oleksii Furman",
      "Kacper Kozaczko",
      "Szymon Leszkiewicz",
      "Jakub Polczyk",
      "Piotr Gramacki",
      "Piotr Szyma\u0144ski"
    ],
    "published": "2025-10-07",
    "abstract": "GeoAI is evolving rapidly, fueled by diverse geospatial datasets like traffic patterns, environmental data, and crowdsourced OpenStreetMap (OSM) information. While sophisticated AI models are being developed, existing benchmarks are often concentrated on single tasks and restricted to a single modality. As such, progress in GeoAI is limited by the lack of a standardized, multi-task, modality-agnostic benchmark for their systematic evaluation. This paper introduces a novel benchmark designed to assess the performance, accuracy, and efficiency of geospatial embedders. Our benchmark is modality-agnostic and comprises 7 distinct datasets from diverse cities across three continents, ensuring generalizability and mitigating demographic biases. It allows for the evaluation of GeoAI embedders on various phenomena that exhibit underlying geographic processes. Furthermore, we establish a simple and intuitive task-oriented model baselines, providing a crucial reference point for comparing more complex solutions.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models",
    "url": "http://arxiv.org/abs/2509.23482v1",
    "authors": [
      "Zhangyu Wang",
      "Nemin Wu",
      "Qian Cao",
      "Jiangnan Xia",
      "Zeping Liu",
      "Yiqun Xie",
      "Akshay Nambi",
      "Tanuja Ganu",
      "Ni Lao",
      "Ninghao Liu",
      "Gengchen Mai"
    ],
    "published": "2025-09-27",
    "abstract": "The widespread adoption of AI models, especially foundation models (FMs), has made a profound impact on numerous domains. However, it also raises significant ethical concerns, including bias issues. Although numerous efforts have been made to quantify and mitigate social bias in AI models, geographic bias (in short, geo-bias) receives much less attention, which presents unique challenges. While previous work has explored ways to quantify geo-bias, these measures are model-specific (e.g., mean absolute deviation of LLM ratings) or spatially implicit (e.g., average fairness scores of all spatial partitions). We lack a model-agnostic, universally applicable, and spatially explicit geo-bias evaluation framework that allows researchers to fairly compare the geo-bias of different AI models and to understand what spatial factors contribute to the geo-bias. In this paper, we establish an information-theoretic framework for geo-bias evaluation, called GeoBS (Geo-Bias Scores). We demonstrate the generalizability of the proposed framework by showing how to interpret and analyze existing geo-bias measures under this framework. Then, we propose three novel geo-bias scores that explicitly take intricate spatial factors (multi-scalability, distance decay, and anisotropy) into consideration. Finally, we conduct extensive experiments on 3 tasks, 8 datasets, and 8 models to demonstrate that both task-specific GeoAI models and general-purpose foundation models may suffer from various types of geo-bias. This framework will not only advance the technical understanding of geographic bias but will also establish a foundation for integrating spatial fairness into the design, deployment, and evaluation of AI systems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025",
    "url": "http://arxiv.org/abs/2509.03191v1",
    "authors": [
      "Taiga Saito",
      "Yu Otake",
      "Stephen Wu"
    ],
    "published": "2025-09-03",
    "abstract": "This paper presents a novel application of the Tabular Prior-Data Fitted Network (TabPFN) - a transformer-based foundation model for tabular data - to geotechnical site characterization problems defined in the GEOAI benchmark BM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the spatial variation of undrained shear strength (su) across borehole depth profiles, and (2) imputing missing mechanical parameters in a dense-site dataset. We apply TabPFN in a zero-training, few-shot, in-context learning setting - without hyper-parameter tuning - and provide it with additional context from the big indirect database (BID). The study demonstrates that TabPFN, as a general-purpose foundation model, achieved superior accuracy and well-calibrated predictive distributions compared to a conventional hierarchical Bayesian model (HBM) baseline, while also offering significant gains in inference efficiency. In Benchmark Problem #1 (spatial su prediction), TabPFN outperformed the HBM in prediction accuracy and delivered an order-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical parameter imputation), TabPFN likewise achieved lower RMSE for all target parameters with well-quantified uncertainties, though its cumulative computation cost was higher than HBM's due to its one-variable-at-a-time inference. These results mark the first successful use of a tabular foundation model in geotechnical modeling, suggesting a potential paradigm shift in probabilistic site characterization.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation",
    "url": "http://arxiv.org/abs/2509.01341v1",
    "authors": [
      "Yunus Serhat Bicakci",
      "Joseph Shingleton",
      "Anahid Basiri"
    ],
    "published": "2025-09-01",
    "abstract": "Street-level geolocalization from images is crucial for a wide range of essential applications and services, such as navigation, location-based recommendations, and urban planning. With the growing popularity of social media data and cameras embedded in smartphones, applying traditional computer vision techniques to localize images has become increasingly challenging, yet highly valuable. This paper introduces a novel approach that integrates open-weight and publicly accessible multimodal large language models with retrieval-augmented generation. The method constructs a vector database using the SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query images are augmented with prompts containing both similar and dissimilar geolocation information retrieved from this database before being processed by the multimodal large language models. Our approach has demonstrated state-of-the-art performance, achieving higher accuracy compared against three widely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our solution eliminates the need for expensive fine-tuning or retraining and scales seamlessly to incorporate new data sources. The effectiveness of retrieval-augmented generation-based multimodal large language models in geolocation estimation demonstrated by this paper suggests an alternative path to the traditional methods which rely on the training models from scratch, opening new possibilities for more accessible and scalable solutions in GeoAI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "LLM"
    ],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision",
    "url": "http://arxiv.org/abs/2508.03745v1",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Maosheng Hu"
    ],
    "published": "2025-08-01",
    "abstract": "Recent interest in geospatial artificial intelligence (GeoAI) has fostered a wide range of applications using artificial intelligence (AI), especially deep learning, for geospatial problem solving. However, major challenges such as a lack of training data and the neglect of spatial principles and spatial effects in AI model design remain, significantly hindering the in-depth integration of AI with geospatial research. This paper reports our work in developing a deep learning model that enables object detection, particularly of natural features, in a weakly supervised manner. Our work makes three contributions: First, we present a method of object detection using only weak labels. This is achieved by developing a spatially explicit model based on Tobler's first law of geography. Second, we incorporate attention maps into the object detection pipeline and develop a multistage training strategy to improve performance. Third, we apply this model to detect impact craters on Mars, a task that previously required extensive manual effort. The model generalizes to both natural and human-made features on the surfaces of Earth and other planets. This research advances the theoretical and methodological foundations of GeoAI.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Partitioning of Eddy Covariance Footprint Evapotranspiration Using Field Data, UAS Observations and GeoAI in the U.S. Chihuahuan Desert",
    "url": "http://arxiv.org/abs/2507.14829v1",
    "authors": [
      "Habibur R. Howlider",
      "Hernan A. Moreno",
      "Marguerite E. Mauritz",
      "Stephanie N. Marquez"
    ],
    "published": "2025-07-20",
    "abstract": "This study proposes a new method for computing transpiration across an eddy covariance footprint using field observations of plant sap flow, phytomorphology sampling, uncrewed aerial system (UAS), deep learning-based digital image processing, and eddy covariance micrometeorological measurements. The method is applied to the Jornada Experimental Range, New Mexico, where we address three key questions: (1) What are the daily summer transpiration rates of Mesquite (Prosopis glandulosa) and Creosote (Larrea tridentata) individuals, and how do these species contribute to footprint-scale evapotranspiration? (2) How can the plant-level measurements be integrated for terrain-wide transpiration estimates? (3) What is the contribution of transpiration to total evapotranspiration within the eddy covariance footprint? Data collected from June to October 2022, during the North American Monsoon season, include hourly evapotranspiration and precipitation rates from the Ameriflux eddy covariance system (US Jo-1 Bajada site) and sap flux rates from heat-balance sensors. We used plant biometric measurements and supervised classification of multispectral imagery to upscale from the patch to footprint-scale estimations. A proportional relationship between the plant's horizontal projected area and the estimated number of water flow conduits was extended to the eddy covariance footprint via UAS data. Our results show that Mesquite's average daily summer transpiration is 2.84 mm/d, while Creosote's is 1.78 mm/d (a ratio of 1.6:1). The summer footprint integrated transpiration to evapotranspiration ratio (T/ET) was 0.50, decreasing to 0.44 during dry spells and increasing to 0.63 following significant precipitation. Further testing of this method is needed in different regions to validate its applicability. With appropriate adjustments, it could be relevant for other areas with similar ecological conditions.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "CLIP the Landscape: Automated Tagging of Crowdsourced Landscape Images",
    "url": "http://arxiv.org/abs/2506.12214v1",
    "authors": [
      "Ilya Ilyankou",
      "Natchapon Jongwiriyanurak",
      "Tao Cheng",
      "James Haworth"
    ],
    "published": "2025-06-13",
    "abstract": "We present a CLIP-based, multi-modal, multi-label classifier for predicting geographical context tags from landscape photos in the Geograph dataset--a crowdsourced image archive spanning the British Isles, including remote regions lacking POIs and street-level imagery. Our approach addresses a Kaggle competition\\footnote{https://www.kaggle.com/competitions/predict-geographic-context-from-landscape-photos} task based on a subset of Geograph's 8M images, with strict evaluation: exact match accuracy is required across 49 possible tags. We show that combining location and title embeddings with image features improves accuracy over using image embeddings alone. We release a lightweight pipeline\\footnote{https://github.com/SpaceTimeLab/ClipTheLandscape} that trains on a modest laptop, using pre-trained CLIP image and text embeddings and a simple classification head. Predicted tags can support downstream tasks such as building location embedders for GeoAI applications, enriching spatial understanding in data-sparse regions.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "CLIP"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "From Bias to Accountability: How the EU AI Act Confronts Challenges in European GeoAI Auditing",
    "url": "http://arxiv.org/abs/2505.18236v1",
    "authors": [
      "Natalia Matuszczyk",
      "Craig R. Barnes",
      "Rohit Gupta",
      "Bulent Ozel",
      "Aniket Mitra"
    ],
    "published": "2025-05-23",
    "abstract": "Bias in geospatial artificial intelligence (GeoAI) models has been documented, yet the evidence is scattered across narrowly focused studies. We synthesize this fragmented literature to provide a concise overview of bias in GeoAI and examine how the EU's Artificial Intelligence Act (EU AI Act) shapes audit obligations. We discuss recurring bias mechanisms, including representation, algorithmic and aggregation bias, and map them to specific provisions of the EU AI Act. By applying the Act's high-risk criteria, we demonstrate that widely deployed GeoAI applications qualify as high-risk systems. We then present examples of recent audits along with an outline of practical methods for detecting bias. As far as we know, this study represents the first integration of GeoAI bias evidence into the EU AI Act context, by identifying high-risk GeoAI systems and mapping bias mechanisms to the Act's Articles. Although the analysis is exploratory, it suggests that even well-curated European datasets should employ routine bias audits before 2027, when the AI Act's high-risk provisions take full effect.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw",
    "url": "http://arxiv.org/abs/2504.17822v1",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Sizhe Wang",
      "Zhining Gu",
      "Yili Yang",
      "Brendan M. Rogers",
      "Anna Liljedahl"
    ],
    "published": "2025-04-23",
    "abstract": "Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost landforms with significant environmental impacts. Mapping these RTS is crucial because their appearance serves as a clear indication of permafrost thaw. However, their small scale compared to other landform features, vague boundaries, and spatiotemporal variation pose significant challenges for accurate detection. In this paper, we employed a state-of-the-art deep learning model, the Cascade Mask R-CNN with a multi-scale vision transformer-based backbone, to delineate RTS features across the Arctic. Two new strategies were introduced to optimize multimodal learning and enhance the model's predictive performance: (1) a feature-level, residual cross-modality attention fusion strategy, which effectively integrates feature maps from multiple modalities to capture complementary information and improve the model's ability to understand complex patterns and relationships within the data; (2) pre-trained unimodal learning followed by multimodal fine-tuning to alleviate high computing demand while achieving strong model performance. Experimental results demonstrated that our approach outperformed existing models adopting data-level fusion, feature-level convolutional fusion, and various attention fusion strategies, providing valuable insights into the efficient utilization of multimodal data for RTS mapping. This research contributes to our understanding of permafrost landforms and their environmental implications.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "CNN",
      "Transformer"
    ],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Geospatial Artificial Intelligence for Satellite-Based Flood Extent Mapping: Concepts, Advances, and Future Perspectives",
    "url": "http://arxiv.org/abs/2504.02214v3",
    "authors": [
      "Hyunho Lee",
      "Wenwen Li"
    ],
    "published": "2025-04-03",
    "abstract": "Geospatial Artificial Intelligence (GeoAI) for satellite-based flood extent mapping systematically integrates artificial intelligence techniques with satellite data to identify flood events and assess their impacts, for disaster management and spatial decision-making. The primary output often includes flood extent maps, which delineate the affected areas, along with additional analytical outputs such as uncertainty estimation and change detection.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "RegionGCN: Spatial-Heterogeneity-Aware Graph Convolutional Networks",
    "url": "http://arxiv.org/abs/2501.17599v2",
    "authors": [
      "Hao Guo",
      "Han Wang",
      "Di Zhu",
      "Lun Wu",
      "A. Stewart Fotheringham",
      "Yu Liu"
    ],
    "published": "2025-01-29",
    "abstract": "Modeling spatial heterogeneity in the data generation process is essential for understanding and predicting geographical phenomena. Despite their prevalence in geospatial tasks, neural network models usually assume spatial stationarity, which could limit their performance in the presence of spatial process heterogeneity. By allowing model parameters to vary over space, several approaches have been proposed to incorporate spatial heterogeneity into neural networks. However, current geographically weighting approaches are ineffective on graph neural networks, yielding no significant improvement in prediction accuracy. We assume the crux lies in the over-fitting risk brought by a large number of local parameters. Accordingly, we propose to model spatial process heterogeneity at the regional level rather than at the individual level, which largely reduces the number of spatially varying parameters. We further develop a heuristic optimization procedure to learn the region partition adaptively in the process of model training. Our proposed spatial-heterogeneity-aware graph convolutional network, named RegionGCN, is applied to the spatial prediction of county-level vote share in the 2016 US presidential election based on socioeconomic attributes. Results show that RegionGCN achieves significant improvement over the basic and geographically weighted GCNs. We also offer an exploratory analysis tool for the spatial variation of non-linear relationships through ensemble learning of regional partitions from RegionGCN. Our work contributes to the practice of Geospatial Artificial Intelligence (GeoAI) in tackling spatial heterogeneity.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "GNN"
    ],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Human-centered Geospatial Data Science",
    "url": "http://arxiv.org/abs/2501.05595v1",
    "authors": [
      "Yuhao Kang"
    ],
    "published": "2025-01-09",
    "abstract": "This entry provides an overview of Human-centered Geospatial Data Science, highlighting the gaps it aims to bridge, its significance, and its key topics and research. Geospatial Data Science, which derives geographic knowledge and insights from large volumes of geospatial big data using advanced Geospatial Artificial Intelligence (GeoAI), has been widely used to tackle a wide range of geographic problems. However, it often overlooks the subjective human experiences that fundamentally influence human-environment interactions, and few strategies have been developed to ensure that these technologies follow ethical guidelines and prioritize human values. Human-centered Geospatial Data Science advocates for two primary focuses. First, it advances our understanding of human-environment interactions by leveraging Geospatial Data Science to measure and analyze human subjective experiences at place including emotion, perception, cognition, and creativity. Second, it advocates for the development of responsible and ethical Geospatial Data Science methods that protect geoprivacy, enhance fairness and reduce bias, and improve the explainability and transparency of geospatial technologies. With these two missions, Human-centered Geospatial Data Sciences brings a fresh perspective to develop and utilize geospatial technologies that positively impact society and benefit human well-being and the humanities.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "A comprehensive GeoAI review: Progress, Challenges and Outlooks",
    "url": "http://arxiv.org/abs/2412.11643v1",
    "authors": [
      "Anasse Boutayeb",
      "Iyad Lahsen-cherif",
      "Ahmed El Khadimi"
    ],
    "published": "2024-12-16",
    "abstract": "In recent years, Geospatial Artificial Intelligence (GeoAI) has gained traction in the most relevant research works and industrial applications, while also becoming involved in various fields of use. This paper offers a comprehensive review of GeoAI as a synergistic concept applying Artificial Intelligence (AI) methods and models to geospatial data. A preliminary study is carried out, identifying the methodology of the work, the research motivations, the issues and the directions to be tracked, followed by exploring how GeoAI can be used in various interesting fields of application, such as precision agriculture, environmental monitoring, disaster management and urban planning. Next, a statistical and semantic analysis is carried out, followed by a clear and precise presentation of the challenges facing GeoAI. Then, a concrete exploration of the future prospects is provided, based on several informations gathered during the census. To sum up, this paper provides a complete overview of the correlation between AI and the geospatial domain, while mentioning the researches conducted in this context, and emphasizing the close relationship linking GeoAI with other advanced concepts such as geographic information systems (GIS) and large-scale geospatial data, known as big geodata. This will enable researchers and scientific community to assess the state of progress in this promising field, and will help other interested parties to gain a better understanding of the issues involved.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "GeoConformal prediction: a model-agnostic framework of measuring the uncertainty of spatial prediction",
    "url": "http://arxiv.org/abs/2412.08661v3",
    "authors": [
      "Xiayin Lou",
      "Peng Luo",
      "Liqiu Meng"
    ],
    "published": "2024-12-05",
    "abstract": "Spatial prediction is a fundamental task in geography. In recent years, with advances in geospatial artificial intelligence (GeoAI), numerous models have been developed to improve the accuracy of geographic variable predictions. Beyond achieving higher accuracy, it is equally important to obtain predictions with uncertainty measures to enhance model credibility and support responsible spatial prediction. Although geostatistic methods like Kriging offer some level of uncertainty assessment, such as Kriging variance, these measurements are not always accurate and lack general applicability to other spatial models. To address this issue, we propose a model-agnostic uncertainty assessment method called GeoConformal Prediction, which incorporates geographical weighting into conformal prediction. We applied it to two classic spatial prediction cases, spatial regression and spatial interpolation, to evaluate its reliability. First, in the spatial regression case, we used XGBoost to predict housing prices, followed by GeoConformal to calculate uncertainty. Our results show that GeoConformal achieved a coverage rate of 93.67%, while Bootstrap methods only reached a maximum coverage of 81.00% after 2000 runs. Next, we applied GeoConformal to spatial interpolation models. We found that the uncertainty obtained from GeoConformal aligned closely with the variance in Kriging. Finally, using GeoConformal, we analyzed the sources of uncertainty in spatial prediction. We found that explicitly including local features in AI models can significantly reduce prediction uncertainty, especially in areas with strong local dependence. Our findings suggest that GeoConformal holds potential not only for geographic knowledge discovery but also for guiding the design of future GeoAI models, paving the way for more reliable and interpretable spatial prediction frameworks.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Regression",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GeoAI-Enhanced Community Detection on Spatial Networks with Graph Deep Learning",
    "url": "http://arxiv.org/abs/2411.15428v1",
    "authors": [
      "Yunlei Liang",
      "Jiawei Zhu",
      "Wen Ye",
      "Song Gao"
    ],
    "published": "2024-11-23",
    "abstract": "Spatial networks are useful for modeling geographic phenomena where spatial interaction plays an important role. To analyze the spatial networks and their internal structures, graph-based methods such as community detection have been widely used. Community detection aims to extract strongly connected components from the network and reveal the hidden relationships between nodes, but they usually do not involve the attribute information. To consider edge-based interactions and node attributes together, this study proposed a family of GeoAI-enhanced unsupervised community detection methods called region2vec based on Graph Attention Networks (GAT) and Graph Convolutional Networks (GCN). The region2vec methods generate node neural embeddings based on attribute similarity, geographic adjacency and spatial interactions, and then extract network communities based on node embeddings using agglomerative clustering. The proposed GeoAI-based methods are compared with multiple baselines and perform the best when one wants to maximize node attribute similarity and spatial interaction intensity simultaneously within the spatial network communities. It is further applied in the shortage area delineation problem in public health and demonstrates its promise in regionalization problems.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Detection"
    ],
    "is_recent": false
  },
  {
    "title": "Enhancing GeoAI and location encoding with spatial point pattern statistics: A Case Study of Terrain Feature Classification",
    "url": "http://arxiv.org/abs/2411.14560v1",
    "authors": [
      "Sizhe Wang",
      "Wenwen Li"
    ],
    "published": "2024-11-21",
    "abstract": "This study introduces a novel approach to terrain feature classification by incorporating spatial point pattern statistics into deep learning models. Inspired by the concept of location encoding, which aims to capture location characteristics to enhance GeoAI decision-making capabilities, we improve the GeoAI model by a knowledge driven approach to integrate both first-order and second-order effects of point patterns. This paper investigates how these spatial contexts impact the accuracy of terrain feature predictions. The results show that incorporating spatial point pattern statistics notably enhances model performance by leveraging different representations of spatial relationships.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "GeOT: A spatially explicit framework for evaluating spatio-temporal predictions",
    "url": "http://arxiv.org/abs/2410.11709v3",
    "authors": [
      "Nina Wiedemann",
      "Th\u00e9o Uscidda",
      "Martin Raubal"
    ],
    "published": "2024-10-15",
    "abstract": "When predicting observations across space and time, the spatial layout of errors impacts a model's real-world utility. For instance, in bike sharing demand prediction, error patterns translate to relocation costs. However, commonly used error metrics in GeoAI evaluate predictions point-wise, neglecting effects such as spatial heterogeneity, autocorrelation, and the Modifiable Areal Unit Problem. We put forward Optimal Transport (OT) as a spatial evaluation metric and loss function. The proposed framework, called GeOT, assesses the performance of prediction models by quantifying the transport costs associated with their prediction errors. Through experiments on real and synthetic data, we demonstrate that 1) the spatial distribution of prediction errors relates to real-world costs in many applications, 2) OT captures these spatial costs more accurately than existing metrics, and 3) OT enhances comparability across spatial and temporal scales. Finally, we advocate for leveraging OT as a loss function in neural networks to improve the spatial accuracy of predictions. Experiments with bike sharing, charging station, and traffic datasets show that spatial costs are significantly reduced with only marginal changes to non-spatial error metrics. Thus, this approach not only offers a spatially explicit tool for model evaluation and selection, but also integrates spatial considerations into model training. All code is available at https://github.com/mie-lab/geospatialOT.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Encoding Agent Trajectories as Representations with Sequence Transformers",
    "url": "http://arxiv.org/abs/2410.09204v1",
    "authors": [
      "Athanasios Tsiligkaridis",
      "Nicholas Kalinowski",
      "Zhongheng Li",
      "Elizabeth Hou"
    ],
    "published": "2024-10-11",
    "abstract": "Spatiotemporal data faces many analogous challenges to natural language text including the ordering of locations (words) in a sequence, long range dependencies between locations, and locations having multiple meanings. In this work, we propose a novel model for representing high dimensional spatiotemporal trajectories as sequences of discrete locations and encoding them with a Transformer-based neural network architecture. Similar to language models, our Sequence Transformer for Agent Representation Encodings (STARE) model can learn representations and structure in trajectory data through both supervisory tasks (e.g., classification), and self-supervisory tasks (e.g., masked modelling). We present experimental results on various synthetic and real trajectory datasets and show that our proposed model can learn meaningful encodings that are useful for many downstream tasks including discriminating between labels and indicating similarity between locations. Using these encodings, we also learn relationships between agents and locations present in spatiotemporal data.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "GeoAI in resource-constrained environments",
    "url": "http://arxiv.org/abs/2408.17361v1",
    "authors": [
      "Marc B\u00f6hlen",
      "Gede Sughiarta",
      "Atiek Kurnianingsih",
      "Srikar Reddy Gopaladinne",
      "Sujay Shrivastava",
      "Hemanth Kumar Reddy Gorla"
    ],
    "published": "2024-08-30",
    "abstract": "This paper describes spatially aware Artificial Intelligence, GeoAI, tailored for small organizations such as NGOs in resource constrained contexts where access to large datasets, expensive compute infrastructure and AI expertise may be restricted. We furthermore consider future scenarios in which resource-intensive, large geospatial models may homogenize the representation of complex landscapes, and suggest strategies to prepare for this condition.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications",
    "url": "http://arxiv.org/abs/2408.14806v2",
    "authors": [
      "Maria Despoina Siampou",
      "Jialiang Li",
      "John Krumm",
      "Cyrus Shahabi",
      "Hua Lu"
    ],
    "published": "2024-08-27",
    "abstract": "Encoding geospatial objects is fundamental for geospatial artificial intelligence (GeoAI) applications, which leverage machine learning (ML) models to analyze spatial information. Common approaches transform each object into known formats, like image and text, for compatibility with ML models. However, this process often discards crucial spatial information, such as the object's position relative to the entire space, reducing downstream task effectiveness. Alternative encoding methods that preserve some spatial properties are often devised for specific data objects (e.g., point encoders), making them unsuitable for tasks that involve different data types (i.e., points, polylines, and polygons). To this end, we propose Poly2Vec, a polymorphic Fourier-based encoding approach that unifies the representation of geospatial objects, while preserving the essential spatial properties. Poly2Vec incorporates a learned fusion module that adaptively integrates the magnitude and phase of the Fourier transform for different tasks and geometries. We evaluate Poly2Vec on five diverse tasks, organized into two categories. The first empirically demonstrates that Poly2Vec consistently outperforms object-specific baselines in preserving three key spatial relationships: topology, direction, and distance. The second shows that integrating Poly2Vec into a state-of-the-art GeoAI workflow improves the performance in two popular tasks: population prediction and land use inference.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Cross-View Geolocalization and Disaster Mapping with Street-View and VHR Satellite Imagery: A Case Study of Hurricane IAN",
    "url": "http://arxiv.org/abs/2408.06761v1",
    "authors": [
      "Hao Li",
      "Fabian Deuser",
      "Wenping Yina",
      "Xuanshu Luo",
      "Paul Walther",
      "Gengchen Mai",
      "Wei Huang",
      "Martin Werner"
    ],
    "published": "2024-08-13",
    "abstract": "Nature disasters play a key role in shaping human-urban infrastructure interactions. Effective and efficient response to natural disasters is essential for building resilience and a sustainable urban environment. Two types of information are usually the most necessary and difficult to gather in disaster response. The first information is about disaster damage perception, which shows how badly people think that urban infrastructure has been damaged. The second information is geolocation awareness, which means how people whereabouts are made available. In this paper, we proposed a novel disaster mapping framework, namely CVDisaster, aiming at simultaneously addressing geolocalization and damage perception estimation using cross-view Street-View Imagery (SVI) and Very High-Resolution satellite imagery. CVDisaster consists of two cross-view models, where CVDisaster-Geoloc refers to a cross-view geolocalization model based on a contrastive learning objective with a Siamese ConvNeXt image encoder, and CVDisaster-Est is a cross-view classification model based on a Couple Global Context Vision Transformer (CGCViT). Taking Hurricane IAN as a case study, we evaluate the CVDisaster framework by creating a novel cross-view dataset (CVIAN) and conducting extensive experiments. As a result, we show that CVDisaster can achieve highly competitive performance (over 80% for geolocalization and 75% for damage perception estimation) with even limited fine-tuning efforts, which largely motivates future cross-view models and applications within a broader GeoAI research community. The data and code are publicly available at: https://github.com/tum-bgd/CVDisaster.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "CMAB: A First National-Scale Multi-Attribute Building Dataset in China Derived from Open Source Data and GeoAI",
    "url": "http://arxiv.org/abs/2408.05891v3",
    "authors": [
      "Yecheng Zhang",
      "Huimin Zhao",
      "Ying Long"
    ],
    "published": "2024-08-12",
    "abstract": "Rapidly acquiring three-dimensional (3D) building data, including geometric attributes like rooftop, height and orientations, as well as indicative attributes like function, quality, and age, is essential for accurate urban analysis, simulations, and policy updates. Current building datasets suffer from incomplete coverage of building multi-attributes. This paper introduces a geospatial artificial intelligence (GeoAI) framework for large-scale building modeling, presenting the first national-scale Multi-Attribute Building dataset (CMAB), covering 3,667 spatial cities, 29 million buildings, and 21.3 billion square meters of rooftops with an F1-Score of 89.93% in OCRNet-based extraction, totaling 337.7 billion cubic meters of building stock. We trained bootstrap aggregated XGBoost models with city administrative classifications, incorporating features such as morphology, location, and function. Using multi-source data, including billions of high-resolution Google Earth images and 60 million street view images (SVIs), we generated rooftop, height, function, age, and quality attributes for each building. Accuracy was validated through model benchmarks, existing similar products, and manual SVI validation, mostly above 80%. Our dataset and results are crucial for global SDGs and urban planning.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "Partial Label Learning with Focal Loss for Sea Ice Classification Based on Ice Charts",
    "url": "http://arxiv.org/abs/2406.03645v2",
    "authors": [
      "Behzad Vahedi",
      "Benjamin Lucas",
      "Farnoush Banaei-Kashani",
      "Andrew P. Barrett",
      "Walter N. Meier",
      "Siri Jodha Khalsa",
      "Morteza Karimzadeh"
    ],
    "published": "2024-06-05",
    "abstract": "Sea ice, crucial to the Arctic and Earth's climate, requires consistent monitoring and high-resolution mapping. Manual sea ice mapping, however, is time-consuming and subjective, prompting the need for automated deep learning-based classification approaches. However, training these algorithms is challenging because expert-generated ice charts, commonly used as training data, do not map single ice types but instead map polygons with multiple ice types. Moreover, the distribution of various ice types in these charts is frequently imbalanced, resulting in a performance bias towards the dominant class. In this paper, we present a novel GeoAI approach to training sea ice classification by formalizing it as a partial label learning task with explicit confidence scores to address multiple labels and class imbalance. We treat the polygon-level labels as candidate partial labels, assign the corresponding ice concentrations as confidence scores to each candidate label, and integrate them with focal loss to train a Convolutional Neural Network (CNN). Our proposed approach leads to enhanced performance for sea ice classification in Sentinel-1 dual-polarized SAR images, improving classification accuracy (from 87% to 92%) and weighted average F-1 score (from 90% to 93%) compared to the conventional training approach of using one-hot encoded labels and Categorical Cross-Entropy loss. It also improves the F-1 score in 4 out of the 6 sea ice classes.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [
      "CNN"
    ],
    "applications": [
      "Classification"
    ],
    "is_recent": false
  },
  {
    "title": "GeoAI Reproducibility and Replicability: a computational and spatial perspective",
    "url": "http://arxiv.org/abs/2404.10108v2",
    "authors": [
      "Wenwen Li",
      "Chia-Yu Hsu",
      "Sizhe Wang",
      "Peter Kedron"
    ],
    "published": "2024-04-15",
    "abstract": "GeoAI has emerged as an exciting interdisciplinary research area that combines spatial theories and data with cutting-edge AI models to address geospatial problems in a novel, data-driven manner. While GeoAI research has flourished in the GIScience literature, its reproducibility and replicability (R&R), fundamental principles that determine the reusability, reliability, and scientific rigor of research findings, have rarely been discussed. This paper aims to provide an in-depth analysis of this topic from both computational and spatial perspectives. We first categorize the major goals for reproducing GeoAI research, namely, validation (repeatability), learning and adapting the method for solving a similar or new problem (reproducibility), and examining the generalizability of the research findings (replicability). Each of these goals requires different levels of understanding of GeoAI, as well as different methods to ensure its success. We then discuss the factors that may cause the lack of R&R in GeoAI research, with an emphasis on (1) the selection and use of training data; (2) the uncertainty that resides in the GeoAI model design, training, deployment, and inference processes; and more importantly (3) the inherent spatial heterogeneity of geospatial data and processes. We use a deep learning-based image analysis task as an example to demonstrate the results' uncertainty and spatial variance caused by different factors. The findings reiterate the importance of knowledge sharing, as well as the generation of a \"replicability map\" that incorporates spatial autocorrelation and spatial heterogeneity into consideration in quantifying the spatial replicability of GeoAI research.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [],
    "is_recent": false
  },
  {
    "title": "Cross-Modal Learning of Housing Quality in Amsterdam",
    "url": "http://arxiv.org/abs/2403.08915v1",
    "authors": [
      "Alex Levering",
      "Diego Marcos",
      "Devis Tuia"
    ],
    "published": "2024-03-13",
    "abstract": "In our research we test data and models for the recognition of housing quality in the city of Amsterdam from ground-level and aerial imagery. For ground-level images we compare Google StreetView (GSV) to Flickr images. Our results show that GSV predicts the most accurate building quality scores, approximately 30% better than using only aerial images. However, we find that through careful filtering and by using the right pre-trained model, Flickr image features combined with aerial image features are able to halve the performance gap to GSV features from 30% to 15%. Our results indicate that there are viable alternatives to GSV for liveability factor prediction, which is encouraging as GSV images are more difficult to acquire and not always available.",
    "categories": [
      "geo_reasoning"
    ],
    "architectures": [],
    "applications": [
      "Recognition",
      "Forecast"
    ],
    "is_recent": false
  },
  {
    "title": "Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing",
    "url": "http://arxiv.org/abs/2512.17224v1",
    "authors": [
      "Xuyang Li",
      "Chenyu Li",
      "Danfeng Hong"
    ],
    "published": "2025-12-19",
    "abstract": "Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation",
    "url": "http://arxiv.org/abs/2512.16740v1",
    "authors": [
      "Yunkai Yang",
      "Yudong Zhang",
      "Kunquan Zhang",
      "Jinxiao Zhang",
      "Xinying Chen",
      "Haohuan Fu",
      "Runmin Dong"
    ],
    "published": "2025-12-18",
    "abstract": "With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation",
    "url": "http://arxiv.org/abs/2512.15564v1",
    "authors": [
      "Roni Blushtein-Livnon",
      "Osher Rafaeli",
      "David Ioffe",
      "Amir Boger",
      "Karen Sandberg Esquenazi",
      "Tal Svoray"
    ],
    "published": "2025-12-17",
    "abstract": "Remote sensing (RS) image segmentation is constrained by the limited availability of annotated data and a gap between overhead imagery and natural images used to train foundational models. This motivates effective adaptation under limited supervision. SAM3 concept-driven framework generates masks from textual prompts without requiring task-specific modifications, which may enable this adaptation. We evaluate SAM3 for RS imagery across four target types, comparing textual, geometric, and hybrid prompting strategies, under lightweight fine-tuning scales with increasing supervision, alongside zero-shot inference. Results show that combining semantic and geometric cues yields the highest performance across targets and metrics. Text-only prompting exhibits the lowest performance, with marked score gaps for irregularly shaped targets, reflecting limited semantic alignment between SAM3 textual representations and their overhead appearances. Nevertheless, textual prompting with light fine-tuning offers a practical performance-effort trade-off for geometrically regular and visually salient targets. Across targets, performance improves between zero-shot inference and fine-tuning, followed by diminishing returns as the supervision scale increases. Namely, a modest geometric annotation effort is sufficient for effective adaptation. A persistent gap between Precision and IoU further indicates that under-segmentation and boundary inaccuracies remain prevalent error patterns in RS tasks, particularly for irregular and less prevalent targets.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [],
    "applications": [
      "Segmentation"
    ]
  },
  {
    "title": "Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries",
    "url": "http://arxiv.org/abs/2512.14102v1",
    "authors": [
      "Emanuele Mezzi",
      "Gertjan Burghouts",
      "Maarten Kruithof"
    ],
    "published": "2025-12-16",
    "abstract": "Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.",
    "categories": [
      "foundation_model"
    ],
    "architectures": [
      "LLM",
      "LVM"
    ],
    "applications": [
      "Detection"
    ]
  },
  {
    "title": "SARMAE: Masked Autoencoder for SAR Representation Learning",
    "url": "http://arxiv.org/abs/2512.16635v1",
    "authors": [
      "Danxu Liu",
      "Di Wang",
      "Hebaixu Wang",
      "Haoyang Chen",
      "Wentao Jiang",
      "Yilin Cheng",
      "Haonan Guo",
      "Wei Cui",
      "Jing Zhang"
    ],
    "published": "2025-12-18",
    "abstract": "Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Autoencoder"
    ],
    "applications": [
      "Segmentation",
      "Detection",
      "Classification"
    ]
  },
  {
    "title": "Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections",
    "url": "http://arxiv.org/abs/2512.16950v1",
    "authors": [
      "Adrian Straker",
      "Paul Magdon",
      "Marco Zullich",
      "Maximilian Freudenberg",
      "Christoph Kleinn",
      "Johannes Breidenbach",
      "Stefano Puliti",
      "Nils N\u00f6lke"
    ],
    "published": "2025-12-17",
    "abstract": "Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Forecast"
    ]
  },
  {
    "title": "CF-Net: A Cross-Feature Reconstruction Network for High-Accuracy 1-Bit Target Classification",
    "url": "http://arxiv.org/abs/2512.15105v1",
    "authors": [
      "Jundong Qi",
      "Weize Sun",
      "Shaowu Chen",
      "Lei Huang",
      "Qiuchen Liu"
    ],
    "published": "2025-12-17",
    "abstract": "Target classification is a fundamental task in radar systems, and its performance critically depends on the quantization precision of the signal. While high-precision quantization (e.g. 16-bit) is well established, 1-bit quantization offers distinct advantages by enabling direct sampling at high frequencies and eliminating complex intermediate stages. However, its extreme quantization leads to significant information loss. Although higher sampling rates can compensate for this loss, such oversampling is impractical at the high frequencies targeted for direct sampling. To achieve high-accuracy classification directly from 1-bit radar data under the same sampling rate, this paper proposes a novel two-stage deep learning framework, CF-Net. First, we introduce a self-supervised pre-training strategy based on a dual-branch U-Net architecture. This network learns to restore high-fidelity 16-bit images from their 1-bit counterparts via a cross-feature reconstruction task, forcing the 1-bit encoder to learn robust features despite extreme quantization. Subsequently, this pre-trained encoder is repurposed and fine-tuned for the downstream multi-class target classification task. Experiments on two radar target datasets demonstrate that CF-Net can effectively extract discriminative features from 1-bit imagery, achieving comparable and even superior accuracy to some 16-bit methods without oversampling.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": [
      "Classification"
    ]
  },
  {
    "title": "MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater",
    "url": "http://arxiv.org/abs/2512.12142v1",
    "authors": [
      "Bj\u00f6rn L\u00fctjens",
      "Patrick Alexander",
      "Raf Antwerpen",
      "Til Widmann",
      "Guido Cervone",
      "Marco Tedesco"
    ],
    "published": "2025-12-13",
    "abstract": "The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as \"ground truth\", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "UNET"
    ],
    "applications": []
  },
  {
    "title": "Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results",
    "url": "http://arxiv.org/abs/2512.16013v1",
    "authors": [
      "Ruolei Zeng",
      "Arun Sharma",
      "Shuai An",
      "Mingzhou Yang",
      "Shengya Zhang",
      "Licheng Liu",
      "David Mulla",
      "Shashi Shekhar"
    ],
    "published": "2025-12-17",
    "abstract": "Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Multi-Modal Semantic Communication",
    "url": "http://arxiv.org/abs/2512.15691v1",
    "authors": [
      "Matin Mortaheb",
      "Erciyes Karakaya",
      "Sennur Ulukus"
    ],
    "published": "2025-12-17",
    "abstract": "Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.",
    "categories": [
      "remote_sensing"
    ],
    "architectures": [
      "Transformer"
    ],
    "applications": []
  },
  {
    "title": "Reconstructing Pre-Satellite Tropical Cyclogenesis Climatology Using Deep Learning",
    "url": "http://arxiv.org/abs/2512.17711v1",
    "authors": [
      "Chanh Kieu",
      "Thanh T. N. Nguyen",
      "Duc-Trong Le",
      "Duc Gia-Anh Hoang",
      "Quang-Lap Luu",
      "Binh T. Dang",
      "Truong X. Ngo",
      "Quang-Trung Luu",
      "Tien D. Du",
      "Khiem V. Mai"
    ],
    "published": "2025-12-19",
    "abstract": "A reliable tropical cyclone (TC) climatology is the key to assessing historical and future changes in TC activities. While global TC records have been systematically maintained since the early 1940s, substantial uncertainties remain for the pre-satellite era during which TC observations relied mostly on scattered aircraft reconnaissance and sporadic ship reports. This study presents a deep learning (DL) approach to reconstruct historical TC activity in the western North Pacific (WNP) basin, with a main focus on the pre-satellite era. Using data feature enrichment tailored for tropical cyclogenesis (TCG), we demonstrate that DL can effectively capture the main characteristics and changes in TCG climatology during the post-satellite era. With additional cross-validations, the reconstruction of TCG climatology is then extended to a pre-satellite period (1940-1960) during which TC base-track datasets are most uncertain. Our DL reconstruction reveals a significant missing of TCG in the current best-track data between September and November during the pre-satellite era. Such a TCG undercount in the best track data occurs mainly around 10-15$^\\circ$N in the central WNP, while coastal regions show better consistency with DL reconstruction. These findings not only highlight the potential of DL for improving historical assessments of TC activity, but also advance our understanding of TCG processes by identifying key environmental conditions conducive to TC formation. The DL approach presented herein can be applied to other ocean basins, climate proxies, or reanalysis datasets for future TC climate studies.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": []
  },
  {
    "title": "Am I Confused or Is This Confusing?: Deep Ensembles for ENSO Uncertainty Quantification",
    "url": "http://arxiv.org/abs/2512.17153v1",
    "authors": [
      "Devin M. McAfee",
      "Elizabeth A. Barnes"
    ],
    "published": "2025-12-19",
    "abstract": "Faithful uncertainty quantification (UQ) is paramount in high stakes climate prediction. Deep ensembles, or ensembles of probabilistic neural networks, are state of the art for UQ in machine learning (ML) and are growing increasingly popular for weather and climate prediction. However, detailed analyses of the mechanisms, strengths, and limitations of ensembles in these complex problem settings are lacking. We take a step towards filling this gap by deploying deep ensembles for predictability analysis of the El-Ni\u00f1o Southern Oscillation (ENSO) in the Community Earth System Model 2 Large Ensemble (CESM2-LE). Principally, we show that epistemic uncertainty, modeled by ensemble disagreement, robustly signals predictive error growth associated with shifts in the distributions of monthly sea-surface temperature (SST), ocean heat content (OHC), and zonal surface wind stress ($\u03c4_x$) anomalies under a climate change scenario. Conversely, we find that aleatoric uncertainty, which remains a popular measure of model confidence, becomes less reliable and behaves counterintuitively under climate-change-induced distributional shift. We highlight that, because ensemble performance improvement relative to the expected single model scales with epistemic uncertainty, ensemble improvement increases with distributional shift from climate change. This work demonstrates the utility of deep ensembles for modeling aleatoric and epistemic uncertainty in ML climate prediction, as well as the growing importance of robustly quantifying these two forms of uncertainty under anthropogenic warming.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Forecast"
    ]
  },
  {
    "title": "Enhancing lithological interpretation from petrophysical well log of IODP expedition 390/393 using machine learning",
    "url": "http://arxiv.org/abs/2512.13529v1",
    "authors": [
      "Raj Sahu",
      "Saumen Maiti"
    ],
    "published": "2025-12-15",
    "abstract": "Enhanced lithological interpretation from well logs plays a key role in geological resource exploration and mapping, as well as in geo-environmental modeling studies. Core and cutting information is useful for making sound interpretations of well logs; however, these are rarely collected at each depth due to high costs. Moreover, well log interpretation using traditional methods is constrained by poor borehole conditions. Traditional statistical methods are mostly linear, often failing to discriminate between lithology and rock facies, particularly when dealing with overlapping well log signals characterized by the structural and compositional variation of rock types. In this study, we develop multiple supervised and unsupervised machine learning algorithms to jointly analyze multivariate well log data from Integrated Ocean Drilling Program (IODP) expeditions 390 and 393 for enhanced lithological interpretations. Among the algorithms, Logistic Regression, Decision Trees, Gradient Boosting, Support Vector Machines (SVM), k-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) neural network models, the Decision Tree and Gradient Boosting models outperformed the others, achieving an accuracy of 0.9950 and an F1-score of 0.9951. While unsupervised machine learning (ML) provides the foundation for cluster information that inherently supports the classification algorithm, supervised ML is applied to devise a data-driven lithology clustering mechanism for IODP datasets. The joint ML-based method developed here has the potential to be further explored for analyzing other well log datasets from the world's oceans.",
    "categories": [
      "ocean"
    ],
    "architectures": [],
    "applications": [
      "Classification",
      "Regression"
    ]
  }
]